{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "RS_TOKEN = \"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\"\n",
    "\n",
    "JV_GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n",
    "RS_GEMINI_TOKEN = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "VM_GEMINI_TOKEN = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56b0c",
   "metadata": {},
   "source": [
    "## Estrazione del testo dal libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(text)\n",
    "        \n",
    "        print(f\"Testo estratto e salvato in {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'estrazione del testo: {e}\")\n",
    "\n",
    "pdf_file_path = \"data/book.pdf\"  \n",
    "output_text_path = \"data/book.txt\"\n",
    "extract_text_from_pdf(pdf_file_path, output_text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a3a4",
   "metadata": {},
   "source": [
    "## Inizializzazione del Retriever (FAISS) del LIBRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/book.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n",
    "\n",
    "vectorstore.save_local(\"data/faiss_index/BOOK__bge-m3__MAX_INNER_PRODUCT\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab885cc7",
   "metadata": {},
   "source": [
    "## *3*  Preprocessing delle informazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f57cfe",
   "metadata": {},
   "source": [
    "#### Estrarre info dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536de412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pypdf\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Estrae il testo direttamente da un file PDF usando pypdf.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = pypdf.PdfReader(f)\n",
    "        # Accumula il testo estratto da ogni pagina, se disponibile\n",
    "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "def extract_text_and_ocr_from_pdf(pdf_path, lang, ocr_dpi=300):\n",
    "    \n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Converti il PDF in immagini (una per ogni pagina)\n",
    "    images = convert_from_path(pdf_path, dpi=ocr_dpi)\n",
    "    \n",
    "    # Estrai il testo dalle immagini utilizzando pytesseract\n",
    "    ocr_text_list = []\n",
    "    for idx, image in enumerate(images):\n",
    "        ocr_text = pytesseract.image_to_string(image, lang=lang)\n",
    "        # Rimuove eventuali spazi o righe vuote\n",
    "        ocr_text_list.append(ocr_text.strip())\n",
    "    \n",
    "    # Combina il testo estratto e quello ottenuto con OCR\n",
    "    combined_text = (\n",
    "        \"=== Extracted text from PDF ===\\n\" + extracted_text.strip() +\n",
    "        \"\\n\\n=== Extracted Text from images (OCR) ===\\n\" + \"\\n\\n\".join(ocr_text_list)\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "def process_pdfs(input_directory, output_directory, ocr_dpi=300, lang=\"eng\"):\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(input_directory, \"*.pdf\")))\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Processing {pdf_path} ...\")\n",
    "        combined_text = extract_text_and_ocr_from_pdf(pdf_path, ocr_dpi=ocr_dpi, lang=lang)\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_directory, base_name + \".txt\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_text)\n",
    "        print(f\"Saved output to {output_path}\")      \n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------#\n",
    "\n",
    "input_dir = \"data/slides/original\"   \n",
    "output_dir = \"data/slides/preprocessed/STEP_1\" \n",
    "process_pdfs(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f6674",
   "metadata": {},
   "source": [
    "### Riscrivere meglio il testo dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a17822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=VM_GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "def generate_better_text_of_slide(text):\n",
    "    prompt = (\n",
    "        \"The following text has been extracted from a PDF and is poorly formatted, with inconsistent spacing, line breaks, and structure. \"\n",
    "        \"Your task is to rewrite the text to improve its readability and formatting. Specifically:\\n\\n\"\n",
    "        \"1. Remove unnecessary line breaks and spaces to create a smooth, continuous flow of text.\\n\"\n",
    "        \"2. Correct any formatting issues, such as misplaced punctuation, inconsistent capitalization, or fragmented sentences.\\n\"\n",
    "        \"3. Ensure the text is clean and easy to read, with proper spacing and structure.\\n\"\n",
    "        \"4. Is important that you don't lose any information!.\\n\"\n",
    "        \"5. When formulas or code pieces are recognize, rewrite them better using your knowledge.\\n\"\n",
    "        \"Here is the text to reformat:\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    \n",
    "    return call_llm(prompt)\n",
    "\n",
    "def improve_slides(input_directory, output_directory):\n",
    "    # Crea la directory di output se non esiste\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 19\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        # Estrai il testo dal file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Migliora il testo\n",
    "        improved_text = generate_better_text_of_slide(extracted_text)\n",
    "        \n",
    "        # Salva il testo migliorato in un file nella directory di output\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        \n",
    "        print(f\"Saved: {output_path}\")\n",
    "        \n",
    "        \n",
    "        tempo_casuale_ms = random.randint(10000, 15000) / 1000 \n",
    "        time.sleep(tempo_casuale_ms)\n",
    "\n",
    "# Directory di input e output\n",
    "input_directory = \"data/slides/preprocessed/STEP_1\" \n",
    "output_directory = \"data/slides/preprocessed/STEP_2\" \n",
    "\n",
    "improve_slides(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512919f",
   "metadata": {},
   "source": [
    "### Unire Informazioni di slides e libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "faiss_index = \"data/faiss_index/BOOK_faiss_index__bge-m3\"\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    faiss_index, \n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "def retrieve_relevant_info(text, k=5):\n",
    "    docs = vectorstore.similarity_search(text, k=k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def generate_better_text(text, additional_info):  \n",
    "    prompt = (  \n",
    "        \"### Task Description\\n\"\n",
    "        \"Enhance the given text by preserving all its original information while improving clarity, coherence, and depth. \"\n",
    "        \"Expand on key concepts by integrating relevant insights and additional context without altering the meaning or omitting any details. \"\n",
    "        \"Ensure that the enhanced text flows naturally and remains logically structured.\\n\\n\"\n",
    "\n",
    "        \"### Provided Information\\n\"\n",
    "        \"**Original Text:**\\n\" + text + \"\\n\\n\"\n",
    "        \"**Additional Context:**\\n\" + additional_info + \"\\n\\n\"\n",
    "\n",
    "        \"### Guidelines & Constraints\\n\"\n",
    "        \"- Retain all information from the original text without omitting any details.\\n\"\n",
    "        \"- Add relevant explanations and context to enrich understanding.\\n\"\n",
    "        \"- Improve readability, coherence, and logical flow.\\n\"\n",
    "        \"- Do not introduce personal opinions or unverifiable information.\\n\"\n",
    "        \"- Maintain a structured format with sections separated by the delimiter:\\n\"\n",
    "        \"  `<----------section---------->`\\n\\n\"\n",
    "    )  \n",
    "    return call_llm(prompt)  \n",
    "\n",
    "\n",
    "def generate_final_textfile(input_directory, output_directory):\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 4\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        additional_info = retrieve_relevant_info(extracted_text, k=20)\n",
    "        improved_text = generate_better_text(extracted_text, additional_info)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        print(f\"Salvato: {output_path}\")\n",
    "\n",
    "\n",
    "input_directory  = \"data/slides/preprocessed/STEP_2\"\n",
    "output_directory = \"data/slides/preprocessed/STEP_3_BGE-m\"\n",
    "generate_final_textfile(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1ed2a",
   "metadata": {},
   "source": [
    "## *4* Unisco in un unico File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_text_files(folder_path, output_file):\n",
    "\n",
    "    text_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "    \n",
    "    additional_files = [\"insights.txt\", \"course.txt\", \"curricula.txt\"]\n",
    "\n",
    "    separator = \"\\n<----------section---------->\\n\\n\"\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Scrittura dei file originali con separatore\n",
    "        for file_name in text_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read() + separator)\n",
    "\n",
    "        # Scrittura dei file aggiuntivi con separatore\n",
    "        for extra_file in additional_files:\n",
    "            extra_file_path = os.path.join(\"data\", extra_file)\n",
    "            if os.path.exists(extra_file_path):\n",
    "                with open(extra_file_path, 'r', encoding='utf-8') as infile:\n",
    "                    outfile.write(infile.read() + separator)  \n",
    "                    \n",
    "\n",
    "    print(f\"Unione completata: {output_file}\")\n",
    "\n",
    "folder_path = \"data/slides/preprocessed/STEP_3_BGE-m\"  \n",
    "output_file = \"data/3Steps_15Marzo2025.txt\"  \n",
    "merge_text_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59797994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def conta_token_e_genera_istogramma(file_path, separatore=\"<----------section---------->\"):\n",
    "    \n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "    \n",
    "    # Lettura del file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenuto = f.read()\n",
    "    \n",
    "    # Divisione del testo in sezioni\n",
    "    sezioni = [c in contenuto.split(separatore) if len(c.strip()) > 0]\n",
    "    \n",
    "    # Conta dei token per ogni sezione usando il WordPieceTokenizer\n",
    "    token_per_sezione = []\n",
    "    for sezione in sezioni:\n",
    "        sezione = sezione.strip()\n",
    "        # Tokenizza la sezione\n",
    "        tokens = tokenizer.tokenize(sezione)\n",
    "        token_per_sezione.append(len(tokens))\n",
    "    \n",
    "    # Genera l'istogramma\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    numeri_sezioni = list(range(1, len(token_per_sezione) + 1))\n",
    "    plt.bar(numeri_sezioni, token_per_sezione, color='skyblue')\n",
    "    plt.xlabel(\"Numero Sezione\")\n",
    "    plt.ylabel(\"Numero di Token (WordPiece)\")\n",
    "    plt.title(\"Conteggio Token per Sezione con WordPieceTokenizer\")\n",
    "    plt.xticks(numeri_sezioni)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return token_per_sezione\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "file_di_testo = \"data/3Steps_15Marzo2025.txt\"  # modifica con il percorso corretto\n",
    "conteggio = conta_token_e_genera_istogramma(file_di_testo)\n",
    "print(\"Token per sezione:\", conteggio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92515b",
   "metadata": {},
   "source": [
    "#### FAISS A DIMENSIONE VARIABILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943472b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "document_path = \"data/3Steps_15Marzo2025.txt\"\n",
    "\n",
    "# Caricamento del documento\n",
    "with open(document_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Divisione manuale basata sul separatore\n",
    "sections = content.split(\"<----------section---------->\")\n",
    "documents = [Document(page_content=section.strip()) for section in sections if len(section.strip()) > 0] #per eliminare eventuali sezioni vuote\n",
    "\n",
    "# Creazione degli embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n",
    "\n",
    "# Salvataggio del database FAISS\n",
    "vectorstore.save_local(\"data/faiss_index/ALL__15Marzo2025_split__bge-m3__MAX_INNER_PRODUCT\")\n",
    "print(\"Retriever FAISS inizializzato e salvato con suddivisione basata su separatore.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
