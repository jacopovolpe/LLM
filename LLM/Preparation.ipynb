{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b22bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56b0c",
   "metadata": {},
   "source": [
    "## **1* Estrazione del testo dal libro*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(text)\n",
    "        \n",
    "        print(f\"Testo estratto e salvato in {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'estrazione del testo: {e}\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "pdf_file_path = \"data/book.pdf\"  \n",
    "output_text_path = \"data/book.txt\"\n",
    "extract_text_from_pdf(pdf_file_path, output_text_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a3a4",
   "metadata": {},
   "source": [
    "## **2* Inizializzazione del Retriever (FAISS)** üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20161b5",
   "metadata": {},
   "source": [
    "üìå FAISS: Cos'√®, Come Funziona e a Cosa Serve\n",
    "FAISS (Facebook AI Similarity Search) √® una libreria sviluppata da Meta AI per eseguire ricerche veloci su grandi set di dati vettoriali. √à ottimizzata per trovare il Nearest Neighbor (NN) in spazi ad alta dimensionalit√†, rendendola ideale per compiti di similarity search come la ricerca di documenti, immagini o frasi simili.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Generazione degli Embeddings:</b>\n",
    "        Un modello NLP (es. Sentence Transformers) converte il testo in vettori numerici.\n",
    "        Ogni documento viene trasformato in una rappresentazione densa in uno spazio vettoriale.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Creazione dell‚ÄôIndice FAISS:</b>\n",
    "        FAISS memorizza questi vettori in una struttura dati ottimizzata per ricerche veloci.\n",
    "        Supporta diversi tipi di indicizzazione (es. Flat, HNSW, IVF) a seconda delle esigenze.\n",
    "    </li>\n",
    "    <li>\n",
    "    <b>Ricerca e Recupero:</b>\n",
    "    Un nuovo testo viene trasformato in un embedding.\n",
    "    FAISS trova i vettori pi√π vicini nel database (nearest neighbors) restituendo i documenti pi√π simili.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/book.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "\n",
    "# Split del testo per migliorare la ricerca\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salviamo il database FAISS\n",
    "vectorstore.save_local(\"faiss_index__all-MiniLM-L6-v2\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab885cc7",
   "metadata": {},
   "source": [
    "## *3*  Preprocessing delle informazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16a17822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pypdf\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Carica il database FAISS\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_index__all-MiniLM-L6-v2\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Estrae il testo da un file PDF.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = pypdf.PdfReader(f)\n",
    "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "def retrieve_relevant_info(text, k=5):\n",
    "    \"\"\"Recupera informazioni pertinenti dal database FAISS.\"\"\"\n",
    "    docs = vectorstore.similarity_search(text, k=k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def generate_better_text(text, additional_info):\n",
    "    \"\"\"Genera un testo migliorato e approfondito basandosi sul contenuto estratto.\"\"\"\n",
    "    prompt = (\n",
    "        \"Rewrite the provided text to make it clearer, more detailed, and comprehensive. \"\n",
    "        \"Analyze the main topics discussed, integrate relevant insights, and enrich the content with additional details. \"\n",
    "        \"Ensure that the resulting text is structured naturally and flows smoothly, without explicit references to the original sources. \"\n",
    "        \"Maintain the original meaning while incorporating pertinent explanations and context.\\n\\n\"\n",
    "        \"Original Text:\\n\" + text + \"\\n\\n\"\n",
    "        \"Additional Information:\\n\" + additional_info\n",
    "    )\n",
    "\n",
    "    return call_llm(prompt)\n",
    "\n",
    "\n",
    "\n",
    "def process_pdfs(input_directory, output_directory):\n",
    "    \"\"\"Processa tutti i PDF in una directory e salva i testi migliorati e approfonditi.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(input_directory, \"*.pdf\")))  # Ordina alfabeticamente\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Processando: {pdf_path}\")\n",
    "        extracted_text = extract_text_from_pdf(pdf_path)\n",
    "        if not extracted_text.strip():\n",
    "            print(f\"Nessun testo estratto da {pdf_path}\")\n",
    "            continue\n",
    "        \n",
    "        additional_info = retrieve_relevant_info(extracted_text)\n",
    "        improved_text = generate_better_text(extracted_text, additional_info)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(pdf_path).replace(\".pdf\", \".txt\"))\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        print(f\"Salvato: {output_path}\")\n",
    "\n",
    "\n",
    "input_directory = \"data/slides/original/\" \n",
    "output_directory = \"data/merged/preprocessed_by_gemini\" \n",
    "process_pdfs(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1ed2a",
   "metadata": {},
   "source": [
    "## *4* Unisco in un unico File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_text_files(folder_path, output_file):\n",
    "\n",
    "    text_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for file_name in text_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read() + '\\n')  \n",
    "    \n",
    "    print(f\"Unione completata: {output_file}\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "folder_path = \"data/merged/preprocessed_by_gemini\"  \n",
    "output_file = \"data/all_preprocessed_by_gemini.txt\"  \n",
    "merge_text_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4810b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/all_preprocessed_by_gemini.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "\n",
    "# Split del testo per migliorare la ricerca\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salviamo il database FAISS\n",
    "vectorstore.save_local(\"ALL_faiss_index__all-MiniLM-L6-v2\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242a9ac",
   "metadata": {},
   "source": [
    "## *5* RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404cddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Configurazione del modello\n",
    "MODEL_LLM_PATH = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_LLM_PATH}\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\n",
    "\n",
    "# Caricamento embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"ALL_faiss_index__all-MiniLM-L6-v2\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "def generate_response(question, debug = False):\n",
    "    docs = vectorstore.similarity_search(question, k=10)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an AI assistant using Retrieval-Augmented Generation (RAG). \"\n",
    "        \"Use the following context to answer the question. If the answer is not in the context, say you don't know. \" \n",
    "        \"Give discursive answers\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"-----END_QUESTION-----\"\n",
    "    )\n",
    "\n",
    "    payload = {\"inputs\": prompt}\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"API Response Code: {response.status_code}\")\n",
    "        print(f\"Total Response: \\n{response}\\n\\n\\n\\n\")\n",
    "        print(f\"API Raw Response: {response.text}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Errore API ({response.status_code}): {response.text}\"\n",
    "    \n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        generated_text = response_json[0].get(\"generated_text\", \"Errore nella generazione della risposta\")\n",
    "        clean_response = generated_text.split(\"-----END_QUESTION-----\")[-1].strip() \n",
    "        return clean_response\n",
    "\n",
    "      \n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        return \"Errore nel parsing della risposta JSON\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c53837",
   "metadata": {},
   "source": [
    "#### ESEMPIO DI UTILIZZO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e45df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrails are crucial mechanisms designed to mitigate risks associated with Large Language Models (LLMs) by implementing policies and technical solutions. They ensure that LLMs generate outputs that are safe, accurate, and contextually relevant, fostering trust and enabling reliable real-world applications.\n",
      "\n",
      "Without guardrails, LLMs can unintentionally perpetuate harmful stereotypes, generate misinformation, or produce outputs that are illegal or unethical. They may also be susceptible to adversarial attacks, where users deliberately try to circumvent safety measures, further highlighting the importance of robust guardrails.\n",
      "\n",
      "There are several types of guardrails, including ethical guardrails and operational guardrails. Ethical guardrails focus on avoiding bias, misinformation, and ensuring fairness in the LLM's responses. Operational guardrails align outputs with business or user objectives and can incite politeness, help, and consistency with brand guidelines.\n",
      "\n",
      "Several techniques can be employed to add guardrails to LLMs, including using frameworks like Guardrails AI or LangChain. It's also important to continuously monitor, adapt to evolving adversarial techniques, and combine multiple techniques for a robust defense against unexpected outputs. Leveraging established NLU techniques and quantifiable accuracy metrics also provides more reliable and consistent guardrail performance.\n",
      "\n",
      "The limitation of static embeddings such as Word2Vec, GloVe, and FastText is that they represent each word with a single vector, disregarding context. Contextual embeddings, like ELMo and BERT, generate word vectors based on surrounding context to address this issue. However, challenges like polysemy, semantic drift, perpetuation of social biases, out-of-vocabulary words, and a lack of transparency and interpretability continue to present difficulties.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "response = generate_response(\"Talk me about Gardrails\", debug = False)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
