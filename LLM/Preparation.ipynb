{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b22bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "RS_TOKEN = \"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\"\n",
    "\n",
    "JV_GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n",
    "RS_GEMINI_TOKEN = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "VM_GEMINI_TOKEN = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56b0c",
   "metadata": {},
   "source": [
    "## Recupero dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(text)\n",
    "        \n",
    "        print(f\"Testo estratto e salvato in {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'estrazione del testo: {e}\")\n",
    "\n",
    "pdf_file_path = \"data/book.pdf\"  \n",
    "output_text_path = \"data/book.txt\"\n",
    "extract_text_from_pdf(pdf_file_path, output_text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a3a4",
   "metadata": {},
   "source": [
    "#### Inizializzazione del Retriever (FAISS) del LIBRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/book.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n",
    "\n",
    "vectorstore.save_local(\"data/faiss_index/BOOK__bge-m3__MAX_INNER_PRODUCT\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab885cc7",
   "metadata": {},
   "source": [
    "## Preprocessing delle informazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f57cfe",
   "metadata": {},
   "source": [
    "### *<b>STEP 1:</b>*  Estrarre info dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536de412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pypdf\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Estrae il testo direttamente da un file PDF usando pypdf.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = pypdf.PdfReader(f)\n",
    "        # Accumula il testo estratto da ogni pagina, se disponibile\n",
    "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "def extract_text_and_ocr_from_pdf(pdf_path, lang, ocr_dpi=300):\n",
    "    \n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Converti il PDF in immagini (una per ogni pagina)\n",
    "    images = convert_from_path(pdf_path, dpi=ocr_dpi)\n",
    "    \n",
    "    # Estrai il testo dalle immagini utilizzando pytesseract\n",
    "    ocr_text_list = []\n",
    "    for idx, image in enumerate(images):\n",
    "        ocr_text = pytesseract.image_to_string(image, lang=lang)\n",
    "        # Rimuove eventuali spazi o righe vuote\n",
    "        ocr_text_list.append(ocr_text.strip())\n",
    "    \n",
    "    # Combina il testo estratto e quello ottenuto con OCR\n",
    "    combined_text = (\n",
    "        \"=== Extracted text from PDF ===\\n\" + extracted_text.strip() +\n",
    "        \"\\n\\n=== Extracted Text from images (OCR) ===\\n\" + \"\\n\\n\".join(ocr_text_list)\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "def process_pdfs(input_directory, output_directory, ocr_dpi=300, lang=\"eng\"):\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(input_directory, \"*.pdf\")))\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Processing {pdf_path} ...\")\n",
    "        combined_text = extract_text_and_ocr_from_pdf(pdf_path, ocr_dpi=ocr_dpi, lang=lang)\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_directory, base_name + \".txt\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_text)\n",
    "        print(f\"Saved output to {output_path}\")      \n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------#\n",
    "\n",
    "input_dir = \"data/slides/original\"   \n",
    "output_dir = \"data/slides/preprocessed/STEP_1\" \n",
    "process_pdfs(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f6674",
   "metadata": {},
   "source": [
    "### *<b>STEP 2:</b>*  Riscrivere meglio il testo dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16a17822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=VM_GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "def generate_better_text_of_slide(text):\n",
    "    prompt = (\n",
    "        \"The following text has been extracted from a PDF and is poorly formatted, with inconsistent spacing, line breaks and structure. \"\n",
    "        \"Your task is to rewrite the text to improve its readability and formatting. Specifically:\\n\\n\"\n",
    "        \"1. Remove unnecessary line breaks and spaces to create a smooth, continuous flow of text.\\n\"\n",
    "        \"2. Correct any formatting issues, such as misplaced punctuation, inconsistent capitalization, or fragmented sentences.\\n\"\n",
    "        \"3. Ensure the text is clean and easy to read, with proper spacing and structure.\\n\"\n",
    "        \"4. Is important that you don't lose any information!.\\n\"\n",
    "        \"5. When formulas or code pieces are recognized, rewrite them better using your knowledge.\\n\"\n",
    "        \"6. Avoid to use o recite any copyrighted content.\\n\"\n",
    "        \"7. Do not add unnecessary information, such as 'here is a reformatted text'.\\n\"\n",
    "        \"Here is the text to reformat:\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    # 6. Elimina le ridonanze ed evita di ripetere le stesse cose \n",
    "    return call_llm(prompt)\n",
    "\n",
    "def improve_slides(input_directory, output_directory):\n",
    "    # Crea la directory di output se non esiste\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 20\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        # Estrai il testo dal file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Migliora il testo\n",
    "        improved_text = generate_better_text_of_slide(extracted_text)\n",
    "        \n",
    "        # Salva il testo migliorato in un file nella directory di output\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        \n",
    "        print(f\"Saved: {output_path}\")\n",
    "        \n",
    "        \n",
    "        tempo_casuale_ms = random.randint(10000, 15000) / 1000 \n",
    "        time.sleep(tempo_casuale_ms)\n",
    "\n",
    "# Directory di input e output\n",
    "input_directory = \"data/slides/preprocessed/STEP_1\" \n",
    "output_directory = \"data/slides/preprocessed/NEW_STEP_2\" \n",
    "\n",
    "improve_slides(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512919f",
   "metadata": {},
   "source": [
    "### *<b>STEP 3:</b>* Unire Informazioni di slides e libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "faiss_index = \"data/faiss_index/BOOK__bge-m3__MAX_INNER_PRODUCT\"\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    faiss_index, \n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "def retrieve_relevant_info(text, k=5):\n",
    "    docs = vectorstore.similarity_search(text, k=k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def generate_better_text(text, additional_info):  \n",
    "    prompt = (  \n",
    "        \"### Task Description\\n\"\n",
    "        \"Enhance the given text by preserving all its original information while improving clarity, coherence, and depth. \"\n",
    "        \"Expand on key concepts by integrating relevant insights and additional context without altering the meaning or omitting any details. \"\n",
    "        \"Ensure that the enhanced text flows naturally and remains logically structured.\\n\\n\"\n",
    "\n",
    "        \"### Provided Information\\n\"\n",
    "        \"**Original Text:**\\n\" + text + \"\\n\\n\"\n",
    "        \"**Additional Context:**\\n\" + additional_info + \"\\n\\n\"\n",
    "\n",
    "        \"### Guidelines & Constraints\\n\"\n",
    "        \"- Retain all information from the original text without omitting any details.\\n\"\n",
    "        \"- Add relevant explanations and context to enrich understanding.\\n\"\n",
    "        \"- Improve readability, coherence, and logical flow.\\n\"\n",
    "        \"- Do not introduce personal opinions or unverifiable information.\\n\"\n",
    "        \"- Maintain a structured format with sections separated by the delimiter:\\n\"\n",
    "        \"  `<----------section---------->`\\n\"\n",
    "        \"- Keep the dimension of each section under 6000-7000 characters.\\n\\n\"\n",
    "\n",
    "        \"### Verification Step\\n\"\n",
    "        \"After generating the enhanced text, perform a rigorous self-check to ensure that no information from the original text has been lost or misrepresented.\\n\"\n",
    "        \"Compare the enhanced version with the original text and confirm:\\n\"\n",
    "        \"- That all key points and details are present.\\n\"\n",
    "        \"- That nothing has been omitted, reinterpreted incorrectly, or altered in meaning.\\n\"\n",
    "        \"If any information is missing or distorted, refine the output accordingly before finalizing it.\"\n",
    "    )  \n",
    "    return call_llm(prompt)\n",
    "\n",
    "\n",
    "\n",
    "def generate_final_textfile(input_directory, output_directory):\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 1\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        additional_info = retrieve_relevant_info(extracted_text, k=20)\n",
    "        improved_text = generate_better_text(extracted_text, additional_info)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        print(f\"Salvato: {output_path}\")\n",
    "\n",
    "\n",
    "input_directory  = \"data/preprocessing/V2__19_03_2025/STEP_2\"\n",
    "output_directory = \"data/preprocessing/V4__22_03_2025/STEP_3\"\n",
    "generate_final_textfile(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1ed2a",
   "metadata": {},
   "source": [
    "## *4* Unisco in un unico File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def merge_text_files(folder_path, output_file):\n",
    "\n",
    "        \n",
    "\n",
    "    def extract_number(file_name):\n",
    "        match = re.match(r\"(\\d+)\", file_name)  # Trova il numero iniziale\n",
    "        return int(match.group(1)) if match else float('inf')  # Se non c'Ã¨ numero, metti alla fine\n",
    "\n",
    "    text_files = sorted(\n",
    "        (f for f in os.listdir(folder_path) if f.endswith('.txt')),\n",
    "        key=extract_number\n",
    "    )   \n",
    "    \n",
    "    additional_files = [\"source/insights.txt\", \"source/course.txt\", \"source/curricula.txt\"]\n",
    "\n",
    "    separator = \"\\n<----------section---------->\\n\\n\"\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Scrittura dei file originali con separatore\n",
    "        for file_name in text_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read() + separator)\n",
    "\n",
    "        # Scrittura dei file aggiuntivi con separatore\n",
    "        for extra_file in additional_files:\n",
    "            extra_file_path = os.path.join(\"data\", extra_file)\n",
    "            if os.path.exists(extra_file_path):\n",
    "                with open(extra_file_path, 'r', encoding='utf-8') as infile:\n",
    "                    outfile.write(infile.read() + separator)  \n",
    "                    \n",
    "\n",
    "    print(f\"Unione completata: {output_file}\")\n",
    "\n",
    "folder_path = \"data/preprocessing/V4__22_03_2025/STEP_3\"  \n",
    "output_file = \"data/FINAL_DOC/V4__22_03_25.txt\"  \n",
    "\n",
    "merge_text_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59797994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def conta_token_e_genera_istogramma(file_path, separatore=\"<----------section---------->\"):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenuto = f.read()\n",
    "    \n",
    "    sezioni = [c for c in contenuto.split(separatore) if len(c.strip()) > 0]\n",
    "    print(\"Numero di sezioni: \", len(sezioni))\n",
    "    \n",
    "    token_per_sezione = []\n",
    "    for sezione in sezioni:\n",
    "        sezione = sezione.strip()\n",
    "        # Tokenizza la sezione\n",
    "        tokens = tokenizer.tokenize(sezione)\n",
    "        token_per_sezione.append(len(tokens))\n",
    "        if len(tokens)>1800:\n",
    "            print(sezione)\n",
    "    \n",
    "    # Genera l'istogramma\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    numeri_sezioni = list(range(1, len(token_per_sezione) + 1))\n",
    "    plt.bar(numeri_sezioni, token_per_sezione, color='skyblue')\n",
    "    plt.xlabel(\"Numero Sezione\")\n",
    "    plt.ylabel(\"Numero di Token (WordPiece)\")\n",
    "    plt.title(\"Conteggio Token per Sezione con WordPieceTokenizer\")\n",
    "    plt.xticks(numeri_sezioni)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return token_per_sezione\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "file_di_testo = \"data/FINAL_DOC/V4__22_03_25.txt\" \n",
    "conteggio = conta_token_e_genera_istogramma(file_di_testo)\n",
    "print(\"Token per sezione:\", conteggio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92515b",
   "metadata": {},
   "source": [
    "#### FAISS A DIMENSIONE VARIABILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943472b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "document_path = \"data/FINAL_DOC/V4__22_03_25.txt\" \n",
    "\n",
    "with open(document_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "sections = content.split(\"<----------section---------->\")\n",
    "documents = [Document(page_content=section.strip()) for section in sections if len(section.strip()) > 0] #per eliminare eventuali sezioni vuote\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
    "\n",
    "vectorstore.save_local(\"data/faiss_index/NEW__ALL__22Marzo2025__COSINE\")\n",
    "print(\"Retriever FAISS inizializzato e salvato con suddivisione basata su separatore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1953d",
   "metadata": {},
   "source": [
    "#### analisi dei chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import faiss\n",
    "import numpy as np\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Set, List, Tuple\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm \n",
    "\n",
    "def clean_faiss_path(faiss_index: str) -> str:\n",
    "    return faiss_index[5:] if faiss_index.startswith(\"LLM//\") else faiss_index\n",
    "\n",
    "def normalize_faiss_name(faiss_index: str) -> str:\n",
    "    return faiss_index.split(\"faiss_index/\", 1)[-1] if \"faiss_index/\" in faiss_index else faiss_index\n",
    "\n",
    "def load_faiss_index(faiss_index_path: str, embedding_model):\n",
    "    cleaned_index = clean_faiss_path(faiss_index_path)\n",
    "    try:\n",
    "        # Use the exact same model that was used to create your index\n",
    "        # Based on your index name, it looks like you used BGE-M3\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        \n",
    "        print(f\"Loading FAISS index from {cleaned_index}...\")\n",
    "        vectorstore = FAISS.load_local(cleaned_index, embeddings, allow_dangerous_deserialization=True)\n",
    "        print(f\"FAISS index loaded successfully\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index {faiss_index_path}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "def load_questions(questions_file: str) -> List[dict]:\n",
    "    if not os.path.exists(questions_file):\n",
    "        raise FileNotFoundError(f\"Questions file not found: {questions_file}\")\n",
    "    \n",
    "    with open(questions_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        questions_data = json.load(f)\n",
    "    \n",
    "    return questions_data\n",
    "\n",
    "def query_faiss_index(vectorstore, question: str, k: int = 5) -> List[str]:\n",
    "    if not vectorstore:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        docs_and_scores = vectorstore.similarity_search_with_score(question, k=k)\n",
    "        return [doc.page_content for doc, _ in docs_and_scores]\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying FAISS index: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print full error trace\n",
    "        return []\n",
    "\n",
    "def analyze_queries(faiss_indices: List[str], embedding_model:str, questions_file: str, top_k: int = 5) -> Dict:\n",
    "    questions = load_questions(questions_file)\n",
    "    \n",
    "    results = {}\n",
    "    index_chunk_usage = defaultdict(Counter)\n",
    "    chunk_details = defaultdict(dict)\n",
    "    \n",
    "    for faiss_index in faiss_indices:\n",
    "        print(f\"Loading FAISS index: {faiss_index}\")\n",
    "        vectorstore = load_faiss_index(faiss_index, embedding_model)\n",
    "        if not vectorstore:\n",
    "            continue\n",
    "        \n",
    "        normalized_index = normalize_faiss_name(faiss_index)\n",
    "        results[normalized_index] = []\n",
    "        \n",
    "        \n",
    "        for question_data in tqdm(questions, desc=f\"Processing queries for {normalized_index}\"):\n",
    "            question = question_data[\"question\"]\n",
    "            section_index = question_data.get(\"section_index\", \"N/A\")\n",
    "            \n",
    "            retrieved_chunks = query_faiss_index(vectorstore, question, k=top_k)\n",
    "            \n",
    "            results[normalized_index].append({\n",
    "                \"section_index\": section_index,\n",
    "                \"question\": question,\n",
    "                \"retrieved_chunks\": retrieved_chunks\n",
    "            })\n",
    "            \n",
    "            # Track chunk usage (without scores)\n",
    "            for chunk in retrieved_chunks:\n",
    "                chunk_id = chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "                chunk_size = len(chunk)\n",
    "                index_chunk_usage[normalized_index][chunk_id] += 1\n",
    "                \n",
    "                # Store chunk details (without scores)\n",
    "                if chunk_id not in chunk_details[normalized_index]:\n",
    "                    chunk_details[normalized_index][chunk_id] = {\n",
    "                        \"size\": chunk_size,\n",
    "                        \"full_content\": chunk,\n",
    "                        \"questions\": []\n",
    "                    }\n",
    "                \n",
    "                chunk_details[normalized_index][chunk_id][\"questions\"].append({\n",
    "                    \"question\": question,\n",
    "                    \"section_index\": section_index\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"index_chunk_usage\": index_chunk_usage,\n",
    "        \"chunk_details\": chunk_details\n",
    "    }\n",
    "\n",
    "def export_usage_to_csv(index_chunk_usage, chunk_details, output_file: str):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['FAISS Index', 'Chunk', 'Size (chars)', 'Usage Count', 'Top Question'])\n",
    "        \n",
    "        for faiss_index, chunk_counter in sorted(index_chunk_usage.items()):\n",
    "            for chunk_id, count in sorted(chunk_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "                chunk_info = chunk_details[faiss_index][chunk_id]\n",
    "                chunk_size = chunk_info[\"size\"]\n",
    "                \n",
    "                # Just get the first question as an example\n",
    "                example_question = chunk_info[\"questions\"][0][\"question\"] if chunk_info[\"questions\"] else \"N/A\"\n",
    "                \n",
    "                writer.writerow([\n",
    "                    faiss_index, \n",
    "                    chunk_id, \n",
    "                    chunk_size, \n",
    "                    count,\n",
    "                    example_question[:100] + \"...\" if len(example_question) > 100 else example_question\n",
    "                ])\n",
    "    \n",
    "    print(f\"\\nUsage report exported to {output_file}\")\n",
    "\n",
    "def export_detailed_results(analysis_results, output_file: str):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDetailed results exported to {output_file}\")\n",
    "    \n",
    "def main(indices, embedding_model, questions_file, output_dir, top_k):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%d_%m_%Y\")\n",
    "    \n",
    "    analysis_results = analyze_queries(indices, embedding_model, questions_file, top_k)\n",
    "    \n",
    "    # Export results\n",
    "    export_usage_to_csv(\n",
    "        analysis_results[\"index_chunk_usage\"],\n",
    "        analysis_results[\"chunk_details\"],\n",
    "        os.path.join(output_dir, f\"chunk_usage_report_{timestamp}.csv\")\n",
    "    )\n",
    "    \n",
    "    export_detailed_results(\n",
    "        analysis_results,\n",
    "        os.path.join(output_dir, f\"detailed_query_results_{timestamp}.json\")\n",
    "    )\n",
    "\n",
    "main(\n",
    "    indices=[\"data/faiss_index/NEW__ALL__22Marzo2025__COSINE\"],\n",
    "    embedding_model=\"BAAI/bge-m3\",\n",
    "    questions_file=\"data/questions/6Marzo2025__ALL.json\",\n",
    "    output_dir=\"data/logs/\",\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15b8d3",
   "metadata": {},
   "source": [
    "## Altre Prove Non utilizzate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd6e1d",
   "metadata": {},
   "source": [
    "### Tagging Extraction (non utilizzato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"efederici/text2tags\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def estrai_tag(testo, max_length=20):\n",
    "    \"\"\"Genera tag dal testo fornito.\"\"\"\n",
    "    testo = testo.strip().replace('\\n', ' ')\n",
    "    input_text = 'summarize: ' + testo\n",
    "    tokenized_text = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    tags_ids = model.generate(tokenized_text,\n",
    "                              num_beams=4,\n",
    "                              no_repeat_ngram_size=2,\n",
    "                              max_length=max_length,\n",
    "                              early_stopping=True)\n",
    "\n",
    "    output = tokenizer.decode(tags_ids[0], skip_special_tokens=True)\n",
    "    return [tag.strip() for tag in output.split(',')]\n",
    "\n",
    "document_path = \"data/FINAL_DOC/V4__22_03_25.txt\"\n",
    "\n",
    "# Caricamento del documento\n",
    "with open(document_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Divisione manuale basata sul separatore\n",
    "sections = content.split(\"<----------section---------->\")\n",
    "documents = [Document(page_content=section.strip()) for section in sections if len(section.strip()) > 0]\n",
    "\n",
    "all_tags = []\n",
    "for sezione in tqdm(sections, desc=\"Estrazione dei tag\", unit=\"sezione\"):\n",
    "    tag = estrai_tag(sezione)\n",
    "    all_tags.extend(tag)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac51986",
   "metadata": {},
   "source": [
    "### Outline extraction (non Utilizzato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5388178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_path = \"data/FINAL_DOC/V4__22_03_25.txt\" \n",
    "\n",
    "# Caricamento del documento\n",
    "with open(document_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Divisione manuale basata sul separatore\n",
    "sections = content.split(\"<----------section---------->\")\n",
    "\n",
    "# Filtra le sezioni contenenti la parola \"outline\"\n",
    "outline_sections = [section.strip() for section in sections if \"outline\" in section.lower()]\n",
    "\n",
    "# Concatena le sezioni filtrate in un'unica stringa\n",
    "outline_text = \"\\n\\n\".join(outline_sections)\n",
    "with open(\"data/FINAL_DOC/outline_extracted.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(outline_text)\n",
    "\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract a structured list of key topics covered in the following text.  \n",
    "The output must be a clean, comma-separated list of concise topic names, without explanations or extra formatting.  \n",
    "\n",
    "### Input Text:  \n",
    "{outline_text}  \n",
    "\n",
    "### Output Format Example:  \n",
    "#Transformer, #Transformer_Architecture, #LLM, #GUARLAYS_TECHNIQUE, #Neural_Networks, #DeepLearning\n",
    "\"\"\"\n",
    "\n",
    "result = call_llm(prompt)\n",
    "with open(\"data/FINAL_DOC/outline_extracted.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
