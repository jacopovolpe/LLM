{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "RS_TOKEN = \"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\"\n",
    "\n",
    "JV_GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n",
    "RS_GEMINI_TOKEN = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "VM_GEMINI_TOKEN = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56b0c",
   "metadata": {},
   "source": [
    "## Estrazione del testo dal libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(text)\n",
    "        \n",
    "        print(f\"Testo estratto e salvato in {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'estrazione del testo: {e}\")\n",
    "\n",
    "pdf_file_path = \"data/book.pdf\"  \n",
    "output_text_path = \"data/book.txt\"\n",
    "extract_text_from_pdf(pdf_file_path, output_text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a3a4",
   "metadata": {},
   "source": [
    "## Inizializzazione del Retriever (FAISS) del LIBRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20161b5",
   "metadata": {},
   "source": [
    "ðŸ“Œ FAISS: Cos'Ã¨, Come Funziona e a Cosa Serve\n",
    "FAISS (Facebook AI Similarity Search) Ã¨ una libreria sviluppata da Meta AI per eseguire ricerche veloci su grandi set di dati vettoriali. Ãˆ ottimizzata per trovare il Nearest Neighbor (NN) in spazi ad alta dimensionalitÃ , rendendola ideale per compiti di similarity search come la ricerca di documenti, immagini o frasi simili.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Generazione degli Embeddings:</b>\n",
    "        Un modello NLP (es. Sentence Transformers) converte il testo in vettori numerici.\n",
    "        Ogni documento viene trasformato in una rappresentazione densa in uno spazio vettoriale.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Creazione dellâ€™Indice FAISS:</b>\n",
    "        FAISS memorizza questi vettori in una struttura dati ottimizzata per ricerche veloci.\n",
    "        Supporta diversi tipi di indicizzazione (es. Flat, HNSW, IVF) a seconda delle esigenze.\n",
    "    </li>\n",
    "    <li>\n",
    "    <b>Ricerca e Recupero:</b>\n",
    "    Un nuovo testo viene trasformato in un embedding.\n",
    "    FAISS trova i vettori piÃ¹ vicini nel database (nearest neighbors) restituendo i documenti piÃ¹ simili.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/book.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "vectorstore.save_local(\"data/faiss_index/BOOK_faiss_index__all-MiniLM-L6-v2\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab885cc7",
   "metadata": {},
   "source": [
    "## *3*  Preprocessing delle informazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f57cfe",
   "metadata": {},
   "source": [
    "#### Estrarre info dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536de412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pypdf\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Estrae il testo direttamente da un file PDF usando pypdf.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = pypdf.PdfReader(f)\n",
    "        # Accumula il testo estratto da ogni pagina, se disponibile\n",
    "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "def extract_text_and_ocr_from_pdf(pdf_path, lang, ocr_dpi=300):\n",
    "    \n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Converti il PDF in immagini (una per ogni pagina)\n",
    "    images = convert_from_path(pdf_path, dpi=ocr_dpi)\n",
    "    \n",
    "    # Estrai il testo dalle immagini utilizzando pytesseract\n",
    "    ocr_text_list = []\n",
    "    for idx, image in enumerate(images):\n",
    "        ocr_text = pytesseract.image_to_string(image, lang=lang)\n",
    "        # Rimuove eventuali spazi o righe vuote\n",
    "        ocr_text_list.append(ocr_text.strip())\n",
    "    \n",
    "    # Combina il testo estratto e quello ottenuto con OCR\n",
    "    combined_text = (\n",
    "        \"=== Extracted text from PDF ===\\n\" + extracted_text.strip() +\n",
    "        \"\\n\\n=== Extracted Text from images (OCR) ===\\n\" + \"\\n\\n\".join(ocr_text_list)\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "def process_pdfs(input_directory, output_directory, ocr_dpi=300, lang=\"eng\"):\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(input_directory, \"*.pdf\")))\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Processing {pdf_path} ...\")\n",
    "        combined_text = extract_text_and_ocr_from_pdf(pdf_path, ocr_dpi=ocr_dpi, lang=lang)\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_directory, base_name + \".txt\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_text)\n",
    "        print(f\"Saved output to {output_path}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------#\n",
    "\n",
    "input_dir = \"data/slides/original\"   \n",
    "output_dir = \"data/slides/preprocessed/STEP_1\" \n",
    "process_pdfs(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f6674",
   "metadata": {},
   "source": [
    "### Riscrivere meglio il testo dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a17822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=VM_GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def generate_better_text_of_slide(text):\n",
    "    prompt = (\n",
    "        \"The following text has been extracted from a PDF and is poorly formatted, with inconsistent spacing, line breaks, and structure. \"\n",
    "        \"Your task is to rewrite the text to improve its readability and formatting. Specifically:\\n\\n\"\n",
    "        \"1. Remove unnecessary line breaks and spaces to create a smooth, continuous flow of text.\\n\"\n",
    "        \"2. Correct any formatting issues, such as misplaced punctuation, inconsistent capitalization, or fragmented sentences.\\n\"\n",
    "        \"3. Ensure the text is clean and easy to read, with proper spacing and structure.\\n\"\n",
    "        \"4. Is important that you don't lose any information!.\\n\\n\"\n",
    "        \"Here is the text to reformat:\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    \n",
    "    return call_llm(prompt)\n",
    "\n",
    "def improve_slides(input_directory, output_directory):\n",
    "    # Crea la directory di output se non esiste\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 19\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        # Estrai il testo dal file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Migliora il testo\n",
    "        improved_text = generate_better_text_of_slide(extracted_text)\n",
    "        \n",
    "        # Salva il testo migliorato in un file nella directory di output\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        \n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "# Directory di input e output\n",
    "input_directory = \"data/slides/preprocessed/STEP_1\" \n",
    "output_directory = \"data/slides/preprocessed/STEP_2\" \n",
    "\n",
    "improve_slides(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512919f",
   "metadata": {},
   "source": [
    "### Unire Informazioni di slides e libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "#faiss_index = \"data/faiss_index/BOOK_faiss_index__all-MiniLM-L6-v2\" \n",
    "faiss_index = \"data/faiss_index/BOOK_faiss_index__bge-m3\"\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    faiss_index, \n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "def retrieve_relevant_info(text, k=5):\n",
    "    docs = vectorstore.similarity_search(text, k=k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def generate_better_text(text, additional_info):  \n",
    "    prompt = (  \n",
    "        \"### Task Description\\n\"\n",
    "        \"Enhance the given text by preserving all its original information while improving clarity, coherence, and depth. \"\n",
    "        \"Expand on key concepts by integrating relevant insights and additional context without altering the meaning or omitting any details. \"\n",
    "        \"Ensure that the enhanced text flows naturally and remains logically structured.\\n\\n\"\n",
    "\n",
    "        \"### Provided Information\\n\"\n",
    "        \"**Original Text:**\\n\" + text + \"\\n\\n\"\n",
    "        \"**Additional Context:**\\n\" + additional_info + \"\\n\\n\"\n",
    "\n",
    "        \"### Guidelines & Constraints\\n\"\n",
    "        \"- Retain all information from the original text without omitting any details.\\n\"\n",
    "        \"- Add relevant explanations and context to enrich understanding.\\n\"\n",
    "        \"- Improve readability, coherence, and logical flow.\\n\"\n",
    "        \"- Do not introduce personal opinions or unverifiable information.\\n\"\n",
    "        \"- Maintain a structured format with sections separated by the delimiter:\\n\"\n",
    "        \"  `<----------section---------->`\\n\\n\"\n",
    "    )  \n",
    "    return call_llm(prompt)  \n",
    "\n",
    "\n",
    "def generate_final_textfile(input_directory, output_directory):\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 4\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        additional_info = retrieve_relevant_info(extracted_text, k=20)\n",
    "        improved_text = generate_better_text(extracted_text, additional_info)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        print(f\"Salvato: {output_path}\")\n",
    "\n",
    "\n",
    "input_directory  = \"data/slides/preprocessed/STEP_2\"\n",
    "output_directory = \"data/slides/preprocessed/STEP_3_BGE-m\"\n",
    "generate_final_textfile(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1ed2a",
   "metadata": {},
   "source": [
    "## *4* Unisco in un unico File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7fe5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unione completata: data/3Steps_10Marzo2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def merge_text_files(folder_path, output_file):\n",
    "\n",
    "    text_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "    \n",
    "    additional_files = [\"course.txt\", \"curricula.txt\"]\n",
    "\n",
    "    separator = \"\\n<----------section---------->\\n\\n\"\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Scrittura dei file originali con separatore\n",
    "        for file_name in text_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read() + separator)\n",
    "\n",
    "        # Scrittura dei file aggiuntivi con separatore\n",
    "        for extra_file in additional_files:\n",
    "            extra_file_path = os.path.join(\"data\", extra_file)\n",
    "            if os.path.exists(extra_file_path):\n",
    "                with open(extra_file_path, 'r', encoding='utf-8') as infile:\n",
    "                    outfile.write(infile.read() + separator)  \n",
    "\n",
    "    print(f\"Unione completata: {output_file}\")\n",
    "\n",
    "folder_path = \"data/slides/preprocessed/STEP_3_BGE-m\"  \n",
    "output_file = \"data/3Steps_10Marzo2025.txt\"  \n",
    "merge_text_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482bbb1",
   "metadata": {},
   "source": [
    "#### FAISS A DIMENSIONE FISSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4810b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/all_preprocessed_by_gemini.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "\n",
    "# Split del testo per migliorare la ricerca\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salviamo il database FAISS\n",
    "vectorstore.save_local(\"ALL_faiss_index__all-MiniLM-L6-v2\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92515b",
   "metadata": {},
   "source": [
    "#### FAISS A DIMENSIONE VARIABILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacop\\AppData\\Local\\Temp\\ipykernel_27788\\3417620120.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "document_path = \"data/3Steps_10Marzo2025.txt\"\n",
    "\n",
    "# Caricamento del documento\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(    \n",
    "    separators=[\"<----------section---------->\"],\n",
    "    chunk_overlap=0,  # Nessuna sovrapposizione poichÃ© giÃ  suddiviso logicamente\n",
    "    keep_separator=False  # Rimuove il separatore dai chunk\n",
    ")\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salvataggio del database FAISS\n",
    "vectorstore.save_local(\"data/faiss_index/ALL__11Marzo2025__bge-m3\")\n",
    "print(\"Retriever FAISS inizializzato e salvato con suddivisione basata su separatore.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
