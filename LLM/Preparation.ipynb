{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "RS_TOKEN = \"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\"\n",
    "\n",
    "GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n",
    "RS_GEMINI_TOKEN = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "VM_GEMINI_TOKEN = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a56b0c",
   "metadata": {},
   "source": [
    "## Estrazione del testo dal libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(text)\n",
    "        \n",
    "        print(f\"Testo estratto e salvato in {output_txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'estrazione del testo: {e}\")\n",
    "\n",
    "# Esempio di utilizzo\n",
    "pdf_file_path = \"data/book.pdf\"  \n",
    "output_text_path = \"data/book.txt\"\n",
    "extract_text_from_pdf(pdf_file_path, output_text_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07a3a4",
   "metadata": {},
   "source": [
    "## Inizializzazione del Retriever (FAISS) del LIBRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20161b5",
   "metadata": {},
   "source": [
    "ðŸ“Œ FAISS: Cos'Ã¨, Come Funziona e a Cosa Serve\n",
    "FAISS (Facebook AI Similarity Search) Ã¨ una libreria sviluppata da Meta AI per eseguire ricerche veloci su grandi set di dati vettoriali. Ãˆ ottimizzata per trovare il Nearest Neighbor (NN) in spazi ad alta dimensionalitÃ , rendendola ideale per compiti di similarity search come la ricerca di documenti, immagini o frasi simili.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Generazione degli Embeddings:</b>\n",
    "        Un modello NLP (es. Sentence Transformers) converte il testo in vettori numerici.\n",
    "        Ogni documento viene trasformato in una rappresentazione densa in uno spazio vettoriale.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Creazione dellâ€™Indice FAISS:</b>\n",
    "        FAISS memorizza questi vettori in una struttura dati ottimizzata per ricerche veloci.\n",
    "        Supporta diversi tipi di indicizzazione (es. Flat, HNSW, IVF) a seconda delle esigenze.\n",
    "    </li>\n",
    "    <li>\n",
    "    <b>Ricerca e Recupero:</b>\n",
    "    Un nuovo testo viene trasformato in un embedding.\n",
    "    FAISS trova i vettori piÃ¹ vicini nel database (nearest neighbors) restituendo i documenti piÃ¹ simili.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/book.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "\n",
    "# Split del testo per migliorare la ricerca\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salviamo il database FAISS\n",
    "vectorstore.save_local(\"data/faiss_index/BOOK_faiss_index__all-MiniLM-L6-v2\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab885cc7",
   "metadata": {},
   "source": [
    "## *3*  Preprocessing delle informazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f57cfe",
   "metadata": {},
   "source": [
    "#### Estrarre info dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536de412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pypdf\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Estrae il testo direttamente da un file PDF usando pypdf.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = pypdf.PdfReader(f)\n",
    "        # Accumula il testo estratto da ogni pagina, se disponibile\n",
    "        return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "def extract_text_and_ocr_from_pdf(pdf_path, lang, ocr_dpi=300):\n",
    "    \n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Converti il PDF in immagini (una per ogni pagina)\n",
    "    images = convert_from_path(pdf_path, dpi=ocr_dpi)\n",
    "    \n",
    "    # Estrai il testo dalle immagini utilizzando pytesseract\n",
    "    ocr_text_list = []\n",
    "    for idx, image in enumerate(images):\n",
    "        ocr_text = pytesseract.image_to_string(image, lang=lang)\n",
    "        # Rimuove eventuali spazi o righe vuote\n",
    "        ocr_text_list.append(ocr_text.strip())\n",
    "    \n",
    "    # Combina il testo estratto e quello ottenuto con OCR\n",
    "    combined_text = (\n",
    "        \"=== Extracted text from PDF ===\\n\" + extracted_text.strip() +\n",
    "        \"\\n\\n=== Extracted Text from images (OCR) ===\\n\" + \"\\n\\n\".join(ocr_text_list)\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "def process_pdfs(input_directory, output_directory, ocr_dpi=300, lang=\"eng\"):\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    pdf_files = sorted(glob.glob(os.path.join(input_directory, \"*.pdf\")))\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"Processing {pdf_path} ...\")\n",
    "        combined_text = extract_text_and_ocr_from_pdf(pdf_path, ocr_dpi=ocr_dpi, lang=lang)\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_directory, base_name + \".txt\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_text)\n",
    "        print(f\"Saved output to {output_path}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------#\n",
    "\n",
    "input_dir = \"data/slides/original\"   \n",
    "output_dir = \"data/slides/preprocessed/STEP_1\" \n",
    "process_pdfs(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f6674",
   "metadata": {},
   "source": [
    "### Riscrivere meglio il testo dalle slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a17822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=RS_GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def generate_better_text_of_slide(text):\n",
    "    prompt = (\n",
    "        \"The following text has been extracted from a PDF and is poorly formatted, with inconsistent spacing, line breaks, and structure. \"\n",
    "        \"Your task is to rewrite the text to improve its readability and formatting. Specifically:\\n\\n\"\n",
    "        \"1. Remove unnecessary line breaks and spaces to create a smooth, continuous flow of text.\\n\"\n",
    "        \"2. Correct any formatting issues, such as misplaced punctuation, inconsistent capitalization, or fragmented sentences.\\n\"\n",
    "        \"3. Ensure the text is clean and easy to read, with proper spacing and structure.\\n\"\n",
    "        \"4. Is important that you don't lose any information!.\\n\\n\"\n",
    "        \"Here is the text to reformat:\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    \n",
    "    return call_llm(prompt)\n",
    "\n",
    "def improve_slides(input_directory, output_directory):\n",
    "    # Crea la directory di output se non esiste\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 19\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        # Estrai il testo dal file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Migliora il testo\n",
    "        improved_text = generate_better_text_of_slide(extracted_text)\n",
    "        \n",
    "        # Salva il testo migliorato in un file nella directory di output\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        \n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "# Directory di input e output\n",
    "input_directory = \"data/slides/preprocessed/STEP_1\" \n",
    "output_directory = \"data/slides/preprocessed/STEP_2\" \n",
    "\n",
    "improve_slides(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512919f",
   "metadata": {},
   "source": [
    "### Unire Informazioni di slides e libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"data/faiss_index/BOOK_faiss_index__all-MiniLM-L6-v2\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "\n",
    "def retrieve_relevant_info(text, k=5):\n",
    "    docs = vectorstore.similarity_search(text, k=k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def generate_better_text(text, additional_info):  \n",
    "    prompt = (  \n",
    "        \"### Task Description\\n\"\n",
    "        \"Enhance the given text by making it clearer, more detailed, and comprehensive. \"\n",
    "        \"Analyze the main topics, integrate relevant insights, and enrich the content with additional context. \"\n",
    "        \"Ensure that the rewritten text flows naturally and maintains coherence without explicitly referencing the original sources.\\n\\n\"\n",
    "\n",
    "        \"### Provided Information\\n\"\n",
    "        \"**Original Text:**\\n\" + text + \"\\n\\n\"\n",
    "        \"**Additional Context:**\\n\" + additional_info + \"\\n\\n\"\n",
    "\n",
    "        \"### Guidelines & Constraints\\n\"\n",
    "        \"- Maintain the original meaning while improving clarity and depth.\\n\"\n",
    "        \"- Expand on key concepts using relevant explanations.\\n\"\n",
    "        \"- Ensure smooth and logical transitions between ideas.\\n\"\n",
    "        \"- Do not include direct references to the original sources.\\n\"\n",
    "        \"- Structure the response into sections separated by the delimiter:\\n\"\n",
    "        \"  `<----------section---------->`\\n\\n\"\n",
    "    )  \n",
    "    return call_llm(prompt)  \n",
    "\n",
    "\n",
    "\n",
    "def generate_final_textfile(input_directory, output_directory):\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    files = glob.glob(os.path.join(input_directory, \"*\"))\n",
    "    \n",
    "    skip_first = 0\n",
    "    cont = 0\n",
    "    \n",
    "    for file_path in files:\n",
    "        cont += 1\n",
    "        if cont <= skip_first:\n",
    "            continue  \n",
    "        \n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                extracted_text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Errore di codifica nel file {file_path}. Tentativo con codifica 'latin-1'.\")\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                extracted_text = file.read()\n",
    "        \n",
    "        if not extracted_text.strip():\n",
    "            print(f\"No text extracted from {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        additional_info = retrieve_relevant_info(extracted_text, k=3)\n",
    "        improved_text = generate_better_text(extracted_text, additional_info)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(improved_text)\n",
    "        print(f\"Salvato: {output_path}\")\n",
    "\n",
    "\n",
    "input_directory  = \"data/slides/preprocessed/STEP_2\"\n",
    "output_directory = \"data/slides/preprocessed/STEP_3\" \n",
    "generate_final_textfile(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1ed2a",
   "metadata": {},
   "source": [
    "## *4* Unisco in un unico File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_text_files(folder_path, output_file):\n",
    "\n",
    "    text_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for file_name in text_files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read() + '\\n')  \n",
    "    \n",
    "    print(f\"Unione completata: {output_file}\")\n",
    "\n",
    "folder_path = \"data/merged/preprocessed_by_gemini\"  \n",
    "output_file = \"data/all_preprocessed_by_gemini.txt\"  \n",
    "merge_text_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482bbb1",
   "metadata": {},
   "source": [
    "#### FAISS A DIMENSIONE FISSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4810b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "document_path = \"data/all_preprocessed_by_gemini.txt\"\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "\n",
    "# Split del testo per migliorare la ricerca\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salviamo il database FAISS\n",
    "vectorstore.save_local(\"ALL_faiss_index__all-MiniLM-L6-v2\")\n",
    "print(\"Retriever FAISS inizializzato e salvato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e92515b",
   "metadata": {},
   "source": [
    "#### FAISS A DIMENSIONE VARIABILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Percorso del documento\n",
    "document_path = \"data/all_preprocessed_by_gemini2Marzo2025.txt\"\n",
    "\n",
    "# Caricamento del documento\n",
    "loader = TextLoader(document_path, encoding=\"utf-8\")  \n",
    "doc_loader = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"<----------section---------->\", \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0,  # Nessuna sovrapposizione poichÃ© giÃ  suddiviso logicamente\n",
    "    keep_separator=False  # Rimuove il separatore dai chunk\n",
    ")\n",
    "split_docs = text_splitter.split_documents(doc_loader)\n",
    "\n",
    "# Creazione degli embeddings con un modello open-source\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Creazione del database FAISS\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# Salvataggio del database FAISS\n",
    "vectorstore.save_local(\"faiss_index/ALL_bge-m3\")\n",
    "print(\"Retriever FAISS inizializzato e salvato con suddivisione basata su separatore.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242a9ac",
   "metadata": {},
   "source": [
    "## *5* RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404cddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Configurazione del modello\n",
    "MODEL_LLM_PATH = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_LLM_PATH}\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\n",
    "\n",
    "# Caricamento embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"ALL_faiss_index__all-MiniLM-L6-v2\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "def generate_response(question, debug = False):\n",
    "    docs = vectorstore.similarity_search(question, k=10)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an AI assistant using Retrieval-Augmented Generation (RAG). \"\n",
    "        \"Use the following context to answer the question. If the answer is not in the context, say you don't know. \" \n",
    "        \"Give discursive answers\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"-----END_QUESTION-----\"\n",
    "    )\n",
    "\n",
    "    payload = {\"inputs\": prompt}\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"API Response Code: {response.status_code}\")\n",
    "        print(f\"Total Response: \\n{response}\\n\\n\\n\\n\")\n",
    "        print(f\"API Raw Response: {response.text}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Errore API ({response.status_code}): {response.text}\"\n",
    "    \n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        generated_text = response_json[0].get(\"generated_text\", \"Errore nella generazione della risposta\")\n",
    "        clean_response = generated_text.split(\"-----END_QUESTION-----\")[-1].strip() \n",
    "        return clean_response\n",
    "\n",
    "      \n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        return \"Errore nel parsing della risposta JSON\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
