{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3d3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 14:27:19.710267: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 14:27:19.713054: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 14:27:19.721027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740752839.734277   18871 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740752839.738130   18871 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 14:27:19.753166: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Assistant import Assistant\n",
    "assistant  = Assistant(faiss_index=\"data/faiss_index/ALL_faiss_index__all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff795dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to discuss Transformers with you! Transformers are a popular and powerful type of machine learning model used in natural language processing (NLP). They're particularly effective at understanding the context and relationships within text data, and they've led to significant improvements in NLP tasks like text generation, translation, and question answering.\n",
      "\n",
      "The Transformer model is unique in that it uses attention mechanisms instead of the traditional recurrent or convolutional neural networks. Attention mechanisms help the model decide how much weight to give each word in the input sequence when processing a particular word in the target sequence. This mechanism allows the model to focus on important context, even when dealing with long input sequences, thus overcoming the limitations of earlier models in handling long-range dependencies.\n",
      "\n",
      "To better understand Transformers, let's touch upon a few key concepts:\n",
      "\n",
      "1. Transformers for Text Representation and Generation:\n",
      "1.1 Encoder-Only Transformers:\n",
      "\n",
      "Encoder-only Transformers are an essential and powerful component of the Transformer architecture. They comprise an Encoder that converts input text into a contextualized and fixed-length representation using self-attention mechanisms. This encoded representation is then typically passed to a classification head or a sequence decoder, depending on the application. Notable applications of encoder-only Transformers include:\n",
      "\n",
      "   * Text Classification: Identifying the genre, sentiment, or topic of a text sample.\n",
      "   * Named Entity Recognition (NER): Tagging named entities in a text such as \"Apple Inc,\" \"Paris, France,\" or \"John Smith.\"\n",
      "   * Dependency Parsing: Learning the hierarchical and oriented relationships between words in a sentence.\n",
      "\n",
      "The slides you looked at showed an in-depth walkthrough of BERT (Bidirectional Encoder Representations from Transformers), a popular open-source transformer model for NLP tasks. BERT has been highly effective in various applications, setting the benchmark for many text classification tasks.\n",
      "\n",
      " Encoder-only transformers are the flavors of the Transformer model applied to text classification tasks. By solely using the Encoder (without the separate Decoder), text classification and other fixed-length tasks can be efficiently solved.\n",
      "\n",
      "1. Transformers for Text Generation:\n",
      "1.1 Decoder-Only Transformers:\n",
      "\n",
      "On the other hand, Decoder-only Transformers come into play when tasks like text generation, conversational AI, text summarization, question answering, code generation, debugging, and others require a sequence output. In these scenarios, the model can generate an output sequence one token at a time through masked self-attention. Decoder-only Transformers generate text using a left-to-right autoregressive paradigm. Examples of decoder-only Transformers are T5, GPT-3, and BART.\n",
      "\n",
      "There you have it! Transformers are a game-changer in NLP. Understanding these concepts will give you a solid foundation to deepen your knowledge and apply these models to various text-related applications. Let me know if you have any other questions or if you'd like to delve deeper into any of these topics!\n"
     ]
    }
   ],
   "source": [
    "print(assistant.ask(\"talk me about transformers\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
