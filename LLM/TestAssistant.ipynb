{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55119255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JV_GEMINI_TOKEN  = \n",
    "RS_GEMINI_TOKEN  = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "EZ_GEMINI_TOKEN  = \"AIzaSyAVi3tobZhK9uBL-eGyXUcCnRTiEPChsF4\"\n",
    "VM_GEMINI_TOKEN  = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\"\n",
    "VM_GEMINI_TOKEN2 = \"AIzaSyAfNu529ZSMYVc2cPrCzaPi5XKlWpi09X0\"\n",
    "JV_COHERE_TOKEN  = \n",
    "RS_COHERE_TOKEN  = \"t1GNKsIULpTSgepiuMsOtGicLqpTgVMX5UNnpiMg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from GenerationModel import GoogleGemini\n",
    "from GenerationModel import Cohere\n",
    "from Assistant import Assistant\n",
    "\n",
    "model1 = GoogleGemini(model=\"gemini-2.0-flash\",\n",
    "                      api_key=\"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\",\n",
    "                      temperature=0.5,\n",
    "                      top_p=0.5\n",
    "                      )\n",
    "\n",
    "model2 = Cohere(model=\"command-r-plus-04-2024\",\n",
    "                api_key=\"XjJ6nkqZabaMHpq4aehIfyyksudq5LSm80QvUqcV\",\n",
    "                temperature=0.5,\n",
    "                top_p=0.5\n",
    "                )\n",
    "\n",
    "assistant =  Assistant(faiss_index=\"data/faiss_index/ALL__22_03_2025__BGE-M3__MAX_INNER_PRODUCT\", \n",
    "                       log_file=\"data/logs/assistant.log\",\n",
    "                       embedding_model=\"BAAI/bge-m3\",\n",
    "                       generation_model1=model1,\n",
    "                       generation_model2=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff795dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "Transformers are a groundbreaking innovation in the field of Natural Language Processing (NLP). They revolutionized NLP by introducing a mechanism that allows models to handle long-range dependencies in text, overcoming the limitations of previous sequential models like Recurrent Neural Networks (RNNs).\n",
      "\n",
      "At the heart of transformers is the **Attention mechanism**. Unlike traditional sequential processing, transformers consider all parts of the input text simultaneously. They achieve this by assigning dynamic weights to each part of the input based on its relevance to other parts, enabling the model to capture relationships between words, regardless of their distance in the sequence.\n",
      "\n",
      "The key advantage of transformers is their ability to process text in parallel, significantly speeding up the training process. This parallel processing is made possible by the attention mechanism, which allows transformers to \"pay attention\" to all input tokens at once.\n",
      "\n",
      "Transformers come in different architectural flavors, including Encoder-Only models (e.g., BERT), Decoder-Only models (e.g., GPT), and Encoder-Decoder models (e.g., T5, BART). Each architecture is suited for specific tasks, such as text understanding, generation, translation, summarization, and question answering.\n",
      "\n",
      "The impact of transformers on Large Language Models (LLMs) is significant. LLMs, such as GPT-4, ChatGPT, and others, leverage the transformer architecture to process and generate human-like text at a scale and sophistication never seen before. The ability to handle long-range dependencies and parallel processing makes transformers ideal for training massive LLMs that can perform a wide range of NLP tasks with impressive accuracy and fluency.\n",
      "\n",
      "Overall, transformers are a fundamental concept in modern NLP, enabling models to understand and generate text with unprecedented capabilities. They have transformed the way machines interpret and create language, paving the way for more advanced language understanding and generation in the future.\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "result = assistant.ask(\"talk me about transformers\")\n",
    "response = result['final_response']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dad6b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk me about transformers\n",
      "talk me about transformers\n",
      "['**Transformers for text representation and generation**\\n\\nTransformers have revolutionized the field of Natural Language Processing (NLP) by offering a powerful mechanism for both understanding (representation) and creating (generation) text. They leverage the attention mechanism to handle long-range dependencies in text, which was a limitation in previous sequential models like Recurrent Neural Networks (RNNs). Furthermore, transformers enable parallel training, significantly speeding up the training process.\\n\\nThe core idea behind Transformers is the **Attention mechanism**. Unlike recurrent networks that process text sequentially, transformers consider all parts of the input at once, assigning dynamic weights to each part based on its relevance to other parts. This enables the model to capture relationships between words regardless of their distance in the sequence.\\n\\nTransformers come in different architectural flavors, each suited to specific tasks:\\n\\n*   **Encoder-Only (e.g., BERT):** This architecture focuses on understanding the input text. It takes input tokens and outputs hidden states representing the contextualized embeddings of the input. Encoder-only models can see all timesteps (i.e., the entire input sequence) at once. They are not inherently auto-regressive, meaning they don\\'t predict the next token sequentially, but rather generate representations of the entire input. BERT (Bidirectional Encoder Representations from Transformers) is a prime example, designed for tasks like sentiment analysis, named entity recognition, and question answering where understanding the context of the entire input is crucial. BERT was released in October 2018. Other prominent encoder-only architectures include:\\n    *   DistilBERT (2019): A smaller, faster version of BERT.\\n    *   RoBERTa (2019): A robustly optimized BERT pre-training approach.\\n    *   ALBERT (2019): A Lite BERT, employing parameter reduction techniques.\\n    *   ELECTRA (2020): Efficiently Learning an Encoder that Classifies Token Replacements Accurately.\\n    *   DeBERTa (2020): Decoding-enhanced BERT with disentangled attention.\\n\\n*   **Decoder-Only (e.g., GPT):** This architecture is designed for generating text. It takes output tokens and hidden states as input and outputs the next predicted token. Decoder-only models can only \"see\" the previous timesteps, making them auto-regressive. This means they predict the next token based on the tokens generated so far, making them suitable for text generation tasks like language modeling and creative writing. GPT (Generative Pre-trained Transformer) is a flagship decoder-only model. Key models in this family are:\\n    *   GPT (Jun 2018): The original Generative Pre-trained Transformer.\\n    *   GPT-2 (2019): An improved version of GPT with a larger parameter set.\\n    *   GPT-3 (2020): A very large language model demonstrating impressive generation capabilities.\\n    *   GPT-Neo (2021): An open-source alternative to GPT-3.\\n    *   GPT-3.5 (ChatGPT) (2022): Further refinement of GPT-3, optimized for conversational interactions.\\n    *   LLaMA (2023): A powerful and efficient open-source language model.\\n    *   GPT-4 (2023): OpenAI\\'s latest generation model, exhibiting advanced reasoning and creative abilities.\\n\\n*   **Encoder-Decoder (e.g., T5, BART):** This architecture combines both encoder and decoder components, allowing it to map input tokens to output tokens. The encoder processes the input sequence, and the decoder generates the output sequence based on the encoder\\'s representation. This architecture is well-suited for tasks like machine translation, text summarization, and question answering where understanding the input and generating a related output are both required. Examples of encoder-decoder models include:\\n    *   T5 (2019): Text-to-Text Transfer Transformer, framing all NLP tasks in a text-to-text format.\\n    *   BART (2019): Bidirectional and Auto-Regressive Transformer, designed for denoising sequence-to-sequence tasks.\\n    *   mT5 (2021): A multilingual version of T5.', \"**Additional Context:**\\n\\nThis section includes additional context gleaned from a practical example of using the base PyTorch implementation of the Transformer module to implement a language translation model. In a language translation model, self-attention enables a leap forward in capability for problems where LSTMs struggled, such as: Conversation — Generate plausible responses to conversational prompts, queries, or utterances.\\nAbstractive summarization or paraphrasing\\n:: Generate a new shorter wording of a long text summarization of sentences, paragraphs, and\\neven several pages of text.\\nOpen domain question answering\\n:: Answering a general question about\\nanything the transformer has ever read.\\nReading comprehension question answering\\n:: Answering questions\\nabout a short body of text (usually less than a page).\\nEncoding\\n:: A single vector or sequence of embedding vectors that\\nrepresent the meaning of body of text in a vector space\\u2009—\\u2009sometimes\\ncalled task-independent sentence embedding\\n.\\nTranslation and code generation\\n\\u2009—\\u2009Generating plausible software\\nexpressions and programs based on plain English descriptions of the\\nprogram’s purpose.\\nThere are two ways to implement the linear algebra of an attention algorithm: additive attention or dot-product attention. The one that was most effective in transformers is a scaled version of dot-production attention. For dot-product attention, the scalar products between the query vectors Q and the key vectors K, are scaled down based on how many dimensions there are in the model.\\nThis makes the dot product more numerically stable for large dimensional embeddings and longer text sequences. For dot-product attention, the scalar products between the query vectors Q and the key vectors K, are scaled down based on how many dimensions there are in the model.\\nThis makes the dot product more numerically stable for large dimensional embeddings and longer text sequences.\\nUnlike RNNs, where there is recurrence and shared weights, in self-attention all of the vectors used in the query, key, and value matrices come from the input sequences' embedding vectors. The entire mechanism can be\\nimplemented with highly optimized matrix multiplication operations. And the Q\\n \\nK product forms a square matrix that can be understood as the connection\\nbetween words in the input sequence.\\nMulti-head self-attention is an expansion of the self-attention approach to\\ncreating multiple attention heads that each attend to different aspects of the\\nwords in a text. So if a token has multiple meanings that are all relevant to the\\ninterpretation of the input text, they can each be accounted for in the separate\\nattention heads. You can think of each attention head as another dimension of\\nthe encoding vector for a body of text, similar to the additional dimensions of\\nan embedding vector for an individual token .\\n\\nThe multiple heads allow the model to focus on different positions, not just\\nones centered on a single word. This effectively creates several different vector subspaces where the transformer can encode a particular generalization for a subset of the word patterns in your text. In the original transformers paper, the model uses n\\n=8 attention heads such that \\\\(d_k = d_v\\n= \\\\frac{d_{model}}{n} = 64\\\\). The reduced dimensionality in the multi-head\\nsetup is to ensure the computation and concatenation cost is nearly equivalent\\nto the size of a full-dimensional single-attention head.\\n\\nThe attention matrices (attention heads) created by the product of Q and K all have the same shape, and they are all square (same number of rows as columns). This means that the attention matrix merely rotates the input sequence of embeddings into a new sequence of embeddings, without affecting the shape or magnitude of the embeddings.\\nEncoder-decoders based on RNNs don’t work very well for longer passages of text\\nwhere related word patterns are far apart. Even long sentences are a challenge\\nfor RNNs doing translation. And the attention mechanism compensates for\\nthis by allowing a language model to pick up important concepts at the\\nbeginning of a text and connect them to text that is towards the end. The\\nattention mechanism gives the transformer a way to reach back to any word it\\nhas ever seen. Unfortunately, adding the attention mechanism forces you to\\nremove all recurrence from the transformer.\\n\\nThe loss of recurrence in a transformer creates a new challenge because the\\ntransformer operates on the entire sequence all at once. A transformer is\\nreading the entire token sequence all at once. And it outputs the tokens all at\\nonce as well, making bi-directional transformers an obvious approach.\\nTransformers do not care about the normal causal order of tokens while it is\\nreading or writing text. To give transformers information about the causal\\nsequence of tokens, positional encoding was added.\\nThe combination of BPE plus attention plus positional encoding combine together to create unprecedented scalability. These three innovations and simplifications of neural networks combined to create a\\nnetwork that is both much more stackable and much more parallelizable.\\nStackability\\n:: The inputs and outputs of a transformer layer have the\\nexact same structure so they can be stacked to increase capacity\\nParallelizability\\n:: The cookie cutter transformer layers all rely heavily\\non large matrix multiplications rather than complex recurrence and\\nlogical switching gates. The increased intelligence that\\ntransformers bring to AI is transforming culture, society, and the economy.\\nFor the first time, transformers are making us question the long-term\\neconomic value of human intelligence and creativity.\\nThe attention matrix enables a transformer to accurately model the connections\\nbetween all the words in a long body of text, all at once.\\n\\nAnd the attention matrix within each layer spans the entire length of the input\\ntext, so each transformer layer has the same internal structure and math. You\\ncan stack as many transformer encoder and decoder layers as you like\\ncreating as deep a neural network as you need for the information content of\\nyour data. Every transformer layer outputs a consistent encoding with the same size and shape.\", '### Large Language Models (LLM)\\n\\nAfter transformers, the next step was scaling...\\n\\n*   LLM leverage extensive data and computational power to understand and generate human-like text, pushing the boundaries of NLP capabilities.\\n\\n*   List of LLMs: GPT-4, ChatGPT, InstructGPT, Codex, Flan-PaLM, LLaMA, BLOOM, OPT, UL2, PaLM, Gopher, Chinchilla, Titan, Jurassic-1, Ernie 3.0, PanGu, etc.*\\n\\nThese models, with their massive scale and sophisticated architectures, can perform a wide range of NLP tasks with impressive accuracy and fluency.', '**Transformer**\\n\\nIn 2017, a group of researchers at Google Brain proposed an alternative model for processing sequential data. In this model, the elements of the sequence can be processed in parallel. The number of layers traversed does not depend on the length of the sequence (so, no problems with the gradient). The model was introduced for language translation (sequence to sequence with different lengths); so, it was called Transformer. Subsets of the model can be used for other sequence processing tasks. Transformers address the limitations of RNNs by enabling parallel processing of sequence elements, mitigating the vanishing gradient problem, and reducing training time.', '## Adapters\\n\\n*   Adapters are lightweight, task-specific neural modules inserted between the layers of a pre-trained transformer block.\\n*   These modules are trainable, while the original pre-trained model parameters remain frozen during fine-tuning.\\n*   Adapters require training only the small fully connected layers, resulting in significantly fewer parameters compared to full fine-tuning.\\n*   Since the base model remains frozen, the general-purpose knowledge learned during pre-training is preserved.\\n\\nAdapters provide an alternative approach to PEFT by introducing small, task-specific neural modules within the layers of a pre-trained transformer block. These adapters are inserted between the existing layers, allowing the model to adapt to the new task without directly modifying the original pre-trained weights. Key features of adapters include:\\n\\n*   **Lightweight Modules:** Adapters are designed to be small and computationally inexpensive, containing a relatively small number of parameters compared to the full model.\\n*   **Trainable Parameters:** Only the adapter modules are trained for the specific task, while the original pre-trained model parameters remain frozen. This approach significantly reduces the number of trainable parameters.\\n*   **Preservation of General Knowledge:** Because the base model remains frozen, the general-purpose knowledge learned during pre-training is preserved. This allows the model to leverage its existing knowledge while adapting to the nuances of the new task.\\n*   **Fully Connected Layers:** Adapters typically consist of small fully connected layers, which are relatively easy to train and require fewer parameters than other types of neural network layers.']\n"
     ]
    }
   ],
   "source": [
    "print(result['original_question'])\n",
    "print(result['reformulated_query'])\n",
    "# . . .\n",
    "print(result['retrieved_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
