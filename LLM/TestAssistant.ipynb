{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55119255",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_TOKEN = \"INSERISCI_TOKEN_QUI\"\n",
    "COHERE_TOKEN = \"INSERISCI_TOKEN_QUI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacop\\OneDrive\\Documenti\\Università\\NATURAL LANGUAGE PROCESSING\\GIT_LLM\\LLM\\LLM\\Assistant.py:104: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  return LLMChain(llm=self.generation_model1.llm, prompt=prompt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jacop\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from GenerationModel import GoogleGemini\n",
    "from GenerationModel import Cohere\n",
    "from Assistant import Assistant\n",
    "\n",
    "model1 = GoogleGemini(model=\"gemini-2.0-flash\",\n",
    "                      api_key=GEMINI_TOKEN,\n",
    "                      temperature=0.5,\n",
    "                      top_p=0.5\n",
    "                      )\n",
    "\n",
    "model2 = Cohere(model=\"command-r-plus-04-2024\",\n",
    "                api_key=COHERE_TOKEN,\n",
    "                temperature=0.5,\n",
    "                top_p=0.5\n",
    "                )\n",
    "\n",
    "assistant =  Assistant(faiss_index=\"data/faiss_index/ALL__22_03_2025__BGE-M3__MAX_INNER_PRODUCT\", \n",
    "                       log_file=\"data/logs/assistant.log\",\n",
    "                       embedding_model=\"BAAI/bge-m3\",\n",
    "                       generation_model1=model1,\n",
    "                       generation_model2=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff795dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacop\\OneDrive\\Documenti\\Università\\NATURAL LANGUAGE PROCESSING\\GIT_LLM\\LLM\\LLM\\Assistant.py:137: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs_current = self.retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>\n",
      "<p>Transformers are a key component in the evolution of Natural Language Processing (NLP) and the development of Large Language Models (LLMs). They have revolutionized text representation and generation, marking a paradigm shift in the field of NLP.</p>\n",
      "\n",
      "<p>The introduction of transformer architectures, particularly encoder-only models like BERT and decoder-only models like GPT, has been a pivotal moment. BERT, for instance, uses bidirectional encoder representations, enabling it to understand the context of words based on their surrounding sentences. This enhances the model's ability to accurately classify tokens and recognize named entities. On the other hand, decoder-only transformers, such as GPT, focus on language generation tasks, demonstrating impressive capabilities in generating human-like text.</p>\n",
      "\n",
      "<p>The impact of transformers extends to the creation of LLMs, which take NLP capabilities to the next level. LLMs, including GPT-4, ChatGPT, InstructGPT, Codex, and many others, leverage massive amounts of data and computational power. They build upon the transformer architecture, incorporating massive scale and sophisticated design choices, to achieve remarkable performance across a wide range of NLP tasks.</p>\n",
      "\n",
      "<p>In the educational context provided, students are encouraged to explore both traditional NLP techniques and state-of-the-art LLM-based approaches for their projects. This blended approach allows for a hybrid chatbot development, combining the strengths of LLMs and conventional NLP tools. By understanding and utilizing these transformer-based models, students can create advanced chatbots, gaining a deep insight into their inner workings and making informed design decisions.</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "result = assistant.ask(\"talk me about transformers\")\n",
    "response = result['final_response']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dad6b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk me about transformers\n",
      "\"Talk about transformers in the context of NLP and LLMs.\"\n",
      "['## LESSON 11 ##\\n\\nFrom Transformers to LLMs\\n\\n**Outline**\\n\\n*   Transformers for text representation and generation\\n*   Paradigm shift in NLP\\n*   Pre-training of LLMs\\n*   Datasets and data pre-processing\\n*   Using LLMs after pre-training', '### Large Language Models (LLM)\\n\\nAfter transformers, the next step was scaling...\\n\\n*   LLM leverage extensive data and computational power to understand and generate human-like text, pushing the boundaries of NLP capabilities.\\n\\n*   List of LLMs: GPT-4, ChatGPT, InstructGPT, Codex, Flan-PaLM, LLaMA, BLOOM, OPT, UL2, PaLM, Gopher, Chinchilla, Titan, Jurassic-1, Ernie 3.0, PanGu, etc.*\\n\\nThese models, with their massive scale and sophisticated architectures, can perform a wide range of NLP tasks with impressive accuracy and fluency.', '## LESSON 13 ##\\n\\n## Outline\\nThe lesson covers the following topics:\\n*   Encoder-only transformer architectures\\n*   BERT (Bidirectional Encoder Representations from Transformers) model\\n*   Practical exercises on token classification and named entity recognition\\n\\nThis outline sets the stage for understanding how encoder-only transformers, like BERT, are used in specific NLP tasks. Token classification and named entity recognition will provide hands-on experience.', '`\\n\\n**Tools to Use for the Project (Expanded):**\\n\\nStudents have significant flexibility in selecting the tools and technologies for this project. They are encouraged to utilize any methods covered during the course. This includes both state-of-the-art LLM-based approaches and traditional NLP techniques.\\n\\nA blended approach is also acceptable, where some chatbot components are developed using LLMs, while others rely on more conventional NLP tools. Students can freely explore this hybrid approach, provided they thoroughly justify their choices and design decisions in the final report. The goal is to encourage thoughtful selection and integration of the most suitable tools for each specific task.\\n\\nExisting LLMs or other pre-trained models can be used, either with or without modifications. However, groups must demonstrate a thorough understanding of *all* tools and models incorporated into their chatbot. Students should be prepared to answer questions regarding every aspect of the code and models used, demonstrating their expertise in the technologies employed. This requirement ensures that students not only use the tools effectively but also gain a deep understanding of their inner workings.\\n\\n`', '## Lesson 14 #\\n\\n## Outline\\nThe lecture covers the following key topics:\\n\\n*   **Decoder-only Transformers:** An introduction to the architecture and functionalities of decoder-only transformers.\\n*   **GPT (Generative Pre-trained Transformer):** Exploration of the GPT model family, including its architecture, training, and applications.\\n*   **LLAMA (Large Language Model Meta AI):** An overview of the LLAMA models developed by Meta, focusing on their design and capabilities.\\n*   **Practice on Text Generation:** Practical exercises and demonstrations on generating text using various models and tools.']\n"
     ]
    }
   ],
   "source": [
    "print(result['original_question'])\n",
    "print(result['reformulated_query'])\n",
    "# . . .\n",
    "print(result['retrieved_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
