{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    sections = re.split(r'<----------section---------->', text)\n",
    "    \n",
    "    sections = [section.strip() for section in sections if section.strip()]\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"data/3Steps_6Marzo2025.txt\"  \n",
    "sections = extract_sections(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "RS_TOKEN = \"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\"\n",
    "\n",
    "JV_GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n",
    "RS_GEMINI_TOKEN = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "VM_GEMINI_TOKEN = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\"\n",
    "\n",
    "TOKENS = [VM_GEMINI_TOKEN, RS_GEMINI_TOKEN, JV_GEMINI_TOKEN]\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=JV_GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in question_data:\n",
    "    print(f\"{q}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "TOKENS = [ VM_GEMINI_TOKEN, \"AIzaSyAWD3pJb6KvzWcTvUeecgMSOQpKtkAP5wU\", RS_GEMINI_TOKEN]\n",
    "current_token_index = 0\n",
    "\n",
    "def call_llm(prompt, token):\n",
    "    genai.configure(api_key=token)\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "for section_index, section in enumerate(tqdm(sections, desc=\"Generating Questions\"), start=1):\n",
    "    \n",
    "    prompt = (\n",
    "        \"generate 5 questions based on the following text: \\n\\n\"\n",
    "        f\"{section}\\n\\n\\n\"\n",
    "        \"separate the questions with separator <----------question---------->\"\n",
    "        \"give me only the questions separated by a row and the separator <----------question---------->\"\n",
    "        \"not add any other text or information like answers or context or enumeration\"\n",
    "    )\n",
    "    \n",
    "    response = call_llm(prompt, TOKENS[current_token_index])\n",
    "    questions = response.split(\"<----------question---------->\")\n",
    "    \n",
    "    for question in questions:\n",
    "        if question.strip():  # Evita di salvare stringhe vuote\n",
    "            question_data.append({\"section_index\": section_index, \"question\": question.strip()})\n",
    "    \n",
    "    cont += 1\n",
    "    if cont % 3 == 0:\n",
    "        tempo_casuale_ms = random.randint(5000, 10000) / 1000 \n",
    "        time.sleep(tempo_casuale_ms)\n",
    "        current_token_index = (current_token_index + 1) % len(TOKENS)  #token prossimo nella coda circolare\n",
    "    tempo_casuale_ms = random.randint(4000, 7500) / 1000 \n",
    "    time.sleep(tempo_casuale_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"data/questions/6Marzo2025__ALL.json\"\n",
    "\n",
    "import json\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(question_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le domande sono state salvate in questions.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Percorso del file JSON\n",
    "input_file = \"data/questions/6Marzo2025__ALL.json\"\n",
    "output_file = \"data/questions/questions.txt\"\n",
    "\n",
    "# Legge il file JSON\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Estrae le domande\n",
    "questions = [item[\"question\"] for item in data]\n",
    "\n",
    "# Salva le domande in un file di testo\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for question in questions:\n",
    "        file.write(question + \"\\n\")\n",
    "\n",
    "print(f\"Le domande sono state salvate in {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
