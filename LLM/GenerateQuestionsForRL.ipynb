{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    sections = re.split(r'<----------section---------->', text)\n",
    "    \n",
    "    sections = [section.strip() for section in sections if section.strip()]\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"data/3Steps_6Marzo2025.txt\"  \n",
    "sections = extract_sections(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_TOKEN = \"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\"\n",
    "RS_TOKEN = \"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\"\n",
    "\n",
    "JV_GEMINI_TOKEN = \"AIzaSyArDcTFUTzztpgCIlogXSYQwBhUieZxv7Y\"\n",
    "RS_GEMINI_TOKEN = \"AIzaSyAS0kVBJkyFyosoCwqAQyJM0ElyKEzrmgM\"\n",
    "VM_GEMINI_TOKEN = \"AIzaSyD22Kr3nfSrvkE45KJlbIZHLuTA_cYuBYM\"\n",
    "\n",
    "TOKENS = [VM_GEMINI_TOKEN, RS_GEMINI_TOKEN, JV_GEMINI_TOKEN]\n",
    "\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=JV_GEMINI_TOKEN)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in question_data:\n",
    "    print(f\"{q}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "TOKENS = [ VM_GEMINI_TOKEN, JV_GEMINI_TOKEN, RS_GEMINI_TOKEN]\n",
    "current_token_index = 0\n",
    "\n",
    "def call_llm(prompt, token):\n",
    "    genai.configure(api_key=token)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "for section_index, section in enumerate(tqdm(sections, desc=\"Generating Questions\"), start=1):\n",
    "    \n",
    "    prompt = (\n",
    "        \"generate 5 questions based on the following text: \\n\\n\"\n",
    "        f\"{section}\\n\\n\\n\"\n",
    "        \"separate the questions with separator <----------question---------->\"\n",
    "        \"give me only the questions separated by a row and the separator <----------question---------->\"\n",
    "        \"not add any other text or information like answers or context or enumeration\"\n",
    "    )\n",
    "    \n",
    "    response = call_llm(prompt, TOKENS[current_token_index])\n",
    "    questions = response.split(\"<----------question---------->\")\n",
    "    \n",
    "    for question in questions:\n",
    "        if question.strip():  # Evita di salvare stringhe vuote\n",
    "            question_data.append({\"section_index\": section_index, \"question\": question.strip()})\n",
    "    \n",
    "    cont += 1\n",
    "    if cont % 3 == 0:\n",
    "        tempo_casuale_ms = random.randint(5000, 10000) / 1000 \n",
    "        time.sleep(tempo_casuale_ms)\n",
    "        current_token_index = (current_token_index + 1) % len(TOKENS)  #token prossimo nella coda circolare\n",
    "    tempo_casuale_ms = random.randint(4000, 7500) / 1000 \n",
    "    time.sleep(tempo_casuale_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"data/questions/6Marzo2025__ALL.json\"\n",
    "\n",
    "import json\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(question_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le domande sono state salvate in questions.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Percorso del file JSON\n",
    "input_file = \"data/questions/6Marzo2025__ALL.json\"\n",
    "output_file = \"data/questions/questions.txt\"\n",
    "\n",
    "# Legge il file JSON\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Estrae le domande\n",
    "questions = [item[\"question\"] for item in data]\n",
    "\n",
    "# Salva le domande in un file di testo\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for question in questions:\n",
    "        file.write(question + \"\\n\")\n",
    "\n",
    "print(f\"Le domande sono state salvate in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 925 questions\n",
      "Starting from question index 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/923 [02:30<1:38:59,  6.58s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 23: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 52/923 [06:31<1:45:02,  7.24s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 55: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 69/923 [08:28<1:28:58,  6.25s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 72: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 85/923 [10:26<1:32:07,  6.60s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 88: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 86/923 [10:34<1:35:05,  6.82s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      " 12%|█▏        | 109/923 [13:32<1:35:51,  7.07s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 112: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 125/923 [15:32<1:35:01,  7.14s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 128: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 188/923 [23:34<1:30:06,  7.36s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      " 21%|██        | 196/923 [24:27<1:11:52,  5.93s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 199: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 252/923 [31:34<1:15:03,  6.71s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 255: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 284/923 [35:32<1:25:57,  8.07s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 287: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 324/923 [40:29<1:09:22,  6.95s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 327: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 333/923 [41:34<1:06:52,  6.80s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      " 37%|███▋      | 341/923 [42:34<1:10:06,  7.23s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      " 43%|████▎     | 396/923 [49:31<1:03:48,  7.26s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 399: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 405/923 [50:34<1:08:19,  7.91s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      " 47%|████▋     | 436/923 [54:31<50:33,  6.23s/it]  Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 439: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 615/923 [1:17:29<36:37,  7.13s/it]  Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 618: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 639/923 [1:20:31<34:34,  7.31s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 642: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 663/923 [1:23:32<27:22,  6.32s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 666: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 672/923 [1:24:30<29:01,  6.94s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 675: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 688/923 [1:26:23<25:36,  6.54s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing question 691: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 689/923 [1:26:34<30:10,  7.74s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      " 75%|███████▌  | 696/923 [1:27:30<28:50,  7.62s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from Assistant import Assistant\n",
    "\n",
    "# Set up directories\n",
    "RESULTS_DIR = \"data/questions/response_13Marzo2025\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Checkpoint file\n",
    "CHECKPOINT_FILE = \"data/questions/13Marzo2025.json\"\n",
    "\n",
    "# Load questions\n",
    "try:\n",
    "    with open( \"data/questions/6Marzo2025__ALL.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        questions = json.load(f)\n",
    "    print(f\"Loaded {len(questions)} questions\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Questions file not found. Please generate questions first.\")\n",
    "    raise\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f).get(\"last_index\", 0)\n",
    "    return 0\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(index):\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_index\": index, \"timestamp\": datetime.now().isoformat()}, f)\n",
    "\n",
    "def test_questions():\n",
    "    assistant = Assistant(\n",
    "        faiss_index=\"data/faiss_index/ALL__11Marzo2025__bge-m3\", \n",
    "        log_file=\"data/logs/TestAssistant13Marzo2025.log\"\n",
    "    )\n",
    "    \n",
    "    start_index = load_checkpoint()\n",
    "    print(f\"Starting from question index {start_index}\")\n",
    "    \n",
    "    # Process questions\n",
    "    for i in tqdm(range(start_index, len(questions))):\n",
    "        question_data = questions[i]\n",
    "        question = question_data[\"question\"]\n",
    "        section_index = question_data[\"section_index\"]\n",
    "        \n",
    "        try:\n",
    "            assistant.clear_history()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = assistant.ask(question)\n",
    "            response = result[\"final_response\"]\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Create result object\n",
    "            result = {\n",
    "                \"question_id\": i + 1,\n",
    "                \"section_index\": section_index,\n",
    "                \"question\": question,\n",
    "                \"response\": response,\n",
    "                \"response_time\": end_time - start_time,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Save individual result\n",
    "            result_file = f\"{RESULTS_DIR}/question_{i+1}.json\"\n",
    "            with open(result_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Update checkpoint\n",
    "            save_checkpoint(i + 1)\n",
    "            \n",
    "            # Random delay to avoid patterns\n",
    "            delay = random.uniform(3, 8)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i+1}: {str(e)}\")\n",
    "            \n",
    "            # Save error information\n",
    "            error_result = {\n",
    "                \"question_id\": i + 1,\n",
    "                \"section_index\": section_index,\n",
    "                \"question\": question,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            error_file = f\"{RESULTS_DIR}/question_{i+1}_error.json\"\n",
    "            with open(error_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(error_result, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Still update checkpoint to continue from next question\n",
    "            save_checkpoint(i + 1)\n",
    "            \n",
    "            # Longer delay after error\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "\n",
    "    print(f\"Testing complete! Results saved to {RESULTS_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_questions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
