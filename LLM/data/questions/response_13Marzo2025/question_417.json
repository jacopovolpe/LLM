{
  "question_id": 417,
  "section_index": 84,
  "question": "What is Parameter-Efficient Fine-Tuning (PEFT) and how does it work?",
  "response": "Parameter-Efficient Fine-Tuning (PEFT) methods reduce the computational and storage burden of full fine-tuning by only modifying a small fraction of the model's parameters. This makes fine-tuning large language models feasible for resource-constrained environments and applications requiring frequent model updates. Some popular PEFT techniques include Low-Rank Adaptation (LoRA), Adapters, and Prefix Tuning.\n\n**Low-Rank Adaptation (LoRA):** LoRA injects trainable rank decomposition matrices into each layer of the Transformer model, allowing efficient fine-tuning with minimal parameter updates. It operates on the principle that the changes needed to adapt a pre-trained model to a new task can be effectively captured by a low-rank representation. The pre-trained transformer model has weight matrices *W*. Instead of directly modifying *W*, LoRA learns a low-rank decomposition of the weight update Δ*W*, represented as the product of two low-rank matrices, *A* (m×r) and *B* (r×n), where *r* is the rank, significantly smaller than the dimensions *m* and *n* of *W*. During fine-tuning, the effective weight matrix *W'* is computed as the sum of the original weights *W* and the low-rank update Δ*W*. The original weights *W* remain frozen, and the low-rank matrices *A* and *B* encode the task-specific knowledge, requiring significantly fewer parameters than updating the full weight matrix.\n\n**Adapters:** Adapters are small, pluggable modules integrated within the Transformer architecture. They introduce task-specific parameters while keeping the original model parameters frozen, achieving a balance between performance and efficiency. Adapters are trained to learn task-specific representations, while the pre-trained model provides a robust and general foundation. This modularity also facilitates multi-task learning.\n\n**Prefix Tuning:** Prefix Tuning optimizes a small set of continuous prefix vectors that are prepended to the input sequence. These prefixes guide the model's attention mechanism, allowing it to adapt to different tasks without modifying the underlying model weights. This approach is highly parameter-efficient, as only the prefix vectors are trained.",
  "response_time": 3.2308542728424072,
  "timestamp": "2025-03-13T18:17:13.718803"
}