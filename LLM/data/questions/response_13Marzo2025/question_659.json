{
  "question_id": 659,
  "section_index": 132,
  "question": "What information is exclusively used in text classification to categorize documents?",
  "response": "Text classification assigns predefined categories to text documents based solely on their content, disregarding metadata. The text classification process differs from document classification, which considers metadata, and document clustering, which discovers categories from unlabeled data. Text classification is crucial for various applications, including spam detection, sentiment analysis, topic categorization, and intent recognition in conversational AI.\n\nThere are several types of classification:\n\n*   **Single-label classification:** Each document is assigned to exactly one class, suitable when categories are mutually exclusive. A special case of this is binary classification.\n*   **Binary classification:** A special case of single-label classification with only two classes (e.g., spam/not spam).\n*   **Multi-label classification:** A document can belong to multiple classes simultaneously, acknowledging that documents can address multiple topics or themes.\n\nMachine learning is commonly employed for text classification. The process generally involves:\n\n1.  **Text Representation:** Converting text into a numerical vector format using methods like TF-IDF.\n2.  **Model Training:** Training an ML algorithm using a labeled dataset.\n3.  **Prediction:** Applying the trained model to classify new documents.\n\nTransformers have revolutionized NLP by offering a powerful architecture for both text representation and generation. Encoder-only models (e.g., BERT) excel at text representation, while decoder-only models (e.g., GPT) are specialized for text generation. Seq2Seq models combine encoder and decoder components and are well-suited for tasks requiring mapping input sequences to output sequences, such as machine translation.\n\nBefore LLMs, NLP pipelines heavily relied on manually crafted features, requiring domain expertise and significant effort. Since LLMs, pre-training and fine-tuning have become standard practices. LLMs can leverage vast amounts of unlabeled data during pre-training to learn general language representations. Fine-tuning then adapts these representations to specific downstream tasks with smaller labeled datasets.\n\nEvaluating classifier performance requires appropriate metrics. For multi-class and multi-label problems, several averaging methods are used: Micro Average, Macro Average, Weighted Average and Samples Average.\n\nBeyond topic labeling and sentiment analysis, text classification has diverse applications, including spam filtering, intent detection, language detection, and content moderation.",
  "response_time": 3.5569472312927246,
  "timestamp": "2025-03-13T18:48:12.608291"
}