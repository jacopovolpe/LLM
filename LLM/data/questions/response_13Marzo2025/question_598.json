{
  "question_id": 598,
  "section_index": 120,
  "question": "Provide two examples of how guardrails can be implemented in LLMs.",
  "response": "Guardrails are essential mechanisms and policies that govern the behavior of LLMs, acting as a safety net to ensure that the model's responses are safe, accurate, relevant, and aligned with ethical and operational guidelines. Several techniques can be employed to implement robust guardrails, including rule-based filters, fine-tuning with custom data, prompt engineering, external validation layers, and real-time monitoring and feedback.\n\nFor example, rule-based filters can block offensive terms or filter sensitive information using regular expressions. Another example is fine-tuning an LLM on medical data to restrict responses to accurate and safe medical advice. Specialized frameworks like Guardrails AI, LangChain, and OpenAI Moderation simplify the implementation of guardrails, offering pre-built functionalities and easy integration with LLM APIs.",
  "response_time": 2.0047638416290283,
  "timestamp": "2025-03-13T18:40:21.497511"
}