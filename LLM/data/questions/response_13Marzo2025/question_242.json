{
  "question_id": 242,
  "section_index": 49,
  "question": "How does WordPiece tokenization handle out-of-vocabulary words?",
  "response": "BERT utilizes WordPiece tokenization, a subword tokenization method, to handle out-of-vocabulary words. WordPiece breaks words into smaller units (subwords), allowing BERT to represent a wide range of vocabulary with a smaller vocabulary size and to handle out-of-vocabulary words efficiently. Common words are treated as single tokens, while rarer words are split into constituent subwords. For example, \"unhappiness\" could be tokenized into \"un,\" \"happy,\" and \"##ness,\" where \"##\" signifies a subword continuing a previous word. A key advantage of WordPiece embedding is that it facilitates the representation of rare or unknown words by breaking them into known subwords.",
  "response_time": 1.7849125862121582,
  "timestamp": "2025-03-13T17:55:25.500295"
}