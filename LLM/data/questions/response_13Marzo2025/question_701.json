{
  "question_id": 701,
  "section_index": 141,
  "question": "What is the purpose of using a minimum document frequency of 5 during TF-IDF vectorization?",
  "response": "Term Frequency (TF) quantifies the importance of a word within a document based on its frequency of occurrence, assuming that words appearing more frequently are more central to the document's topic. A common approach to represent text using TF is the Bag of Words (BoW) model, which represents text as a vector of word counts, disregarding word order and grammar. There are several variations of the BoW model, including One-hot Encoding, Binary BoW, Standard BoW, and Term Frequency (TF). Normalized TF addresses the limitations of raw TF by dividing the word count by the total number of words in the document, providing a more accurate representation of a word's importance relative to the document length. The Vector Space Model (VSM) represents documents as vectors in a multidimensional space, where each dimension corresponds to a term in the vocabulary, and the values in each dimension typically represent the TF or normalized TF of the corresponding term in the document. Document similarity can be calculated using metrics like Euclidean Distance or Cosine Similarity.\n\nTF-IDF (Term Frequency-Inverse Document Frequency) improves upon TF by considering a term's prevalence across the entire corpus, reducing the weight of common words and increasing the weight of rare words that are more likely to be informative. TF-IDF is calculated as the product of TF and IDF. While TF-IDF is a valuable tool for gauging term importance within a document, it has limitations when it comes to capturing semantic relationships, relying on exact word matching. Techniques like stemming and lemmatization can help address this issue by grouping similar words under a single token. TF-IDF remains effective for various NLP applications, including information retrieval, information filtering, and text classification.\n\nT5 (Text-to-Text Transfer Transformer) is a language model that frames all tasks as text-to-text problems. T5 utilizes a SentencePiece tokenizer with a vocabulary size of 32,000 tokens and incorporates special tokens such as `<pad>`, `<unk>`, `<eos>`, and task-specific prefixes. T5's pre-training employs a denoising autoencoder objective called span corruption, masking random spans of text within the input and training the model to reconstruct the original text, leveraging the C4 (Colossal Clean Crawled Corpus) dataset.",
  "response_time": 3.774857997894287,
  "timestamp": "2025-03-13T18:53:15.200987"
}