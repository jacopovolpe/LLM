{
  "question_id": 74,
  "section_index": 15,
  "question": "How has NLP developed historically?",
  "response": "The history of Natural Language Processing (NLP) has seen periods of progress and setbacks, influenced by available computational resources and evolving approaches. In the 1950s and 1960s, machine translation emerged as a primary focus, but early systems struggled with ambiguity, leading to the ALPAC Report in 1966, which reduced research funding. Noam Chomsky's work on generative grammar in 1957 influenced NLP, but early translation systems struggled with language complexity. The ALPAC Report recommended a shift away from fully automated machine translation. ELIZA, developed in the 1960s, simulated a Rogerian psychotherapist using pattern matching, while Alan Turing's Turing Test in 1950 aimed to evaluate a machine's ability to exhibit human-like intelligence; however, both had limitations.\n\nThe 1970s and 1980s saw the development of rule-based systems and ontologies, but these systems struggled with flexibility and scalability. The 1990s brought a statistical revolution, where increased computing power enabled statistically based models trained on large corpora of data to surpass rule-based systems, also seeing the invention of Long Short-Term Memory (LSTM) networks. In the 2000s, neural networks and word embeddings gained prominence, and Google Translate launched in 2006. The 2010s marked the deep learning era, with LSTM and Convolutional Neural Network (CNN) architectures becoming widely adopted, Word2Vec revolutionizing word embedding learning in 2013, sequence-to-sequence models introducing the encoder-decoder framework in 2014, and the emergence of virtual assistants. The Transformer architecture in 2017 significantly advanced NLP capabilities. More recently, Large Language Models (LLMs) have emerged as powerful tools for various NLP tasks, and Multimodal LLMs integrate and process diverse data types.",
  "response_time": 2.9053335189819336,
  "timestamp": "2025-03-13T17:34:15.412721"
}