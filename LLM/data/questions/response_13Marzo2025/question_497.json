{
  "question_id": 497,
  "section_index": 100,
  "question": "How does the \"Bag of Words\" approach represent textual information, and what are its limitations?",
  "response": "The Bag-of-Words (BoW) model represents text as a vector of word counts, disregarding word order and grammar. In this model, words are represented as one-hot vectors, where each word is assigned a unique index in the vocabulary. One-hot encoding represents each word in a vocabulary as a vector, the vector's length equals the vocabulary size, and only the element corresponding to the word's index is 1; all others are 0. The BoW model addresses the sparsity issue of one-hot vectors by summing the one-hot vectors for all words in a document, resulting in a single vector where each element represents the count of a specific word in the document. A binary variant of BoW simply indicates the presence (1) or absence (0) of a word, disregarding its frequency.\n\nHowever, the Bag-of-Words model has limitations. The distance between any two one-hot vectors is always the same, failing to capture semantic relationships between words. One-hot vectors are sparse, requiring significant memory and processing resources, especially for large vocabularies. BoW loses word order information, which can be crucial for understanding meaning.",
  "response_time": 2.376910924911499,
  "timestamp": "2025-03-13T18:27:24.824904"
}