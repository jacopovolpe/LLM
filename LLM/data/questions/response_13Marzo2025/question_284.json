{
  "question_id": 284,
  "section_index": 57,
  "question": "What is the role of the causal (unidirectional) mask in the self-attention mechanism of decoder-only transformers?",
  "response": "In decoder-only transformers, the self-attention mechanism within decoder layers uses a causal (unidirectional or forward) mask. This mask ensures that each token only attends to previous tokens, mimicking the natural flow of language generation and preventing the model from \"looking ahead\" at future tokens during training and generation. This sequential processing allows the model to accumulate contextual information and learn complex relationships between tokens, effectively replacing the need for explicit encoder-decoder attention. During the generation of the *i*-th output word, the decoder should only consider the preceding words (positions 1 to *i-1*) and not \"peek\" into the future. This is achieved by applying a mask to the attention scores, effectively setting the scores corresponding to future positions to negative infinity, forcing the softmax function to assign zero probability to these future positions.",
  "response_time": 1.842768669128418,
  "timestamp": "2025-03-13T18:00:32.846497"
}