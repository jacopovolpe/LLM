{
  "question_id": 116,
  "section_index": 24,
  "question": "What is the primary purpose of using Masked Multi-Head Attention in decoder training?",
  "response": "Masked Multi-Head Attention is crucial for training the decoder because, during the generation of the *i*-th output word, the decoder should only consider the preceding words (positions 1 to *i-1*) and not \"peek\" into the future. This is achieved by applying a mask to the attention scores, effectively setting the scores corresponding to future positions to negative infinity. This forces the softmax function to assign zero probability to these future positions.",
  "response_time": 1.679396629333496,
  "timestamp": "2025-03-13T17:39:31.064660"
}