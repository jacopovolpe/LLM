{
  "question_id": 13,
  "section_index": 3,
  "question": "Describe the differences between Encoder-only, Decoder-only, and Encoder-Decoder Transformer architectures, and provide an example NLP task suitable for each.",
  "response": "Here's a comparison of Encoder-only, Decoder-only, and Encoder-Decoder transformer architectures, highlighting their differences and suitable NLP tasks.\n\n**Encoder-Only Transformers (e.g., BERT):** These models utilize only encoder blocks and employ bidirectional self-attention, processing the entire input sequence simultaneously to capture context from both directions. They are trained using Masked Language Modeling (MLM), where the model predicts masked tokens based on the surrounding context. Encoder-only transformers process the entire sequence in parallel, allowing for rich contextual understanding. They are primarily suited for tasks requiring understanding of the entire input sequence, such as text classification, Named Entity Recognition (NER), and question answering. The output typically generates contextualized embeddings for each input token, which can then be used for downstream tasks. When the input and output sequences have the same length, only the encoder is necessary, and the output vectors are derived directly from the encoder, allowing for direct loss computation, as seen in part-of-speech tagging or named entity recognition. For sequence classification, where the output is a single value, the encoder is also sufficient by prepending a special `[CLS]` token to the input sequence, and its corresponding output vector represents the entire sequence, used for computing the loss function, as in sentiment analysis.\n\n**Decoder-Only Transformers (e.g., GPT):** These models consist of only decoder blocks and employ causal (unidirectional) self-attention, processing tokens sequentially from left to right, conditioning each prediction on previous tokens. They are trained using Autoregressive Language Modeling, predicting the next token in a sequence given the preceding tokens. Decoder-only transformers process tokens sequentially, building up context as it progresses. They are ideal for generative tasks like text generation, story generation, code generation, and translation. They produce sequential token generation (text or other content). The core principle behind text generation in decoder-only transformers is autoregression, generating text token by token, where each new token is predicted based on the preceding tokens in the sequence. A causal (unidirectional or forward) mask is applied to ensure that each token attends only to previous tokens, mimicking the natural flow of language generation.\n\n**Seq2Seq models (Encoder-Decoder Architectures):** These models combine encoder and decoder components and are well-suited for tasks requiring mapping input sequences to output sequences, such as machine translation. The encoder processes the input sequence, and the decoder generates the output sequence based on the encoder's representation. The decoder generates the output sequence one element at a time, using the contextualized representation generated by the encoder. At each step, the decoder considers the encoder's output and the previously generated words in the output sequence. The decoder block structure mirrors the encoder block, with the addition of a second multi-head attention mechanism, called encoder-decoder attention, allowing the decoder to attend to the relevant parts of the encoder's output when generating each word in the output sequence. Masked Multi-Head Attention is crucial for training the decoder, where the decoder should only consider the preceding words and not \"peek\" into the future. Encoder-Decoder Attention bridges the encoder and decoder, enabling the decoder to leverage the contextualized information encoded by the encoder.",
  "response_time": 4.275465726852417,
  "timestamp": "2025-03-13T17:26:46.534166"
}