{
  "question_id": 455,
  "section_index": 91,
  "question": "What related topics, mentioned in the original text, were omitted from this version for clarity and focus?",
  "response": "The course \"Natural Language Processing and Large Language Models\" covers a range of topics, including Fundamentals of NLP, Transformers, Prompt Engineering, and LLM Fine Tuning. The Fundamentals of NLP include basic concepts, evolution and applications of NLP, representing text using techniques like tokenization, stemming, lemmatization, and POS tagging, math with words (Bag of Words, Vector Space Model, TF-IDF, Search Engines), text classification (topic labelling, sentiment analysis), word embeddings (Word2Vec, CBOW, Skip-Gram, GloVe, FastText), neural networks for NLP (RNN, LSTM, GRU, CNN, Introduction to Text Generation), information extraction (parsing, named entity recognition), and question answering and dialog engines (chatbots). The section on Transformers covers self-attention, multi-head attention, positional encoding, masking, the encoder and decoder of a Transformer, an introduction to HuggingFace, encoder-decoder or Seq2Seq models (translation and summarization), encoder-only Models (sentence classification and named entity recognition), decoder-only Models (text generation), and the definition and training of a Large Language Model. Prompt Engineering includes zero-shot and few-shot prompting, chain-of-thought, self-consistency, prompt chaining, role prompting, structured prompts and system prompts, and retrieval augmented generation. LLM Fine Tuning covers feature-based fine tuning, parameter efficient fine tuning and low rank adaptation, and reinforcement learning with human feedback.\n\nThe textbook for the course is \"Natural Language Processing in Action: Understanding, analyzing, and generating text with Python\" by H. Lane, C. Howard, and H. M. Hapke, published by Manning in 2019. A second edition is expected in fall 2024, with an early access version available online.\n\nRegarding GPT fine-tuning, it allows tailoring GPT models for specific tasks by training the model on a smaller, labeled dataset curated for the target application, adapting the pre-trained knowledge to the specific nuances of the downstream task, leading to improved performance. Examples of fine-tuning tasks include customer support automation, medical assistance, legal document processing, coding assistance, educational tutoring, content creation, and virtual personal assistants.\n\nGPT models have strengths such as fluency and coherence, a broad knowledge base, few-shot and zero-shot learning capabilities, and creative and contextual writing abilities. However, they also have limitations, including a lack of true understanding, sensitivity to prompting, ethical and bias concerns, limited reasoning and calculation abilities, high computational requirements, and limited memory across interactions.\n\nSeveral popular GPT variants exist, including Codex (fine-tuned for coding tasks), MT-NLG (a massive language model), GLaM (employing a sparse mixture-of-experts architecture), PanGu-α (a Chinese language model), Chinchilla (focusing on optimizing the balance between training data and model parameters), OPT (a series of open-source models), and BLOOM (an open-source multilingual model).\n\nT5 fine-tuning adapts the pre-trained T5 model for specific downstream tasks, maintaining the text-to-text paradigm with task-specific prefixes guiding the model. Example tasks and prefixes include summarization (`summarize: <document>` → `<summary>`), translation (`translate English to French: <text>` → `<translated_text>`), and question answering (`question: <question> context: <context>` → `<answer>`).\n\nSeveral T5 variants have been developed for specific purposes and improvements, including mT5 (Multilingual NLP), Flan-T5 (Instruction-following), ByT5 (No tokenization), T5-3B/11B (High-capacity NLP), UL2 (Unified objectives), Multimodal T5 (Vision-language tasks), and Efficient T5 (Resource-constrained NLP).\n\nFor practice on translation and summarization, the Hugging Face guides provide practical examples and code for implementing these tasks using pre-trained models, including T5. Resources are available for fine-tuning a pre-trained model on a specific dataset to enhance its performance on the target task.",
  "response_time": 5.200911521911621,
  "timestamp": "2025-03-13T18:22:06.076878"
}