{
  "question_id": 422,
  "section_index": 85,
  "question": "How does full fine-tuning differ from other fine-tuning methods in terms of parameter updates?",
  "response": "Fine-tuning tailors a pre-trained LLM to a specific task or domain by further training it on a dataset relevant to the target application. Several fine-tuning strategies address the limitations of full fine-tuning by updating only a subset of the model's parameters. Full fine-tuning updates all the parameters of the pre-trained LLM, which can lead to high accuracy but has significant drawbacks, including substantial computational cost and the risk of overfitting with small datasets. Other types of fine-tuning include Parameter-Efficient Fine-Tuning (PEFT), which encompasses techniques like LoRA, Adapters, and Prefix-Tuning, Instruction Fine-Tuning, which focuses on aligning the LLM with specific instructions or prompts, and Reinforcement Learning from Human Feedback (RLHF), which combines supervised learning with reinforcement learning to train LLMs to generate outputs that align with human preferences and values. PEFT methods reduce the computational and storage burden by modifying only a small fraction of the model's parameters. LoRA injects trainable rank decomposition matrices into each layer of the Transformer model, while Adapters are small, task-specific modules inserted within the Transformer layers. Prefix Tuning prepends a sequence of trainable prefix tokens to the input sequence.",
  "response_time": 2.4396698474884033,
  "timestamp": "2025-03-13T18:17:56.618619"
}