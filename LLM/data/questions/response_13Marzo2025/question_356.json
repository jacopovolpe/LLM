{
  "question_id": 356,
  "section_index": 72,
  "question": "What are the key differences between causal language modeling and masked language modeling, and why is the `mlm=False` setting crucial for fine-tuning a causal language model like GPT-2?",
  "response": "Encoder-Decoder Transformers are neural networks designed for sequence-to-sequence tasks, utilizing the attention mechanism to capture dependencies between input and output sequences. The encoder transforms the input sequence into hidden representations, while the decoder generates the output sequence, attending to relevant parts of the encoded input.\n\nFine-tuning tailors a pre-trained LLM to a specific task or domain by further training it on a relevant dataset, allowing the model to specialize its knowledge and improve performance. Full fine-tuning updates all the model's parameters, potentially leading to high accuracy but also increased computational cost and overfitting risk. Parameter-Efficient Fine-Tuning (PEFT), instruction fine-tuning, and Reinforcement Learning from Human Feedback (RLHF) are alternative strategies.\n\nGPT models generate fluent and coherent text with a broad knowledge base and excel at few-shot and zero-shot learning and creative writing. However, they may lack true understanding, are sensitive to prompting, can exhibit biases, and have limitations in reasoning and calculation. Popular GPT variants include Codex, MT-NLG, GLaM, PanGu-Î±, Chinchilla, OPT, and BLOOM.\n\nLLMs are pre-trained using self-supervised learning on massive text datasets. Autoencoding models (e.g., BERT) use Masked Language Modeling (MLM) to predict masked words, while autoregressive models (e.g., GPT) use Causal Language Modeling (CLM) to predict the next word in a sequence. Seq2seq models reconstruct corrupted sections of text.\n\nTraining LLMs requires enormous text corpora from books, CommonCrawl, Wikipedia, and other sources. Data pre-processing involves quality filtering, deduplication, privacy scrubbing, and filtering toxic and biased text. After pre-training, LLMs can be adapted through fine-tuning or prompting. Fine-tuning adjusts the model's weights using a task-specific dataset. Prompting guides the model's output without modifying its parameters.\n\nThe Transformer architecture offers advantages such as self-attention, scalability, and stackability. Byte Pair Encoding (BPE) handles large vocabularies, and positional encodings account for word order.",
  "response_time": 3.583448886871338,
  "timestamp": "2025-03-13T18:09:36.552586"
}