{
  "question_id": 407,
  "section_index": 82,
  "question": "Is a hybrid approach using both LLMs and traditional NLP techniques allowed, and if so, what is required in the final report?",
  "response": "This course provides a comprehensive introduction to Natural Language Processing (NLP) with a strong emphasis on Large Language Models (LLMs). The instructors for the course are Nicola Capuano and Antonio Greco. It is part of the Master's Degree Program in Computer Engineering at DIEM, University of Salerno. The course covers fundamental NLP concepts, explores the underlying architecture of LLMs (Transformers), and delves into practical applications, including prompt engineering and fine-tuning techniques. Students will acquire both theoretical knowledge and practical skills in designing and implementing NLP systems using LLMs.\n\nThe course objectives are divided into knowledge acquisition and skill development. Students will learn basic concepts of NLP, including the history, evolution, and core challenges, such as ambiguity, variability, and knowledge dependence. They will also study Natural Language Understanding (NLU) and Generation (NLG), statistical approaches to NLP, Large Language Models (LLM) based on Transformers, NLP applications with LLM, and Prompt Engineering and Fine Tuning of LLM. Students will develop skills in the design and implementation of an NLP system based on LLMs, integrating existing technologies and tools.\n\nThe Transformer architecture offers several advantages, including self-attention, scalability, stackability, Byte Pair Encoding (BPE), and positional encodings. Self-attention allows the model to weigh the importance of different parts of the input sequence. The parallelizable nature of attention computations allows Transformers to be scaled to much larger datasets and model sizes. The consistent input and output dimensions of Transformer layers facilitate stacking multiple layers. Byte Pair Encoding (BPE) plays a critical role in handling large vocabularies efficiently. Positional Encodings compensate for the lack of inherent positional information in self-attention.\n\nNLP's history includes early stages and machine translation, generative grammars and the ALPAC Report, ELIZA and the Turing Test, symbolic approaches, the statistical revolution, NLP advancements in the 2000s, the deep learning era, Large Language Models (LLMs), and multimodal LLMs.\n\nFine-tuning tailors a pre-trained LLM to a specific task or domain by further training it on a dataset relevant to the target application. Fine-tuning allows for domain specialization, enhanced accuracy and relevance, and effective utilization of smaller datasets. Types of fine-tuning include full fine-tuning, parameter-efficient fine-tuning (PEFT), instruction fine-tuning, and reinforcement learning from human feedback (RLHF).",
  "response_time": 4.00011944770813,
  "timestamp": "2025-03-13T18:15:53.736682"
}