This course on Natural Language Processing (NLP) and Large Language Models (LLMs) provides a comprehensive introduction to the field, covering fundamental concepts, advanced techniques using transformers, and practical application through prompt engineering and fine-tuning.  The course is designed for graduate students in Computer Engineering and aims to equip them with both theoretical knowledge and practical skills in designing and implementing NLP systems.

The course begins with the fundamentals of NLP, exploring its evolution and diverse applications across various domains. Students will learn about core concepts like tokenization, stemming, and lemmatization, which are crucial for preparing text data for computational analysis.  The course then delves into representing text mathematically using techniques like Bag-of-Words, TF-IDF, and Vector Space Models, laying the groundwork for understanding how search engines function.  Further, the course explores text classification tasks such as topic labeling and sentiment analysis, essential for understanding and interpreting textual data.  Word embeddings, including Word2Vec, GloVe, and FastText, will be covered, demonstrating how words can be represented in a continuous vector space to capture semantic relationships. Finally, the foundational section concludes with an introduction to neural networks for NLP, including recurrent neural networks (RNNs), Long Short-Term Memory networks (LSTMs), Gated Recurrent Units (GRUs), and Convolutional Neural Networks (CNNs), along with an introduction to text generation.  This section also includes critical NLP tasks like Information Extraction through techniques like Parsing and Named Entity Recognition, and touches upon the basics of Question Answering and Dialog Systems (chatbots).

Building upon the fundamentals, the course then explores the transformative architecture of Transformers.  This section covers key concepts like self-attention, multi-head attention, positional encoding, and masking, which are crucial for understanding how transformers process sequential data.  Students will gain an understanding of the encoder and decoder components of a transformer model and be introduced to the Hugging Face library, a popular tool for working with transformer models.  The different types of transformer models, including encoder-decoder (used in tasks like translation and summarization), encoder-only (used for sentence classification and named entity recognition), and decoder-only models (used in text generation) will be examined.  Finally, the course delves into the intricacies of defining and training Large Language Models.

The final segment of the course focuses on practical application and optimization of LLMs.  Students will learn about prompt engineering techniques, which are essential for effectively utilizing LLMs.  This includes zero-shot and few-shot prompting, advanced techniques like Chain-of-Thought prompting and Self-Consistency, and techniques like prompt chaining for complex tasks.  Furthermore, students will explore different prompting strategies such as role prompting, structured prompts, and system prompts.  The course also covers Retrieval Augmented Generation, a powerful technique for enhancing LLM outputs by incorporating external knowledge sources. Finally, the course will cover LLM fine-tuning techniques, including feature-based fine-tuning, parameter-efficient fine-tuning methods, and low-rank adaptation, which allow adapting pre-trained LLMs to specific downstream tasks.  Advanced techniques using Reinforcement Learning with Human Feedback will also be introduced.

Throughout the course, students will develop the ability to design, implement, and evaluate NLP systems using LLMs, integrate existing technologies and tools, critically evaluate different NLP techniques, and adapt and refine LLM outputs for improved performance.  The course will utilize the textbook "Natural Language Processing in Action" (Lane, Howard, & Hapke, 2nd edition) and will be supplemented with online materials and hands-on project work, culminating in an oral exam and project discussion.
