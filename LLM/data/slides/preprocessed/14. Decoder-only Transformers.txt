 is a mechanism that allows the model to focus on specific parts of
the input sequence when generating an output. This is achieved by assigning
weights to each word in the input sequence, which determines the importance of
each word in the output. The weights are learned during training and are
dynamically adjusted based on the input sequence.
The self-attention mechanism can be thought of as a weighted sum of the input
sequence, where the weights are learned during training. The weights are
computed using a dot product of the input sequence and a set of learned
parameters called query, key, and value vectors. The query, key, and value vectors
are all derived from the input sequence, but they are transformed using different
linear transformations.
The query, key, and value vectors are computed as follows:
1. The input sequence is first transformed into three different vectors using
different linear transformations: query, key, and value vectors.
2. The dot product of the query and key vectors is computed, resulting in a
matrix of scores.
3. The scores are then softmaxed to obtain a matrix of attention weights.
4. The value vectors are multiplied by the attention weights to obtain the
attention output.
The attention output is then added to the output of the encoder or decoder layer
to obtain the final output.
The self-attention mechanism allows the model to focus on specific parts of the
input sequence when generating an output. This is particularly useful in tasks
such as translation, where the model needs to focus on specific words or
phrases in the input sequence to generate the correct output.
The self-attention mechanism is implemented using a multi-head attention
architecture, which allows the model to attend to multiple parts of the input
sequence simultaneously. This is achieved by computing multiple sets of query,
key, and value vectors and combining their attention outputs using a weighted
sum.
The multi-head attention architecture is implemented as follows:
1. The input sequence is first transformed into four different vectors using
different linear transformations: query, key, value, and output vectors.
2. The query, key, and value vectors are split into multiple sets using a
different linear transformation for each set.
3. For each set of query, key, and value vectors, the dot product of the query
and key vectors is computed, resulting in a