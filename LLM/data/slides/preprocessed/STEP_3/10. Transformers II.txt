## Natural Language Processing and Large Language Models

**Corso di Laurea Magistrale in Ingegneria Informatica**

**Lesson 10: Transformers II**

Nicola Capuano and Antonio Greco

DIEM – University of Salerno


This lesson delves into the inner workings of the Transformer architecture, a revolutionary model in natural language processing. We'll explore the key components that enable Transformers to effectively process and generate text, focusing on the mechanisms within both the encoder and decoder stacks.  We will also discuss the practical application of Transformers, particularly in language translation tasks, highlighting their power and elegant design.

`<----------section---------->`

### Multi-Head Attention

The core of the Transformer's power lies in its attention mechanism, specifically multi-head attention.  This mechanism allows the model to weigh the importance of different parts of the input sequence when processing each word.  Instead of relying on a single attention mechanism, multi-head attention employs multiple "heads" that operate in parallel. Each head learns a different linear projection of the input sequence into query, key, and value vectors.  These vectors are used to calculate attention weights, representing the relevance of each word in the input sequence to the word being currently processed.

The multiple heads enable the model to capture different aspects of the relationships between words.  For instance, one head might focus on syntactic relationships, while another focuses on semantic connections. The outputs of these individual attention heads are then concatenated and linearly transformed to produce a unified representation. This allows the model to attend to information from different representation subspaces at different positions, something that wouldn’t be possible with a single, averaged attention mechanism.  This multi-faceted understanding of the input sequence enhances the model's ability to discern nuanced meaning and context.  Furthermore, each multi-head attention block incorporates Add & Norm layers (skip connections) and Feed Forward layers. The Add & Norm layers normalize the output, stabilizing training and acting as a regularizer. The skip connections help prevent vanishing gradients, a common problem in deep networks, allowing for the training of deeper models.  The Feed Forward layers introduce non-linearity, enabling the model to learn complex relationships within the data.


`<----------section---------->`

### Transformer’s Encoder

The encoder's role is to create a contextualized representation of the input sequence.  It achieves this by stacking multiple identical encoder blocks. Each block consists of a multi-head self-attention layer and a position-wise feed-forward network, both enveloped by Add & Norm layers.  The self-attention mechanism within the encoder allows each word in the input sequence to attend to all other words, including itself, within the same sequence. This enables the model to capture long-range dependencies and contextual relationships between words.

Crucially, since Transformers don't inherently process sequential information in the same way recurrent networks do, positional encoding is added to the input embeddings. This provides information about the position of each word in the sequence, making the model sensitive to word order. The output of each encoder block is a sequence of vectors, each representing a word in the input sequence enriched with contextual information. This output can then be fed as input to the next encoder block, allowing for deeper processing and a hierarchical representation of the input.

`<----------section---------->`

### Decoder

The decoder generates the output sequence one element at a time, utilizing the contextualized representation generated by the encoder and the previously generated parts of the output sequence. Like the encoder, the decoder is composed of stacked identical decoder blocks.  Each decoder block contains a masked multi-head attention layer, an encoder-decoder attention layer, and a position-wise feed-forward network, all with Add & Norm layers.

The masked multi-head attention mechanism within the decoder ensures that when generating the output at a specific position, the model only attends to the preceding output elements, preventing it from "peeking" into the future.  The encoder-decoder attention layer allows the decoder to attend to the entire encoded input sequence, providing access to the contextualized representation created by the encoder.  This mechanism allows the decoder to focus on relevant parts of the input when generating each element of the output.  Finally, a linear layer followed by a softmax function transforms the output of the last decoder block into probabilities over the output vocabulary, allowing the model to predict the next element in the output sequence.


`<----------section---------->`

### Masked Multi-Head Attention

The masked multi-head attention is a crucial component of the decoder, preventing information leakage from future tokens during training.  It works by masking future positions in the input sequence, effectively setting their attention weights to zero.  This ensures that the prediction for the current token only relies on the preceding tokens, maintaining the autoregressive property of the decoder and allowing for accurate training.

`<----------section---------->`

### Encoder-Decoder Attention

The encoder-decoder attention mechanism bridges the encoder and decoder, enabling the decoder to leverage the contextualized representations produced by the encoder.  The queries for this attention mechanism come from the decoder, while the keys and values come from the encoder output. This allows the decoder to focus on the most relevant parts of the input sequence when generating each output token, effectively aligning the input and output sequences.

`<----------section---------->`

### Output

The final layer of the decoder is a linear layer followed by a softmax function. This layer maps the decoder's output to a probability distribution over the output vocabulary. The output with the highest probability is then selected as the next token in the generated sequence.  The weights of this linear layer are often tied with the input embedding matrix, which can improve performance and reduce the number of parameters.

`<----------section---------->`

### Transformer’s Pipeline

The Transformer operates by first encoding the entire input sequence using the encoder stack. This creates a rich, contextualized representation of the input. The decoder then generates the output sequence autoregressively, one token at a time. At each step, the decoder considers the previously generated tokens and the encoder output, using the attention mechanisms to weigh the importance of different parts of the input and output sequences. This process continues until an end-of-sequence token is generated, completing the output sequence.  This pipeline enables Transformers to excel at tasks like machine translation, where understanding the relationships between words in both the source and target languages is crucial.
