
RNNs are a type of neural network that can process sequences of tokens or
vectors. They extract generally useful features and representations of your
data according to whatever problem you set up in your pipeline. And modern
neural networks work especially well even for information-rich data such as
natural language text.
5.1.1 Neural networks for words
With neural networks, you don t have to guess whether the proper nouns or
the average word length or hand-crafted word sentiment scores are going
to be what your model needs. You can avoid the temptation to use readability
scores, or sentiment analyzers to reduce the dimensionality of your data.
You don t even have to squash your vectors with blind (unsupervised)
dimension reduction approaches such as stop word filtering, stemming,
lemmatizing, LDA, PCA, TSNE, or clustering. A neural network mini-brain can
do this for you, and it will do it optimally, based on the statistics of the
relationship between words and your target.
tip
Don t use stemmers, lemmatizers or other keyword-based preprocessing in
main approaches:
1. Continuous bag-of-words (CBOW)
2. Continuous skip-gram
The continuous bag-of-words (CBOW) approach predicts the target word (the
output or "target" word) from the nearby context words (input words). The
only difference with the bag-of-words (BOW) vectors you learned about in
chapter 3 is that a CBOWs are created for a continuously sliding window of
words within each document. So you will have almost as many CBOW
vectors as you have words in the sequence of words from all of your
documents. Whereas for the BOW vectors you only had one vector for each
document. This gives your word embedding training set a lot more
information to work with so it will produce more accurate embedding
vectors. With the CBOW approach, you create a huge number of tiny
synthetic
Natural Language Processing in Action, Second
Edition
1. Welcome
2. 1_Machines_that_read_and_write_(NLP_overview)
3. 2_Tokens_of_thought