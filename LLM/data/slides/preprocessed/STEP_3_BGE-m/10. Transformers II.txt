## Natural Language Processing and Large Language Models

**Corso di Laurea Magistrale in Ingegneria Informatica**

**Lesson 10: Transformers II**

Nicola Capuano and Antonio Greco

DIEM – University of Salerno


**Abstract:** This lesson delves into the inner workings of the Transformer architecture, expanding on the concepts introduced in the previous lesson. We will explore the Multi-Head Attention mechanism, the Encoder and Decoder structures, Masked Multi-Head Attention, Encoder-Decoder Attention, and the overall pipeline of the Transformer. This understanding is crucial for comprehending how Transformers process sequential data and achieve state-of-the-art results in various NLP tasks.

`<----------section---------->`

### Outline

* Multi-Head Attention
* Encoder Output
* Decoder
* Masked Multi-Head Attention
* Encoder-Decoder Attention
* Output
* Transformer’s Pipeline

`<----------section---------->`

### Multi-head Attention

Self-attention allows a model to consider the relationships between different words in a sequence when processing each word. However, a single attention mechanism might not capture all the nuances of these relationships. Multi-head attention addresses this limitation by employing multiple "heads," each with its own set of learned weight matrices.  This allows the model to attend to different aspects of the input sequence simultaneously.

Each head performs a scaled dot-product attention operation. The results from these parallel computations are then concatenated and transformed through another linear layer. This final transformation combines the various perspectives offered by the different heads into a unified representation.  Multi-head attention allows the model to capture a richer understanding of the relationships within the sequence compared to a single-head approach.

Just like in single-head attention, Multi-Head Attention incorporates Add & Norm (skip connections and layer normalization) and Feed Forward layers. The Add & Norm component normalizes the output, stabilizing training and providing a regularization effect, while the residual connections (Add) help mitigate vanishing gradients, which is crucial for training deep networks. The Feed Forward layers introduce non-linear transformations, enabling the model to learn complex, non-linear relationships between words in the sequence.


`<----------section---------->`

### Transformer’s Encoder

The Transformer’s Encoder processes the input sequence to generate a contextualized representation for each word.  Crucially, because self-attention mechanisms don't inherently account for word order, the encoder incorporates positional encoding. This is typically achieved by adding sinusoidal functions to the input embeddings, providing information about the position of each word in the sequence.

Each encoder block consists of a multi-head self-attention layer followed by the Add & Norm and Feed Forward layers as described previously. This architecture allows the encoder to process each word in the context of all other words in the input sequence, including itself.

The encoder's design, where each block maintains the same output dimensionality as the input, allows for stacking multiple encoder blocks. The output of one block becomes the input to the next, enabling the network to learn hierarchical representations of the input sequence.


`<----------section---------->`

### Decoder

The Decoder generates the output sequence one element at a time, using the contextualized representation generated by the encoder.  At each step, the decoder considers the encoder's output and the previously generated words in the output sequence.

The decoder block structure mirrors the encoder block, with the addition of a second multi-head attention mechanism, called encoder-decoder attention. This mechanism allows the decoder to attend to the relevant parts of the encoder's output when generating each word in the output sequence.

The original Transformer architecture used 6 decoder blocks. A key difference from the encoder's self-attention is the "masked" self-attention within the decoder. This mask prevents the decoder from attending to future positions in the output sequence during training, ensuring that predictions are made only based on the information available up to the current time step.

The final layer of the decoder is a linear layer followed by a softmax function.  This projects the decoder's output to the vocabulary size, producing a probability distribution over all possible words for the next position in the output sequence.


`<----------section---------->`

### Masked Multi-Head Attention

Masked Multi-Head Attention is crucial for training the decoder. During the generation of the *i*-th output word, the decoder should only consider the preceding words (positions 1 to *i-1*) and not "peek" into the future.  This is achieved by applying a mask to the attention scores, effectively setting the scores corresponding to future positions to negative infinity. This forces the softmax function to assign zero probability to these future positions.


`<----------section---------->`

### Encoder-Decoder Attention

Encoder-Decoder Attention bridges the encoder and decoder, enabling the decoder to leverage the contextualized information encoded by the encoder. In this mechanism, the queries come from the decoder, while the keys and values are derived from the encoder's output.  This allows the decoder to focus on the relevant parts of the input sequence when generating each word in the output sequence.


`<----------section---------->`

### Output

The final decoder output for each time step is a vector representing the generated word. This vector is fed through a linear layer, which is often tied (shared weights) with the input embedding matrix, and a softmax function to compute the probability distribution over the output vocabulary.  The word with the highest probability is then selected as the output for that time step.


`<----------section---------->`

### Transformer’s Pipeline

The Transformer processes sequential data through a distinct pipeline. First, the encoder processes the entire input sequence, generating a set of contextualized representations.  Next, the decoder generates the output sequence one element at a time.  At each step, the decoder receives the encoder's output and the previously generated words.  This process repeats until an end-of-sequence token is generated, signaling the completion of the output sequence.

The linked resource ([https://poloclub.github.io/transformer-explainer/](https://poloclub.github.io/transformer-explainer/)) provides an interactive visualization of the Transformer architecture, which can further aid understanding.

`<----------section---------->`

### Additional Context and Insights (Integrating Provided Context)


The Transformer architecture offers several advantages over traditional recurrent and convolutional models for sequence-to-sequence tasks. Key innovations include:

* **Self-Attention:**  This mechanism allows the model to weigh the importance of different parts of the input sequence when processing each word, capturing long-range dependencies more effectively than recurrent networks, which struggle with vanishing gradients over long sequences.

* **Scalability:** The lack of recurrence and the parallelizable nature of attention computations allow Transformers to be scaled to much larger datasets and model sizes than RNNs or CNNs. This scalability has been instrumental in the development of large language models.

* **Stackability:** The consistent input and output dimensions of Transformer layers facilitate stacking multiple layers, enabling the model to learn increasingly complex representations of the input data.

* **Byte Pair Encoding (BPE):** While not part of the core Transformer architecture, BPE plays a critical role in handling large vocabularies efficiently by representing words as subword units.  This helps reduce the vocabulary size and addresses the out-of-vocabulary problem.

* **Positional Encodings:**  These encodings compensate for the lack of inherent positional information in self-attention, allowing the model to account for word order in the input sequence.

These innovations, combined with the ability to train on massive datasets, have led to significant performance improvements in various NLP tasks, including machine translation, text summarization, question answering, and text generation. The Transformer’s ability to capture long-range dependencies and be trained efficiently has been particularly crucial for the development of Large Language Models (LLMs) which demonstrate impressive abilities in understanding and generating human-like text.
