
token.
The second trick that helped Transformers outperform RNNs is the use of
self-attention. Self-attention is a mechanism that allows a transformer to
focus on different parts of the input sequence when encoding or decoding a
token. This is a much simpler approach than the complicated math (and
computational complexity) of CNNs and RNNs. The attention mechanism
removes the recurrence of the encoder and decoder networks. So a
transformer has neither the vanishing gradients nor the exploding gradients
problem of an RNN.
The third trick that helped Transformers outperform RNNs is the use of
positional encoding. Positional encoding is a way to add information about
the position of a token in the input sequence to the transformer. This is
important because the transformer does not have any inherent sense of
position. Without positional encoding, the transformer would not be able to
distinguish between the sentence The cat is on the mat and The mat is on the
cat.
The fourth trick that helped Transformers outperform RNNs is the use of
multi-head attention. Multi-head attention allows a transformer to focus on
multiple parts of the input sequence simultaneously. This is a much more
powerful approach than the single-head attention used in previous
transformer models.
The fifth trick that helped Transformers outperform RNNs is the use of
residual connections. Residual connections allow a transformer to learn
features at multiple levels of abstraction. This is a much more powerful
approach than the simple feedforward networks used in previous transformer
models.
The sixth trick that helped Transformers outperform RNNs is the use of
layer normalization. Layer normalization allows a transformer to learn
features at multiple levels of abstraction. This is a much more powerful
approach than the simple feedforward networks used in previous transformer
models.
The seventh trick that helped Transformers outperform RNNs is the use of
dropout. Dropout is a technique that randomly drops out some of the
neurons during training to prevent overfitting. This is a much more powerful
approach than the simple feedforward networks used in previous transformer
models.
The eighth trick that helped Transformers outperform RNNs