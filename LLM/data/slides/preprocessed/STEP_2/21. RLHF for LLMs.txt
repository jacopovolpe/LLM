## Natural Language Processing and Large Language Models

**Corso di Laurea Magistrale in Ingegneria Informatica**

**Lesson 21: Reinforcement Learning from Human Feedback**

**Nicola Capuano and Antonio Greco**

**DIEM – University of Salerno**


### Outline

* Reinforcement Learning from Human Feedback (RLHF)
* Transformers trl library
* Try it yourself


### Reinforcement Learning from Human Feedback (RLHF)

**What is RLHF?**

RLHF is a technique to improve large language models (LLMs) using human feedback as guidance. It's a strategy to balance model performance with alignment to human values and preferences.

**Why RLHF?**

RLHF may be a strategy to ground the focus of the LLM. It can enhance safety, ethical responses, and user satisfaction.

**Workflow of RLHF**

(See included image of workflow diagram)

**Key components of RLHF**

* **Pre-trained Language Model:** A base LLM trained on large corpora (e.g., BERT, GPT, T5).
* **Reward Model:** A secondary model that scores LLM outputs based on human feedback.
* **Fine-Tuning with Reinforcement Learning:** Optimization of the LLM using reinforcement learning guided by the reward model.

**Reward model**

The inputs for training a reward model are:

* Multiple LLM-generated outputs for given prompts
* Corresponding human rank responses according to their preferences

The goal is to train a model to predict human preference scores. The methodology uses a ranking loss function to teach the reward model which outputs humans prefer.


**Fine-tuning with Proximal Policy Optimization (PPO)**

The goal is to align the LLM’s outputs with human-defined quality metrics.  The process involves:

1. Generating responses using the LLM.
2. Scoring responses with the reward model.
3. Updating the LLM to maximize reward scores.

(See included image of PPO diagram)

**Pros and Cons of RLHF**

**Pros:**

* **Iterative Improvement:**  Possibility to collect human feedback as the model evolves and update the reward model and fine-tune iteratively.
* **Improved Alignment:** Generates responses closer to human intent.
* **Ethical Responses:** Reduces harmful or biased outputs.
* **User-Centric Behavior:** Tailors interactions to user preferences.

**Cons:**

* **Subjectivity:** Human feedback may vary widely.
* **Scalability:** Collecting sufficient, high-quality feedback is resource-intensive.
* **Reward Model Robustness:** Misaligned reward models can lead to suboptimal fine-tuning.


**Tasks to enhance with RLHF**

* **Text Generation:** Enhance the quality of text produced by LLMs.
* **Dialogue Systems:** Enhance the performance of dialogue systems.
* **Language Translation:** Increase the precision of language translation.
* **Summarization:** Raise the standard of summaries produced by LLMs.
* **Question Answering:** Increase the accuracy of question answering.
* **Sentiment Analysis:** Increase the accuracy of sentiment identification for particular domains or businesses.
* **Computer Programming:** Speed up and improve software development.


**Case study: GPT-3.5 and GPT-4**

The pre-trained models have been fine-tuned using RLHF. OpenAI declares that RLHF achieved:

* Enhanced alignment
* Fewer unsafe outputs
* More human-like interactions.

These models were or are widely used in real-world applications like ChatGPT.  The models are still incrementally improved with additional human feedback.


### Transformers trl library

**TRL (Transformer Reinforcement Learning)**

TRL is a full-stack library that provides tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM), to the Proximal Policy Optimization (PPO) step. The library is integrated with HuggingFace transformers.

(See included image of TRL steps diagram)


### Try it yourself

Study the trl library on HuggingFace: [https://huggingface.co/docs/trl/v0.7.8/index](https://huggingface.co/docs/trl/v0.7.8/index)

Give a careful look to:

* PPOTrainer: [https://huggingface.co/docs/trl/v0.7.8/ppo_trainer](https://huggingface.co/docs/trl/v0.7.8/ppo_trainer)
* RewardTrainer: [https://huggingface.co/docs/trl/v0.7.8/reward_trainer](https://huggingface.co/docs/trl/v0.7.8/reward_trainer)

Study the examples that are closer to your purposes:

* Sentiment analysis tuning: [https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning](https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning)
* Detoxifying a Large Language Model with PPO: [https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm](https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm)

Try to apply RLHF to your project.
