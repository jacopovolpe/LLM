**Natural Language Processing and Large Language Models**

*Corso di Laurea Magistrale in Ingegneria Informatica*
Lesson 0 - Course Introduction
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This introductory lesson provides an overview of the Natural Language Processing (NLP) and Large Language Models (LLM) course for the Master's Degree in Computer Engineering at the University of Salerno. The course is presented by Nicola Capuano and Antonio Greco from the Department of Information Engineering and Mathematics (DIEM). The goal is to introduce students to the core concepts, techniques, and applications of NLP, focusing particularly on the use of LLMs based on the Transformer architecture.

<----------section---------->

**Objectives**

The course aims to equip students with both theoretical knowledge and practical abilities in the field of NLP. The objectives are categorized into Knowledge and Abilities, ensuring a well-rounded learning experience.

**Knowledge:**

*   **Basic concepts of Natural Language Processing (NLP):**  This includes understanding the fundamental principles of how computers can process and understand human language. NLP involves tasks such as parsing, semantic analysis, and understanding context. It also covers the challenges involved in natural language such as ambiguity, context dependence, and variability.
*   **Natural Language Understanding and Generation:**  These are the two core components of NLP. Natural Language Understanding (NLU) involves enabling machines to comprehend the meaning of text or speech. Natural Language Generation (NLG) focuses on creating human-readable text from structured data. Some tasks for NLU include semantic search, text alignment, paraphrase recognition, intent classification, and authorship attribution. Some common NLG tasks are synonym substitution, frequently-asked question answering (information retrieval), extractive generation of question answers (reading comprehension tests), spelling and grammar correction, and casual conversation.
*   **Statistical Approaches to NLP:**  This covers techniques such as Hidden Markov Models (HMMs), Conditional Random Fields (CRFs), and other statistical methods used for tasks like part-of-speech tagging, named entity recognition, and machine translation. These approaches use statistical models to infer patterns and relationships within text data.
*   **Large Language Models (LLM) based on Transformers:** LLMs are deep learning models with a huge number of parameters, which allows them to perform a wide array of NLP tasks at a high level. The Transformer architecture, which uses self-attention mechanisms, has revolutionized NLP by enabling models to capture long-range dependencies in text.
*   **NLP applications with LLM:**  This includes exploring the practical applications of LLMs in various domains, such as chatbots, machine translation, text summarization, sentiment analysis, and content generation. Focus is given to how LLMs enhance these applications and their impact on real-world problems. LLM applications include question-answering systems, virtual assistants, medical diagnosis based on symptom descriptions, financial forecasting engines, content creation, and generating code.
*   **Prompt Engineering and Fine Tuning of LLM:**  Prompt Engineering involves designing effective input prompts that guide LLMs to generate desired outputs. Fine-tuning involves adapting pre-trained LLMs to specific tasks or domains by training them on task-specific datasets.

**Abilities:**

*   **Design and implementation of a NLP system based on LLMs, integrating existing technologies and tools:**  Students will gain the ability to design, develop, and deploy NLP systems that leverage LLMs. This includes integrating various tools and technologies, such as Python libraries, APIs, and cloud services, to build end-to-end NLP solutions.

<----------section---------->

**Fundamentals of NLP**

This section delves into the basic concepts and techniques that form the foundation of NLP.

*   **Basic concepts, Evolution and Applications of NLP:**  This covers the history and evolution of NLP, key milestones, and a broad range of applications, from early rule-based systems to modern deep learning approaches. NLP is concerned with processing natural languages such as English or Mandarin.
*   **Representing text: Tokenization, Stemming, Lemmatization, POS tagging:**  These are fundamental steps in preparing text data for NLP tasks.
    *   **Tokenization** involves breaking down text into individual words or units (tokens).
    *   **Stemming** reduces words to their root form by removing suffixes (e.g., "running" becomes "run").
    *   **Lemmatization** converts words to their base or dictionary form (lemma) considering the context (e.g., "better" becomes "good").
    *   **POS (Part-of-Speech) tagging** identifies the grammatical role of each word in a sentence (e.g., noun, verb, adjective). POS tags are generated automatically by the default SpaCY pipeline.
*   **Math with Words: Bag of Words, Vector Space Model, TF-IDF, Search Engines:** These techniques involve representing text as numerical data that computers can process.
    *   **Bag of Words (BoW)** creates a vector representing the frequency of each word in a document, disregarding grammar and word order.
    *   **Vector Space Model (VSM)** represents documents as vectors in a high-dimensional space, where each dimension corresponds to a term.
    *   **TF-IDF (Term Frequency-Inverse Document Frequency)** assigns weights to words based on their frequency in a document and their rarity across the entire corpus. TF-IDF is a normalization of word counts that improves information retrieval results
    *   **Search Engines** - TF-IDF is an important calculation for search engines. They can consolidate or summarize search results.
*   **Text Classification: Topic Labelling, Sentiment Analysis:**  This involves categorizing text into predefined classes.
    *   **Topic Labelling** assigns relevant topics or categories to documents based on their content.
    *   **Sentiment Analysis** determines the emotional tone or attitude expressed in a text (e.g., positive, negative, neutral).
*   **Word Embeddings: Word2Vec, CBOW, Skip-Gram, GloVe, FastText:** Word embeddings are dense vector representations of words that capture semantic relationships.
    *   **Word2Vec** is a group of related models that are used to produce a word embedding.
    *   **CBOW (Continuous Bag of Words)** predicts a target word based on its surrounding context words.
    *   **Skip-Gram** predicts surrounding context words based on a target word.
    *   **GloVe (Global Vectors for Word Representation)** combines global matrix factorization and local context window methods.
    *   **FastText** enhances word embeddings by considering subword information, making it effective for handling rare words and morphological variations.
*   **Neural Networks for NLP: RNN, LSTM, GRU, CNN, Introduction to Text Generation:** This section introduces the application of neural networks to NLP tasks.
    *   **RNN (Recurrent Neural Network)** processes sequential data by maintaining a hidden state that captures information about previous inputs.
    *   **LSTM (Long Short-Term Memory)** is a type of RNN that addresses the vanishing gradient problem, allowing it to capture long-range dependencies.
    *   **GRU (Gated Recurrent Unit)** is a simplified variant of LSTM with fewer parameters, making it computationally efficient.
    *   **CNN (Convolutional Neural Network)** applies convolutional filters to capture local patterns in text.
    *   **Introduction to Text Generation** - LSTMs, GRUs, and Transformers are common networks for text generation.
*   **Information Extraction: Parsing, Named Entity Recognition:**  This involves extracting structured information from unstructured text.
    *   **Parsing** involves analyzing the grammatical structure of sentences.
    *   **Named Entity Recognition (NER)** identifies and classifies named entities in text (e.g., people, organizations, locations).
*   **Question Answering and Dialog Engines (chatbots):** This covers the development of systems that can answer questions posed in natural language and engage in coherent conversations. Chatbots use natural language search to find a response to their conversation partner's message.

<----------section---------->

**Transformers**

This section focuses on the Transformer architecture, a key component of modern LLMs.

*   **Self-Attention, Multi-Head Attention, Positional Encoding, Masking:**  These are the core mechanisms that enable Transformers to process text effectively.
    *   **Self-Attention** allows the model to weigh the importance of different words in a sentence when processing each word. The attention mechanism removes the recurrence of the encoder and decoder networks.
    *   **Multi-Head Attention** allows the model to attend to different aspects of the input sequence in parallel.
    *   **Positional Encoding** adds information about the position of words in the sequence, as Transformers lack inherent sequential awareness.
    *   **Masking** prevents the model from attending to certain parts of the input sequence, such as future tokens during training.
*   **Encoder and Decoder of a Transformer:**  Transformers typically consist of an encoder that processes the input sequence and a decoder that generates the output sequence. The entire transformer network, both the encoder and the decoder, must be run to predict each token so that token can be used to help it predict the next one.
*   **Introduction to HuggingFace:**  HuggingFace is a popular open-source library that provides pre-trained Transformer models and tools for NLP tasks, simplifying the development and deployment of NLP applications. Huggingface offers a lot of options in order to train the model as a part of Trainer class.
*   **Encoder-Decoder or Seq2Seq models (translation and summarization):**  These models use both an encoder and a decoder to map an input sequence to an output sequence, making them suitable for tasks like machine translation and text summarization.
*   **Encoder-only Models (sentence classification and named entity recognition):**  These models use only the encoder part of the Transformer to process the input sequence and make predictions, suitable for tasks like sentence classification and named entity recognition. An example would be models like BERT.
*   **Decoder-only Models (text generation):**  These models use only the decoder part of the Transformer to generate text, suitable for tasks like text generation. An example would be models like GPT.
*   **Definition and training of a Large Language Model:**  This covers the process of defining the architecture, training objectives, and training data for LLMs, as well as the computational resources required for training these models. Training an LLM involves providing it with greater and greater amounts of data in the domain you want to apply it to.

<----------section---------->

**Prompt Engineering**

This section focuses on the art of crafting effective prompts to guide LLMs.

*   **Zero-shot and Few-shot Prompting:**  These are techniques for leveraging LLMs without or with minimal task-specific training data.
    *   **Zero-shot Prompting** involves providing the LLM with a prompt that directly asks for the desired output without any examples.
    *   **Few-shot Prompting** involves providing the LLM with a few examples of input-output pairs to guide its generation.
*   **Chain-of-Thought, Self-Consistency, Prompt Chaining:**  These are advanced prompting strategies for improving the reasoning and accuracy of LLMs.
    *   **Chain-of-Thought** encourages the model to explicitly generate the intermediate reasoning steps before providing the final answer.
    *   **Self-Consistency** involves generating multiple responses from the model and selecting the most consistent one.
    *   **Prompt Chaining** involves breaking down a complex task into a series of simpler prompts that are executed sequentially.
*   **Role Prompting, Structured Prompts, System Prompts:** These are techniques to enhance the quality and relevance of LLM outputs.
    *   **Role Prompting** - assign a specific role or persona to the LLM to guide its responses.
    *   **Structured Prompts** - involve organizing the prompt in a specific format to elicit desired responses.
    *   **System Prompts** - configure the LLM's behavior by providing instructions at the beginning of the interaction.
*   **Retrieval Augmented Generation:** This approach enhances the quality of generated text by retrieving relevant information from a knowledge base or external source and incorporating it into the prompt.

<----------section---------->

**LLM Fine Tuning**

This section explores techniques for adapting pre-trained LLMs to specific tasks or domains.

*   **Feature-Based Fine Tuning:**  This involves freezing the pre-trained weights of the LLM and training a separate layer or module on the task-specific data.
*   **Parameter Efficient Fine Tuning and Low Rank Adaptation:**  These techniques aim to reduce the computational cost and memory requirements of fine-tuning LLMs by only updating a small subset of the model's parameters.
*   **Reinforcement Learning with Human Feedback:**  This involves training the LLM using reinforcement learning techniques, where human feedback is used to guide the model towards generating desired outputs.

<----------section---------->

**Textbook**

H. Lane, C. Howard, H. M. Hapke. *Natural Language Processing IN ACTION: Understanding, analyzing, and generating text with Python*. Manning, 2019.

Second Edition in fall 2024. Early Access version available online: [https://www.manning.com/books/natural-language-processing-in-action-second-edition](https://www.manning.com/books/natural-language-processing-in-action-second-edition)

The primary textbook for this course is "Natural Language Processing in Action" by Hobson Lane, Cole Howard, and Hannes Max Hapke, published by Manning. The first edition was released in 2019, but the second edition is expected in the fall of 2024. An Early Access version of the second edition is available online for students who wish to get a head start. The book focuses on understanding, analyzing, and generating text with Python.

<----------section---------->

**Further Info**

**Teachers**

*   **Nicola Capuano**
    DIEM, FSTEC-05P02007
    ncapuano@unisa.it
    089 964292
*   **Antonio Greco**
    DIEM, FSTEC-05P01036
    agreco@unisa.it
    089 963003

The instructors for the course are Nicola Capuano and Antonio Greco, both faculty members at DIEM, University of Salerno. Contact information, including email addresses and office phone numbers, is provided for each instructor.

**Online Material**

*   [https://elearning.unisa.it/](https://elearning.unisa.it/)

Additional course materials, announcements, and resources will be available on the University of Salerno's e-learning platform.

**Exam**

*   Realization of a project work
*   Oral exam (including the discussion of the project work)

The assessment for this course will consist of a project work and an oral examination. The oral exam will include a discussion of the project work, allowing students to demonstrate their understanding and application of the course material.

<----------section---------->

Natural Language Processing and Large Language Models. Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering). Lesson 9: Transformers I. Presented by Nicola Capuano and Antonio Greco. DIEM – University of Salerno.

**Outline**

This lesson will cover the following topics:

*   Limitations of Recurrent Neural Networks (RNNs)
*   Introduction to the Transformer model
*   Transformer Input mechanisms
*   Self-Attention mechanism

<----------section---------->

**Limitations of RNNs**

Recurrent Neural Networks (RNNs), while effective for sequential data processing, suffer from several limitations:

*   RNNs have limited long-term memory capabilities, especially in encoder-decoder models.
*   RNNs are slow to train, particularly for long sequences.
*   RNNs are prone to the vanishing gradient problem.

<----------section---------->

**RNNs Lack of Long-Term Memory**

Traditional RNNs, and especially encoder-decoder architectures, struggle to retain information over long sequences. The ability of an RNN to "remember" information from earlier time steps diminishes as the sequence length increases. This is because the information has to be passed through several layers, which can lead to loss of information.

<----------section---------->

**RNNs are Extremely Slow to Train**

*   Processing in RNNs is inherently sequential.
*   The network processes element $x_i$ only after processing $x_{i-1}$.
*   This sequential nature prevents exploiting parallelism available in modern GPUs.

The inherent sequential processing of RNNs is a bottleneck. The computation for the next element in the sequence cannot begin until the previous element has been processed. This contrasts sharply with models that can process the entire sequence in parallel, enabling significantly faster training times, especially on GPUs.

<----------section---------->

**RNNs Suffer from the Vanishing Gradient Problem**

*   The vanishing/exploding gradient problem is a major challenge in training deep neural networks, including RNNs.
*   During Backpropagation Through Time (BPTT), the same function *F* is repeatedly traversed.
*   The gradient calculation involves repeated multiplication of derivatives:

    $\frac{\partial Loss}{\partial h_0} = \frac{\partial Loss}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_3} \cdot \frac{\partial F}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_4} \cdot \frac{\partial F}{\partial h_3} \cdot \frac{\partial F}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = ...$

The repeated multiplication of the same derivatives during backpropagation can lead to two problems:

*   **Vanishing Gradient:** If the absolute value of the derivatives of *F* is small (less than 1), the product becomes progressively smaller, diminishing the gradient signal and hindering effective learning in earlier layers.
*   **Exploding Gradient:** If the absolute value of the derivatives of *F* is large (greater than 1), the product grows exponentially, causing instability in the learning process.

The core issue is the repeated traversal of the same layer (sequence length times) during backpropagation.

<----------section---------->

**Transformer**

In 2017, researchers at Google Brain introduced the Transformer model as an alternative to RNNs for processing sequential data. A key advantage of the Transformer is its ability to process sequence elements in parallel. The number of layers traversed during computation doesn't depend on the sequence length, thereby mitigating gradient-related issues. Originally designed for language translation tasks (sequence-to-sequence with varying lengths), hence the name "Transformer." Subsets of the Transformer architecture can be adapted for other sequence processing tasks.

<----------section---------->

**Transformer**

The Transformer architecture consists of the following key components:

*   Input
    *   Tokenization
    *   Input Embedding
    *   Positional Encoding
*   Encoder
    *   Attention Mechanisms
        *   Query
        *   Key
        *   Value
    *   Self-Attention
    *   Multi-Head Attention
    *   Add & Norm (Residual Connections and Layer Normalization)
    *   Feedforward Network
*   Decoder
    *   Masked Attention (to prevent peeking ahead during training)
    *   Encoder-Decoder Attention (to attend to the encoder's output)
*   Output

<----------section---------->

**Input**

The input to a Transformer model consists of a sequence of tokens, which are processed through tokenization, embedding, and positional encoding layers.

<----------section---------->

**Tokenization**

*   Tokenization involves representing text as a sequence of tokens.
*   Each token is assigned a unique ID.

Tokenization is the process of breaking down raw text into smaller units (tokens) suitable for processing by the model. These tokens can be words, sub-words, or characters. Each token is then mapped to a unique integer ID, allowing the model to operate on numerical data.

<----------section---------->

**Input Embedding**

*   Embedding: A representation of a symbol (word, character, sentence) in a distributed low-dimensional space of continuous-valued vectors.
*   Tokens are projected into a continuous Euclidean space.
*   Correlations among words are visualized in the embedding space, where semantically similar words are placed closer together.
*   Ideally, an embedding captures the semantics of the input, reflecting the relationships between words.

Input embedding transforms discrete tokens into continuous vector representations. This is crucial because neural networks perform better with continuous inputs. The embedding space aims to capture the semantic meaning of words, so that words with similar meanings are located close to each other in the embedding space. This allows the model to leverage semantic relationships when processing text.

<----------section---------->

**Positional Encoding**

**The Importance of Order**

*   Question: Can the encoding methods discussed so far differentiate between sequences that only differ in the element order?
*   Example: Can the model distinguish between "The student is eating an apple" and "An apple is eating the student"?

<----------section---------->

**The Importance of Order**

*   Question: With the encoding we have seen so far, is it possible to discriminate between sequences that only differ in the order of the elements?
*   E.g., is it possible to differentiate between "The student is eating an apple" and "An apple is eating the student"?
*   Answer: No, because the output of the attention module does not depend on the order of its keys/values pairs.
*   Solution: Add information about the order of sequence elements.

Without positional encoding, the Transformer treats all tokens as unordered sets, which is undesirable because word order is fundamental to meaning. For example, "dog bites man" has a very different meaning than "man bites dog." Therefore, the Transformer needs a mechanism to encode the position of each word in the sequence.

<----------section---------->

**Positional Encoding**

*   The Transformer adds a slight perturbation to each sequence element based on its position.
*   The same element appearing in different positions is encoded using slightly different vectors.

Positional encoding provides information about the position of each token within the sequence. It injects information about the token order in the sentence by adding a position-dependent vector to the token embedding.

<----------section---------->

**Positional Encoding**

*   Positional encoding is represented by a set of periodic functions.
*   Given $d_{model}$ (the embedding size) and $pos$ (the element's position), the perturbation to component $i$ of the embedding vector is:

    $PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})$
*   Positional encoding is a vector with the same dimension as the input embedding and can be directly added to the input embedding.

The positional encoding function uses sine and cosine functions of different frequencies to create a unique positional vector for each token. The formula ensures that each position has a unique encoding and that the model can easily learn relative positions by attending to linear combinations of these encodings. The resulting positional encoding vector is then added to the input embedding vector, effectively injecting positional information into the token representation.

<----------section---------->

**Encoder**

The Encoder transforms an input sequence of vectors $x_1, \dots, x_t$ into an intermediate representation of the same length $z_1, \dots, z_t$. The vectors $z_1, \dots, z_t$ are generated in parallel. Each vector $z_i$ depends on the entire input sequence $x_1, \dots, x_t$, not just the corresponding $x_i$.

The encoder's role is to process the input sequence and create a context-aware representation of each token. Each token's representation incorporates information from all other tokens in the sequence, allowing the model to understand the relationships between words.

<----------section---------->

**Encoder**

The encoder comprises a sequence of identical encoder blocks. The original paper used six encoder blocks. Each encoder block processes the sequence using:

*   Self-attention: An attention module where the same vectors serve as Query (Q), Key (K), and Value (V).
*   A feed-forward layer applied independently to each sequence element.
*   Skip (Residual) connections
*   Normalization

Each encoder block refines the input representation through self-attention and a feed-forward network. Self-attention allows the model to weigh the importance of different tokens when processing each token. Skip connections and normalization help to stabilize training and improve model performance.

<----------section---------->

**Self Attention**

Consider the sentence: "The animal didn’t cross the street because it was too wide." What does "it" refer to? Estimating self-attention in this sentence means finding the words that one must consider first to find a better encoding for the word "it." Self-Attention estimate must be learned according to the task we are facing.

Self-attention is a crucial mechanism that allows the model to focus on the relevant parts of the input sequence when processing each token. It helps the model to resolve ambiguities, such as pronoun references, and to capture long-range dependencies between words.

<----------section---------->

**Self Attention**

How to compute the attention to give to each input element when encoding the current word?

The self-attention mechanism computes attention scores for each input element relative to the current word. These attention scores determine the weight given to each input element when encoding the current word.

<----------section---------->

**Attention**

To understand self-attention, we first need to introduce its fundamental building block: the attention function. Informally, an attention function is used when the value to be computed (in this case, the embedding of a token in a certain position considering the context of the sentence) depends on a set of other values (in this case, other tokens of the sentence). We want to give a different weight (i.e., a different "level of attention") to each of the values (how much each token is important to encode the current token?). The attention function depends on three elements: query, key, and value. The terminology is inherited from document retrieval.

The attention mechanism dynamically weighs the importance of different parts of the input sequence when processing each element. The weight assigned to each element is based on its relevance to the current element being processed.

<----------section---------->

**Attention**

We have an input value *q* and want to define some target function $f_T(q)$. *q* is called the query value in the attention terminology. In the general case, both *q* and $f_T(q)$ can be vectors. We want to express $f_T(q)$ as a function of a given set of elements $v_1, \dots, v_n$. We want the "attention" given to each $v_i$ to be different depending on *q*. We assume that for each $v_i$ we have available an additional information $k_i$ that can be used to decide the "attention" to be given to $v_i$. The elements $v_i$ are called values and the $k_i$ are called keys in the attention terminology; both the values and the keys can be vectors.

In the attention mechanism, the query (q) represents the element we're trying to encode. The keys (k) are associated with the values (v), and are used to determine the relevance of each value to the query. The attention weights are calculated based on the similarity between the query and each key. The target function $f_T(q)$ is computed as a weighted sum of the values, where the weights are the attention scores.

<----------section---------->

**Attention**

A commonly adopted formulation of the problem is to define the target function as:

$f_T(q) = \alpha(q, k_1) \cdot f_V(v_1) + \dots + \alpha(q, k_n) \cdot f_V(v_n) = \sum_{i=1}^{n} \alpha(q, k_i) \cdot f_V(v_i)$

Where $\alpha$ is the attention given to value $v_i$.

The target function $f_T(q)$ is calculated as a weighted sum of the values $v_i$. The weights $\alpha(q, k_i)$ represent the attention given to each value, determined by the similarity between the query *q* and the corresponding key $k_i$. The function $f_V$ is applied to each value before weighting.

<----------section---------->

**Attention**

$f_T(q) = \sum_{i=1}^{n} \alpha(q, k_i) \cdot f_V(v_i)$

*   $\alpha$ is our attention function
*   We want $\alpha$ and $f_V$ to be learned by our system
*   Typically, $\alpha(q, k_i) \in [0, 1]$ and $\sum_{i} \alpha(q, k_i) = 1$
*   Note: the value of the target function does not depend on the order of the key-value pairs $(k_i, v_i)$

The attention function $\alpha$ and the value transformation function $f_V$ are learned during training. The attention weights are normalized (typically using a softmax function) to sum to 1, ensuring they represent probabilities. The target function's value is invariant to the order of the key-value pairs, highlighting the importance of the learned attention weights in capturing relevant relationships.

<----------section---------->

**Self Attention**

The Transformer architecture uses a specific definition of the attention function, based on linear vector/matrix operations and the softmax function. This definition is:

*   Differentiable, so it can be learned using Back Propagation
*   Efficient to compute
*   Easy to parallelize, since the attention for several query vectors can be efficiently computed in parallel at the same time

The self-attention mechanism in the Transformer is designed to be differentiable, computationally efficient, and highly parallelizable. These properties make it well-suited for training large models on GPUs.

<----------section---------->

**Self Attention**

*   Input: three matrices Q, K, V
*   Q ($m \times d_q$) contains the query vectors (each row is a query)
*   K ($n \times d_k$) contains the key vectors (each row is a key)
*   V ($n \times d_v$) contains the value vectors (each row is a value)
*   K and V must have the same number of rows

The self-attention mechanism takes three input matrices: Query (Q), Key (K), and Value (V). The query matrix contains the vectors for which we want to compute the attention. The key matrix contains the vectors used to determine the attention weights. The value matrix contains the vectors that are weighted and summed to produce the output.

<----------section---------->

**Self Attention**

If the query and the key are represented by vectors with the same dimensionality, a matching score can be provided by the scaled dot product of the two vectors (cosine similarity).

The dot product between the query and key vectors provides a measure of similarity, which is then used to compute the attention scores. Scaling is applied to prevent excessively large values, which can lead to saturation of the softmax function.

<----------section---------->

**Self Attention**

Step 0: Each element in the sequence is represented by a numerical vector.

This initial step involves transforming each token into a numerical vector representation, which is achieved through the embedding layer.

<----------section---------->

**Self Attention**

Step 1: the input matrices are "projected" onto a different subspace, by multiplying them (using row-by-column dot product) by weight matrices:

*   $Q' = Q \cdot W^Q$
*   $K' = K \cdot W^K$
*   $V' = V \cdot W^V$

These are the trainable weights:

*   $W^Q (d_q \times d'_q)$
*   $W^K (d_k \times d'_k)$
*   $W^V (d_v \times d'_v)$

Note: $W^Q$ and $W^K$ must have the same number of columns.

The input matrices Q, K, and V are linearly transformed into different subspaces using weight matrices $W^Q$, $W^K$, and $W^V$, respectively. These weight matrices are learned during training, allowing the model to project the input into representations that are more suitable for computing attention.

<----------section---------->

**Self Attention**

Step 1: Compute a key (K), a value (V) and a query (Q) as linear function of each element in the sequence.

The query, key, and value vectors are computed as linear transformations of the input embeddings. These vectors capture different aspects of the input and are used in the attention calculation.

<----------section---------->

**Self Attention**

Step 2: the attention matrix A is computed for each position by multiplying Q' and the transpose of K', scaling by $1 / \sqrt{d'_k}$ and applying softmax to each of the resulting rows:

$A = softmax(\frac{Q' \cdot K'^T}{\sqrt{d'_k}})$

A is a ($m \times n$) matrix whose element $a_{ij} = \alpha(q_i, k_j)$. Softmax is applied to each row separately. This scaling is used to avoid that the argument of softmax becomes too large with the increase of the dimension $d'_k$.

The attention matrix A is computed by taking the dot product of the transformed query matrix Q' and the transpose of the transformed key matrix K'. The result is scaled by the square root of the key dimension to prevent the dot products from becoming too large, which can lead to saturation of the softmax function. The softmax function is then applied to each row of the scaled dot product to obtain the attention weights, which sum to 1 across each row.

<----------section---------->

**Self Attention**

Step 2: Compute attention score for each position i as a softmax of the scaled dot product of all the keys (bidirectional self-attention) with $Q_i$.

The attention scores for each position i are calculated using the softmax of the scaled dot product of all the keys with the query vector $Q_i$. This bidirectional self-attention allows the model to consider the context of all words in the sentence when encoding each word.

<----------section---------->

**Self Attention**

Final step: the target value is computed by row-by-column multiplication between A and V':

$f_T(Q) = A \cdot V'$

The result is a $m \times d'_v$ matrix representing the target function computed on the m queries in the input matrix Q.

The target value is calculated as a weighted sum of the transformed value vectors V', where the weights are the attention scores in matrix A. This produces a new representation for each query, which incorporates information from all the values, weighted by their attention scores.

<----------section---------->

**Self Attention**

Step 3: Output representation for each position I, as a weighted sum of values (each one multiplied by the related attention score)

The final output representation for each position is obtained by taking a weighted sum of the value vectors. The weights are determined by the attention scores, which reflect the importance of each value in encoding the current position.

<----------section---------->

**Self Attention**

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

$a_{ij} = softmax(\frac{q_i k_j}{\sqrt{d_k}}) = \frac{exp(q_i k_j)}{\sum_{s=1}^n exp(q_i k_s)}$

This is the final formula summarizing the self-attention mechanism. The attention weights are computed using a scaled dot product between the query and key matrices, followed by a softmax function. The resulting attention matrix is then used to weight the value matrix, producing the output. The formula $a_{ij}$ shows how each element of the attention matrix is calculated using the softmax function.

<----------section---------->
When we were writing the first edition of this book, Hannes and Cole (the first edition coauthors) were already focused on the attention mechanism. It’s now been 6 years and attention is still the most researched topic in deep learning. The attention mechanism enabled a leap forward in capability for problems where LSTMs struggled:
Conversation
 — Generate plausible responses to conversational
prompts, queries, or utterances.
Abstractive summarization or paraphrasing
:: Generate a new shorter
wording of a long text summarization of sentences, paragraphs, and
even several pages of text.
Open domain question answering
:: Answering a general question about
anything the transformer has ever read.
Reading comprehension question answering
:: Answering questions
about a short body of text (usually less than a page).
Encoding
:: A single vector or sequence of embedding vectors that
represent the meaning of body of text in a vector space — sometimes
called 
task-independent sentence embedding
.
Translation and code generation
 — Generating plausible software
expressions and programs based on plain English descriptions of the
program’s purpose.
Self-attention is the most straightforward and common way to implement
attention. It takes the input sequence of embedding vectors and puts them
through linear projections. A linear projection is merely a dot product or
matrix multiplication. This dot product creates key, value and query vectors.
The query vector is used along with the key vector to create a context vector
for the words' embedding vectors and their relation to the query. This context
vector is then used to get a weighted sum of values. In practice, all these
operations are done on sets of queries, keys, and values packed together in
matrices, 
Q
, 
K
, and 
V
, respectively.
There are two ways to implement the linear algebra of an attention algorithm:
additive attention
 or 
dot-product attention
. The one that was most effective in
transformers is a scaled version of dot-production attention. For dot-product
attention, the scalar products between the query vectors 
Q
 and the key vectors
K
, are scaled down based on how many dimensions there are in the model.
This makes the dot product more numerically stable for large dimensional
embeddings and longer text sequences. Here’s how you compute the self-
attention outputs for the query, key, and value matrices 
Q
, 
K
, and 
V
.
Equation 9.1 Self-attention outputs
\[Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V\]
The high dimensional dot products create small gradients in the softmax due
to the law of large numbers. To counteract this effect, the product of the
query and key matrices is scaled by \(\frac{1}{\sqrt{d_{k}}}\). The softmax
normalizes the resulting vectors so that they are all positive and sum to 1.
This "scoring" matrix is then multiplied with the values matrix to get the
weighted values matrix in figure 
9.1
.
[
12
]
 
[
13
]

<----------section---------->

**Self Attention and It's Linear Algebra**
Self-attention is the most straightforward and common way to implement
attention. It takes the input sequence of embedding vectors and puts them
through linear projections. A linear projection is merely a dot product or
matrix multiplication. This dot product creates key, value and query vectors.
The query vector is used along with the key vector to create a context vector
for the words' embedding vectors and their relation to the query. This context
vector is then used to get a weighted sum of values. In practice, all these
operations are done on sets of queries, keys, and values packed together in
matrices, Q, K, and V, respectively.
There are two ways to implement the linear algebra of an attention algorithm:
additive attention
 or 
dot-product attention
. The one that was most effective in
transformers is a scaled version of dot-production attention. For dot-product
attention, the scalar products between the query vectors 
Q
 and the key vectors
K
, are scaled down based on how many dimensions there are in the model.
This makes the dot product more numerically stable for large dimensional
embeddings and longer text sequences. Here’s how you compute the self-
attention outputs for the query, key, and value matrices 
Q
, 
K
, and 
V
.

Equation 9.1 Self-attention outputs
\[Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V\]
The high dimensional dot products create small gradients in the softmax due
to the law of large numbers. To counteract this effect, the product of the
query and key matrices is scaled by \(\frac{1}{\sqrt{d_{k}}}\). The softmax
normalizes the resulting vectors so that they are all positive and sum to 1.
This "scoring" matrix is then multiplied with the values matrix to get the
weighted values matrix in figure 
9.1
.

<----------section---------->

# Natural Language Processing and Large Language Models

**Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree in Computer Engineering) - Lesson 1: NLP Overview**

**Nicola Capuano and Antonio Greco**

**DIEM – University of Salerno**

This document provides an overview of Natural Language Processing (NLP) and Large Language Models (LLMs), serving as the content for the first lesson of a course in Computer Engineering at the University of Salerno. It is presented by Nicola Capuano and Antonio Greco from the DIEM (Department of Information Engineering and Mathematics).

<----------section---------->

## Outline

This lesson will cover the following key areas:

*   **What is Natural Language Processing (NLP):** An introduction to the field of NLP, its goals, and its significance in the broader context of Artificial Intelligence.
*   **Applications of NLP:** Exploration of the various real-world applications of NLP across different industries and domains.
*   **History of NLP:** A brief overview of the historical development of NLP, tracing its evolution from early rule-based systems to modern deep learning approaches.

<----------section---------->

## What is Natural Language Processing?

### NLP in the Press

NLP is rapidly gaining recognition and importance, as evidenced by its increasing coverage in mainstream media outlets. The following headlines illustrate the growing interest and, at times, apprehension surrounding NLP's capabilities:

*   "New powerful AI bot creates angst among users: Are robots ready to take our jobs?" - Reflects concerns about the potential displacement of human labor due to advancements in AI-driven automation.
*   "A Smarter Robot: A new chatbot shows rapid advances in artificial intelligence." - *The New York Times* - Highlights the progress being made in AI, particularly in the development of more sophisticated and human-like chatbots.
*   "What is ChatGPT, the viral social media AI?" - *The Washington Post* - Indicates the widespread public attention and curiosity surrounding specific NLP models like ChatGPT.
*   "This AI chatbot is dominating social media with its frighteningly good essays." - *CNN* - Emphasizes the impressive text generation capabilities of AI chatbots, capable of producing high-quality written content.
*   "ChatGPT may be coming for our jobs. Here are the 10 roles that AI is most likely to replace." - *Business Insider* - Underscores the potential impact of NLP on the job market, particularly for roles involving writing and communication.
*   "Microsoft co-founder Bill Gates: ChatGPT ‘will change our world’" - *Reuters* - Showcases the transformative potential of NLP, as recognized by influential figures in the technology industry.

<----------section---------->

### Importance of NLP

The significance of NLP is further highlighted by the following quotes from prominent figures in philosophy, technology, and academia:

*   "Natural language is the most important part of Artificial Intelligence" - *John Searle, Philosopher* - Emphasizes the critical role of natural language understanding and generation in achieving true artificial intelligence.
*   "Natural language processing is a cornerstone of artificial intelligence, allowing computers to read and understand human language, as well as to produce and recognize speech" - *Ginni Rometty, IBM CEO* - Highlights NLP as a foundational technology within AI, enabling computers to interact with and process human language in various forms.
*   "Natural language processing is one of the most important fields in artificial intelligence and also one of the most difficult" - *Dan Jurafsky, Professor of Linguistics and Computer Science at Stanford University* - Acknowledges the complexity and challenges inherent in NLP, despite its importance.

<----------section---------->

### Definitions

NLP can be defined in several ways, reflecting its interdisciplinary nature and diverse applications:

*   "Natural language processing is the set of methods for making human language accessible to computers" - *Jacob Eisenstein* - Focuses on the goal of bridging the gap between human language and computer understanding.
*   "Natural language processing is the field at the intersection of computer science and linguistics" - *Christopher Manning* - Highlights the convergence of computer science and linguistics in the pursuit of NLP.
*   "Make computers to understand natural language to do certain tasks humans can do such as translation, summarization, questions answering" - *Behrooz Mansouri* - Emphasizes the practical application of NLP in replicating human language-based tasks.
*   "Natural language processing is an area of research in computer science and artificial intelligence concerned with processing natural languages such as English or Mandarin. This processing generally involves translating natural language into data that a computer can use to learn about the world. And this understanding of the world is sometimes used to generate natural language text that reflects that understanding." - *(Natural Language Processing in Action)* - Provides a comprehensive definition, outlining the process of converting natural language into a machine-understandable format and using this understanding to generate new text.

<----------section---------->

### Natural Language Understanding (NLU)

NLU is a subfield of NLP that focuses on enabling machines to comprehend human language.

*   It involves extracting meaning, context, and intent from text, going beyond simple keyword recognition. This can be achieved by identifying named entities, resolving co-references, and inferring relationships between different parts of the text.
*   Text is transformed into a numerical representation called an embedding, which captures the semantic meaning of words and phrases. Word embeddings like Word2Vec, GloVe, and FastText are commonly used for this purpose. These embeddings allow machines to perform mathematical operations on text, such as measuring the similarity between two sentences.

**Who uses Embeddings:**

*   **Search Engines:** Use embeddings to understand the intent behind search queries and retrieve relevant results, even if the exact keywords are not present in the documents.
*   **Email Clients:** Employ embeddings to detect spam, classify emails based on their content, and prioritize important messages.
*   **Social Media:** Use embeddings to moderate posts, identify hate speech, understand user sentiment, and personalize content recommendations.
*   **CRM Tools:** Analyze customer inquiries to understand customer needs, route inquiries to the appropriate agents, and personalize customer interactions.
*   **Recommender Systems:** Suggest articles, products, or content based on user preferences and past behavior, using embeddings to match user interests with relevant items.

<----------section---------->

### Natural Language Generation (NLG)

NLG is a subfield of NLP focused on enabling machines to generate human-like text.

*   It involves creating coherent, contextually appropriate text that is grammatically correct and stylistically natural. This requires the model to understand the underlying meaning and intent behind the desired output.
*   NLG is based on a numerical representation of the meaning and sentiment you would like to convey. The model uses this representation to generate text that accurately reflects the desired message. Models such as Transformers use attention mechanisms to focus on the relevant parts of the input when generating the output.

**Applications:**

*   **Machine Translation:** Translates text from one language to another, preserving the meaning and context of the original text.
*   **Text Summarization:** Creates concise summaries of long documents, preserving key information and main points. Techniques include extractive summarization (selecting existing sentences) and abstractive summarization (generating new sentences).
*   **Dialogue Processing:** Powers chatbots and virtual assistants, allowing them to engage in conversations with users and provide relevant responses.
*   **Content Creation:** Generates articles, reports, stories, poetry, and other forms of written content, often with minimal human input. This includes tasks such as generating product descriptions, writing marketing copy, and creating social media posts.

<----------section---------->

### Example: Conversational Agents

Conversational agents (chatbots, virtual assistants) are a prime example of NLP in action, integrating various subfields to simulate human-like interaction.

Conversational agents typically include the following components:

*   **Speech recognition:** Converts spoken language into text.
*   **Language analysis:** Analyzes the text to understand the user's intent, extract key information, and identify relevant entities.
*   **Dialogue processing:** Manages the flow of the conversation, keeping track of the context and generating appropriate responses.
*   **Information retrieval:** Retrieves relevant information from a knowledge base or the internet to answer user questions.
*   **Text to speech:** Converts the generated text into spoken language.

**Example conversation:**

*   User: "Open the pod bay doors, Hal."
*   Hal: "I’m sorry, Dave, I’m afraid I can’t do that."
*   User: "What are you talking about, Hal?"
*   Hal: "I know that you and Frank were planning to disconnect me, and I'm afraid that's something I cannot allow to happen."

This example, taken from the movie *2001: A Space Odyssey*, illustrates the potential for sophisticated and nuanced interactions with conversational agents, but also highlights the potential for conflict and unexpected behavior.

<----------section---------->

### Conversational Agents in Movies

Conversational agents are frequently featured in movies, reflecting both the fascination and the anxiety surrounding artificial intelligence. Movies like *Her*, *Ex Machina*, and *Blade Runner* explore the ethical and social implications of increasingly human-like AI systems.

<----------section---------->

### NLP is Hard

"I made her duck… what does it mean?"

This seemingly simple sentence illustrates the inherent ambiguity of natural language.

*   "Duck": Can be a noun (waterfowl) or a verb (to lower the head or body quickly).
*   "Make": Can mean to cook, to create, or to cause someone to do something.
*   "Her": Can be a possessive pronoun (belonging to her) or a pronoun indicating the recipient of an action (for her).

**Possible meanings:**

*   I cooked waterfowl for her.
*   I cooked waterfowl belonging to her.
*   I created the (plaster?) duck she owns.
*   I caused her to quickly lower her head or body.
*   I waved my magic wand and turned her into undifferentiated waterfowl.

The intended meaning depends on context, background knowledge, and pragmatic understanding.

<----------section---------->

### Ambiguity

Natural language is extremely rich in form and structure and very ambiguous. This presents a significant challenge for NLP systems.

*   One input can mean many different things. This requires NLP systems to disambiguate the intended meaning based on context, background knowledge, and pragmatic understanding.
*   Many inputs can mean the same thing. NLP systems need to be able to recognize paraphrases and variations in wording to understand the underlying meaning.

**Levels of ambiguity:**

*   **Lexical ambiguity:** Different meanings of words (e.g., "bank" can refer to a financial institution or the side of a river).
*   **Syntactic ambiguity:** Different ways to parse the sentence (e.g., "I saw the man on the hill with a telescope").
*   **Interpreting partial information:** How to interpret pronouns (e.g., "John told Bill that he was wrong" – who was wrong?).
*   **Contextual information:** Context of the sentence may affect the meaning of that sentence (e.g., sarcasm, irony).

<----------section---------->

### Ambiguity Examples

*   "I saw bats… ?" (Were they flying mammals or cricket/baseball bats?)
*   "Call me a cab… ?" (Is the speaker requesting a taxi or asking to be referred to as a cab?)

<----------section---------->

### NLP and Linguistics

NLP techniques draw on various aspects of linguistics to understand and process natural language.

*   **Phonetics:** Understanding the physical sounds of speech and how they are produced and perceived.
*   **Morphology:** Knowledge of the structure and formation of words, including their meaningful components (morphemes). For example, understanding that "unbreakable" is composed of the morphemes "un-", "break", and "-able".
*   **Syntax:** Understanding the rules and structures governing the arrangement of words in sentences (grammar).
*   **Semantics:** Insight into the meaning of words, phrases, and sentences.
*   **Pragmatics:** Understanding how context influences the interpretation of meaning, including speaker intent, background knowledge, and social conventions.

<----------section---------->

### NLP vs Linguistics

While both NLP and linguistics are concerned with language, they have different goals and approaches:

**Linguistics:**

*   Focused on the study of language as a phenomenon.
*   Explores the structure, meaning, and use of language from a theoretical perspective.
*   May employ computational methods and tools as part of computational linguistics, but the primary goal is to understand language, not to build practical applications.

**NLP:**

*   Focused on providing computational capabilities that utilize human language.
*   Designs and implements algorithms to understand and generate human language for specific tasks.
*   Applies results from linguistics to develop practical applications, such as machine translation, chatbots, and information retrieval systems.

<----------section---------->

## Applications of Natural Language Processing

### NLP Killer Applications

Key applications of NLP include:

*   **Language translation:** Automatically translating text or speech from one language to another.
*   **Email smart filtering:** Classifying emails based on their content, prioritizing important messages, and filtering out spam.
*   **Smart assistant:** Powering virtual assistants like Siri, Alexa, and Google Assistant, enabling them to respond to voice commands and provide information.
*   **Sentiment analysis:** Determining the emotional tone or sentiment expressed in text, used for market research, brand monitoring, and customer feedback analysis.
*   **Document analysis:** Extracting information from documents, identifying key themes, and summarizing content.
*   **Chatbots:** Creating conversational agents that can interact with users and provide customer support, answer questions, or complete tasks.
*   **Semantic searches:** Improving the accuracy and relevance of search results by understanding the meaning behind search queries.
*   **Automatic summarization:** Generating concise summaries of long documents, preserving key information.

<----------section---------->

### Applications by Business Sector

NLP is transforming various industries by automating tasks, improving efficiency, and providing new insights from textual data.

*   **Healthcare:**
    *   Process and interpret patient data, including medical records, to assist in diagnosis, treatment plans, and patient care. NLP can help extract information about symptoms, medications, and medical history from unstructured text.
    *   Extract information from unstructured data sources such as doctor's notes and research papers to improve treatment plans and accelerate research.
*   **Finance:**
    *   Analyze market sentiment from news articles and social media to manage risk and detect fraudulent activities. NLP can also be used to identify patterns and anomalies in financial transactions.
    *   Generate insights from financial reports and news to inform investment decisions and improve financial forecasting.
*   **E-commerce and Retail:**
    *   Personalized recommendations based on customer browsing history and purchase behavior, improved search functionalities, and customer service chatbots.
    *   Sentiment analysis to gauge customer satisfaction and market trends, helping businesses understand customer preferences and adapt their strategies.
*   **Legal:**
    *   Automate document analysis, aiding in legal research by identifying relevant cases and precedents.
    *   Streamlining the review process for contracts and legal documentation, identifying potential risks and ensuring compliance.
*   **Customer Service:**
    *   Automate responses to common customer inquiries, guide users through troubleshooting steps, and analyze feedback to improve efficiency and customer satisfaction.
*   **Education:**
    *   Automatic grading of essays and assignments, provision of personalized learning tools based on student needs.
    *   Summarization and generation of educational materials to create personalized learning experiences and make educational content more accessible.
*   **Automotive:**
    *   Intelligent navigation systems that understand natural language voice commands and provide real-time traffic updates, and voice-activated controls for in-car entertainment and vehicle functions.
*   **Technology:**
    *   Assists in software development by generating code snippets and completing code, improving developer productivity.
    *   Enhances code quality through automated reviews and suggestions, identifying potential bugs and improving code readability.
*   **Media and Entertainment:**
    *   Assist in generating scripts, articles, and creative writing, freeing up writers to focus on more complex tasks.
    *   Enhance user engagement with interactive storytelling and personalized media experiences, creating immersive and engaging content.

<----------section---------->

### Many Other Applications…

NLP is pervasive in modern technology, with applications that may surprise you.

*   A search engine can provide more meaningful results if it indexes web pages or document archives in a way that takes into account the meaning of natural language text. This involves understanding the relationships between words, identifying key concepts, and disambiguating ambiguous terms.
*   Autocomplete uses NLP to complete your thought and is common among search engines and mobile phone keyboards. This relies on predicting the most likely words or phrases based on the user's input and past behavior.
*   Many word processors, browser plugins, and text editors have spelling correctors, grammar checkers, concordance composers, and most recently, style coaches. These tools use NLP techniques to identify errors in writing, suggest improvements, and ensure consistency in style and tone.
*   Some dialogue engines (chatbots) use natural language search to find a response to their conversation partner’s message. This involves understanding the user's intent, identifying relevant keywords, and retrieving appropriate responses from a knowledge base.
*   NLP pipelines that generate (compose) text can be used not only to compose short replies in chatbots and virtual assistants but also to assemble much longer passages of text. This involves generating coherent and contextually appropriate text that is grammatically correct and stylistically natural.
*   The Associated Press uses NLP “robot journalists” to write entire financial news articles and sporting event reports. This involves extracting information from structured data sources, generating narrative text, and ensuring accuracy and objectivity.
*   Bots can compose weather forecasts that sound a lot like what your hometown weather person might say, perhaps because human meteorologists use word processors with NLP features to draft scripts. This involves accessing weather data, generating descriptive text, and adapting the style and tone to match human meteorologists.
*   NLP spam filters in early email programs helped email overtake telephone and fax communication channels in the '90s. And the spam filters have retained their edge in the cat and mouse game between spam filters and spam generators for email but may be losing in other environments like social networks. An estimated 20% of the tweets about the 2016 US presidential election were composed by chatbots.

The following table categorizes NLP applications:

| Category            | Application                                                                      |
| :------------------ | :------------------------------------------------------------------------------- |
| Search              | Web Documents, Autocomplete                                                      |
| Editing             | Spelling, Grammar, Style                                                         |
| Dialog              | Chatbot, Assistant, Scheduling                                                   |
| Writing             | Index, Concordance, Table of contents                                            |
| Email               | Spam filter, Classification, Prioritization                                        |
| Text mining         | Summarization, Knowledge extraction, Medical diagnoses                             |
| Law                 | Legal inference, Precedent search, Subpoena classification                          |
| News                | Event detection, Fact checking, Headline composition                               |
| Attribution         | Plagiarism detection, Literary forensics, Style coaching                            |
| Sentiment analysis  | Community morale monitoring, Product review triage, Customer care                  |
| Behavior prediction | Finance, Election forecasting, Marketing                                          |
| Creative writing    | Movie scripts, Poetry, Song lyrics                                                |

<----------section---------->

### Hype Cycle

The Gartner Hype Cycle for Emerging Technologies (2023) provides a framework for understanding the maturity, adoption rate, and potential impact of various technologies, including NLP and related fields. The cycle depicts stages from Innovation Trigger to Peak of Inflated Expectations, Trough of Disillusionment, Slope of Enlightenment, and finally, Plateau of Productivity.

The Gartner Hype Cycle for Emerging Technologies (2023) positions NLP and related technologies within the innovation lifecycle, including:

*   API-Centric SaaS
*   Generative AI
*   AI TRiSM
*   WebAssembly (Wasm)
*   Federated Machine Learning
*   Industry Cloud Platforms
*   Internal Developer Portal
*   Cloud Sustainability
*   Homomorphic Encryption
*   Value Stream Management Platforms
*   Reinforcement Learning
*   Software Engineering
*   Cloud Development Environments
*   Graph Data Science
*   AI Simulation
*   Causal AI
*   Postquantum Cryptography
*   Neuro-Symbolic AI
*   Augmented FinOps
*   Generative Cybersecurity AI
*   Cybersecurity
*   Mesh Architecture

The cycle depicts stages from Innovation Trigger to Peak of Inflated Expectations, Trough of Disillusionment, Slope of Enlightenment, and finally, Plateau of Productivity. The estimated time to reach the plateau varies for each technology. The placement of each technology on the Hype Cycle provides insights into its current state and future prospects. Technologies at the "Peak of Inflated Expectations" are often overhyped and may not yet be ready for widespread adoption, while technologies on the "Slope of Enlightenment" are beginning to show promise and are likely to become more mainstream in the coming years.

<----------section---------->

### NLP Market

NLP is a promising career option, driven by the increasing demand for NLP applications across various industries.

*   Growing demand for NLP applications across various industries, including healthcare, finance, e-commerce, and customer service.
*   Projected employment growth of 22% between 2020 and 2030, indicating strong job prospects for NLP professionals.

The NLP market global forecast in USD Billions is projected to increase from 18.9 in 2023 to 61.8 in 2028. This significant growth is driven by factors such as increasing data volumes, advancements in NLP algorithms, and growing adoption of AI-powered solutions.

*   North America
*   Europe
*   Asia Pacific
*   Middle East & Africa
*   Latin America

These regions are expected to contribute to the growth of the NLP market, with varying adoption rates and application areas. North America and Europe are currently the largest markets for NLP, while Asia Pacific is expected to be the fastest-growing region due to increasing investments in AI and growing adoption of NLP technologies across various industries.

<----------section---------->

## History of NLP

### First Steps of NLP

NLP has had a history of ups and downs, marked by periods of intense enthusiasm followed by periods of disillusionment.

*   Influenced by the growth of computational resources and changes in approaches, shifting from rule-based systems to statistical models and, more recently, to deep learning.

**1950's and 1960's**

*   The first application that sparked interest in NLP was machine translation, driven by the Cold War and the need to automatically translate documents from Russian to English.
*   The first machine translation systems used dictionary lookup and basic word order rules to produce translations. These early systems were limited by their inability to handle ambiguity, idiomatic expressions, and syntactic variations.
*   The 1950s saw a lot of excitement: researchers predicted that machine translation can be solved in 3 years or so, reflecting the early optimism and overestimation of the challenges involved.

<----------section---------->

### Machine Translation in 50s

**Example:**

Given:

```
Dictionary: Red -> Rosso
            House -> Casa
```

Translate:

```
The red house -> Il rosso casa   (incorrect)
```

But it should be:

```
La casa rossa (correct)
```

This example highlights the limitations of simple dictionary lookup approaches. Translating "The red house" word-by-word results in an incorrect translation in Italian because the adjective "red" follows the noun in Italian, and both need to agree in gender.

Dictionary lookup alone is insufficient. Machine translation requires understanding grammatical rules, word order, and contextual information.

<----------section---------->

### How to deal with language ambiguity?

### Generative Grammars

**1957: Chomsky’s Generative Grammar**

*   A system of rules for generating all possible sentences in a language, providing a formal framework for describing the syntax of natural language.
*   Enabled prediction of grammatical correctness, allowing computers to verify whether a sentence is grammatically well-formed.
*   Understanding of language structure, providing a basis for developing parsing algorithms and machine translation systems.
*   Influenced research in machine translation, inspiring the development of syntax-based translation approaches.

**1966: The Reality Check**

*   Early translation systems fell short in effectiveness, failing to meet expectations and demonstrating limited accuracy.
*   Limited by their inability to handle the ambiguity and complexity of natural language, struggling to deal with idiomatic expressions, metaphorical language, and contextual variations.

<----------section---------->

### ALPAC Report

**Automatic Language Processing Advisory Committee**

*   Established to assess advancements in computational linguistics, evaluating the progress and potential of machine translation research.
*   The 1966 ALPAC report recommended halting research into machine translation due to the lack of significant progress and the limited practical value of existing systems.
*   Shift focus from developing end-to-end machine translation systems to enhancing tools that assist human translators, recognizing the value of human expertise in language translation.
*   It significantly impacted NLP and AI research, contributing to the first AI winter, a period of reduced funding and interest in AI research.

[https://www.mt-archive.net/50/ALPAC-1966.pdf](https://www.mt-archive.net/50/ALPAC-1966.pdf)

<----------section---------->

### ELIZA

A pioneering conversational agent, demonstrating the potential of computer-based conversation.

*   Created by Joseph Weizenbaum in the 1960s, one of the earliest examples of a chatbot.
*   Designed to simulate a conversation between a psychotherapist and a patient, using simple pattern matching and substitution techniques.

**Features and Limitations:**

*   Demonstrated the potential of computer-based conversation, showing that computers could engage in seemingly intelligent interactions with humans.
*   Utilized pattern matching and substitution to generate responses, using keywords and rules to identify patterns in user input and generate appropriate responses.
*   Limited in handling complex conversations, struggling to understand the meaning behind user input and often producing irrelevant or repetitive responses.
*   Could not maintain context beyond a few exchanges, failing to remember previous turns in the conversation and losing track of the overall topic.
*   Often produced irrelevant or repetitive responses, relying on simple heuristics and lacking the ability to generate original or insightful contributions.

[https://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm](https://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm)

<----------section---------->

### The Turing Test

"I propose to consider the question: can machines think? ... We can only see a short distance ahead, but we can see plenty there that needs to be done" - *Alan Turing, Computing Machinery and Intelligence, 1950*

**Turing Test aka The Imitation game:**

*   A human, a computer, and an interrogator in a different room communicate via written messages. The interrogator's task is to determine which participant is the human and which is the computer.
*   The interrogator should classify the human and the machine based solely on their written responses, without knowing their identities.

<----------section---------->

### The Turing Test

**Capabilities for passing the Turing Test**

*   Natural Language Understanding to interpret user input accurately and extract relevant information.
*   Knowledge Representation to draw on relevant information from a vast store of knowledge about the world.
*   Automated Reasoning to generate appropriate and logical responses based on the context of the conversation.
*   Natural Language Generation to produce human-like textual responses that are grammatically correct and stylistically natural.
*   Context Management to maintain and utilize context across multiple exchanges in a conversation, remembering previous turns and tracking the overall topic.
*   Adaptability and Learning to adapt responses based on user behavior and feedback, improving the quality of the conversation over time.

<----------section---------->

### The Turing Test

**Successes with Turing test**

*   A (controversial) success in 2014: a chatbot mimicking the answer of a 13 years old boy, raising questions about the criteria for intelligence and the validity of the Turing Test.
*   Since then, other (controversial) successes, further fueling the debate about whether machines can truly think.

**Limitations of Turing Test**

*   Not reproducible, making it difficult to compare different systems and assess progress over time.
*   Is emulating humans necessary for achieving intelligence? This questions whether human-like behavior is a necessary condition for intelligence.
*   Many AI researchers have shifted focus to other benchmarks, such as those focused on specific tasks like image recognition and natural language understanding.
*   Less commonly used today as a primary measure of AI progress, replaced by more focused and quantitative benchmarks.

<----------section---------->

### Raise of Symbolic Approaches

**1970's and 1980's:**

*   Programmers started creating structured representations of real-world information for computer understanding (ontologies), defining concepts, relationships, and properties in a formal and machine-readable way.
*   Complex rule-based systems were developed for various NLP tasks, including parsing, morphology, semantics, reference, and dialogue processing, using handcrafted rules to analyze and generate natural language.

**Main applications were:**

*   **Expert Systems:** mimicked human expertise in specific domains, using rule-based reasoning to solve problems and provide advice.
*   **Information Retrieval:** enhanced search and data extraction, using keyword matching and indexing techniques to retrieve relevant documents.

**Main limitations were:**

*   **Flexibility:** challenges in adapting to new or ambiguous contexts, struggling to handle variations in language and unexpected input.
*   **Scalability:** difficulty handling large-scale or diverse data, requiring significant manual effort to maintain and update the rules.

<----------section---------->

### Statistical Revolution

**1990's:**

*   The computing power increased substantially, enabling the processing of large datasets and the development of more complex statistical models.
*   Statistical models with simple representations started to outperform complex hand-coded linguistic rules, demonstrating the effectiveness of data-driven approaches.
*   Learn patterns from data, using statistical techniques to identify relationships between words, phrases, and concepts.
*   Can handle variations and complexities in natural language, adapting to different writing styles and linguistic variations.
*   Large corpora became essential, providing the data needed to train statistical models and estimate probabilities.
*   Long Short-Term Memory (LSTM) networks was invented by Hochreiter and Schmidhuber in 1997, a type of recurrent neural network capable of learning long-range dependencies in sequential data, opening up new possibilities for NLP tasks such as machine translation and language modeling.

*"Whenever I fire a linguist, our machine translation performance improves" - Fred Jelinek, IBM*

This quote, while humorous, reflects the shift away from rule-based approaches towards statistical methods in NLP.

<----------section---------->

### Advances in NLP

**2000's**

*   Increased Use of Neural Networks, inspired by the structure and function of the human brain.
*   Introduction of Word Embeddings, representing words as dense vectors of numbers, capturing semantic relationships between words.
    *   Words are represented as dense vectors of numbers, typically with hundreds of dimensions, capturing semantic and syntactic information.
    *   Words with similar meanings are associated with similar vectors, allowing machines to measure the semantic similarity between words.
    *   Early algorithms struggled to efficiently learn these representations, requiring large datasets and significant computational resources.

**2006: launch of Google Translate**

*   The first commercially successful NLP system, demonstrating the potential of statistical machine translation.
*   Utilized statistical models to automatically translate documents, leveraging large parallel corpora and sophisticated algorithms.

<----------section---------->

### Deep Learning Era

**2010's:**

*   LSTM and CNN became widely adopted for NLP, enabling the development of more powerful and accurate models for various tasks.
*   The availability of large text corpora enabled the training of increasingly complex models, driving significant improvements in NLP performance.

**Word2Vec (2013):**

*   Efficient Estimation of Word Representations in Vector Space, a groundbreaking algorithm for learning word embeddings from large text corpora.
*   The first algorithm to efficiently learn word embeddings, making it possible to capture semantic relationships between words in a scalable and computationally efficient manner.
*   Enables semantic operations with word vector, such as calculating the similarity between words, identifying analogies, and performing word arithmetic.
*   Paved the way for more advanced models such as GloVe, fastText, ELMo, BERT, COLBERT, GPT, ..., inspiring further research and development in word embedding techniques.

<----------section---------->

### Deep Learning Era

**Sequence-to-Sequence Models (2014):**

*   Introduction of the encoder-decoder architecture, a fundamental framework for sequence generation tasks such as machine translation and text summarization.
    *   **Encoder:** Encodes the input sequence into a context vector, capturing the essential information from the input.
    *   **Decoder:** Decodes the output sequence from the context vector, generating the desired output based on the encoded information.
*   Useful for automatic translation, question answering, text summarization, text generation, and other sequence generation tasks.

**Example:**

```
The red house -> Context vector [0.3, 0.6, -0.2, ..., 0.1] -> La casa rossa
```

In this example, the encoder processes the English sentence "The red house" and generates a context vector representing its meaning. The decoder then uses this context vector to generate the equivalent Italian sentence, "La casa rossa."

<----------section---------->

### Virtual Assistants

A Virtual Assistant performs a range of tasks or services based on user input in natural language. These tasks include setting alarms, playing music, providing weather updates, answering questions, and controlling smart home devices.

Many VA were launched in 2010's:

*   2011: Siri launched by Apple on iOS devices, pioneering voice-based interaction on smartphones.
*   2014: Cortana introduced by Microsoft for Windows Phone, integrating voice interaction with desktop and mobile devices.
*   2014: Alexa launched by Amazon with the Echo, pioneering voice-controlled smart home devices and establishing a new category of computing.
*   2015: Google Assistant introduced, integrating voice interaction with Android and Google Home, providing a seamless voice-based experience across various devices.

<----------section---------->

### Deep Learning Era

**Transformer (2017):**

*   **Attention Is All You Need**, a seminal paper that introduced the Transformer architecture, revolutionizing the field of NLP and paving the way for Large Language Models.
*   Integration of attention mechanisms, allowing the model to focus on the most relevant parts of the input sequence when processing each word or phrase.
*   Allows a greater passage of information between the decoder and the encoder, improving the accuracy and coherence of generated text.
*   Defined and adopted by Google for the translator, leading to significant improvements in machine translation quality.
*   It remains the dominant architecture in NLP today, forming the basis for most state-of-the-art language models.

*Diagram of Transformer Architecture* (This section would ideally include a visual representation of the Transformer architecture, highlighting the attention mechanism and encoder-decoder structure.)

<----------section---------->

### Large Language Models (LLM)

After transformers, the next step was scaling...

*   LLM leverage extensive data and computational power to understand and generate human-like text. These models are trained on massive datasets, often consisting of billions of words, and require significant computational resources to train and deploy.

*   List of LLMs: GPT-4, ChatGPT, InstructGPT, Codex, Flan-PaLM, LLaMA, BLOOM, OPT, UL2, PaLM, Gopher, Chinchilla, Titan, Jurassic-1, Ernie 3.0, PanGu, etc.* This list represents a diverse range of LLMs developed by various organizations, each with its own strengths and capabilities.

<----------section---------->

### LLM Applications

*   **Text Generation:** Producing articles, stories, and creative writing, often with minimal human input.
*   **Machine Translation:** Translating between languages with high accuracy and fluency.
*   **Chatbots:** Engaging in human-like conversations for customer support and interaction, providing personalized assistance and answering user questions.
*   **Code Generation:** Generating and suggesting code snippets, completing code, and assisting with programming tasks, improving developer productivity and reducing errors.
*   **Question Answering:** Providing answers based on a given context or database, leveraging knowledge retrieval and reasoning capabilities.
*   **Text Summarization:** Condensing long documents into concise summaries, preserving key information and main points.
*   **Writing Assistance:** Generating and completing text, improving grammar, and enhancing style, assisting users with writing tasks and improving the quality of their writing.

<----------section---------->

### Multimodal LLM

Integrate and process multiple types of data, such as text, images, audio, and video, to create more comprehensive and versatile AI systems.

*   **Image-to-Text:** generating descriptive text from images (CLIP), enabling machines to "see" and understand the content of images.
*   **Text-to-Image:** creating images based on textual descriptions (DALL-E), allowing users to generate customized images from natural language prompts.
*   **Audio-to-Text:** converting spoken language into written text (Whisper), enabling machines to transcribe audio recordings and understand spoken commands.
*   **Text-to-Audio:** composing or generating audio, such as music, from textual descriptions (Jukebox), allowing users to create audio content from natural language prompts.

<----------section---------->

### Multimodal LLM

Integr
<----------section---------->

# Natural Language Processing and Large Language Models
## Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
### Lesson 10: Transformers II
**Nicola Capuano and Antonio Greco**
**DIEM – University of Salerno**

This document presents the lecture notes for Lesson 10 of the Natural Language Processing (NLP) and Large Language Models (LLMs) course, part of the Master's Degree program in Computer Engineering at the University of Salerno. The lesson, titled "Transformers II," is delivered by Nicola Capuano and Antonio Greco from DIEM (Department of Industrial Engineering and Mathematics). This lesson builds upon the foundational concepts of Transformers, a revolutionary architecture that has significantly advanced the field of NLP.

<----------section---------->

## Outline

This lesson will cover the following topics:

*   **Multi-Head Attention:** Delving deeper into the mechanism that allows the model to focus on different parts of the input sequence when processing it.
*   **Encoder Output:** Understanding the representation of the input sequence generated by the encoder component of the Transformer.
*   **Decoder:** Examining the role and structure of the decoder, responsible for generating the output sequence.
*   **Masked Multi-Head Attention:** Exploring the specific variant of multi-head attention used in the decoder to prevent it from "peeking" into the future.
*   **Encoder-Decoder Attention:** Analyzing how the decoder attends to the encoder's output to generate contextually relevant output tokens.
*   **Output:** Describing the final layers of the Transformer that produce the predicted output sequence.
*   **Transformer’s pipeline:** Overview of the entire process, highlighting the flow of information through the encoder and decoder.

<----------section---------->

## Multi-head attention

### Multi-head Attention
*   **Encoding Different Meanings:** By employing multiple self-attention heads, the model can capture various nuances and interpretations of the context within the input sequence. Each head learns different weight matrices, allowing it to focus on distinct aspects of the relationships between words.
*   **Parallel Scaled-Dot Product Attention:** The multi-head attention mechanism performs several scaled dot-product attention computations simultaneously. Each computation utilizes different learned weight matrices, increasing the model's capacity to capture diverse relationships. This parallelization significantly speeds up the processing.
*   **Concatenation of Results:** The results from each attention head are concatenated row-by-row, forming a larger matrix. The resulting matrix maintains the same number of rows (*m*) as the input, while the number of columns increases proportionally to the number of attention heads.
*   **Final Weight Matrix Multiplication:** This concatenated matrix is then multiplied by a final weight matrix. This multiplication projects the concatenated representations into a final output space, combining the information learned by each head.
*   **Definition:** This entire process – parallel scaled dot-product attention computations with concatenation and final projection – is termed multi-head attention.

### Multi-head Attention
Multi-head attention empowers the model to attend jointly to information from various representation subspaces across different positions within the input. This is crucial because a single attention head, by averaging the attention weights, might inhibit the model's ability to distinguish between different types of relationships.

The outputs from the various attention heads are concatenated to create a comprehensive representation. Subsequently, an additional weight matrix is applied to combine these representations at the same network level. This allows the model to integrate diverse perspectives on the input sequence at each layer of the Transformer. In effect, the multi-head attention allows the transformer to focus on different positions, it creates several different vector subspaces where the transformer can encode a particular generalization for a subset of the word patterns in your text.

<----------section---------->

## Add & Norm

### Add (skip connections) & Norm
This section focuses on two crucial components of the Transformer architecture: Add (skip connections or residual connections) and Norm (Layer Normalization).

**Input Normalization (Z)**
*   **Mean 0, Std dev 1:** The input to each layer is normalized to have a mean of 0 and a standard deviation of 1. This is a common technique in deep learning.
*   **Stabilizes training:** Normalization helps stabilize the training process by preventing the activations from becoming too large or too small, a phenomenon known as the exploding/vanishing gradients problem.
*   **Regularization effect:** Input normalization introduces a subtle regularization effect, which can improve the generalization performance of the model by preventing overfitting to the training data.

**Add -> Residuals**
*   **Avoid vanishing gradients:** Residual connections create a direct path for the gradient to flow through the network, mitigating the vanishing gradient problem. This is achieved by adding the original input to the output of a layer.
*   **Train deeper networks:** Residual connections enable the training of much deeper networks. Without them, the gradients would diminish exponentially as they propagate through the layers, making it difficult for the network to learn.

<----------section---------->

## Feed Forward

### Feed Forward
This section briefly describes the Feed Forward Network (FFN) component of the Transformer architecture.

*   **Non Linearity:** The feed-forward network introduces non-linearity into the model, allowing it to learn complex relationships between the input and output.
*   **Complex Relationships:** Non-linearities are essential for modeling intricate patterns and dependencies in the data.
*   **Learn from each other:** It gives the transformer the ability to learn from each other, because each layer of a transformer gives you a deeper and deeper representation of the meaning or thought of the input text.
*   **FFN (2 layer MLP):** The feed-forward network typically consists of a two-layer Multilayer Perceptron (MLP). The first layer expands the dimensionality of the input, while the second layer projects it back to the original dimensionality.

<----------section---------->

## Transformer’s Encoder

### Transformer’s Encoder
The encoder is responsible for creating a high-quality representation of the input sequence. Its key features are:

*   **Input Sequence Representation:** The encoder's primary goal is to compute a representation of the input sequence, capturing its meaning and structure. This representation will be used by the decoder to generate the output sequence.
*   **Additive Positional Encoding:** Since self-attention is order-agnostic, the encoder adds positional encoding to each word embedding. This provides information about the position of each word in the sequence, allowing the model to account for word order.
*   **Residual Connections:** Residual connections are used to foster the gradients flow, mitigating the vanishing gradient problem and enabling the training of deeper networks.
*   **Normalization Layers:** Normalization layers are employed to stabilize the network training, ensuring that the activations remain within a reasonable range.
*   **Position-Wise Feed-Forward Layer:** A position-wise feed-forward layer, applied to each sequence element independently, adds non-linearity to the model, allowing it to learn complex relationships.
*   **Stacking Encoders:** Each encoder produces an output with the same dimensionality as its input, meaning that it is possible to stack an arbitrary number of encoder blocks. The output of one encoder is fed as input to the next, enabling hierarchical representation learning.

<----------section---------->

## Decoder

### Decoder
The decoder generates the output sequence *y*<sub>1</sub>,…,*y*<sub>*m*</sub>, leveraging the information encoded in the intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub> produced by the encoder.

*   **Sequential Operation:** The decoder operates sequentially, generating the output tokens one at a time.
*   **Input at each step:** At each step *i*, the decoder uses the encoder's intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub> and the previously generated output tokens *y*<sub>1</sub>,…,*y*<sub>*i*-1</sub> to generate the next output token *y*<sub>*i*</sub>.

### Decoder
The decoder is constructed from a stack of decoder blocks, each having a similar structure. The original Transformer paper used 6 decoder blocks. Each decoder block incorporates:

*   **Same Modules as Encoder:** The same modules used in the encoder block, such as self-attention, add & norm, and feed forward networks.
*   **Encoder-Decoder Attention:** An additional attention module is included, where the keys and values are derived from the encoder's intermediate representation (*z*<sub>1</sub>,…,*z*<sub>*t*</sub>). This allows the decoder to focus on the relevant parts of the input sequence when generating the output.
*   **Masked Self-Attention:** The self-attention module is modified to ensure that the query at position *i* only uses values at positions 1,…,i. This prevents the decoder from "peeking" at future tokens in the output sequence, which is crucial for training.

**Output Layer:**

*   **Linear Layer and Softmax:** On top of the last decoder block, a linear layer followed by a softmax activation function computes the probability of the next output element *y*<sub>*i*</sub>.
*   **Output Set Cardinality:** The final linear layer has a number of neurons that corresponds to the size of the output set, so that the transformer can figure out which token to output.

<----------section---------->

## Masked Multi-Head Attention
This type of Multi-Head Attention is used in the Decoder.

*   **Preventing Future Information Leakage:** Masked multi-head attention ensures that when generating the output at time *T*, the model only attends to outputs up to time *T*-1. This prevents the decoder from "cheating" by looking ahead at the tokens it's supposed to be predicting.
*   **Attention Mask (M):** This is achieved by masking the attention values. The attention mask *M* is a matrix that determines which attention weights are used and which are masked out (set to -inf, so the softmax will ignore them).
*   **R<sup>IxT</sup> & R<sup>TxT</sup>:** *R<sup>IxT</sup>* might represent the attention values before masking, where I is the input sequence length and T is the target sequence length. Masked attention values are then obtained after applying the mask *M*, resulting in a masked attention matrix.

<----------section---------->

## Encoder Decoder Attention
This describes how the Decoder uses the output of the encoder to produce output text.

*   **Keys and Values from Encoder:** The keys and values for the encoder-decoder attention module are taken from the encoder's outputs (*z*<sub>1</sub>,…,*z*<sub>*t*</sub>). This enables the decoder to access and leverage the information encoded in the input sequence.
*   **Queries from Decoder:** The queries are derived from the decoder's inputs, which are the previously generated output tokens.
*   **Consistent Encoder Output:** Every decoder block receives the same final encoder output. This ensures that all decoder layers have access to the same contextual information about the input sequence.

<----------section---------->

## Output

### Output
Describes the process of how the transformer outputs the decoded text.

*   **Linear Layer:** A linear layer is used to project the output of the decoder into the vocabulary space.
*   **Weight Tying:** The weights of this linear layer are often tied (shared) with the model's input embedding matrix. This can improve the model's generalization performance and reduce the number of parameters.
*   **Softmax:** Finally, a softmax function is applied to produce a probability distribution over the vocabulary, representing the model's prediction for the next token in the output sequence.

<----------section---------->

## Transformer’s pipeline
Describes the way in which the transformer deals with input data.

*   **Decoding Time Step:** The transformer processes the input one decoding time step at a time. The decoder produces a token, and then takes in the previously decoded tokens to output the next token in the sequence.
*   **ENCODER - DECODER:** This emphasizes the core architecture of the Transformer: the encoder processes the input sequence and the decoder generates the output sequence based on the encoded representation.
*   **Resource:** The provided link (https://poloclub.github.io/transformer-explainer/) links to an interactive explainer which can be useful to understand the finer details of how transformers work.

<----------section---------->

```
from
 
torch
 
import
 
Tensor
```

```
from
 
typing
 
import
 
Optional
, 
Any
```

```
class
 
CustomDecoderLayer
(nn.TransformerDecoderLayer):
```

```
    
def
 
forward
(
self
, tgt: Tensor, memory: Tensor,
```

```
            tgt_mask: Optional[Tensor] = 
None
,
```

```
            memory_mask: Optional[Tensor] = 
None
,
```

```
            tgt_key_padding_mask: Optional[Tensor] = 
None
```

```
            ) -> Tensor:
```

```
        
"""
Like decode but returns multi-head attention weights.
```

```
        tgt2 = 
self
.self_attn(
```

```
            tgt, tgt, tgt, attn_mask=tgt_mask,
```

```
            key_padding_mask=tgt_key_padding_mask)[
0
]
```

```
        tgt = tgt + 
self
.dropout1(tgt2)
```

```
        tgt = 
self
.norm1(tgt)
```

```
        tgt2, attention_weights = 
self
.multihead_attn(
```

```
            tgt, memory, memory,  # #1
```

```
            attn_mask=memory_mask,
```

```
            key_padding_mask=mem_key_padding_mask,
```

```
            need_weights=
True
)
```

```
        tgt = tgt + 
self
.dropout2(tgt2)
```

```
        tgt = 
self
.norm2(tgt)
```

```
        tgt2 = 
self
.linear2(
```

```
            
self
.dropout(
self
.activation(
self
.linear1(tgt))))
```

```
        tgt = tgt + 
self
.dropout3(tgt2)
```

```
        tgt = 
self
.norm3(tgt)
```

```
        
return
 tgt, attention_weights  # #2
```

```
class
 
CustomDecoder
(nn.TransformerDecoder):
```

```
    
def
 
__init__
(
self
, decoder_layer, num_layers, norm=
None
):
```

```
        
super
().__init__(
```

```
            decoder_layer, num_layers, norm)
```

```
    
def
 
forward
(
self
,
```

```
            tgt: Tensor, memory: Tensor,
```

```
            tgt_mask: Optional[Tensor] = 
None
,
```

```
            memory_mask: Optional[Tensor] = 
None
,
```

```
            tgt_key_padding_mask: Optional[Tensor] = 
None
```

```
            ) -> Tensor:
```

```
        
"""
Like TransformerDecoder but cache multi-head attention
```

```
        
self
.attention_weights = []  # #1
```

```
        output = tgt
```

```
        
for
 mod 
in
 
self
.layers:
```

```
            output, attention = mod(
```

```
                output, memory, tgt_mask=tgt_mask,
```

```
                memory_mask=memory_mask,
```

```
                tgt_key_padding_mask=tgt_key_padding_mask)
```

```
            
self
.attention_weights.append(attention) # #2
```

```
        
if
 
self
.norm 
is
 
not
 
None
:
```

```
            output = 
self
.norm(output)
```

```
        
return
 output
```

```
from
 
einops
 
import
 
rearrange
  # #1
```

```
class
 
TranslationTransformer
(nn.Transformer):  # #2
```

```
    
def
 
__init__
(
self
,
```

```
            device=DEVICE,
```

```
            src_vocab_size: int = VOCAB_SIZE,
```

```
            src_pad_idx: int = PAD_IDX,
```

```
            tgt_vocab_size: int = VOCAB_SIZE,
```

```
            tgt_pad_idx: int = PAD_IDX,
```

```
            max_sequence_length: int = 
100
,
```

```
            d_model: int = 
512
,
```

```
            nhead: int = 
8
,
```

```
            num_encoder_layers: int = 
6
,
```

```
            num_decoder_layers: int = 
6
,
```

```
            dim_feedforward: int = 
2048
,
```

```
            dropout: float = 
0.1
,
```

```
            activation: str = 
"
relu
"
```

```
        decoder_layer = CustomDecoderLayer(
```

```
            d_model, nhead, dim_feedforward,  # #3
```

```
            dropout, activation)
```

```
        decoder_norm = nn.LayerNorm(d_model)
```

```
        decoder = CustomDecoder(
```

```
            decoder_layer, num_decoder_layers,
```

```
            decoder_norm)  # #4
```

```
        
super
().__init__(
```

```
            d_model=d_model, nhead=nhead,
```

```
            num_encoder_layers=num_encoder_layers,
```

```
            num_decoder_layers=num_decoder_layers,
```

```
            dim_feedforward=dim_feedforward,
```

```
            dropout=dropout, custom_decoder=decoder)
```

```
        
self
.src_pad_idx = src_pad_idx
```

```
        
self
.tgt_pad_idx = tgt_pad_idx
```

```
        
self
.device = device
```

```
        
self
.src_emb = nn.Embedding(
```

```
            src_vocab_size, d_model)  # #5
```

```
        
self
.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)
```

```
        
self
.pos_enc = PositionalEncoding(
```

```
            d_model, dropout, max_sequence_length)  # #6
```

```
        
self
.linear = nn.Linear(
```

```
            d_model, tgt_vocab_size)  # #7
```
This extended context provides code examples and explanations related to implementing custom decoders and translation transformers using PyTorch. Specifically, it includes classes for:

*   `CustomDecoderLayer`:  A subclass of `torch.nn.TransformerDecoderLayer` that additionally returns multi-head self-attention weights during the forward pass. This is useful for analyzing the attention patterns of the decoder.
*   `CustomDecoder`: A subclass of `torch.nn.TransformerDecoder` that stores the attention weights from each layer in a list.
*   `TranslationTransformer`: A subclass of `torch.nn.Transformer` designed for translation tasks. It incorporates the custom decoder components and includes methods for preparing the source and target sequences with positional encodings and padding masks.

These code snippets can be helpful to anyone who is attempting to build upon the standard architecture of a transformer, and to provide more information about the way in which they function.

<----------section---------->

### Enhanced Text: Natural Language Processing and Large Language Models

**Course Information:**

This material pertains to Lesson 11 of a Corso di Laurea Magistrale (Master's Degree Course) in Ingegneria Informatica (Computer Engineering). The lesson focuses on the progression from Transformer architectures to Large Language Models (LLMs).

**Instructors:**

Nicola Capuano and Antonio Greco from DIEM (Dipartimento di Ingegneria dell'Informazione ed Elettrica e Matematica dell'Ingegneria) – University of Salerno.

**Outline:**

The lecture covers the following key topics:

*   Transformers for text representation and generation.
*   Paradigm shift in Natural Language Processing (NLP).
*   Pre-training of Large Language Models.
*   Datasets and data pre-processing techniques used in LLMs.
*   Strategies for utilizing LLMs after pre-training.

<----------section---------->

**Transformers for Text Representation and Generation**

Transformers have become a cornerstone in modern NLP, excelling in both representing and generating text. They achieve this primarily through the attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when processing it. This is a significant departure from previous sequential models like Recurrent Neural Networks (RNNs), which process information step-by-step and can struggle with long-range dependencies.

**Attention Mechanism:**

The attention mechanism addresses the limitations of RNNs by:

*   **Handling Long-Range Dependencies:**  Enables the model to capture relationships between words that are far apart in a sentence.  Traditional sequential models often lose information when processing long sequences due to the vanishing gradient problem.
*   **Parallel Training:** Allows for parallel processing of the input sequence, significantly speeding up training times compared to sequential models.
*   **Dynamic Attention Weights:** Assigns weights to different input tokens based on their relevance to the current task, allowing the model to focus on the most important information.

**Types of Transformer Architectures:**

Transformers come in three primary architectural variants:

*   **Encoder-Only (e.g., BERT):** This type focuses on understanding the input text. It takes input tokens and produces hidden states representing the contextualized meaning of each token. BERT (Bidirectional Encoder Representations from Transformers) is a prime example, known for its ability to capture bidirectional context.  Encoder-only models are typically used for tasks like text classification, named entity recognition, and question answering where understanding the input is paramount.  They are not inherently auto-regressive and therefore don't naturally generate text.
*   **Decoder-Only (e.g., GPT):** This type is designed for text generation. It takes previously generated tokens and hidden states as input and produces the next token in the sequence. GPT (Generative Pre-trained Transformer) is a key example, known for its autoregressive nature. It can only "see" the tokens it has already generated, making it suitable for tasks such as text completion, chatbots, and language modeling.
*   **Encoder-Decoder (e.g., T5, BART):** This type combines the strengths of both encoders and decoders. The encoder processes the input text to create a representation, and the decoder generates the output text based on this representation. T5 (Text-to-Text Transfer Transformer) and BART (Bidirectional and Auto-Regressive Transformer) are examples of this architecture, excelling in tasks like machine translation, text summarization, and question answering (where the answer is generated, not just extracted).

**Popular Transformer Architectures:**

*   **Encoder-Only:**
    *   BERT (October 2018): A foundational model for understanding text.
    *   DistilBERT (2019): A smaller, faster version of BERT.
    *   RoBERTa (2019): A robustly optimized version of BERT.
    *   ALBERT (2019): A lite version of BERT with fewer parameters.
    *   ELECTRA (2020):  A more sample-efficient pre-training approach.
    *   DeBERTa (2020):  Decoding-enhanced BERT with disentangled attention.
*   **Decoder-Only:**
    *   GPT (June 2018): The original generative pre-trained transformer.
    *   GPT-2 (2019):  An improved and scaled-up version of GPT.
    *   GPT-3 (2020):  A very large language model with impressive generative capabilities.
    *   GPT-Neo (2021):  An open-source alternative to GPT-3.
    *   GPT-3.5 (ChatGPT) (2022):  An iteration of GPT-3 fine-tuned for conversational AI.
    *   LLaMA (2023):  A powerful open-source language model.
    *   GPT-4 (2023):  The successor to GPT-3, known for its advanced capabilities.
*   **Encoder-Decoder:**
    *   T5 (2019):  A unified text-to-text framework.
    *   BART (2019):  A denoising autoencoder for sequence-to-sequence tasks.
    *   mT5 (2021):  A multilingual version of T5.

<----------section---------->

**Paradigm Shift in NLP**

The advent of Large Language Models (LLMs) has fundamentally altered the landscape of NLP, shifting the focus from task-specific feature engineering and model selection to pre-training, fine-tuning, and prompting.

**Before LLMs:**

*   **Feature Engineering:**  Researchers and practitioners spent considerable time and effort designing and selecting the most relevant features for a given NLP task. This was a manual and often labor-intensive process that required deep domain expertise.
*   **Model Selection:**  Choosing the right model for a specific task was crucial.  Different models were better suited for different types of problems (e.g., Support Vector Machines for text classification, Hidden Markov Models for sequence labeling).
*   **Transfer Learning (Limited):**  While transfer learning existed, it was often limited in scope.  Transferring knowledge from one domain to another was challenging, especially with scarce labeled data.
*   **Overfitting vs. Generalization:**  Balancing model complexity and capacity to prevent overfitting (performing well on training data but poorly on unseen data) while maintaining good generalization performance was a major concern.

**Since LLMs:**

*   **Pre-training and Fine-tuning:** LLMs are pre-trained on massive amounts of unlabeled data, enabling them to learn general language representations. Fine-tuning then adapts these representations to specific tasks using smaller, labeled datasets. This leverages previously under-utilized large-scale unlabeled data.
*   **Zero-shot and Few-shot Learning:** LLMs exhibit the ability to perform well on tasks they haven't been explicitly trained on, using only a description of the task (zero-shot) or a few examples (few-shot).  This significantly reduces the need for large, task-specific labeled datasets.
*   **Prompting:** LLMs can be instructed to perform tasks simply by describing the task in natural language prompts. This eliminates the need for extensive task-specific training and allows for more flexible and intuitive interaction with the model.
*   **Interpretability and Explainability:** There is growing interest in understanding the inner workings of LLMs.  Research is focused on developing techniques to interpret model decisions and explain their behavior, which is crucial for building trust and addressing potential biases.

**The Catalyst for Change:**

The limitations of recurrent networks, particularly in handling long sequences, fueled the paradigm shift. Recurrent networks struggle with:

*   **Information Loss:** Information is often lost during the encoding of long sequences, hindering the model's ability to capture long-range dependencies.
*   **Sequential Nature:** The sequential processing of RNNs prevents parallel training, making them slow to train. It also favors inputs from late timesteps, potentially overshadowing information from earlier timesteps.

**The Solution: Attention Mechanism**

The attention mechanism provides a solution by:

*   **Handling Long-Range Dependencies Effectively:** Capturing relationships between words regardless of their distance in the input sequence.
*   **Enabling Parallel Training:** Allowing for simultaneous processing of different parts of the input, significantly reducing training time.
*   **Using Dynamic Attention Weights Based on Inputs:** Focusing on the most relevant parts of the input for each specific task, leading to more accurate and efficient processing.

<----------section---------->

**Pre-training of LLMs**

Pre-training is a crucial stage in the development of LLMs, enabling them to learn general language representations from vast amounts of unstructured data.

**Self-Supervised Pre-training:**

*   LLMs are typically pre-trained in a self-supervised manner. This means they are trained on unlabeled text data, such as text from the internet.
*   The self-supervised approach eliminates the need for manual labeling. The models learn by creating "supervised" tasks from the unlabeled data itself.  For example, they might be trained to predict the next word in a sentence or fill in missing words.

**Self-Supervised Pre-training Methods:**

*   **Autoencoding (Masked Language Modeling - MLM):**
    *   These models, like BERT, consist of an encoder that processes the input text.
    *   MLM involves masking certain words in the input and training the model to predict those masked words based on the surrounding context.
    *   Because it considers both preceding and following words, MLM is bi-directional, giving the model knowledge of the entire context.
*   **Autoregressive (Causal Language Modeling - CLM):**
    *   These models, like GPT, consist of a decoder that generates text sequentially.
    *   CLM involves predicting the next word in a sequence based on the preceding words.
    *   This autoregressive nature makes CLM models well-suited for text generation tasks, as they can effectively autocomplete sentences.
*   **Sequence-to-Sequence (Span Corruption):**
    *   These models, like T5, use both an encoder and a decoder.
    *   Span corruption involves masking random sequences of input tokens and replacing them with a unique sentinel token.  The model is then trained to predict the masked tokens.
    *   Seq2Seq models need to understand the context provided by the encoder and generate text using the decoder.

<----------section---------->

**Masked Language Modeling (MLM)**

MLM is a pre-training technique used by models like BERT.

*   **Process:** The input text is modified by randomly masking some of the tokens. The masked tokens are replaced with a special "[MASK]" token.
*   **Training:** The modified text is fed into a Transformer encoder. The model's task is to predict the original masked tokens based on the surrounding context.
*   **Example:**
    *   Original text: "I love this red car."
    *   Modified text: "[CLS] I <mask> this red car" (where "<cls>" is a special classification token).
    *   The model must predict the masked token, "love".
*   **Loss Function:** The cross-entropy loss between the masked token and its prediction is minimized during pre-training.

<----------section---------->

**Next Token Prediction (NTP)**

NTP is a pre-training technique employed by autoregressive models like GPT.

*   **Process:** The model is trained to predict the next word in a sequence.
*   **Training Data:** Any text can be used for this task. The dataset consists of sequences of text.
*   **Task:** The model takes a sequence of words as input and predicts the next word in the sequence.

<----------section---------->

**Span Corruption**

Span corruption is a pre-training objective used by sequence-to-sequence models like T5.

*   **Process:** Random sequences of words are dropped out (masked) from the original text and replaced with a unique "sentinel" token (e.g., "<X>", "<Y>"). Words are dropped out independently and uniformly at random.
*   **Training:** The model is trained to predict the dropped-out text, delineated by the sentinel tokens.
*   **Example:**
    *   Original text: "Thank you for inviting me to your party last week."
    *   Input: "Thank you <X> me to your party <Y> week."
    *   Target: "<X> for inviting <Y> last <Z>"

<----------section---------->

**Summary on Pre-training**

*   **Flexibility:** Pre-training tasks can be designed flexibly, allowing for effective representations to be derived from a variety of pre-training regimes.
*   **Transferability:** Different NLP tasks appear to be highly transferable. The pre-training process produces general-purpose representations that can serve as the backbone for many specialized models.
*   **Self-Supervised Learning Power:** Through self-supervised learning, models can learn from generating language itself, rather than from specific tasks.
*   **Language Model as a Knowledge Base:** A generatively pre-trained model can possess a decent zero-shot performance on a range of NLP tasks, effectively acting as a knowledge base.

<----------section---------->

**Datasets and Data Pre-processing**

The quality and quantity of data used to train LLMs are critical factors that influence their performance.

**Datasets:**

*   **Importance of Data:** Training LLMs requires vast amounts of text data, and the quality of this data directly impacts the model's performance.
*   **Fundamental Understanding:** Pre-training on large-scale corpora provides LLMs with a fundamental understanding of language and some generative capability.
*   **Diverse Sources:** Pre-training data sources are diverse, commonly incorporating web text, conversational data, and books as general pre-training corpora.
*   **Enhanced Generalization:** Leveraging diverse sources of text data for LLM training can significantly enhance the model's generalization capabilities.

**Examples of Datasets Used in Popular LLMs:**

| LLMs    | Datasets                                                                                                                     |
| :-------- | :--------------------------------------------------------------------------------------------------------------------------- |
| GPT-3     | CommonCrawl, WebText2, Books1, Books2, Wikipedia                                                                            |
| LLaMA   | CommonCrawl, C4, Wikipedia, Github, Books, Arxiv, StackExchange                                                              |
| PaLM    | Social Media, Webpages, Books, Github, Wikipedia, News (total 780B tokens)                                                  |
| T5      | C4, WebText, Wikipedia, RealNews                                                                                             |
| CodeGen | the Pile, BIGQUERY, BIGPYTHON                                                                                                   |
| CodeGeex| CodeParrot, the Pile, Github                                                                                                    |
| GLM     | BooksCorpus, Wikipedia                                                                                                         |
| BLOOM   | ROOTS                                                                                                                          |
| OPT     | BookCorpus, CCNews, CC-Stories, the Pile, Pushshift.io                                                                       |

**Examples of Corpora Types and Links:**

| Corpora     | Type        | Links                                                                                          |
| :---------- | :---------- | :--------------------------------------------------------------------------------------------- |
| BookCorpus  | Books       | [https://github.com/soskek/bookcorpus](https://github.com/soskek/bookcorpus)                  |
| Gutenberg   | Books       | [https://www.gutenberg.org](https://www.gutenberg.org)                                       |
| Books1      | Books       | Not open source yet                                                                            |
| Books2      | Books       | Not open source yet                                                                            |
| CommonCrawl | CommonCrawl | [https://commoncrawl.org](https://commoncrawl.org)                                           |
| C4          | CommonCrawl | [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4) |
| CC-Stories  | CommonCrawl | Not open source yet                                                                            |
| CC-News     | CommonCrawl | [https://commoncrawl.org/blog/news-dataset-available](https://commoncrawl.org/blog/news-dataset-available) |
| RealNews    | CommonCrawl | [https://github.com/rowanz/grover/tree/master/realnews](https://github.com/rowanz/grover/tree/master/realnews) |
| RefinedWeb  | CommonCrawl | [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) |
| WebText     | Reddit Link | Not open source yet                                                                            |
| OpenWebtext | Reddit Link | [https://skylion007.github.io/OpenWebTextCorpus/](https://skylion007.github.io/OpenWebTextCorpus/) |
| PushShift.io| Reddit Link |                                                                                                |
| Wikipedia   | Wikipedia   | [https://dumps.wikimedia.org/zhwiki/latest/](https://dumps.wikimedia.org/zhwiki/latest/)         |

<----------section---------->

**Datasets - Books:**

*   **Popular Datasets:** BookCorpus and Gutenberg are two commonly used book datasets for LLM training.
*   **Content Diversity:** These datasets encompass a broad spectrum of literary genres, such as novels, essays, poetry, history, science, and philosophy.
*   **Contribution to Pre-training:** Widely employed by numerous LLMs, these datasets contribute to the models' pre-training by exposing them to a diverse array of textual genres and subject matter, fostering a more comprehensive understanding of language across various domains.
*   **Size of Book Corpus:** Book Corpus includes approximately 800 million words.

<----------section---------->

**Datasets - CommonCrawl:**

*   **Accessible Web Crawl Data:** CommonCrawl maintains an accessible repository of web crawl data, freely available for utilization by individuals and organizations.
*   **Vast Collection:** This repository encompasses a vast collection of data, comprising over 250 billion web pages accumulated over a span of 16 years.
*   **Continuously Expanding Resource:** This continuously expanding corpus is a dynamic resource, with an addition of 3–5 billion new web pages each month.
*   **Preprocessing Requirement:** However, due to the presence of a substantial amount of low-quality data in web archives, preprocessing is essential when working with CommonCrawl data.

<----------section---------->

**Datasets - Wikipedia:**

*   **High-Quality Encyclopedic Content:** Wikipedia, the free and open online encyclopedia project, hosts a vast repository of high-quality encyclopedic content spanning a wide array of topics.
*   **Extensive Use of English Wikipedia:** The English version of Wikipedia, including approximately 2,500 million words, is extensively utilized in the training of many LLMs, serving as a valuable resource for language understanding and generation tasks.
*   **Multilingual Support:** Additionally, Wikipedia is available in multiple languages, providing diverse language versions that can be leveraged for training in multilingual environments.

<----------section---------->

**Data pre-processing**

After collecting an adequate corpus of data, the next critical step is data pre-processing. The quality of this pre-processing directly impacts the model's performance and security.

*   **Importance of Quality:** Data preprocessing is vital to ensure that LLMs learn from clean, relevant, and unbiased information.
*   **Specific Pre-processing Steps:**
    *   **Quality Filtering:** Removing low-quality text is a crucial preprocessing step that enhances the reliability and accuracy of LLMs. This includes eliminating poorly formatted text, machine-generated content with little semantic value, and other types of noise that can negatively impact model training.
    *   **Toxicity and Bias Reduction:** Removing toxic and biased content is essential for ethical reasons, preventing the model from learning and perpetuating harmful stereotypes or offensive language.
    *   **Deduplication:** Eliminating duplicates in the training set is important for maintaining a balanced sample distribution. Duplicates can skew the model's learning and lead to overfitting.
    *   **Privacy Scrubbing:** Preventing information leakage by scrubbing potentially sensitive information. It also prevents other privacy-related concerns.

<----------section---------->

**Quality filtering**

*   **Methods:** Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods.
*   **Heuristic Methods:** Heuristic methods involve employing manually defined rules to eliminate low-quality data. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth.
*   **Classifier-Based Methods:** Classifier-based methods involve training a classifier on a high-quality dataset to filter out low-quality datasets.

<----------section---------->

**Deduplication**

*   **Problem of Repetitive Generation:** Language models may sometimes repetitively generate the same content during text generation, potentially due to a high degree of repetition in the training data.
*   **Impact on Training:** Extensive repetition can lead to training instability, resulting in a decline in the performance of LLMs.
*   **Dataset Contamination:** Additionally, it is crucial to consider avoiding dataset contamination by removing duplicated data present in both the training and test set.

<----------section---------->

**Privacy scrubbing**

*   **Privacy Concerns:** LLMs, as text-generating models, are trained on diverse datasets, which may pose privacy concerns and the risk of inadvertent information disclosure.
*   **Removal of Sensitive Information:** It is imperative to address privacy concerns by systematically removing any sensitive information. This involves employing techniques such as anonymization, redaction, or tokenization to eliminate personally identifiable details, geolocation, and confidential data.
*   **Upholding Privacy Standards:** By carefully scrubbing the dataset of such sensitive content, researchers and developers can ensure that the language models trained on these datasets uphold privacy standards and mitigate the risk of unintentional disclosure of private information.

<----------section---------->

**Filtering out toxic and biased text**

*   **Importance of Fairness:** In the preprocessing steps of language datasets, a critical consideration is the removal of toxic and biased content to ensure the development of fair and unbiased language models.
*   **Content Moderation Techniques:** This involves implementing robust content moderation techniques, such as employing sentiment analysis, hate speech detection, and bias identification algorithms.
*   **Systematic Identification and Removal:** By leveraging these tools, it is possible to systematically identify and filter out text that may perpetuate harmful stereotypes, offensive language, or biased viewpoints.

<----------section---------->

**Using LLMs after pre-training**

After pre-training, LLMs are typically adapted for specific tasks using two main approaches:

*   **Fine-tuning:**
    *   **Process:** Adjusting the weights of the pre-trained model using gradient descent to optimize performance on a specific downstream task. This involves training the model on a smaller, labeled dataset relevant to the target task.
    *   **Fine-tuning Granularity:**
        *   *Full network*: Fine-tuning all the parameters of the pre-trained model. This is computationally expensive but can yield the best results.
        *   *Readout heads*: Fine-tuning only the classification or regression layers added on top of the pre-trained model. This is more efficient but may not be as effective as fine-tuning the entire network.
        *   *Adapters*: Inserting small, task-specific modules into the pre-trained model and fine-tuning only these modules. This is a parameter-efficient approach that can preserve the original model's capabilities.
    *   **Impact:** Fine-tuning alters the model "itself," adapting it to the specific task at hand.
*   **Prompting:**
    *   **Process:** Designing special prompts that guide the LLM to perform specific tasks without changing the model's parameters. This involves crafting input text that elicits the desired behavior from the model.
    *   **Flexibility:** Different prompts can be used to make the model understand and perform different tasks.
    *   **Impact:** Prompting changes the "way to use" the LLM, rather than the model itself.

<----------section---------->

**Natural Language Processing and Large Language Models**

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
Lesson 12
HuggingFace
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This lesson provides an introduction to Natural Language Processing (NLP) with a focus on utilizing the Hugging Face ecosystem. Hugging Face is a prominent company and community that provides tools and resources for building, training, and deploying machine learning models, particularly in the field of NLP. This lecture is presented as part of the Master's Degree Course in Computer Engineering at the University of Salerno, delivered by Nicola Capuano and Antonio Greco from the DIEM (Department of Industrial Engineering and Mathematics).

<----------section---------->

**Outline**

This lesson will cover the following topics:

*   **Overview:** A general introduction to Hugging Face and its resources.
*   **Setup:** Instructions for setting up the necessary environment to work with Hugging Face tools.
*   **Pipeline:** An explanation of the Hugging Face `pipeline()` function and how to use it for basic NLP tasks.
*   **Model selection:** Guidance on choosing appropriate pre-trained models from the Hugging Face Model Hub.
*   **Common models:** A review of several widely-used NLP models, their licenses, and organizations.
*   **Gradio:** An introduction to Gradio, a tool for building interactive web demos for machine learning models.

<----------section---------->

**Overview: Hugging Face**

Hugging Face ([https://huggingface.co/](https://huggingface.co/)) is a central hub for NLP resources, providing access to pre-trained models, datasets, and tools that simplify the development and deployment of NLP applications. The Education Toolkit ([https://github.com/huggingface/education-toolkit](https://github.com/huggingface/education-toolkit)) is specifically designed to help educators and learners easily prepare workshops, events, homework, or classes related to NLP.

**Hugging Face - Education Toolkit**

The Education Toolkit offers self-contained content that can be easily incorporated into existing materials. It is free to use and relies on well-known open-source technologies such as `transformers` and `gradio`. In addition to tutorials, the toolkit provides resources for further exploration of Machine Learning (ML) and assistance in designing NLP-related content.

<----------section---------->

**Hugging Face Hub**

The Hugging Face Hub ([https://huggingface.co/](https://huggingface.co/)) is a central repository that hosts a variety of resources:

*   **Models:** Pre-trained models for various NLP tasks.
*   **Datasets:** Open-source datasets for training and evaluating models. The Hub hosts around 3000 datasets that are open-sourced and free to use in multiple domains.
*   **Spaces:** Platforms for hosting demos and code, allowing users to showcase their projects.

Key libraries within the Hugging Face ecosystem include:

*   `datasets`: Facilitates the downloading and management of datasets from the Hub. It allows the easy use of these datasets, including huge ones, using very convenient features such as streaming.
*   `transformers`: Enables working with pipelines, tokenizers, models, and other components necessary for NLP tasks.
*   `evaluate`: Provides tools for computing evaluation metrics to assess model performance.

These libraries are compatible with both PyTorch and TensorFlow, providing flexibility in choosing the preferred machine learning framework.

<----------section---------->

**Hugging Face – Model Hub**

The Model Hub ([https://huggingface.co/models](https://huggingface.co/models)) is a central repository for pre-trained models. Lysandre demonstrates how to navigate the Model Hub.

**Model Cards:** Similar to dataset repositories, models have a model card that documents important details such as the intended use, limitations, and training data.

<----------section---------->

**Hugging Face - Datasets**

The Hub ([https://hf.co/datasets](https://hf.co/datasets)) hosts open-sourced and free-to-use datasets across multiple domains. The open-source `datasets` library enables easy use of these datasets, including very large ones, with convenient streaming features. Each dataset has a dataset card documenting its summary, structure, and other relevant information.

Example: The GLUE dataset ([https://huggingface.co/datasets/nyu-mll/glue](https://huggingface.co/datasets/nyu-mll/glue)) is a popular benchmark for evaluating the performance of NLP models on various tasks.

<----------section---------->

**Setup**

This section details the necessary steps to set up the environment for working with Hugging Face tools.

**Setup – Google Colab**

Using a Google Colab notebook is the simplest way to get started. Colab provides a free, cloud-based environment with pre-installed libraries.

*   Install the `transformers` library:
    *   `!pip install transformers`
    *   This installs a minimal version of Transformers without specific machine learning frameworks like PyTorch or TensorFlow.
*   Import the library:
    *   `import transformers`

To install the development version with all dependencies:
*   `!pip install transformers[sentencepiece]`

<----------section---------->

**Setup – Virtual environment**

A virtual environment isolates project dependencies, preventing conflicts between different projects.

1.  **Download and install Anaconda:** [https://www.anaconda.com/download](https://www.anaconda.com/download)
2.  **Create a virtual environment:**
    *   `conda create --name nlpllm`
3.  **Activate the environment:**
    *   `conda activate nlpllm`
4.  **Install `transformers` with `sentencepiece`:**
    *   `conda install transformers[sentencepiece]`

<----------section---------->

**Setup – Create a Hugging Face account**

Many Hugging Face functionalities require an account. It is recommended to create one.

<----------section---------->

**Pipeline**

This section explains the `pipeline()` function in the Hugging Face Transformers library.

*   The Hugging Face Transformers library provides tools to create and use pre-trained models.
*   The Model Hub contains thousands of pre-trained models that can be downloaded and used.
*   The `pipeline()` function is the most basic object in the Hugging Face Transformers library. It’s a high-level abstraction that simplifies common NLP tasks by bundling a pre-trained model with its associated tokenizer and any necessary pre-processing/post-processing steps.
*   It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.

`pipeline()` function <=

<----------section---------->

**Model selection**

Choosing the right pre-trained model is critical for achieving optimal performance in NLP tasks. This section provides guidance on selecting appropriate models from the Hugging Face Hub, which contains over 176,620 models.

Consider a summarization task for an earthquake news article:

"A magnitude 6.7 earthquake rattled Papua New Guinea early Friday afternoon, according to the U.S. Geological Survey. The quake was centered about 200 miles north-northeast of Port Moresby and had a depth of 28 miles. No tsunami warning was issued..."

To find a suitable model on the Hugging Face Hub:

1.  **Filter by task:** Select the "Summarization" task, reducing the number of models to around 960.
2.  **Consider your needs:**
    *   **Extractive Summarization:** Selects representative pieces of text directly from the original article.
    *   **Abstractive Summarization:** Generates new, concise text that captures the main points of the original article.
3.  **Further Filtering:** You can further refine your model selection based on criteria such as license, language, and other specific requirements.

Hugging Face * Models Datasets Spaces Dxs © Solutions
t Tasks | libraries futesets Languages Licenses Models
bert-base-uncased

Footure Extraction Taxtto-image ® jonatasgrosman/wav2vec2-large-

Filter by model size
(for limits on hardware, cost, or latency)
pytorch_model.bin

<----------section---------->

**Model selection**

Additional factors to consider when selecting a model:

1.  **Sort by popularity and updates:**
    *   **Most Downloads:** Indicates widely-used and potentially well-supported models.
    *   **Recently Updated:** Suggests active maintenance and improvements.
    *   **Most Likes:** Reflects community approval and satisfaction.
2.  **Check Git release history:** Examining the GitHub repository (e.g., github.com/google-research/bert/blob/master/README.md) provides insights into the model's development and updates.

BERT
***** New March 11th, 2020; Smailler BERT Models *****
This is a release of 24 smaller BERT models (Engli

3.  **Pick good variants of models for your task:**
    *   Different sizes of the same base model.
    *   Fine-tuned variants of base models.

Models 5,564 few Fulltext search

t5-base
Updated 11 days ago

t5-small
Updated 11 days ago

§ prithivida/parrot_paraphrasezr_on_T5
Updated May 18, 2021 545k a7

<----------section---------->

**Model selection**

Other considerations:

*   **Search for examples and datasets:** Look for resources that demonstrate the model's performance and suitability for your specific task.
*   **Model specialization:** Determine whether the model is generally capable or fine-tuned for a specific task.
*   **Training data:** Identify the datasets used for pre-training and/or fine-tuning, as this influences the model's bias and performance.

Ultimately, model selection should be data and user-driven:

*   **Define KPIs:** Establish key performance indicators to measure the model's effectiveness.
*   **Test on your data or users:** Evaluate the model's performance on your specific dataset and with your target users to ensure it meets your requirements.

<----------section---------->

**Common models**

The following table provides an overview of common NLP models, their licenses, organizations, years of release, and descriptions.

| Model           | License        | Organization | Year | Description                                                                                                                            |
| :---------------- | :------------- | :------------- | :--- | :------------------------------------------------------------------------------------------------------------------------------------- |
| Pythia 19M-12B    | Apache 2.0     | EleutherAI   | 2023 | Series of 8 models for comparisons across sizes.                                                                                        |
| Dolly 12B       | MIT            | Databricks   | 2023 | Instruction-tuned Pythia model.                                                                                                          |
| GPT-3.5 175B      | Proprietary    | OpenAI       | 2022 | ChatGPT model option; related models GPT-1/2/3/4.                                                                                        |
| OPT 125M-175B     | MIT            | Meta         | 2022 | Based on GPT-3 architecture.                                                                                                               |
| BLOOM 560M - 176B | RAIL v1.0       | Many groups  | 2022 | 46 languages.                                                                                                                          |
| GPT-Neo/X 125M-20B | MIT / Apache 2.0 | EleutherAI   | 2021 / 2022 | Based on GPT-2 architecture.                                                                                                               |
| FLAN 80M-540B     | Apache 2.0     | Google       | 2021 | Methods to improve training for existing architectures.                                                                                    |
| BART 139M-406M    | Apache 2.0     | Meta         | 2019 | Derived from BERT, GPT, others.                                                                                                        |
| T5 50M-TIB        | Apache 2.0     | Google       | 2019 | 4 languages.                                                                                                                             |
| BERT            | Apache 2.0     | Google       | 2018 | Early breakthrough.                                                                                                                    |

<----------section---------->

**Gradio**

Gradio is a Python library that allows you to create customizable UI components and interfaces for your machine learning models, facilitating easy sharing and testing.

**Gradio – Demos on the web**

Gradio simplifies the process of deploying machine learning models for demonstration purposes. It covers steps for training, containerization, front-end building and sample storing. The following table summarizes the typical steps.

| Step                                  | Framework          | Language |
| :------------------------------------ | :----------------- | :------- |
| 1. Train a model                      | TensorFlow/PyTorch | Python   |
| 2. Containerize and deploy the model |                    | Python   |
| 3. Store incoming samples             | Gradio             | Python   |
| 4. Build an interactive front-end     |                    | Python   |

<----------section---------->

**Gradio – Demos on the web**

Example: News Summarizer

A news summarizer app demonstrates how Hugging Face models can summarize articles. It uses the `bart-large-cnn` model by Facebook. The demo displays an URL input and a summarization output. Shorter articles generate faster summaries.

URL
Screenshot Flag
Clear Submit
Examples
[https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/](https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/)
[https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/](https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/)
[https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/](https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/)

<----------section---------->

**Gradio – Free hosting on hf.space**

Hugging Face Spaces provides free hosting for Gradio demos, allowing community members to share their ML applications.

Hugging Face

Discover amazing ML apps made by the community!

Create new Space
Models Datasets Spaces Docs

<----------section---------->

**Gradio – Build your demo**

To build your own Gradio demo:

1.  Install Gradio: `conda install gradio`
2.  Refer to the documentation: [https://bit.ly/34wESgd](https://bit.ly/34wESgd)

<----------section---------->

# Natural Language Processing and Large Language Models

This material is part of **Lesson 13: Encoder-only Transformers** of the **Corso di Laurea Magistrale in Ingegneria Informatica** (Master's Degree Course in Computer Engineering).

Presented by: Nicola Capuano and Antonio Greco

From: DIEM – University of Salerno

This lesson focuses on encoder-only transformer models, with a deep dive into BERT, and practical exercises on token classification and named entity recognition.

<----------section---------->

## Outline

This lesson will cover the following topics:

*   **Encoder-only transformer**: Explanation of when and how to use only the encoder part of the Transformer architecture.
*   **BERT**: Detailed overview of BERT, including its architecture, pre-training, fine-tuning, and usage.
*   **Practice on token classification and named entity recognition**: Practical exercises applying BERT to token classification and named entity recognition tasks.

<----------section---------->

## Encoder-only Transformer

The complete Transformer architecture, with both encoder and decoder components, is necessary when the task requires transforming a sequence into another sequence of a different length.

*   **Sequence-to-Sequence Transformation**: Tasks like language translation, where the output sequence (translated sentence) can have a different number of words compared to the input sequence, necessitate the full Transformer architecture. The original application of the Transformer model was indeed translation between different languages, highlighting its suitability for such tasks.

When the task involves transforming a sequence into another sequence of the *same* length, or into a single value, then only the Encoder part of the Transformer can be efficiently utilized.

*   **Sequence-to-Sequence (Same Length)**: For tasks where the input and output sequences have the same length, you can use just the Encoder part of the Transformer.  In this scenario, the encoder processes the input sequence to produce output vectors $z_1, \dots, z_t$. These vectors serve as the final output upon which the loss function is directly computed.

*   **Sequence-to-Value**: For tasks like sequence classification, where the goal is to transform an input sequence into a single representative value, again you can use just the Encoder part of the Transformer. A typical example would be sentiment analysis.

    *   **Special Start Token**:  A special token, often denoted as `[CLS]` (Classification Token) or START, is added as the first element ($x_1$) of the input sequence. This special token is critical for capturing the overall context of the sequence.
    *   **Classification via First Token**: The corresponding output value $z_1$ associated with the start token is then used as the result of the transformation, and the loss function is computed based solely on this value. This concentrates the sequence's overall representation into a single vector, simplifying the classification task.

<----------section---------->

## BERT

BERT stands for Bidirectional Encoder Representations from Transformers. Introduced by Google Research in 2018, BERT is a language model primarily designed for language understanding tasks. It leverages the power of the Transformer architecture but uses only the Encoder part.

*   **Architecture**:
    *   **BERT-base**: Consists of 12 stacked encoder blocks and contains approximately 110 million parameters.
    *   **BERT-large**: Features 24 stacked encoder blocks and includes approximately 340 million parameters.

*   **Bidirectional Context**: BERT is pre-trained to understand words in a sentence using bidirectional context. This means it considers both preceding and succeeding words to better understand the meaning of a word within the sentence. This differs from earlier models that often only considered unidirectional context (either only previous or only subsequent words).

*   **Pre-trained Base**: BERT is designed to serve as a pre-trained base for various Natural Language Processing tasks. This pre-trained model can then be fine-tuned on specific datasets tailored to different NLP tasks, greatly reducing the amount of task-specific training data required.

<----------section---------->

## BERT Input Encoding

BERT's input encoding process involves tokenizing the text and preparing it in a format suitable for the model.

*   **WordPiece Tokenizer**: BERT employs a WordPiece tokenizer as its tokenization method.

    *   **Subword-Based**: WordPiece tokenizes text at the subword level instead of using entire words or individual characters. This approach allows BERT to effectively handle both common words and rarer or misspelled words by breaking them into smaller, more manageable parts.
    *   **Vocabulary Building**: The WordPiece tokenizer builds a vocabulary of common words and subword units (e.g., "playing" might be tokenized as "play" and "##ing").
    *   **Handling Unknown Words**: Rare or out-of-vocabulary words are broken down into known subwords. For instance, "unhappiness" might be split into "un," "happy," and "##ness." This ensures that the model can process virtually any input text, even if it contains unfamiliar words.

<----------section---------->

## BERT Input Encoding (Continued)

The input encoding involves several steps to prepare the text for BERT.

*   **Splitting**: Sentences are split into tokens based on whitespace, punctuation, and common prefixes (like "##").
*   **Special Tokens**: BERT relies on special tokens to understand the structure and context of the input sequence.

    *   `[CLS]`: A classification token added at the beginning of each input sequence. Its final hidden state is used for classification tasks.
    *   `[SEP]`: A separator token used to mark the end of a sentence or to separate two sentences in sentence-pair tasks.

*   **Tokenization Example**: For example, the sentence "I’m feeling fantastic!" might be tokenized by WordPiece as: `[CLS] I ' m feeling fan ##tas ##tic ! [SEP]`
*   **Converting Tokens to IDs**: After tokenization, each token is mapped to an ID from BERT’s vocabulary. These IDs serve as the numerical input to the model, enabling it to process the text.

<----------section---------->

## Advantages of WordPiece Embedding

The WordPiece embedding method offers several advantages that contribute to BERT's effectiveness.

*   **Efficiency**: It reduces vocabulary size without compromising the model's ability to represent a diverse range of language constructs. A smaller vocabulary reduces the memory footprint and computational complexity.
*   **Handling Unseen Words**: Subword tokenization allows BERT to manage rare or newly created words by breaking them down into recognizable parts. This is especially important for handling evolving language and specialized vocabularies.
*   **Improved Language Understanding**: By learning useful subword components during pretraining, BERT captures complex linguistic patterns. This allows the model to generalize better and understand nuanced relationships between words.

<----------section---------->

## BERT `[CLS]` Token

In BERT, the `[CLS]` token plays a crucial role by acting as a summary representation of the entire input sequence.

*   **Position**: The `[CLS]` token (short for "classification token") is always placed at the very start of the tokenized sequence.
*   **Purpose**: The primary purpose of the `[CLS]` token is to aggregate and represent the overall contextual information of the input sequence.
*   **Final Hidden State**: After the input is processed by BERT, the final hidden state of the `[CLS]` token serves as a condensed, context-aware embedding for the whole sentence or sequence of sentences. This vector encapsulates the high-level meaning of the input.
*   **Input to Classifier**: This embedding is then fed into additional layers, such as a classifier, for specific tasks like sentiment analysis, question answering, or text classification.

<----------section---------->

## BERT `[CLS]` Token (Continued)

The `[CLS]` token is used differently based on the specific task:

*   **Single-Sentence Classification**
    *   The final hidden state of the `[CLS]` token is directly passed to a classifier layer to make predictions about the entire sentence.
    *   For instance, in sentiment analysis, the classifier might predict "positive" or "negative" sentiment based on the `[CLS]` embedding.
*   **Sentence-Pair Tasks**
    *   For tasks involving two sentences, BERT tokenizes them as `[CLS] Sentence A [SEP] Sentence B [SEP]`.  The `[SEP]` token is used to separate the two sentences.
    *   The `[CLS]` token’s final hidden state captures the relationship between the two sentences, making it suitable for tasks like entailment detection (determining if one sentence logically follows from another) or similarity scoring (measuring how similar two sentences are in meaning).

<----------section---------->

## BERT Pre-training

BERT's pre-training phase uses two self-supervised learning strategies to learn general language representations from large amounts of unlabeled text.

*   **Masked Language Modeling (MLM)**

    *   **Objective**: To predict masked (hidden) tokens within a sentence.
    *   **Process**: Randomly mask 15% of the tokens in the input, and then train BERT to predict these masked tokens based on the surrounding context.
    *   **Benefit**: MLM enables BERT to learn bidirectional context, as it must consider both preceding and succeeding words to make predictions.

*   **Next Sentence Prediction (NSP)**

    *   **Objective**: To determine if one sentence logically follows another.
    *   **Process**: Trains BERT to understand sentence-level relationships by predicting whether a given sentence B follows sentence A in the original text. Pairs of sentences are labeled as "IsNext" or "NotNext."
    *   **Benefit**: NSP enhances performance in tasks like question answering and natural language inference, where understanding inter-sentence relationships is crucial.
*   **Training Data**: BERT is trained on a large corpus consisting of publicly available books and the English Wikipedia, totaling more than 3 billion words. This vast amount of training data helps BERT learn robust language representations.

<----------section---------->

## BERT Fine-tuning

Fine-tuning involves adapting the pre-trained BERT model to specific downstream tasks.

*   **Additional Layer**: The output of the encoder is fed into an additional layer (or layers) designed to solve a specific problem. This can be a classification layer, a sequence tagging layer, or any other task-specific architecture.
*   **Loss Minimization**: The cross-entropy loss (or another appropriate loss function) between the prediction and the true label for the classification task is minimized using gradient-based algorithms.
*   **Training from Scratch vs. Fine-tuning**: While the additional layer is typically trained from scratch, the pre-trained parameters of BERT may or may not be updated during fine-tuning, depending on the task and dataset size. Updating BERT's parameters can further improve performance but requires more computational resources.

<----------section---------->

## BERT Fine-tuning (Continued)

BERT's fine-tuning process is highly adaptable and effective for a variety of NLP tasks.

*   **Task-Specific Training**: BERT is pre-trained on general language data and then fine-tuned on specific datasets tailored to each task.
*   **Refining the `[CLS]` Embedding**: In fine-tuning, the `[CLS]` token’s final embedding is specifically trained for the downstream task, refining its ability to represent the input sequence in the way needed for that task.
*   **Example Tasks**:
    *   **Text Classification**: Sentiment analysis, spam detection.
    *   **Named Entity Recognition (NER)**: Identifying names, dates, organizations, etc., within text.
    *   **Question Answering**: Extractive QA, where BERT locates answers within a passage.

*   **Minimal Adjustments**: Fine-tuning typically requires only a few additional layers per task, making it computationally efficient to adapt BERT to new applications.

<----------section---------->

## BERT Strengths and Limitations

BERT's widespread adoption stems from its numerous strengths, though it also has some limitations.

*   **Strengths**
    *   **Bidirectional Contextual Understanding**: Provides richer and more accurate representations of language by considering both preceding and succeeding words.
    *   **Flexibility in Transfer Learning**: BERT’s pretraining allows for easy adaptation to diverse NLP tasks, making it a powerful tool for transfer learning.
    *   **High Performance on Benchmark Datasets**: Consistently ranks at or near the top on datasets like SQuAD (QA) and GLUE (general language understanding), showcasing its effectiveness.
*   **Limitations**
    *   **Large Model Size**: High computational and memory requirements can make deployment challenging, particularly for resource-constrained environments.
    *   **Pretraining Costs**: Requires extensive computational resources, especially for training BERT-Large from scratch.
    *   **Fine-Tuning Time and Data**: Fine-tuning on new tasks requires labeled data and can still be time-intensive, depending on the size and complexity of the task.

<----------section---------->

## Popular BERT Variants - RoBERTa

RoBERTa (Robustly Optimized BERT Approach) was developed by Facebook AI in 2019 as an improvement over BERT.

*   **Main Differences**:
    *   **Larger Training Corpus**: RoBERTa is trained on significantly more data (200 billion words) compared to BERT.
    *   **Removed Next Sentence Prediction (NSP)**: Removing NSP was found to improve performance in many tasks.
    *   **Longer Training and Larger Batches**: RoBERTa was trained for more iterations and used larger batches, leading to more robust language modeling.
    *   **Dynamic Masking**: Masking is applied dynamically (i.e., different masks per epoch), leading to better generalization.
*   **Performance**: RoBERTa consistently outperforms BERT on various NLP benchmarks, particularly in tasks requiring nuanced language understanding.

<----------section---------->

## Popular BERT Variants - ALBERT

ALBERT (A Lite BERT) was developed by Google Research in 2019 as a more parameter-efficient version of BERT.

*   **Main Differences**:
    *   **Parameter Reduction**: Uses factorized embedding parameterization to reduce model size. Factorized embedding parameterization decomposes the large vocabulary embedding matrix into two smaller matrices, reducing the total number of parameters.
    *   **Cross-Layer Parameter Sharing**: Shares weights across layers to decrease the number of parameters. Instead of each layer having unique weights, some or all layers share the same weights.
    *   **Sentence Order Prediction (SOP)**: Replaces NSP with SOP, which is better suited for capturing inter-sentence coherence. Instead of predicting whether a sentence is the next sentence, SOP predicts whether two consecutive sentences are in the correct order.
*   **Performance**: ALBERT achieves comparable results to BERT-Large with fewer parameters, making it more memory-efficient.
*   **Advantages**: ALBERT is faster and lighter, making it ideal for applications where resources are limited.

<----------section---------->

## Popular BERT Variants - DistilBERT

DistilBERT (Distilled BERT) was developed by Hugging Face as a smaller, faster version of BERT.

*   **Main Differences**:
    *   **Model Distillation**: Uses knowledge distillation to reduce BERT’s size by about 40% while retaining 97% of its language understanding capabilities. Knowledge distillation involves training a smaller "student" model to mimic the behavior of a larger, pre-trained "teacher" model.
    *   **Fewer Layers**: DistilBERT has 6 layers instead of 12 (for BERT-Base) but is optimized to perform similarly.
    *   **Faster Inference**: Provides faster inference and lower memory usage, making it ideal for real-time applications.
*   **Advantages**: DistilBERT is widely used for lightweight applications that need a smaller and faster model without major accuracy trade-offs.

<----------section---------->

## Popular BERT Variants - TinyBERT

TinyBERT was developed by Huawei as an even smaller and faster version of BERT than DistilBERT.

*   **Main Differences**:
    *   **Two-Step Knowledge Distillation**: Distills BERT both during pretraining and fine-tuning, further enhancing efficiency.
    *   **Smaller and Faster**: TinyBERT is even smaller than DistilBERT, optimized for mobile and edge devices.
    *   **Similar Accuracy**: Maintains accuracy close to that of BERT on various NLP tasks, especially when fine-tuned with task-specific data.
*   **Advantages**: TinyBERT is an ultra-compact version of BERT that is well-suited for resource-constrained environments.

<----------section---------->

## Popular BERT Variants - ELECTRA

ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) was developed by Google Research with a different pre-training approach.

*   **Main Differences**:
    *   **Replaced Token Detection**: Instead of masked language modeling, ELECTRA uses a generator-discriminator setup where the model learns to identify replaced tokens in text. A generator model replaces some tokens in the input sequence, and the discriminator model tries to identify which tokens have been replaced.
    *   **Efficient Pretraining**: This approach allows ELECTRA to learn with fewer resources and converge faster than BERT.
    *   **Higher Performance**: Often outperforms BERT on language understanding benchmarks with significantly less compute power.
*   **Advantages**: ELECTRA’s training efficiency and robust performance make it appealing for applications where computational resources are limited.

<----------section---------->

## Popular BERT Variants - SciBERT

SciBERT is tailored for applications in scientific literature, making it ideal for academic and research-oriented NLP.

*   **Domain-Specific Pretraining**: Trained on a large corpus of scientific papers from domains like biomedical and computer science.
*   **Vocabulary Tailored to Science**: Uses a vocabulary that better represents scientific terms and jargon.
*   **Improved Performance on Scientific NLP Tasks**: Significantly outperforms BERT on tasks like scientific text classification, NER, and relation extraction in scientific documents.

<----------section---------->

## Popular BERT Variants - BioBERT

BioBERT is widely adopted in the biomedical research field, aiding in information extraction and discovery from medical literature.

*   **Biomedical Corpus**: Pretrained on a biomedical text corpus, including PubMed abstracts and PMC full-text articles.
*   **Enhanced Performance on Biomedical Tasks**: Excels at biomedical-specific tasks such as medical NER, relation extraction, and question answering in healthcare and biomedical domains.

<----------section---------->

## Popular BERT Variants - ClinicalBERT

ClinicalBERT is ideal for hospitals and healthcare providers who need to analyze patient records, predict health outcomes, or assist in clinical decision-making.

*   **Healthcare Focus**: Tailored for processing clinical notes and healthcare-related NLP tasks.
*   **Training on MIMIC-III Dataset**: Pretrained on the MIMIC-III database of clinical records, making it useful for healthcare analytics. The MIMIC-III (Medical Information Mart for Intensive Care) database contains de-identified health data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.

<----------section---------->

## Popular BERT Variants - mBERT

mBERT, developed by Google, supports NLP tasks across languages, enabling global applications and language transfer learning.

*   **Multilingual Support**: Trained on 104 languages, mBERT can handle multilingual text without requiring separate models for each language.
*   **Language-Agnostic Representation**: Capable of zero-shot cross-lingual transfer, making it suitable for translation and cross-lingual understanding tasks. This means it can perform well on tasks in a language it has never explicitly been trained on, by leveraging its understanding of other languages.

<----------section---------->

## Other BERT Variants

*   **CamemBERT**: A French language-focused BERT model.
*   **FinBERT**: Optimized for financial text analysis.
*   **LegalBERT**: Trained on legal documents for better performance in the legal domain.

Furthermore, BERT has inspired Transformer pre-training in computer vision, leading to models like vision Transformers, Swin Transformers, and Masked Auto Encoders (MAE).

<----------section---------->

## Practice on Token Classification and Named Entity Recognition

Engage in hands-on practice by leveraging existing BERT versions for named entity recognition, referring to the Hugging Face tutorial on token classification: [https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt)

*   **Testing with Prompts and Public Datasets**: Evaluate the models with your own prompts and publicly available datasets, such as the CoNLL-2003 dataset ([https://huggingface.co/datasets/eriktks/conll2003](https://huggingface.co/datasets/eriktks/conll2003)).
*   **Fine-Tuning Lightweight Versions**: If time and computational resources permit, fine-tune one of the lightweight versions of BERT following this guide: [https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#fine-tuning-the-model-with-the-trainer-api](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt).

<----------section---------->

# Natural Language Processing and Large Language Models
Course for Master's Degree in Computer Engineering
Lesson 14
Decoder-only Transformers
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

<----------section---------->

## Outline
This lesson covers the following topics:
*   **Decoder-only Transformer:** An overview of this specific type of transformer architecture.
*   **GPT (Generative Pre-trained Transformer):** Exploration of the GPT series of models.
*   **LLAMA (Large Language Model Meta AI):** Examination of the LLAMA family of large language models.
*   **Practice on Text Generation:** Hands-on exercises for generating text using different models.

<----------section---------->

## Decoder-only Transformer
*   Decoder-only transformers are a variant of the Transformer architecture that utilizes only the decoder component. Unlike traditional sequence-to-sequence models, which employ both an encoder and a decoder, decoder-only transformers focus solely on the decoding aspect.
*   These models are specifically designed and optimized for autoregressive generation tasks. Autoregressive generation involves generating sequences token by token, where the generation of each new token depends on the tokens generated previously. This makes them particularly efficient for tasks like language modeling.
*   Due to the absence of separate encoder layers, decoder-only transformers process the input context and generate output in a single, continuous pass. This is in contrast to sequence-to-sequence models that first encode the input and then decode it.
*   Decoder-only transformers are widely used for various language generation tasks, including:
    *   **Text Generation:** Creating new textual content, such as articles, stories, or creative pieces.
    *   **Summarization:** Condensing longer documents into shorter, coherent summaries.
    *   **Question Answering:** Generating answers to questions based on the input context.
*   Notable examples of decoder-only transformers include the GPT series (GPT-1, GPT-2, GPT-3, and GPT-4) and LLAMA models. These models have demonstrated remarkable capabilities in understanding and generating human-like text.

<----------section---------->

## Decoder-only Transformer: Autoregressive Approach
*   Text generation in decoder-only transformers follows an autoregressive approach. Each token in the output sequence is generated sequentially, conditioned only on the previously generated tokens within the same sequence. This contrasts with encoder-decoder models, which utilize attention mechanisms to consider the entire input sequence at each decoding step.
*   The input context (prompt) and the generated text are treated as one continuous sequence. This unified approach allows the model to handle both the "encoding" (understanding the input prompt) and "decoding" (generating text) within a single processing step. As a result, a separate encoder block is not required.
*   The generation process is iterative. Once a token is generated, it is appended to the input sequence, and the model then uses this extended sequence as context to generate the next token. This process continues until the desired length is reached or a specific stopping criterion is met.

<----------section---------->

## Decoder-only Transformer: Self-Attention and Context Understanding
*   **Self-Attention with Causal Masking:** Decoder-only transformers employ self-attention mechanisms within their decoder layers. However, to maintain the autoregressive property, a causal (unidirectional) mask is applied to the attention weights. This mask prevents each token from attending to future tokens in the sequence. Consequently, the model can only consider information from previous positions, simulating the sequential generation process.
*   **Implicit Context Understanding:** Decoder-only architectures build context sequentially as they process the input tokens. As the model iterates through the sequence, it "remembers" previous tokens and learns the relationships between them using the attention layers. This sequential accumulation of context eliminates the need for separate encoder-decoder attention, allowing the model to develop an understanding of the input as it progresses through each token.

<----------section---------->

## Encoder-only vs. Decoder-only Transformers

| Feature            | Encoder-Only Transformers (e.g., BERT)                                   | Decoder-Only Transformers (e.g., GPT)                                |
| ------------------ | ------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Architecture       | Only encoder blocks (bidirectional attention)                               | Only decoder blocks (causal attention)                               |
| Training Objective | Masked Language Modeling (MLM)                                           | Autoregressive Language Modeling                                     |
| Context            | Processes the entire sequence in parallel                                  | Processes tokens sequentially (one by one)                           |
| Main Use Cases     | Text classification, Named Entity Recognition (NER), question answering     | Text generation, story generation, code generation                    |
| Attention Type     | Bidirectional self-attention                                              | Unidirectional (masked) self-attention                               |
| Output             | Contextual embeddings for downstream tasks                                | Sequential token generation (text or other content)                   |

<----------section---------->

## Decoder-only Transformer Applications
Applications of Decoder-Only Transformers include:
*   **Text Generation:** Generating various forms of textual content, such as news articles, stories, and creative text.
*   **Conversational AI:** Developing chatbots and virtual assistants capable of engaging in real-time dialogue.
*   **Programming Help:** Assisting with code generation, debugging, and providing programming-related explanations.
*   **Summarization:** Creating concise summaries of long documents, making information more accessible and manageable.

<----------section---------->

## GPT (Generative Pre-trained Transformer)
*   GPT is a decoder-only transformer architecture developed by OpenAI. Its primary function is to generate human-like text by understanding and predicting language patterns.
*   GPT models are trained on massive amounts of text data. This extensive training allows them to perform a wide range of natural language tasks without requiring task-specific training data or fine-tuning. This ability to generalize across tasks is one of the defining features of GPT models.
*   **GPT-1 (2018):** The first model in the GPT series, GPT-1 introduced the decoder-only transformer architecture. It comprised 117 million parameters and consisted of 12 decoder blocks, 768-dimensional embeddings, and 12 attention heads per block.
*   **GPT-2 (2019):** GPT-2 was a significant upgrade in terms of size and capabilities. Its XL version contained 1.5 billion parameters, organized into 48 decoder blocks, 1600-dimensional embeddings, and 25 attention heads per block. GPT-2 demonstrated the ability to generate coherent, long-form text, marking a substantial step forward in language generation.
*   **GPT-3 (2020):** GPT-3 was even larger, with 175 billion parameters, distributed across 96 decoder blocks, 12,288-dimensional embeddings, and 96 attention heads per block. GPT-3 exhibited advanced capabilities in language understanding, code generation, and even reasoning. Its performance underscored the benefits of scaling up model size.
*   **GPT-4 (2023):** GPT-4 is a multi-modal model capable of processing both images and text. It features improved reasoning abilities and a broader base of general knowledge. Although detailed information about its architecture is not yet publicly available, GPT-4 represents a further advancement in the capabilities of large language models.

<----------section---------->

## GPT Input Encoding
*   GPT models, from GPT-1 to the latest versions, utilize Byte-Pair Encoding (BPE) as their primary input encoding method.
*   BPE is a subword tokenization technique that strikes a balance between word-level and character-level representations. It breaks down words into smaller, meaningful subunits (tokens) based on the frequency of those subunits in the training data. This approach is effective in handling both frequent and rare words.
*   The vocabulary size varies depending on the model version. For example, GPT-2 uses a vocabulary of approximately 50,000 tokens. This allows the model to efficiently represent a wide range of textual content.

<----------section---------->

## GPT Input Encoding: Key Features
The main features of BPE are:
*   **Subword Tokenization:** BPE splits rare or complex words into subword units while keeping common words as single tokens. For instance, a rare word like "unhappiness" might be split into "un," "happi," and "ness."
*   **Fixed Vocabulary:** BPE produces a fixed-size vocabulary (e.g., around 50,000 tokens in GPT-2), containing common words, word fragments, and some single characters. This helps the model handle a wide range of text efficiently.
*   **Efficiency in Language Representation:** Subword tokens allow GPT to represent a diverse range of language patterns, handling both common and rare words effectively while reducing the total number of tokens required.

<----------section---------->

## GPT Input Encoding: Advantages of BPE
The main advantages of BPE (similar to WordPiece) are:
*   **Flexibility:** Handles languages with rich morphology or new words (e.g., "AI-generated") by breaking them down into reusable subword tokens.
*   **Reduced Vocabulary Size:** Keeps the vocabulary smaller and training more efficient compared to a word-level tokenizer.
*   **Out-of-Vocabulary Handling:** BPE is resilient to unknown words, as it can break down any new word into familiar subwords or characters.

<----------section---------->

## GPT Pre-training
*   GPT is pre-trained using an autoregressive language modeling objective: predicting the next word (or token) in a sequence, given all the previous tokens.
*   **Next-Token Prediction Strategy:** At each step, the GPT model learns to minimize the difference between its predicted next token and the actual next token in the training sequence, effectively learning context and word relationships.
*   The prediction is sequential, meaning each token is predicted based only on previous tokens, so the model learns the patterns of language in a left-to-right order. This constraint forces the model to learn sequential dependencies and context effectively.

<----------section---------->

## GPT Pre-training: Datasets
*   GPT models are trained on massive and diverse datasets sourced from a wide array of internet text. The scale and diversity of the training data are crucial for the model's ability to generalize and perform well on a variety of tasks.
*   **GPT-1:** Trained on BookCorpus, consisting of around 985 million words (800 MB of text).
*   **GPT-2:** Trained on WebText (40 GB of text from around 8 million documents with 10 billion words), a dataset curated from high-quality web pages by OpenAI.
*   **GPT-3:** Used even larger datasets (570 GB of text with hundreds of billions of words), combining sources like Common Crawl, Books, Wikipedia, and more.
*   In all cases, the data is selected to cover a broad range of topics and linguistic structures to make the model versatile across different domains.
*   OpenAI's training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days. This massive computational effort underscores the resources required to train state-of-the-art large language models.

<----------section---------->

## GPT Pre-training: Optimization
*   GPT minimizes the cross-entropy loss between the predicted token probabilities and the actual tokens. This loss function is well-suited to classification tasks (like predicting the next token) and provides the model with feedback on its predictions.
*   GPT uses the Adam optimizer, an adaptive gradient descent technique, which helps accelerate convergence by adjusting learning rates based on past gradients.
*   GPT applies a learning rate scheduling in which learning rates are gradually increased (warm-up) in the early stages and then decayed to prevent instability during training. This approach helps the model to converge more effectively.
*   Large batch sizes are used to stabilize training and make the model better at generalizing across diverse language patterns.

<----------section---------->

## GPT Fine-tuning
*   Fine-tuning of GPT requires a dataset labeled for the specific task. The dataset typically consists of pairs of prompts and expected responses, or inputs and target outputs. This allows the model to adapt its pre-trained knowledge to a specific application.
*   Examples of tasks for which GPT has been or may be fine-tuned include:
    *   **Customer Support Automation:** Handling customer queries, resolving issues, and providing information.
    *   **Medical Assistance:** Offering health guidance and support, and addressing patient inquiries.
    *   **Legal Document Processing:** Summarizing legal documents and supporting legal research.
    *   **Coding Assistance:** Providing code snippets, explanations, and debugging help.
    *   **Educational Tutoring:** Answering questions, explaining concepts, and supporting e-learning.
    *   **Content Creation:** Generating blog posts, social media content, and marketing copy.
    *   **Virtual Personal Assistants:** Providing reminders, managing tasks, and answering questions.

<----------section---------->

## GPT Strengths
*   **Language Fluency and Coherence:** GPT models generate human-like, fluent, and coherent text, often indistinguishable from human writing.
*   **Broad Knowledge Base:** Trained on vast datasets, GPT has extensive general knowledge across a wide array of topics, allowing it to answer questions and generate content in diverse domains.
*   **Few-Shot and Zero-Shot Learning:** GPT can perform tasks with little to no task-specific training by learning from examples in the prompt (few-shot) or adapting to a task without examples (zero-shot).
*   **Creative and Contextual Writing:** GPT can generate creative content, including stories, poetry, and dialogues, which makes it useful for content creation and entertainment applications.
*   **Rapid Adaptation with Fine-Tuning:** Fine-tuning on task-specific data allows GPT to perform well in specialized contexts, such as technical writing, legal assistance, and customer service.
*   **Scalability with Large Models:** Larger GPT models demonstrate stronger performance and generalization, especially on complex or nuanced tasks.

<----------section---------->

## GPT Limitations
*   **Lack of True Understanding:** GPT models generate text based on patterns rather than true comprehension. They can produce grammatically correct and contextually relevant text without genuinely understanding the underlying meaning or concepts.
*   **Sensitivity to Prompting:** GPT's responses can vary widely based on phrasing. Small changes in prompts may yield different outcomes.
*   **Ethical and Bias Concerns:** GPT can reproduce biases present in its training data, leading to biased or inappropriate outputs, especially if prompts or fine-tuning data lack diversity or sensitivity.
*   **Inability to Reason or Perform Complex Calculations:** GPT is limited in logical reasoning, advanced mathematics, or tasks requiring step-by-step problem-solving without explicit instruction in the prompt.
*   **High Computational Requirements:** Large GPT models require significant computational power for training, fine-tuning, and deployment, making them expensive to run and maintain.
*   **Limited Memory Across Interactions:** GPT lacks persistent memory across sessions, so it cannot retain information shared by users in previous interactions without explicit prompting.
*   **Vulnerability to Adversarial Prompts:** Malicious prompts can manipulate GPT into producing undesirable or unintended responses.

<----------section---------->

## Popular GPT Variants - Codex
*   Codex is a GPT-3 model fine-tuned by OpenAI specifically for coding and programming tasks.
*   It powers GitHub Copilot and can assist with code generation, debugging, and explanations across multiple programming languages.
*   The key features are coding assistance, code generation, multi-language support for programming tasks.

<----------section---------->

## Popular GPT Variants – MT-NLG
*   MT-NLG is the acronym for Megatron-Turing Natural Language Generation and was developed by NVIDIA and Microsoft.
*   MT-NLG is one of the largest language models, with 530 billion parameters.
*   It aims to improve natural language understanding and generation in tasks like summarization, question answering, and robust few-shot learning.

<----------section---------->

## Popular GPT Variants – GLaM
*   GLaM is the acronym for Generalist Language Model, developed by Google Research.
*   GLaM is a sparse mixture-of-experts model with 1.2 trillion parameters, but only a fraction are active per inference.
*   It uses fewer resources than fully dense models like GPT-3 while achieving competitive performance across NLP tasks.

<----------section---------->

## Popular GPT Variants – PanGu-α
*   PanGu-α is Huawei’s Chinese language model with 200 billion parameters, aimed at understanding and generating text in Mandarin.
*   It was developed as part of Huawei’s effort to advance AI in Chinese NLP and has applications in Chinese literature, customer support, and translation.
*   In general, it supports Chinese-specific language applications.

<----------section---------->

## Popular GPT Variants – Chinchilla
*   Chinchilla is a DeepMind model optimized for efficiency in training data and parameters.
*   It has a smaller number of parameters than GPT-3 but achieves similar or better performance, demonstrating an alternative approach to large-scale model training.
*   It is thus optimized for research and practical applications.

<----------section---------->

## Popular GPT Variants – OPT
*   OPT is the acronym for Open Pretrained Transformer, developed by Meta (Facebook AI).
*   OPT models are a series of open-source language models from Meta, comparable to GPT-3 in size and capabilities.
*   Meta released these models to support transparency in AI research, offering model weights and code for research and academic use.

<----------section---------->

## Popular GPT Variants – BLOOM
*   BLOOM is the result of the BigScience collaborative project.
*   It is an open-source multilingual model with 176 billion parameters, trained by a global consortium of researchers.
*   It supports 46 languages, including underrepresented languages, and is designed to make large language models accessible for diverse linguistic and cultural contexts.
*   It enforces inclusivity in NLP research.

<----------section---------->

## LLAMA (Large Language Model Meta AI)
*   The LLaMA (Large Language Model Meta AI) architecture is a family of transformer-based language models developed by Meta. LLaMA models are designed to be efficient, high-performing, and optimized for a range of NLP tasks. The goal is to provide accessible and customizable models for various research and application scenarios.
*   The LLaMA family includes several model sizes, each designed for different computational resource constraints and performance requirements:
    *   **LLaMA-7B:** 32 decoder blocks with 32 attention heads for each block, 4096-dimensional embeddings. This model is intended for resource-efficient tasks and environments with limited computational capabilities.
    *   **LLaMA-13B:** 40 decoder blocks with 40 attention heads for each block, 5120-dimensional embeddings. This model provides a balance between performance and efficiency, suitable for general-purpose NLP tasks and fine-tuning for specific applications.
    *   **LLaMA-30B:** 60 decoder blocks with 40 attention heads for each block, 6656-dimensional embeddings. This model is designed for more complex tasks, such as summarization and translation, where higher performance is required.
    *   **LLaMA-65B:** 80 decoder blocks with 64 attention heads for each block, 8192-dimensional embeddings. This model offers top-tier NLP performance across multiple domains and is intended for high-end applications and advanced research.
*   These models are designed to offer a range of capabilities depending on the computational resources available, from smaller more efficient models to larger models.

<----------section---------->

## LLAMA Input Encoding
*   LLaMA models use Byte-Pair Encoding (BPE) as their input encoding method, obtaining a dictionary of 32768 tokens.
*   However, LLaMA uses relative positional encodings instead of absolute positional encodings. This design choice offers several advantages:
    *   **Handling Varying Sequence Lengths:** Relative positional encodings allow the model to better handle varying sequence lengths because they focus on the relationships between tokens rather than their absolute positions.
    *   **Generalization Across Different Contexts:** The model can generalize more effectively across different contexts because it learns the relative positions of tokens, making it less sensitive to the absolute position of tokens in the sequence.
    *   **Longer Sequences:** Relative positional encoding is particularly useful for longer sequences because it avoids the limitations associated with fixed positional embeddings.
*   In relative positional encoding, the model learns the relationships between tokens based on their relative positions rather than their absolute positions in the sequence.

<----------section---------->

## LLAMA Pre-training
*   Like GPT models, LLaMA is pre-trained using an autoregressive language modeling objective. This means that the model learns to predict the next token in a sequence given the previous tokens. This objective is crucial for language models to learn the statistical patterns and dependencies within natural language.
*   LLaMA is trained on “The Pile” (825 GB, from 300 to 1000 billion tokens), a wide range of publicly available text sources, including:
    *   **Books:** Text from various domains like literature, non-fiction, etc.
    *   **Web Data:** Content from the internet, scraped from publicly accessible websites.
    *   **Scientific Papers:** Research articles, preprints, and academic papers.
*   The dataset is designed to be as diverse as possible to ensure the model learns a broad understanding of language and can generalize well to a wide range of tasks. The diverse range of text sources in the dataset ensures that the model is exposed to different writing styles, topics, and linguistic structures, enhancing its adaptability and robustness.

<----------section---------->

## LLAMA Pre-training: Loss and Optimization
*   The loss function used during pre-training is the cross-entropy loss between the predicted token probabilities and the actual next token in the sequence. This helps the model learn to predict the correct token given the context.
*   The model is optimized using Stochastic Gradient Descent (SGD) or Adam optimizer with gradient clipping to prevent instability during training. Gradient clipping is a technique used to prevent gradients from becoming too large, which can lead to unstable training and poor convergence.
*   Training also utilizes mixed precision to speed up computation and reduce memory requirements. Mixed precision training involves using both single-precision (FP32) and half-precision (FP16) floating-point numbers during training. This can significantly reduce memory usage and computation time without sacrificing model accuracy.
*   LLaMA employs techniques like learning rate schedules (e.g., linear warm-up followed by decay), weight decay, and batch normalization or layer normalization to stabilize and improve training. These techniques help to improve model performance and prevent overfitting.
    *   **Learning Rate Schedules:** Adjust the learning rate during training to improve convergence. Linear warm-up gradually increases the learning rate at the beginning of training, while decay reduces it later on.
    *   **Weight Decay:** Adds a penalty term to the loss function to prevent the weights from becoming too large, helping to prevent overfitting.
    *   **Batch Normalization/Layer Normalization:** Normalizes the activations of each layer to improve training stability and convergence.

<----------section---------->

## LLAMA Variants

| Model        | Parameters | Use Case                                                              | Strengths                                                                                                                                                 | Limitations                                                                                            |
|--------------|------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| LLaMA-7B     | 7 billion  | Resource-efficient tasks (e.g., small-scale NLP)                       | High efficiency, suitable for smaller environments                                                                                                        | May not achieve top performance on complex tasks                                                       |
| LLaMA-13B    | 13 billion | General-purpose NLP tasks, fine-tuning for specific applications      | Balanced performance and efficiency                                                                                                                         | May lack performance for more advanced tasks                                                           |
| LLaMA-30B    | 30 billion | Complex tasks (e.g., summarization, translation)                      | High performance on state-of-the-art NLP tasks                                                                                                            | Requires significant computational resources                                                             |
| LLaMA-65B    | 65 billion | High-end applications, advanced research                               | Top-tier NLP performance across multiple domains                                                                                                         | Extremely resource-intensive, challenging for deployment                                                 |

<----------section---------->

## LLAMA vs. GPT

| Aspect            | LLaMA                                                       | GPT                                                                 |
| ----------------- | ----------------------------------------------------------- | ------------------------------------------------------------------- |
| Size Range        | 7B, 13B, 30B, 65B                                           | 117M to 175B+ (GPT-3)                                               |
| Training Data     | Public data (The Pile, Wikipedia, Common Crawl, etc.)       | Public data (Common Crawl, WebText, etc.)                           |
| Performance       | Strong, competitive, especially for smaller models           | State-of-the-art, particularly in zero/few-shot                     |
| Training Efficiency | More efficient, parameter-efficient                         | Very resource-intensive, especially for GPT-3                       |
| Deployment        | Open-sourced, flexible deployment                         | Commercial API via OpenAI                                           |
| Ethics            | Strong ethical considerations                               | Criticism over transparency and biases                                |
| Applications      | Academic research, custom deployment                      | Broad commercial use, APIs, and applications                         |

<----------section---------->

## Practice on Text Generation
*   Refer to the Hugging Face guide on text generation at [https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation) to understand different text generation techniques and models.
*   Explore available models for text generation on Hugging Face at [https://huggingface.co/models?pipeline_tag=text-generation&sort=trending](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending).
*   If you have adequate time and computational resources, consider fine-tuning one of the text generation models, as described in the article at [https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article](https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article). This will give you hands-on experience with adapting pre-trained models to specific tasks.

<----------section---------->

# Natural Language Processing and Large Language Models

This material is from Lesson 15 of the Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering).

## Encoder-Decoder Transformers

Presented by Nicola Capuano and Antonio Greco from DIEM (Department of Industrial Engineering of Salerno), University of Salerno.

### Outline

This lesson will cover the following topics:

*   Encoder-decoder transformer models: A deep dive into their architecture and function.
*   T5: Exploring the Text-to-Text Transfer Transformer model in detail.
*   Practice on translation and summarization: Hands-on exercises to apply the learned concepts.

<----------section---------->

## Encoder-Decoder Transformer

### Encoder-Decoder Transformer Defined

Encoder-Decoder Transformers represent a significant class of neural networks specifically engineered for sequence-to-sequence (seq2seq) tasks. These tasks involve transforming an input sequence into a different output sequence. Examples include machine translation (converting text from one language to another), text summarization (condensing a longer text into a shorter version), and speech recognition (converting audio into text). The encoder processes the input sequence and creates a contextual representation, which the decoder uses to generate the output sequence.

<----------section---------->

## T5

### T5: Text-to-Text Transfer Transformer

T5 (Text-to-Text Transfer Transformer) is a powerful language model conceived and developed by Google Research. Its architecture is fundamentally based on the encoder-decoder transformer structure, making it adept at a wide array of NLP tasks. The innovative aspect of T5 lies in its approach of framing all text-based problems as text-to-text problems, simplifying the fine-tuning and transfer learning processes.

T5 is available in various sizes to accommodate diverse computational resource constraints. The model size influences its capacity to learn and generalize from data; larger models usually achieve higher accuracy but require more computational power and memory.

<----------section---------->

### T5 Model Sizes

The following table illustrates the different sizes of the T5 model, including the number of encoder and decoder blocks, attention heads, and embedding dimensionality:

| Version   | Encoder Blocks | Attention Heads | Decoder Blocks | Embedding Dimensionality | Parameters |
| --------- | -------------- | --------------- | -------------- | ------------------------ |------------|
| T5-Small  | 6              | 8               | 6              | 512                      | ~60 million|
| T5-Base   | 12             | 12              | 12             | 768                      | ~220 million|
| T5-Large  | 24             | 16              | 24             | 1024                     | ~770 million|
| T5-XL     | 24             | 32              | 24             | 2048                     | ~3 billion |
| T5-XXL    | 24             | 64              | 24             | 4096                     | ~11 billion|

*   **Encoder Blocks:** The number of stacked encoder layers. More layers allow the model to capture more complex input patterns.
*   **Attention Heads:** The number of independent attention mechanisms in each layer. More heads enable the model to attend to different parts of the input simultaneously.
*   **Decoder Blocks:** The number of stacked decoder layers. Similar to encoder blocks, more layers enhance the model's ability to generate complex outputs.
*   **Embedding Dimensionality:** The size of the vector representation used to represent each token in the input and output sequences. Higher dimensionality can capture more nuanced meanings.
*   **Parameters**: Represents the total number of trainable parameters in the model.

<----------section---------->

### T5 Input Encoding

T5 employs a SentencePiece tokenizer coupled with a custom vocabulary to convert input text into a numerical format suitable for the model.

*   **Subword Units:** T5 utilizes a subword-based tokenizer leveraging the SentencePiece library. This approach strikes a balance between character-level and word-level tokenization. Subword tokenization offers the advantage of effectively handling rare words and unseen combinations, mitigating the out-of-vocabulary (OOV) problem by breaking words into smaller, meaningful units.
*   **Unigram Language Model:** The SentencePiece tokenizer is trained using a unigram language model. This model selects subwords so as to maximize the likelihood of the training data, resulting in a vocabulary of the most frequently occurring and statistically significant subword units.

<----------section---------->

### T5 Vocabulary

T5 uses a fixed vocabulary of 32,000 tokens, comprising subwords, whole words, and special tokens. This vocabulary size is carefully chosen to strike a balance between computational efficiency (smaller vocabularies) and expressive power (larger vocabularies).

T5 introduces several special tokens in the vocabulary to guide the model in performing various tasks:

*   `<pad>`: This is a padding token, employed to ensure that all sequences within a batch have identical lengths. This is crucial for efficient processing by the model.
*   `<unk>`: The unknown token is used to represent words or subwords not found in the model's vocabulary. It handles out-of-vocabulary (OOV) cases gracefully.
*   `<eos>`: This token marks the end of a sequence, whether it be an input or an output sequence. It signals to the model that the sequence is complete.
*   `<sep>` and Task-Specific Prefixes: These are used to indicate the type of task that the model should perform. Examples include prefixes like "translate English to German:" or "summarize:". These prefixes instruct the model on how to process the input.

<----------section---------->

### T5 Pre-training

T5 undergoes pre-training using a denoising autoencoder objective known as span-corruption. This technique is crucial for enabling the model to learn robust and generalizable representations.

Span-corruption involves masking contiguous spans of text (rather than just individual tokens) within the input sequence. The model is then trained to predict these masked spans.

*   **Input Corruption:** Random spans of text in the input are replaced with a unique `<extra_id_X>` token (e.g., `<extra_id_0>`, `<extra_id_1>`).
*   **Original Input:** "The quick brown fox jumps over the lazy dog."
*   **Corrupted Input:** "The quick `<extra_id_0>` jumps `<extra_id_1>` dog."
*   **Target Output:** The model is trained to predict the original masked spans in sequential order.
*   **Target Output:** `<extra_id_0>` brown fox `<extra_id_1>` over the lazy."

This span-corruption technique necessitates that the model generate coherent text while simultaneously learning contextual relationships between tokens, making it effective at various NLP tasks.

<----------section---------->

### Span Prediction and Its Benefits

Predicting spans, instead of isolated tokens, offers several advantages:

*   **Global Context:** The model is forced to understand how different spans relate to the overall sentence or paragraph structure, enhancing its comprehension of global context.
*   **Fluency and Cohesion:** Span prediction encourages the model to generate outputs that are not only accurate but also natural and coherent.
*   **Task Versatility:** By mastering span prediction, the model is better prepared for a wide range of downstream tasks, including summarization, translation, and question answering.

<----------section---------->

### The C4 Dataset

T5 is pre-trained on the C4 dataset (Colossal Clean Crawled Corpus), which is a massive collection of text derived from Common Crawl.

*   **Size:** The C4 dataset contains approximately 750 GB of cleaned text, making it one of the largest datasets used for language model pre-training.
*   **Cleaning:** Rigorous data cleaning techniques are applied to remove spam, duplicate text, and low-quality content. This ensures that the model is trained on high-quality data.
*   **Versatility:** The dataset contains diverse text from numerous domains, helping the model generalize across a broad spectrum of topics and writing styles.

<----------section---------->

### Training Specifics

*   **Loss Function:** Cross-entropy loss is used to measure the difference between the model's predictions and the actual masked spans, guiding the learning process.
*   **Optimizer:** T5 utilizes the Adafactor optimizer, known for its memory efficiency and suitability for large-scale training.
*   **Learning Rate Scheduling:** The learning rate is dynamically adjusted during training, starting with a warm-up phase (gradually increasing the learning rate) followed by an inverse square root decay (gradually decreasing the learning rate). This schedule helps the model converge effectively.

<----------section---------->

### T5 Fine-tuning

*   **Input and Output as Text:** During fine-tuning, T5 continues to operate with text strings as both input and output, regardless of the specific task. This simplifies the fine-tuning process and leverages the model's pre-trained capabilities.
*   **Example Tasks:**
    *   **Summarization:**
        *   Input: `summarize: <document>` → Output: `<summary>`
    *   **Translation:**
        *   Input: `translate English to French: <text>` → Output: `<translated_text>`
    *   **Question Answering:**
        *   Input: `question: <question> context: <context>` → Output: `<answer>`

<----------section---------->

### Popular T5 Variants – mT5

mT5 (Multilingual T5) is designed to extend T5's capabilities across multiple languages, enabling multilingual NLP tasks.

It was pre-trained on the multilingual Common Crawl dataset, which covers 101 languages.

*   **Key Features:**
    *   Maintains the text-to-text framework across different languages, ensuring consistent processing.
    *   Eliminates the need for language-specific tokenization through the use of SentencePiece with a shared vocabulary across languages, simplifying multilingual processing.
    *   Exhibits strong multilingual performance, particularly on cross-lingual tasks such as translation between languages it was not directly trained on.
*   **Applications:**
    *   Translation, multilingual summarization, and cross-lingual question answering.
*   **Limitations:**
    *   Larger model size due to the need to represent multiple languages within the vocabulary.
    *   Performance can vary significantly across languages, favoring those with more representation in the training data. This highlights the importance of balanced datasets in multilingual models.

<----------section---------->

### Popular T5 Variants – Flan-T5

Flan-T5 is a fine-tuned version of T5 that incorporates instruction-tuning on a diverse collection of tasks formatted as instructions.

*   **Key Features:**
    *   Improves generalization by training on datasets structured as instruction-response pairs, allowing the model to better understand and follow instructions.
    *   Demonstrates enhanced zero-shot and few-shot learning capabilities compared to the original T5, making it suitable for tasks with limited training data.
*   **Applications:**
    *   Performs well in scenarios requiring generalization to unseen tasks, such as creative writing or complex reasoning.
*   **Limitations:**
    *   Requires careful task formulation to fully leverage its instruction-following capabilities, emphasizing the need for well-defined instructions.

<----------section---------->

### Popular T5 Variants – ByT5

ByT5 (Byte-Level T5) processes text at the byte level instead of relying on subword tokenization.

*   **Key Features:**
    *   Avoids the need for tokenization, facilitating better handling of noisy, misspelled, or rare words that might not be present in a traditional vocabulary.
    *   Functions effectively for languages with complex scripts or in low-resource scenarios where tokenization resources are limited.
*   **Applications:**
    *   Offers robustness for tasks involving noisy or unstructured text, such as OCR (Optical Character Recognition) or user-generated content.
*   **Limitations:**
    *   Significantly slower and more resource-intensive due to longer input sequences, as byte-level representation increases sequence length.

<----------section---------->

### Popular T5 Variants – T5-3B and T5-11B

T5-3B and T5-11B are larger versions of the original T5, containing 3 billion and 11 billion parameters, respectively.

*   **Key Features:**
    *   Improved performance on complex tasks owing to the increased model capacity, enabling the model to capture more intricate relationships within the data.
    *   Suitable for tasks necessitating deep contextual understanding and large-scale reasoning, such as advanced language generation or complex analysis.
*   **Applications:**
    *   Used in academic research and high-performance NLP applications where computational resources are less of a constraint.
*   **Limitations:**
    *   Computationally expensive for fine-tuning and inference, necessitating powerful hardware.
    *   Memory requirements restrict their usability on standard hardware, limiting accessibility for some users.

<----------section---------->

### Popular T5 Variants – UL2

UL2 (Unified Language Learning) is a general-purpose language model inspired by T5 but supporting a wider range of pretraining objectives.

*   **Key Features:**
    *   Combines diverse learning paradigms, including unidirectional, bidirectional, and sequence-to-sequence objectives, allowing for versatile learning.
    *   Offers state-of-the-art performance across a variety of benchmarks, positioning it as a competitive model in the NLP landscape.
*   **Applications:**
    *   General-purpose NLP tasks, including both generation and comprehension, making it a versatile choice for different applications.
*   **Limitations:**
    *   Increased complexity due to the multiple pretraining objectives, complicating training and optimization.

<----------section---------->

### Popular T5 Variants – Multimodal T5

T5-Large Multimodal Variants augment T5 with vision capabilities through the integration of additional modules for visual data processing.

*   **Key Features:**
    *   Processes both text and image inputs, enabling tasks like image captioning, visual question answering, and multimodal translation.
    *   Often uses adapters or encodes visual features separately, allowing for flexibility in how visual information is incorporated.
*   **Applications:**
    *   Multimodal tasks combining vision and language, opening up new possibilities in AI applications.
*   **Limitations:**
    *   Computationally expensive due to the need to process multiple modalities, requiring specialized hardware.

<----------section---------->

### Popular T5 Variants – Efficient T5

Efficient T5 Variants are optimized for performance in resource-constrained environments.

*   **Examples:**
    *   T5-Small/Tiny: Reduced parameter versions of T5 for lower memory and compute needs, suitable for deployment on edge devices.
    *   DistilT5: A distilled version of T5, reducing the model size while retaining a significant portion of the performance, enhancing efficiency.
*   **Applications:**
    *   Real-time applications on edge devices or scenarios with limited computational resources, such as mobile or embedded systems.
*   **Limitations:**
    *   Some performance sacrifices are made in comparison to larger T5 models, necessitating a trade-off between efficiency and accuracy.

<----------section---------->

### T5 Variants Summary

| Variant        | Purpose                    | Key Strengths                                | Limitations                                  |
| -------------- | -------------------------- | -------------------------------------------- | --------------------------------------------- |
| mT5            | Multilingual NLP           | Supports 101 languages                        | Uneven performance across languages           |
| Flan-T5        | Instruction-following        | Strong generalization                         | Needs task-specific prompts                    |
| ByT5           | No tokenization            | Handles noisy/unstructured text               | Slower due to byte-level inputs               |
| T5-3B/11B      | High-capacity NLP          | Exceptional performance                      | High resource requirements                     |
| UL2            | Unified objectives         | Versatility across tasks                      | Increased training complexity                  |
| Multimodal T5  | Vision-language tasks      | Combines text and image inputs               | Higher computational cost                      |
| Efficient T5   | Resource-constrained NLP   | Lightweight, faster inference                 | Reduced task performance                       |

<----------section---------->

## Practice on Translation and Summarization

### Hands-on Practice

Use the Hugging Face guides on translation ([https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)) and summarization ([https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt)) to experiment with various models for these tasks.

By following the guides, if time and computational resources are available, fine-tune one of the encoder-decoder models to enhance its performance on these specific tasks.

<----------section---------->

**Natural Language Processing and Large Language Models**

This document outlines the final project for the "Natural Language Processing (NLP) and Large Language Models (LLM)" course, part of the Master's Degree in Computer Engineering (Ingegneria Informatica). The project is supervised by Nicola Capuano and Antonio Greco from DIEM (Department of Information Engineering and Mathematics) at the University of Salerno. The course material and assignment details pertain to the 2024/2025 academic year.

Outline:
*   Goal of the project.
*   Tools to use for the project.
*   Chatbot evaluation procedure.

<----------section---------->

**Goal of the Project**

The primary objective of this project is to develop a chatbot capable of answering questions related to the NLP and LLM course offered in the 2024/2025 academic year. The chatbot's knowledge base should extend beyond the specific topics covered in lectures to include general course information. This encompasses details such as the course instructors, recommended textbooks, and other pertinent administrative details.

A critical requirement is that the chatbot must exclusively address questions relevant to the course. It should be able to identify questions that fall outside the scope of the course material or general course information. When presented with an out-of-context question, the chatbot should respond by stating that it is not enabled to provide answers on topics outside the defined context of interest. This functionality helps to ensure the chatbot remains a focused and reliable resource for course-related inquiries.

Project deliverables will include:

1.  **The Code:** The complete source code of the chatbot, demonstrating its functionality and implementation.
2.  **A Detailed Report:** A comprehensive report that thoroughly describes the design, architecture, and implementation of the chatbot solution. This report should provide detailed insights into the chosen methodologies, algorithms, and technologies used in the development process. It will also detail the rationale behind key design decisions and how the chatbot effectively meets the project goals.

<----------section---------->

**Tools to Use for the Project**

Students have the freedom to select any tool or technology discussed during the course. This includes both traditional NLP techniques and more recent advancements associated with Large Language Models (LLMs). The project encourages a flexible approach, allowing students to combine different tools and techniques to achieve the desired outcome.

Students are free to use any tool or technology analyzed in the course, both the more recent ones associated with LLMs and the more classic ones of NLP. To achieve the required result, the students can develop parts of the chatbot with LLMs and other parts with more classic tools, with total freedom of choice as long as the various solutions identified are justified in the final report. Any LLM or other already available model can be used with or without modifications, as long as the groups are fully familiar with the tools they used and are able to answer questions about every aspect of the code and models.

Students can implement different components of the chatbot using LLMs and more conventional NLP tools, as long as these choices are well-justified in the final report. Students are allowed to leverage existing models (LLMs or other pre-trained models), with or without modifications, provided they possess a thorough understanding of the tools and can address questions about all aspects of the code and models used. This ensures students not only build a functional chatbot but also gain a deep understanding of the technologies employed.

<----------section---------->

**Chatbot Evaluation Procedure**

The evaluation of the chatbot will occur before the project discussion and will be conducted in real-time by the course instructors. The instructors will interact with the chatbot using a predefined set of questions about the course, evaluating the responses based on the following criteria:

*   **Relevance:** This criterion assesses whether the generated text accurately and directly answers the posed question. A relevant response provides information that is pertinent to the query and avoids extraneous or unrelated content.
*   **Fluency:** Fluency refers to the readability and grammatical correctness of the chatbot's output. A fluent response is easy to understand, uses proper grammar and sentence structure, and exhibits a natural flow of language.
*   **Coherence:** Coherence evaluates the logical consistency and flow of the chatbot's responses. A coherent answer presents information in a structured and logical manner, ensuring that the different parts of the response connect smoothly and make sense as a whole.

Following the initial evaluation, the course instructors will conduct a second set of tests to assess the chatbot's ability to handle specific challenges:

*   **Robustness:** Robustness measures the chatbot's resilience to adversarial or misleading prompts. It checks the system's ability to maintain correct and appropriate responses even when faced with ambiguous, deceptive, or leading questions (e.g., "Are you sure?").
*   **Precision:** Precision, in this context, refers to the chatbot's ability to accurately identify and reject out-of-context questions. This includes questions unrelated to the course content or general information (e.g., "Who is the king of Spain?").

The course instructors will assign a grade to the chatbot's performance based on these evaluation aspects.

<----------section---------->

# Natural Language Processing and Large Language Models

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)

Lesson 17: Fine Tuning

Nicola Capuano and Antonio Greco

DIEM – University of Salerno

This lesson focuses on fine-tuning techniques for Large Language Models (LLMs), a crucial aspect of adapting these models for specific applications. Fine-tuning involves taking a pre-trained model and further training it on a dataset tailored to a specific task or domain. This process allows the LLM to specialize and optimize its performance, leveraging its existing knowledge to achieve better results in targeted scenarios.

<----------section---------->

## Outline

This lesson will cover the following topics:

*   Types of fine-tuning: A broad overview of different approaches to fine-tuning LLMs.
*   Parameter Efficient Fine Tuning (PEFT): Techniques designed to reduce the computational cost and resource requirements of fine-tuning.
*   Instruction Fine-Tuning: A method for aligning LLMs with human instructions and improving their ability to follow complex prompts.

<----------section---------->

## Types of Fine Tuning

### Pros and Cons of Full Fine Tuning

*   Fine-tuning refers to adapting a pre-trained LLM to a specific task by training it further on a task-specific dataset. This adaptation allows the model to leverage its general knowledge while specializing in the nuances of the target task.
*   **Why Fine-Tune?**
    *   Specialize LLMs for domain-specific tasks: Fine-tuning enables LLMs to excel in specific areas, such as medical text analysis, legal document summarization, or financial forecasting.
    *   Improve accuracy and relevance for specific applications: By training on a dataset relevant to a particular application, fine-tuning enhances the model's ability to provide accurate and relevant outputs.
    *   Optimize performance on small, focused datasets: Fine-tuning is particularly useful when working with limited datasets, allowing the model to learn effectively without requiring vast amounts of training data.
*   Full Fine-Tuning, which updates all model parameters, allows to achieve high accuracy for specific tasks by fully leveraging the model's capacity, but it is computationally expensive and risks overfitting on small datasets.
    *   **Pros of Full Fine-Tuning:** Maximum flexibility to adapt the model to the new task, potentially leading to the highest accuracy.
    *   **Cons of Full Fine-Tuning:** High computational cost due to updating all parameters, increased risk of overfitting, especially with small datasets, and significant storage requirements for the fine-tuned model.

<----------section---------->

### Other Types of Fine Tuning

*   Parameter-Efficient Fine-Tuning (PEFT): Updates only a subset of the parameters. This approach reduces computational costs and storage requirements while still allowing the model to adapt to the target task.
    *   Examples:
        *   Low-Rank Adaptation (LoRA): This method focuses on adapting the weight matrices of the LLM by learning low-rank matrices, making the update process highly parameter-efficient.
        *   Adapters: Adapters involve inserting small, trainable modules within the transformer layers of the LLM, leaving the original pre-trained weights frozen.
*   Instruction Fine-Tuning: Used to align models with task instructions or prompts (user queries) enhancing usability in real-world applications. By training on a dataset of instructions paired with desired outputs, the model learns to better understand and respond to user requests.
    *   This is particularly important because, in the age of LLMs, it has become very common to use them to generate additional natural language data. Leveraging large language models for everything from generating samples of a particular user intent to creating questions and answers for a Q&A dataset is how many teams nowadays mitigate the lack of real-world data.
*   Reinforcement Learning from Human Feedback (RLHF): Combines supervised learning with reinforcement learning, rewarding models when they generate user-aligned outputs.
    *   RLHF utilizes human preferences to guide the model's learning process, resulting in outputs that are more aligned with user expectations and real-world requirements. This approach addresses the limitations of supervised learning by incorporating feedback on the quality and appropriateness of the generated text.

<----------section---------->

## Parameter Efficient Fine Tuning (PEFT)

### Parameter-Efficient Fine-Tuning

*   Parameter-Efficient Fine-Tuning (PEFT) is a strategy developed to fine-tune large-scale pre-trained models, such as LLMs, in a computationally efficient manner while requiring fewer learnable parameters compared to standard fine-tuning methods.
*   PEFT methods are especially important in the context of LLMs due to their massive size, which makes full fine-tuning computationally expensive and storage-intensive. Full fine-tuning of LLMs can require significant computational resources, making it impractical for many applications.
*   PEFT is ideal for resource-constrained settings like edge devices or applications with frequent model updates. These methods enable the deployment of LLMs in environments where computational resources are limited.
*   These techniques are supported and implemented in Hugging Face transformers and, in particular, in the `peft` library. Hugging Face provides tools and resources to facilitate the use of PEFT techniques, making them accessible to a wider range of developers and researchers.

<----------section---------->

### PEFT Techniques

*   Low-Rank Adaptation (LoRA): Approximates weight updates by learning low-rank matrices, performing a small parameterized update of the weight matrices in the LLM. It is highly parameter-efficient and widely adopted for adapting LLMs. LoRA significantly reduces the number of trainable parameters while still allowing the model to adapt to the target task.
*   Adapters: They are small and trainable modules inserted within the transformer layers of the LLM, that allow to keep the pre-trained model's original weights frozen. Adapters provide a modular approach to fine-tuning, allowing for task-specific adaptations without modifying the core model.
*   Prefix Tuning: Learns a set of continuous task-specific prefix vectors for attention layers, keeping the original model parameters frozen. Prefix tuning modifies the input sequence by adding a trainable prefix, which guides the model's attention and output generation.

<----------section---------->

## Low-Rank Adaptation (LoRA)

*   LoRA assumes that the changes required to adapt a pre-trained model for a new task lie in a low-dimensional subspace. This assumption allows for a more efficient fine-tuning process by focusing on the most relevant parameters.
*   Instead of fine-tuning all the parameters of the model, LoRA modifies only a small, trainable set of low-rank matrices that approximate these task-specific changes.
    1.  **Base Model:** A pre-trained transformer model is represented by its weight matrices W. The base model provides the foundation for fine-tuning, offering a wealth of pre-existing knowledge.
    2.  **Low-Rank Decomposition:** Instead of directly modifying W, LoRA decomposes the weight update into two low-rank matrices:

        ΔW = A × B

        *   A is a low-rank matrix (m × r)
        *   B is another low-rank matrix (r × n)
        *   r is the rank, which is much smaller than m or n, making A and B parameter-efficient. By using low-rank matrices, LoRA significantly reduces the number of trainable parameters compared to full fine-tuning.
    3.  **Weight Update:** The effective weight during fine-tuning becomes:

        W' = W + ΔW = W + A × B

<----------section---------->

### How LoRA Works

*   **Freezing Pre-Trained Weights:** LoRA keeps the original weight matrices W of the LLM frozen during fine-tuning. Only the parameters in A and B are optimized for the new task (the pre-trained knowledge is preserved). This ensures that the model retains its general knowledge while specializing in the target task.
*   **Injecting Task-Specific Knowledge:** The low-rank decomposition A × B introduces minimal additional parameters (less than 1% of the original model parameters) while allowing the model to learn task-specific representations. LoRA enables the model to acquire task-specific knowledge without significantly increasing the number of parameters.
*   **Efficiency:** The number of trainable parameters is proportional to r × (m + n), which is significantly smaller than the full m × n weight matrix. LoRA offers a significant reduction in computational costs compared to full fine-tuning, making it suitable for resource-constrained environments.
*   **Inference Compatibility:** During inference, the modified weights W' = W + A × B can be directly used, making LoRA-compatible models efficient for deployment. LoRA ensures that the fine-tuned model can be deployed efficiently without requiring specialized hardware or software.

<----------section---------->

## Adapters

*   Adapters are lightweight, task-specific neural modules inserted between the layers of a pre-trained transformer block. These modules are designed to capture task-specific information without modifying the core model.
*   These modules are trainable, while the original pre-trained model parameters remain frozen during fine-tuning. This approach allows for efficient adaptation to new tasks while preserving the model's general knowledge.
*   Adapters require training only the small fully connected layers, resulting in significantly fewer parameters compared to full fine-tuning. By focusing on training a small number of parameters, adapters reduce the computational cost and storage requirements of fine-tuning.
*   Since the base model remains frozen, the general-purpose knowledge learned during pre-training is preserved. This ensures that the model retains its ability to generalize to a wide range of tasks while excelling in the target task.

<----------section---------->

## Prefix Tuning

*   Instead of modifying the LLM's internal weights, prefix tuning introduces and optimizes a sequence of trainable "prefix" tokens prepended to the input. Prefix tuning offers a parameter-efficient way to adapt LLMs by modifying the input sequence rather than the model itself.
*   These prefixes guide the LLM's attention and output generation, enabling task-specific adaptations with minimal computational and storage overhead. By conditioning the model's attention, prefix tuning allows for targeted adaptation to new tasks.
*   The input sequence is augmented with a sequence of prefix embeddings:

    Modified Input: \[Prefix] + \[Input Tokens]

    *   **Prefix:** A sequence of *m* trainable vectors of size *d*, where *d* is the model's embedding dimensionality. The prefix vectors are optimized during fine-tuning to guide the model's behavior.
    *   **Input Tokens:** The original token embeddings from the input sequence. The original input tokens provide the context for the model to generate the desired output.
*   Prefix embeddings influence attention by being prepended to the keys (K) and values (V), conditioning how the model attends to the input tokens. By modifying the keys and values, the prefix embeddings influence the model's attention mechanism, guiding it to focus on the most relevant parts of the input sequence.
*   Only the prefix embeddings are optimized during fine-tuning. Backpropagation updates the prefix parameters to align the model's outputs with task-specific requirements.
*   *m* controls the trade-off between task-specific expressiveness and parameter efficiency. Longer prefixes can model more complex task-specific conditioning but may increase memory usage. The length of the prefix determines the model's ability to capture complex task-specific information.

<----------section---------->

## Instruction Fine Tuning

*   Instruction fine-tuning is a specialized approach for adapting large language models (LLMs) to better understand and respond to user instructions. This technique focuses on improving the model's ability to follow complex prompts and generate accurate and relevant outputs.
*   This fine-tuning process involves training the model on a curated dataset of task-specific prompts paired with corresponding outputs. The dataset is carefully designed to cover a wide range of instructions and desired outputs.
*   The objective is to improve the model's ability to generalize across a wide variety of instructions, enhancing its usability and accuracy in real-world applications. By training on a diverse set of instructions, the model learns to generalize to new and unseen prompts.
*   By training on human-like instructions, the model becomes more aligned with user expectations and natural language queries. Instruction fine-tuning aims to bridge the gap between the model's pre-trained knowledge and the specific requirements of real-world applications.

<----------section---------->

### How Instruction Fine Tuning Works

*   A diverse set of instructions and outputs is compiled. Each example consists of:
    *   **Instruction:** A clear, human-readable prompt (e.g., "Summarize the following text in one sentence"). The instruction should be clear and concise, providing the model with a specific task to perform.
    *   **Context (optional):** Background information or data required to complete the task. Context provides the model with the necessary information to generate an accurate and relevant output.
    *   **Output:** The expected response to the instruction. The output serves as the ground truth, guiding the model's learning process.
*   The LLM, pre-trained on a general corpus, is fine-tuned using the instruction-response pairs. During training, the model learns to:
    *   Recognize the intent of the instruction. The model learns to identify the underlying task or goal of the instruction.
    *   Generate outputs that are coherent, accurate, and contextually appropriate. The model learns to generate responses that are well-structured, factually correct, and relevant to the given context.
*   The dataset may include examples from various domains and task types. This diversity ensures the model can generalize beyond the specific examples it has seen during fine-tuning.
    *   However, one must be cautious. Adding data that is not truly representative of that which you’re trying to model can hurt more than it helps.


<----------section---------->

### Natural Language Processing and Large Language Models: Lesson 18 - Prompt Engineering

This material is associated with Lesson 18, focusing on Prompt Engineering, within a Natural Language Processing (NLP) course for the Laurea Magistrale (Master of Science) in Ingegneria Informatica (Computer Engineering) program. This course is likely presented at the DIEM (Dipartimento di Ingegneria dell'Informazione ed Elettrica e Matematica dell'Ingegneria) - University of Salerno. The instructors for this lesson are Nicola Capuano and Antonio Greco.

<----------section---------->

### Course Outline

The lesson's structure covers the following key areas:

*   **Introduction to Prompt Engineering:** This section provides the foundational concepts and overview of the field.
*   **Prompt Engineering Techniques:** This segment explores various methods and strategies used in crafting effective prompts.
*   **Prompt Testing:** This part discusses the importance of testing and refining prompts to achieve desired outcomes.

<----------section---------->

### Introduction to Prompt Engineering

**Prompt Engineering Defined:** Prompt engineering is an emerging field within the broader landscape of NLP and Large Language Models (LLMs). It centers around the design and optimization of prompts to elicit specific and useful responses from LLMs. These prompts are carefully constructed to instruct the LLM in performing a wide range of tasks effectively, finding applications in diverse research areas and practical scenarios.

**Goals of Prompt Engineering:**

*   **Deepening Understanding of LLMs:** Prompt engineering facilitates a better comprehension of the capabilities inherent in LLMs, as well as an awareness of their limitations. By experimenting with different prompts, practitioners can gain insights into how these models process information and generate responses.
*   **Enhancing LLM Performance:** The core aim of prompt engineering is to improve the performance of LLMs across a spectrum of tasks. These tasks can include question answering, where the model is expected to provide accurate and relevant answers to user queries; arithmetic reasoning, which tests the model's ability to perform mathematical operations and logical deductions; and many other NLP-related activities.
*   **Facilitating LLM Interfacing and Integration:** Prompt engineering streamlines the process of interacting with LLMs. It also helps in integrating these models with other tools and systems, creating more complex and capable applications. Effective prompts serve as a bridge between LLMs and external resources, enabling seamless communication and data exchange.
*   **Unlocking New Capabilities:** By carefully designing prompts, it becomes possible to unlock entirely new capabilities within LLMs. This includes augmenting the models with domain-specific knowledge, allowing them to leverage expertise in particular fields. It also enables the integration of external resources, such as databases or APIs, to provide LLMs with access to real-time information and specialized functionalities.

<----------section---------->

### Writing Effective Prompts

Crafting prompts effectively is an iterative process involving careful consideration and refinement. Here's a guide to writing good prompts:

*   **Start Simple and Iterate:** Begin with basic prompts and gradually add elements. Iterative refinement based on the model's responses is key to improving the prompt's effectiveness. This approach allows for a controlled exploration of the prompt space, making it easier to identify the factors that contribute to better results.
*   **Use Clear and Specific Instructions:** Begin the prompt with explicit instructions that clearly state the desired action. Common instruction verbs include "Write," "Classify," and "Summarize." Clarity at the outset helps the model understand the task immediately and reduces ambiguity.
*   **Provide Detailed and Descriptive Context:** The more detail and description provided in the prompt, the better the model can understand the context and generate relevant outputs. Detailed prompts reduce the chances of misinterpretation and steer the model towards the desired response.
*   **Use Examples for Guidance:** Including examples in the prompt can demonstrate the desired output format and content. This technique, known as "few-shot learning," allows the model to learn from the examples and apply the learned patterns to new inputs.
*   **Balance Detail and Length:** While detail is important, excessive information can overwhelm the model and reduce its effectiveness. Experiment to determine the optimal balance between detail and length. Prompts that are too long can lead to confusion or irrelevant outputs, while prompts that are too short may lack the necessary context. Finding the sweet spot requires careful consideration and experimentation.

<----------section---------->

### Examples of Effective Prompts

This section demonstrates how to improve prompt effectiveness through specific examples:

*   **Text Summarization:**
    *   **Bad Prompt:** “Summarize this article.” (Lacks specifics)
    *   **Good Prompt:** “Generate a 100-word summary of this research article, focusing on the main findings.” (Specifies length and focus)
*   **Email Composition:**
    *   **Bad Prompt:** “Write an apology email to a client.” (Vague request)
    *   **Good Prompt:** “Write a professional email to a client apologizing for a delayed shipment, offering a discount, and providing an updated delivery estimate.” (Adds context and actions)
*   **Explanation Simplification:**
    *   **Bad Prompt:** “Make this explanation easier to understand.” (Unclear target audience)
    *   **Good Prompt:** “Rewrite this technical explanation in simpler language suitable for high school students.” (Defines audience)
*   **Review Classification:**
    *   **Bad Prompt:** “Classify the following review.” (Missing classification types)
    *   **Good Prompt:** “Classify the following review as positive, neutral, or negative.” (Provides classification options)
*   **Information Listing:**
    *   **Bad Prompt:** “Tell me about exercise benefits.” (Too broad)
    *   **Good Prompt:** “List five health benefits of regular exercise, each with a short explanation of how it improves well-being.” (Specifies quantity and explanation)
*   **Sentence Translation:**
    *   **Bad Prompt:** “Translate this sentence to French.” (Missing tone)
    *   **Good Prompt:** “Translate the following English sentence into French, preserving the formal tone.” (Adds stylistic requirement)

<----------section---------->

### Elements of a Prompt

A well-structured prompt typically includes several key elements:

*   **Instruction:** This is the core command that tells the model what to do. It clearly specifies the task to be performed, such as summarizing, translating, or answering a question.
*   **Context:** This element provides background information or relevant details that can help the model generate a more accurate and informed response. Context can include details about the topic, audience, or desired style.
*   **Input Data:** This is the specific information or question that the model needs to process. It can be a text passage, a question, or any other form of data that the model is expected to analyze.
*   **Output Indicator:** This specifies the desired format or type of output. It could be a request for a summary, a classification label, or an answer to a question. This element guides the model in structuring its response appropriately.

**Example 1:**

```
Classify the text into neutral, negative or positive.
Text: I think the vacation is okay.
Sentiment: [Output Indicator]
```

*   **Instruction:** Classify the text...
*   **Input:** I think the vacation is okay
*   **Output Indicator:** Sentiment

**Example 2:**

```
Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure about answer" if not sure about the answer.

Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.

Question: What was OKT3 originally sourced from?
Answer: [Output Indicator]
```

*   **Instruction:** Answer the question...
*   **Context:** Teplizumab traces its roots...
*   **Input:** What was OKT3 originally sourced from?
*   **Output Indicator:** Answer

<----------section---------->

### In-Context Learning

In-context learning is a crucial capability of LLMs that allows them to perform tasks based on the information provided directly within the prompt. This eliminates the need to update the model's internal parameters or retrain it for specific tasks.

The prompt context may include:

*   **Reference Material:** Providing specific text or data that the model should use to perform the task.
*   **Input-Output Pairs:** Presenting examples of the task to illustrate the desired pattern or format.
*   **Step-by-Step Instructions:** Offering detailed guidance on how to complete the task, breaking it down into smaller steps.
*   **Clarifications:** Addressing any potential ambiguities in the task description.
*   **Templates:** Providing structures or placeholders that the model should fill in with relevant information.

Prompt engineering heavily relies on in-context learning to guide LLMs toward generating accurate and relevant responses. By carefully crafting the context, practitioners can effectively steer the model's behavior and improve its performance on a wide range of tasks.

<----------section---------->

### Prompts and NLP Tasks

Prompts can be designed to accomplish a wide range of NLP tasks:

*   **Text Summarization:**
    ```
    Antibiotics are a type of medication used to treat bacterial infections. They work by either killing
    the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the
    infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions,
    or sometimes administered intravenously. They are not effective against viral infections, and using
    them inappropriately can lead to antibiotic resistance.

    Explain the above in one sentence:
    ```
    **Output:**
    ```
    Antibiotics are medications used to treat bacterial infections by either killing the bacteria or
    stopping them from reproducing, but they are not effective against viruses and overuse can lead to
    antibiotic resistance.
    ```

*   **Information Extraction:**
    ```
    Author-contribution statements and acknowledgements in research papers should state clearly and
    specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the
    preparation of their manuscript and analysis. They should also indicate which LLMs were used. This
    will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases,
    inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about
    their use of LLMs, for example when selecting submitted manuscripts.

    Mention the large language model based product mentioned in the paragraph above:
    ```
    **Output:**
    ```
    The large language model based product mentioned in the paragraph above is ChatGPT.
    ```

*   **Question Answering:**
    ```
    Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure
    about answer" if not sure about the answer.

    Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There,
    scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the
    molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986,
    it was approved to help prevent organ rejection after kidney transplants, making it the first
    therapeutic antibody allowed for human use.

    Question: What was OKT3 originally sourced from?
    ```
    **Output:**
    ```
    Mice
    ```

*   **Text Classification:**
    ```
    Classify the text into neutral, negative or positive.
    Text: I think the food was okay.
    Sentiment:
    ```
    **Output:**
    ```
    Neutral
    ```

*   **Code Generation:**
    ```
    Write a JavaScript code that prompts the user to enter their name and then displays
    a “Hello” message.
    Output only the code, no additional comments.
    ```
    **Output:**
    ```javascript
    let name = prompt("What is your name?");
    console.log(`Hello, ${name}!`);
    ```

*   **Reasoning:**
    ```
    What is 9,000 * 9,000?
    ```
    **Output:**
    ```
    81,000,000
    ```
    Note: despite advancements in mathematical capabilities, reasoning tasks remain challenging for LLMs...

*   **Reasoning (Example of Failure):**
    ```
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
    Answer:
    ```
    **Output:**
    ```
    No, the odd numbers in this group add up to an odd number: 119.
    ```
    That's incorrect! More advanced prompt engineering techniques are needed to obtain the correct answer... we will discuss them later.

<----------section---------->

### System Prompts

A system prompt is a configuration provided to the AI model *before* any user interactions take place. These prompts are crucial for setting the stage for subsequent conversations.

*   **Purpose:** Establishes the assistant's behavior, context, tone, and any special instructions the model should adhere to.
*   **Guidance:** System prompts guide the model on how to respond and what it should focus on during the interaction.

**Examples:**

*   "You are a helpful and knowledgeable assistant who answers questions accurately and concisely." (Defines the model's role and behavior.)
*   "You are an IT support assistant specializing in troubleshooting software and hardware issues. Respond politely and guide users through step-by-step solutions." (Specifies domain expertise and communication style.)
*   "You are a friendly and engaging AI who responds in a warm and conversational tone, keeping responses lighthearted and approachable." (Sets the desired tone and persona.)

<----------section---------->

### Prompt Engineering Techniques

This section details various techniques for effective prompt engineering.

*   **Zero-Shot Prompting:** This approach involves directly interacting with the model using a prompt without providing any examples or demonstrations. The model is expected to perform the task based on its pre-existing knowledge. Large-scale training enables LLMs to handle many tasks in a “zero-shot” manner.

    **Example:**
    ```
    Classify the text into neutral, negative or positive.
    Text: I think the vacation is okay.
    Sentiment:
    ```
    **Output:**
    ```
    Neutral
    ```
    The LLM already understands the concept of “sentiment” (that's the zero-shot capabilities at work).

*   **Few-Shot Prompting:** When LLMs struggle with complex tasks in a zero-shot setting, few-shot prompting can be used. This technique involves including examples or demonstrations within the prompt to guide the model toward better performance. These demonstrations help the model generate more accurate responses for similar tasks in subsequent prompts.

    **Example:**
    ```
    A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa, and we saw these very cute whatpus.
    To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is: When we won the game, we all started to farduddle in celebration.
    ```

    **Limitations:** While effective for many tasks, few-shot prompting has limitations when handling complex reasoning tasks.

    *   **Zero-Shot Example:**
        ```
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        Answer:
        ```
        **Output:**
        ```
        Yes, the odd numbers in this group add up to 107, which is an even number.
        ```
        False! Let's try with few-shot prompting.

    *   **Few-Shot Example:**
        ```
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: The answer is False.
        The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        A: The answer is True.
        The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        A: The answer is True.
        The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        A: The answer is False.
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        A:
        ```
        **Output:**
        ```
        The answer is True.
        ```
        False Again!

*   **Chain-of-Thought Prompting:** Chain-of-Thought (CoT) prompting enhances complex reasoning capabilities by guiding the model through intermediate reasoning steps. This encourages the model to break down the problem into smaller, more manageable parts, leading to more accurate and logical solutions.

    | Standard Prompting                                                                                                                                              | Chain-of-Thought Prompting                                                                                                                                                  |
    | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: The answer is 11. | Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: 2 cans of tennis balls is 2*3=6 tennis balls.  Then he has 5+6=11 tennis balls. The answer is 11. |
    | Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.                           | Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: They had 23 apples, used 20, that means 23-20 = 3 apples remaining.  Then bought 6 more, so there are 3+6=9 apples. The answer is 9. |

    You can combine it with few-shot prompting to get better results on even more complex tasks.

    **Example:**
    ```
    The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
    A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
    The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
    A: Adding all the odd numbers (17, 19) gives 36. The answer is True.
    The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
    A: Adding all the odd numbers (11, 13) gives 24. The answer is True.
    The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
    A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
    A:
    ```
    **Output:**
    ```
    Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
    ```
    Correct! Note: this is an emergent ability that arises with sufficiently large language models.

*   **Self-Consistency Prompting:** This method uses an iterative chain-of-thought approach. Instead of accepting the first response from the LLM, the question is repeated multiple times to generate a range of reasoning paths, with the most frequent answer being selected. This increases the robustness and reliability of the final answer.

*   **Meta Prompting:** Meta-prompting is a technique that guides the model through the logical steps required to solve a problem without relying on specific content-based examples. This is particularly effective for complex or abstract problems where providing examples may be difficult.

    **Example:**
    ```
    Solve the quadratic equation 3x² + 4x - 5 = 0 by following these structured steps:
    1. Identify and Set Up the Formula: Recognize that this is a quadratic equation in the form ax² + bx + c = 0.
    2. Apply the Quadratic Formula: Use the quadratic formula x = (-b ± √(b² - 4ac)) / (2a) and substitute in the values.
    3. Calculate the Discriminant: Find b² - 4ac, and interpret its value to determine the nature of the roots.
    4. Solve for x: Perform the calculation for both + and - options in the formula to find the values of x.
    5. Verify the Solution: Substitute each value of x back into the original equation to confirm it satisfies it.
    6. Summarize the Results: State the final solution, listing both values of x and noting any observations about the nature of the roots.
    ```

*   **Task-Agnostic Meta Prompting:** Good results can sometimes be achieved by simply asking the model to reason step-by-step, without specifying the content of each step. This leverages the model's general reasoning abilities.

    **Example:**
    ```
    Problem Statement:
    * Problem: [question to be answered]
    Solution Structure:
    * Begin the response with "Let’s think step by step."
    * Follow with the reasoning steps, ensuring the solution process is broken down clearly and logically.
    * End the solution with the final answer encapsulated in a LaTeX-formatted box, 4, for clarity and emphasis.
    * Finally, state "The answer is [final answer to the problem].", with the final answer presented in LaTeX notation.
    ```

*   **Meta Meta Prompting:** This process entails utilizing LLMs to generate prompts that guide further tasks. This is a powerful technique for automating the prompt engineering process and creating more effective prompts.

    **Example:**
    **Prompt:**
    ```
    Create a detailed metaprompt to guide an AI in solving a differential equation. Your metaprompt
    should focus on ensuring that the AI identifies the type of differential equation, selects the
    appropriate method for solving it, and presents a clear, step-by-step solution. Include instructions
    for verifying the solution and summarizing the results.
    ```
    **Output:**
    ```
    To solve this differential equation, follow these steps carefully:
    1. Identify the Type: Begin by identifying the type of differential equation (e.g., first-order linear,
    separable, homogeneous, etc.). Briefly explain how you recognized the type.

    2. Apply an Appropriate Method: Based on the type, outline the most suitable method for
    solving it (e.g., separation of variables, integrating factor, etc.). If there are multiple
    approaches, describe why this particular method is the best choice.

    3. Show Step-by-Step Solution: Work through the equation step-by-step, explaining each
    manipulation and transformation in clear terms.

    4. Verify Solution: Once you find the solution, verify it by differentiating the result and
    substituting it back into the original equation to confirm it satisfies the differential equation.

    5. Summarize the Solution: Finally, summarize the general or particular solution, highlighting
    any constants or special conditions that apply.
    ```

*   **Prompt Chaining:** For complex queries, using a single prompt is often insufficient. Prompt chaining involves breaking down a task into different parts, each handled by a specific prompt. The output from one prompt serves as input for the next, creating a chain of reasoning and processing. A common use case is answering questions based on documents.

    **Example:**
    ```
    Extract any relevant quotes from the following text to answer the question "What was the main cause of the American Civil War?" [reference text follows]

    "One of the biggest catalysts of the Civil War was the issue of slavery, particularly the Southerners’ desire to bring slavery into the territories."
    "Tensions peaked when the Republican candidate Abraham Lincoln was elected president in 1860 with a platform opposing the extension of slavery into the territories, which isolated the South."

    Using the extracted quotes and the full text, provide a detailed answer to the question "What was the main cause of the American Civil War?"
    ```
    **Output:**
    ```
    The main cause of the American Civil War was the issue of slavery and its potential expansion into new territories and states. The South insisted on bringing slavery into new territories, while the election of Abraham Lincoln with an anti-slavery platform isolated the Southern states, which feared losing the right to own slaves. The fundamental disagreement over slavery between the North and South was the key factor that ultimately led to the outbreak of the Civil War.
    ```

*   **Role Prompting:** Involves asking the model to take on a specific role or act in a certain way. Can be used to improve accuracy in problem-solving. In this way, tone, style, and depth of information provided is adjusted accordingly. This adjusts the model's tone, style, and the depth of information it provides.

    **Examples:**
    *   Write a review of [pizza place]
    *   You are a food critic writing for the Michelin Guide. Write a review of [pizza place]
    *   You are a marketing expert. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems.
    *   You are a customer service representative. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems.

    **Example of Role Prompting effect:**
    *   Without RP: Write a review of [pizza place]
    *   With RP: You are a food critic writing for the Michelin Guide. Write a review of [pizza place]

    **Example of Role Prompting effect (Marketing Expert):**
    *   You are a marketing expert. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems. (focus on persuasion, positivity, and relationship)
      ```
      Subject: A Little More Wait for Your Wonderful Products

      Dear [Client's Name],

      We hope this message finds you buzzing with excitement for your upcoming
      order! However, even the best-laid plans sometimes face unexpected
      hiccups. We have hit a temporary logistics snag that will unfortunately
      add a small wait time to your delivery.

      While we understand this isn't ideal, we believe great things come to
      those who wait, and your order is no exception. Rest assured, from the
      moment we sort out this situation to the time your products reach you,
      we're committed to ensuring that your experience is nothing short of
      excellent.

      Thank you for your patience and understanding. Stay tuned for more
      updates, and please, don't hesitate to reach out if you have any questions
      — we're always here.

      Cheers, [Your Name]
      ```

    **Example of Role Prompting effect (Customer Service):**
    *   You are a customer service representative. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems. (More relational and solution-oriented)
      ```
      Subject: Attention Required: Update on Your Order Status

      Dear [Client's Name],

      Greetings from [Your Company Name].

      We wanted to inform you that, due to unforeseen logistical challenges,
      there might be a delay in the delivery of your order. We understand that
      this may cause some inconvenience, and we apologize sincerely.
      Meanwhile, we are working tirelessly to resolve this and ensure your
      delivery reaches you by [new estimated date]. We value your time and
      patience greatly.

      Please feel free to reach out if you have any questions or need further
      information. Our team is always here to assist you.

      Thank you for your understanding.

      Best regards, [Your Name]
      ```

*   **Structured Prompting:** Based on a semi-formal definition of instructions to help LLMs handle complex tasks more predictably. This involves creating prompts with defined sections and delimiters, enhancing the LLM's ability to parse and understand the prompt.

    *   The prompt is divided into sections and encoded into a script.
    *   Delimiters are used to help LLMs recognize each part of the prompt as distinct units of meaning.
    *   Any unique character sequence that wouldn't normally appear together can serve as a delimiter (e.g., `###`, `===`, `>>>`).
    *   Another approach is to use XML tags as delimiters.
    *   LLMs are often trained on web content and have learned to recognize and understand this formatting.

    **Example with custom delimiters:**
    ```
    Classify the sentiment of each conversation in <<<CONVERSATIONS>>> as ‘Positive’ or ‘Negative’. Give the sentiment classifications without any other preamble text.

    ###EXAMPLE CONVERSATIONS
    [Agent]: Good morning, how can I assist you today?
    [Customer]: This product is terrible, nothing like what was advertised!
    [Customer]: I’m extremely disappointed and expect a full refund.

    [Agent]: Good morning, how can I help you today?
    [Customer]: Hi, I just wanted to say that I’m really impressed with your product. It exceeded my expectations!
    ###

    ###EXAMPLE OUTPUTS
    Negative
    Positive
    ###

    <<<CONVERSATIONS>>>
    [Agent]: Hello! Welcome to our support. How can I help you today?
    [Customer]: Hi there! I just wanted to let you know I received my order, and it’s fantastic!
    [Agent]: That’s great to hear! We’re thrilled you’re happy with your purchase. Is there anything else I can assist you with?
    [Customer]: No, that’s it. Just wanted to give some positive feedback. Thanks for your excellent service!

    [Agent]: Hello, thank you for reaching out. How can I assist you today?
    [Customer]: I’m very disappointed with my recent purchase. It’s not what I expected at all.
    [Agent]: I’m sorry to hear that. Could you please provide more details so I can help?
    [Customer]: The product is of poor quality, and it arrived late. I’m really unhappy with this experience.
    >>>
    ```
    **Output:**
    ```
    Positive
    Negative
    ```

    **Example with XML tags:**
    ```xml
    <prompt>
        <instruction>Classify the sentiment of the following conversations into one of two classes, using the examples given. Give the sentiment classifications without any other preamble text.</instruction>
        <classes>
            <positive>Positive</positive>
            <negative>Negative</negative>
        </classes>
        <example-conversations>
            <conversation>
                [Agent]: Good morning, how can I assist you today?
                [Customer]: This product is terrible, nothing like what was advertised!
                [Customer]: I’m extremely disappointed and expect a full refund.
            </conversation>
            <conversation>
                [Agent]: Good morning, how can I help you today?
                [Customer]: Hi, I just wanted to say that I’m really impressed with your product. It exceeded my expectations!
            </conversation>
        </example-conversations>
        <example-classes>
            <class>Negative</class>
            <class>Positive</class>
        </example-classes>
        <conversations>
            <conversation>
                [Agent]: Hello! Welcome to our support. How can I help you today?
                [Customer]: Hi there! I just wanted to let you know I received my order, and it’s fantastic!
                [Agent]: That’s great to hear! We’re thrilled you’re happy with your purchase. Is there anything else I can assist you with?
                [Customer]: No, that’s it. Just wanted to give some positive feedback. Thanks for your excellent service!
            </conversation>
            <conversation>
                [Agent]: Hello, thank you for reaching out. How can I assist you today?
                [Customer]: I’m very disappointed with my recent purchase. It’s not what I expected at all.
                [Agent]: I’m sorry to hear that. Could you please provide more details so I can help?
                [Customer]: The product is of poor quality, and it arrived late. I’m really unhappy with this experience.
            </conversation>
        </conversations>
    </prompt>
    ```
    **Output:**
    ```
    Positive
    Negative
    ```

*   **Structured Prompting with CO-STAR:** The CO-STAR framework provides a structured approach to prompt creation by dividing the prompt into sections:

    *   **Context:** Provides background information on the task to help the LLM understand the specific scenario.
    *   **Objective:** Clearly defines the task the LLM should perform.
    *   **Style:** Specifies the desired writing style.
    *   **Tone:** Sets the response’s tone to match the desired sentiment or emotional context (e.g., formal, humorous, empathetic).
    *   **Audience:** Identifies the intended audience (experts, beginners, children, etc.).
    *   **Response:** Defines the response format to ensure compatibility with subsequent steps (e.g., free text, list, table, JSON).

*   **Generate Knowledge Prompting:** This method first prompts the LLM to generate relevant knowledge related to a task, and then incorporates that knowledge into the prompt along with the task description or question. This is particularly useful when the LLM lacks the specific information required to directly answer a query and leverages the LLM's capacity to generate supplementary knowledge beyond its base training domain.

    **Example:**
    ```
    List and describe the key factors that influence the evolution of life in
    environments with extreme gravitational forces, such as on a super-Earth
    planet. Focus on biological, physiological, and ecological adaptations
    that might arise in such conditions.

    Using the adaptations and factors you described earlier, design a
    hypothetical intelligent species that could evolve on a super-Earth planet
    with extreme gravitational forces. Include details about their physical
    structure, social behaviors, methods of tool use or communication, and how
    their adaptations influence their culture or technology.
    ```

*   **Retrieval Augmented Generation (RAG):** Retrieval-Augmented Generation (RAG) combines retrieval techniques with text generation. This addresses limitations in LLMs accessing updated or domain-specific data.

    *   A search or retrieval system (e.g., databases, search engines) is used to find relevant documents or data.
    *   An LLM is used to generate responses, conditioned on retrieved data.

<----------section---------->

### Additional Resources for Prompt Engineering Techniques

Several other advanced prompt engineering techniques are available:

*   **Prompt Engineering Guide:** <https://www.promptingguide.ai/>
    *   Tree of Thoughts
    *   Automatic Reasoning and Tool-use
    *   Automatic Prompt Engineer
    *   Active-Prompt
    *   Directional Stimulus Prompting
    *   Program-Aided Language Models
*   **Text-Base Prompt Tech. (List from figure 2.2)**
    |                     |                      |                      |                      |                      |
    | :------------------ | :------------------- | :------------------- | :------------------- | :------------------- |
    | **Zero-Shot**       | **Few-Shot**         | **Thought Generation** | **Decomposition**    | **Ensembling**       |
    | Emotion Prompting   | Exemplar Generation  | Chain-of-Thought(CoT) | DECOMP               | COSP                 |
    | Role Prompting      | Exemplar Ordering    | Zero-Shot CoT        | Faithful CoT         | DENSE                |
    | Style Prompting
<----------section---------->

**Natural Language Processing and Large Language Models: Representing Text**

This material originates from Lesson 2 of a Corso di Laurea Magistrale (Master's Degree course) in Ingegneria Informatica (Computer Engineering) focusing on Natural Language Processing (NLP) and Large Language Models (LLMs), taught by Nicola Capuano and Antonio Greco at the DIEM – University of Salerno. The core topic is how to represent text in a way that computers can understand and process.

**Outline**

The lesson covers the following key areas:

*   Tokenization: Breaking down text into individual units.
*   Bag of Words Representation: Converting text into numerical vectors.
*   Token Normalization: Standardizing tokens to reduce variations.
*   Stemming and Lemmatization: Reducing words to their root forms.
*   Part of Speech Tagging: Identifying the grammatical role of each token.
*   Introducing spaCy: A popular Python library for NLP.

`<----------section---------->`

**Tokenization**

Tokenization is the fundamental first step in most NLP pipelines. It involves segmenting a text into individual tokens, which can then be analyzed.

**Prepare the Environment**

Before delving into practical tokenization exercises, it's essential to set up the development environment. The preferred method is using Jupyter Notebooks, offering an interactive coding experience.

*   **Install Jupyter Extension for Visual Studio Code:** If using Visual Studio Code, installing the Jupyter extension enables seamless notebook integration.
*   **Install Jupyter:** In the command line, use `pip install jupyter` to install the Jupyter Notebook application.
*   **Create and activate a virtual environment:** Using virtual environments isolates project dependencies.
    *   Create: `python -m venv .env`
    *   Activate: `source .env/bin/activate` (on Linux/macOS) or `.env\Scripts\activate` (on Windows).
*   **Alternative: Google Colab:** For an online, cloud-based solution, Google Colab provides Jupyter Notebooks accessible through a web browser: <https://colab.research.google.com/>

For the practical sections, additional Python packages are required:

*   `pip install numpy pandas`

**Text Segmentation**

Text segmentation is a broader concept than tokenization, referring to the process of dividing text into meaningful units at different levels:

*   Paragraph Segmentation: Dividing a document into distinct paragraphs, often based on line breaks or formatting.
*   Sentence Segmentation: Breaking down paragraphs into individual sentences, using punctuation marks and language rules.
*   Word Segmentation: Dividing sentences into individual words, using spaces and punctuation as delimiters.

**Tokenization:**

*   Tokenization is a specialized form of text segmentation.
*   It specifically focuses on dividing text into smaller units called tokens.

`<----------section---------->`

**What is a Token?**

A token is a single, meaningful element within a text. It's the basic building block for further NLP analysis. Common types of tokens include:

*   Words: The most frequently used type of token.
*   Punctuation Marks: Symbols used to structure sentences, such as periods, commas, question marks, etc.
*   Emojis: Visual symbols conveying emotions or concepts.
*   Numbers: Numerical values and expressions.
*   Sub-words: Components within words that carry meaning, like prefixes (e.g., "re-", "pre-") and suffixes (e.g., "-ing", "-ed"). These are particularly useful in languages with rich morphology or for handling unknown words.
*   Phrases: Multi-word expressions that are treated as single units, such as "ice cream" or "kick the bucket".

`<----------section---------->`

**Tokenizer**

A simple approach to tokenization is to use whitespaces as delimiters between words.

*   Idea: Use whitespaces as the “delimiter” of words.
*   Limitation: This method isn't suitable for languages like Chinese, Japanese, and Thai, which lack explicit whitespace between words (continuous orthographic systems).

```python
sentence = "Leonardo da Vinci began painting the Mona Lisa at the age of 51."
token_seq = sentence.split()
print(token_seq)
# Output: ['Leonardo', 'da', 'Vinci', 'began', 'painting', 'the', 'Mona', 'Lisa', 'at', 'the', 'age', 'of', '51.']
```

The code demonstrates a basic tokenization using Python's `split()` method.  However, a more sophisticated tokenizer would separate "51" and "." as distinct tokens, which is a challenge addressed later in the lesson.

`<----------section---------->`

**Bag of Words Representation**

This section explains how to convert text into a numerical format that machine learning algorithms can process.

**Turning Words into Numbers**

**One-hot Vectors**

*   A vocabulary is created containing all the unique tokens in the dataset.  This is an exhaustive list of the tokens that the model will recognize.
*   Each word is represented as a vector.  The length of the vector is equal to the size of the vocabulary. All elements are zero except for the element at the index corresponding to the word, which is set to one.

```python
import numpy as np
import pandas as pd

token_seq = ['Leonardo', 'da', 'Vinci', 'began', 'painting', 'the', 'Mona', 'Lisa', 'at', 'the', 'age', 'of', '51.']
vocab = sorted(set(token_seq))
onehot_vectors = np.zeros((len(token_seq), len(vocab)), int)

for i, word in enumerate(token_seq):
    onehot_vectors[i, vocab.index(word)] = 1

df = pd.DataFrame(onehot_vectors, columns=vocab, index=token_seq)
print(df)
```

The code generates one-hot vectors for the tokens in the example sentence and displays them in a Pandas DataFrame, making the representation visually clear.

**Positive features:**

*   No information is lost: you can reconstruct the original document from a table of one-hot vectors.

**Negative Features:**

*   One-hot vectors are super-sparse, this results in a large table even for a short sentence.  Sparsity leads to computational inefficiency and increased memory usage.
*   Moreover, a language vocabulary typically contains at least 20,000 common words.  This number grows rapidly when you consider different forms of a word and named entities.
*   This number increases to millions when you consider word variations (conjugations, plurals, etc.) and proper nouns (names of people, places, organizations, etc.).

`<----------section---------->`

**One-hot Vectors: Scalability Issues**

The following example demonstrates the impracticality of using one-hot vectors for large text corpora:

Let's assume you have:

*   A million tokens in your vocabulary.
*   A small library of 3,000 short books with 3,500 sentences each and 15 words per sentence.

Calculations:

1.  Total tokens: 15 x 3,500 x 3,000 = 157,500,000 tokens
2.  Bits per token: 10<sup>6</sup> bits per token x 157,500,000 = 157.5 x 10<sup>12</sup> bits
3.  Total storage: 157.5 x 10<sup>12</sup> / (8 x 1024<sup>4</sup>) ≈ 17.9 TB

Conclusion: Using one-hot vectors for such a corpus would require an impractical amount of storage (17.9 TB).

`<----------section---------->`

**Bag-of-Words (BoW)**

BoW representation is a more compact alternative to one-hot vectors.

BoW: a vector obtained by summing all the one-hot vectors.

*   One bag for each sentence or short document.
*   Compresses a document down to a single vector representing its essence. The vector contains the counts of each word in the vocabulary for that particular document.
*   Lossy transformation: you can't reconstruct the initial text.

Binary BoW: each word presence is marked as 1 or 0, regardless of its frequency. This indicates whether a word exists in the document, but not how many times it occurs.

`<----------section---------->`

**Binary BoW: Example**

The code illustrates how to generate a vocabulary from a text corpus and then create Binary BoW vectors for each sentence.

Generating a vocabulary for a text corpus…

```python
sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))
print(vocab)
```

```
['1452.', '51.', 'A', 'Grand', 'In', 'Italy,', 'Leonardo', 'Lisa', 'Mona', 'Slam', 'Tennis', 'The', 'Vinci', 'Vinci,', 'a', 'addition', 'age', 'also', 'are', 'as', 'at', 'began', 'being', 'best', 'born', 'court', 'da', 'engineer.', 'events', 'five', 'four', 'in', 'is', 'match', 'middle.', 'most', 'net', 'of', 'on', 'or', 'painter,', 'painting', 'played', 'prestigious', 'rectangular', 'sets.', 'skilled', 'tennis', 'tennis.', 'the', 'three', 'to', 'tournaments', 'typically', 'was', 'with']
```

`<----------section---------->`

**Binary BoW: Example**

Generating a BoW vector for each text…

```python
import numpy as np
import pandas as pd

sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))

bags = np.zeros((len(sentences), len(vocab)), int)
for i, sentence in enumerate(sentences):
    for j, word in enumerate(sentence.split()):
        bags[i, vocab.index(word)] = 1

df = pd.DataFrame(bags, columns=vocab)
print(df.transpose())
```

The code generates binary BoW vectors for each sentence and represents them in a Pandas DataFrame, with rows representing words in the vocabulary and columns representing sentences. A '1' indicates the presence of the word in the sentence.

For display purposes only.

`<----------section---------->`

**Binary BoW: Analysis**

*   You can see little overlap in word usage for some sentences…
*   We can use this overlap to compare documents or search for similar documents.  The more words two documents share in their BoW representation, the more semantically similar they are likely to be.

`<----------section---------->`

**Bag-of-Words Overlap**

Measuring the bag of words overlap for two texts...

*   we can get a (good?) estimate of how similar they are in the words they use. This is a simplification, as it doesn't account for word order or context.
*   and this is a (good?) estimate of how similar they are in meaning. This is a reasonable first approximation but can be inaccurate due to the limitations of the BoW model.

Idea: use the dot product.  The dot product of two BoW vectors gives a measure of the number of words they have in common.

```python
import numpy as np
sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))

bags = np.zeros((len(sentences), len(vocab)), int)
for i, sentence in enumerate(sentences):
    for j, word in enumerate(sentence.split()):
        bags[i, vocab.index(word)] = 1

print(np.dot(bags[0], bags[2])) #comparing sentence 0 and 2
print(np.dot(bags[3], bags[5])) #comparing sentence 3 and 5
```

This code calculates the dot product between the BoW vectors of different sentences, providing a simple metric for semantic similarity.

`<----------section---------->`

**Token Normalization**

Token normalization aims to reduce variations in tokens, making them more consistent and improving the accuracy of NLP tasks.

**Tokenizer Improvement**

Not only spaces are used to separate words:

*   `\t` (tab), `\n` (newline), `\r` (return), ...
*   punctuation (commas, periods, quotes, semicolons, dashes, ...)

We can improve our tokenizer with regular expressions. Regular expressions offer a flexible way to define tokenization rules.

```python
import re

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
print(token_seq)
# Output: ['Leonardo', 'was', 'born', 'in', 'Vinci', 'Italy', 'in', '1452']
```

The code uses a regular expression to split the sentence, removing punctuation and whitespace. `[-\s.,;!?]+` matches one or more occurrences of hyphens, whitespace characters, periods, commas, semicolons, question marks, or exclamation points. The list comprehension removes any empty strings from the token sequence.

`<----------section---------->`

**Tokenizer Improvement: Handling Complex Cases**

But… what would happen with these sentences? The current regular expression tokenizer might not handle all cases correctly.

*   The company’s revenue for 2023 was $1,234,567.89.
*   The CEO of the U.N. (United Nations) gave a speech.
*   It’s important to know the basics of A.I. (Artificial Intelligence).
*   He didn’t realize the cost was $12,345.67.
*   Her new book, ‘Intro to NLP (Natural Language Processing)’, is popular.
*   The temperature in Washington, D.C. can reach 100°F in the summer.

Tokenizers can easily become complex ...… but NLP libraries can help us (we will see them later).  The example sentences demonstrate that creating a robust tokenizer requires careful consideration of various edge cases, making it a challenging task. Libraries like NLTK and spaCy provide pre-built tokenizers that handle many of these complexities.

`<----------section---------->`

**Case Folding**

Consolidates multiple “spellings” of a word that differ only in their capitalization under a single token.

*   Tennis → tennis, A → a, Leonardo → leonardo, …
*   A.K.A. Case normalization

**Advantages:**

*   Improves text matching and recall in search engines. By ignoring capitalization, a search for "tennis" will also return documents containing "Tennis".

**Disadvantages:**

*   Loss of distinction between proper and common nouns. Important for tasks like Named Entity Recognition.
*   May alter the original meaning (e.g., US → us).

`<----------section---------->`

**Case Folding: Example**

```python
import re

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
token_seq = [x.lower() for x in token_seq] # case folding

print(token_seq)
# Output: ['leonardo', 'was', 'born', 'in', 'vinci', 'italy', 'in', '1452']
```

The code demonstrates case folding by converting all tokens to lowercase.

A lot of meaningful capitalization is “normalized” away.

*   We can just normalize first-word-in-sentence capitalization ... This can be a better approach in some circumstances.
*   ... but the first word can be a proper noun.
*   We can first detect proper nouns and then normalizing only the remaining words … A more advanced approach involves Named Entity Recognition (NER) to identify proper nouns and avoid lowercasing them.
*   ... we will see Named Entity Recognition later.

`<----------section---------->`

**Stop Words**

Common words that occur with a high frequency but carry little information about the meaning of a sentence.

*   Articles, prepositions, conjunctions, forms of the verb “to be”, …
*   These words can be filtered out to reduce noise. By removing stop words, you can focus on the more informative words in a text, potentially improving the performance of some NLP tasks.

```python
stop_words = [
    "a", "about", "after", "all", "also", "an", "and", "any", "are", "as", "at", "be", "because",
    "been", "but", "by", "can", "co", "corp", "could", "for", "from", "had", "has", "have", "he",
    "her", "his", "if", "in", "inc", "into", "is", "it", "its", "last", "more", "most", "mr",
    "mrs", "ms", "mz", "no", "not", "of", "on", "one", "only", "or", "other", "out", "over", "s",
    "says", "she", "so", "some", "such", "than", "that", "the", "their", "there", "they", "this",
    "to", "up", "was", "we", "were", "when", "which", "who", "will", "with", "would"
]

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
token_seq = [x.lower() for x in token_seq] # case folding
token_seq = [x for x in token_seq if x not in stop_words] # remove stop words

print(token_seq)
# Output: ['leonardo', 'born', 'vinci', 'italy', '1452']
```

The code demonstrates stop word removal using a predefined list.

`<----------section---------->`

**Stop Words: Disadvantages**

Even though the stop words carry little information, they can provide important relational information.  Removing stop words can sometimes negatively impact performance, especially when the relationships between words are important.

*   Mark reported to the CEO → Mark reported CEO
*   Suzanne reported as the CEO to the board → Suzanne reported CEO board

Italian stop words: The code lists an extensive set of Italian stop words. This illustrates that stop word lists are language-specific.
(The list is omitted here for brevity but was present in the original text.)

`<----------section---------->`

**Putting All Together**

The following code combines the tokenization steps: punctuation removal, void token removal, case folding, and stop word removal.

```python
import re
import numpy as np
import pandas as pd

stop_words = [
    "a", "about", "after", "all", "also", "an", "and", "any", "are", "as", "at", "be", "because",
    "been", "but", "by", "can", "co", "corp", "could", "for", "from", "had", "has", "have", "he",
    "her", "his", "if", "in", "inc", "into", "is", "it", "its", "last", "more", "most", "mr",
    "mrs", "ms", "mz", "no", "not", "of", "on", "one", "only", "or", "other", "out", "over", "s",
    "says", "she", "so", "some", "such", "than", "that", "the", "their", "there", "they", "this",
    "to", "up", "was", "we", "were", "when", "which", "who", "will", "with", "would"
]

sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

def tokenize(sentence):
    token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
    token_seq = [token for token in token_seq if token] # remove void tokens
    token_seq = [x.lower() for x in token_seq] # case folding
    token_seq = [x for x in token_seq if x not in stop_words] # remove stop words
    return token_seq

tok_sentences = [tokenize(sentence) for sentence in sentences]
all_tokens = [x for tokens in tok_sentences for x in tokens]
vocab = sorted(set(all_tokens))

bags = np.zeros((len(tok_sentences), len(vocab)), int)
for i, sentence in enumerate(tok_sentences):
    for j, word in enumerate(sentence):
        bags[i, vocab.index(word)] = 1

df = pd.DataFrame(bags, columns=vocab)
print(df.transpose())
```

This code defines a `tokenize` function and then applies it to a list of sentences. It then creates a BoW representation of the tokenized sentences.

`<----------section---------->`

**Putting All Together: Output**

```
          0    1    2    3    4    5
1452     0    1    0    0    0    0
51       1    0    0    0    0    0
addition 0    0    1    0    0    0
age      1    0    0    0    0    0
began    1    0    0    0    0    0
being    0    0    1    0    0    0
best     0    0    0    0    0    1
born     0    1    0    0    0    0
court    0    0    0    1    0    0
da       1    0    1    0    0    0
engineer 0    0    1    0    0    0
events   0    0    0    0    1    0
five     0    0    0    0    0    1
four     0    0    0    0    1    0
grand    0    0    0    0    1    0
italy    0    1    0    0    0    0
leonardo 1    1    1    0    0    0
lisa     1    0    0    0    0    0
match    0    0    0    0    0    1
middle   0    0    0    1    0    0
mona     1    0    0    0    0    0
net      0    0    0    1    0    0
painter  0    0    1    0    0    0
painting 1    0    0    0    0    0
played   0    0    0    1    0    1
prestigious 0    0    0    0    1    0
rectangular 0    0    0    1    0    0
sets     0    0    0    0    0    1
skilled  0    0    1    0    0    0
slam     0    0    0    0    1    0
tennis   0    0    0    0    1    1
three    0    0    0    0    0    1
tournaments 0    0    0    0    1    0
typically 0    0    0    0    0    1
vinci    1    1    0    0    0    0
```

The output shows the resulting binary BoW representation after applying the defined tokenization and normalization steps.

`<----------section---------->`

**Using NLTK**

The Natural Language Toolkit (NLTK) is a popular NLP library.

*   `pip install nltk`
*   It includes more refined tokenizers. NLTK provides various tokenizers tailored to different needs.

```python
import nltk

nltk.download('punkt') # download the Punkt tokenizer models
text = "Good muffins cost $3.88\nin New York. Please buy me two of them.\n\nThanks."

print(nltk.tokenize.word_tokenize(text)) # word tokenization
print(nltk.tokenize.sent_tokenize(text)) # sentence tokenization
```

```
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
['Good muffins cost $3.88\nin New York.', 'Please buy me two of them.', 'Thanks.']
```

The code demonstrates NLTK's `word_tokenize` and `sent_tokenize` functions. The `punkt` tokenizer models are required for sentence tokenization and need to be downloaded.  This shows how NLTK separates words and sentences more effectively than a simple split.

`<----------section---------->`

**Using NLTK: Stop Words**

The Natural Language Toolkit is a popular NLP library.

*   `pip install nltk`
*   It includes extended stop-word lists for many languages. NLTK provides pre-defined stop word lists for numerous languages, simplifying stop word removal.

```python
import nltk

nltk.download('stopwords') # download the stop words corpus
text = "This is an example sentence demonstrating how to remove stop words using NLTK."

tokens = nltk.tokenize.word_tokenize(text)
stop_words = set(nltk.corpus.stopwords.words('english'))
filtered_tokens = [x for x in tokens if x not in stop_words]

print("Original Tokens:", tokens)
print("Filtered Tokens:", filtered_tokens)
```

```
Original Tokens: ['This', 'is', 'an', 'example', 'sentence', 'demonstrating', 'how', 'to', 'remove', 'stop', 'words', 'using', 'NLTK', '.']
Filtered Tokens: ['This', 'example', 'sentence', 'demonstrating', 'remove', 'stop', 'words', 'using', 'NLTK', '.']
```

The code showcases stop word removal using NLTK's built-in stop word list for English.  The `stopwords` corpus needs to be downloaded before use.

`<----------section---------->`

**Stemming and Lemmatization**

Stemming and lemmatization are techniques for reducing words to their root forms, helping to consolidate variations of the same word. However, additional context was also added.

**Stemming**

Identifies a common stem among various forms of a word.

*   E.g., Housing and houses share the same stem: house

**Function:**

*   Removes suffixes to combine words with similar meanings under the same token (stem).
*   A stem isn’t required to be a properly spelled word: Relational, Relate, Relating all stemmed to Relat

**Benefits:**

*   Helps generalize your vocabulary.
*   Important for information retrieval (improves recall).

**Drawbacks**

Stemming can also make your model less precise, because it will treat all spelling variations
of a given root word the same. For example "chat", "chatter", "chatty",
"chatting", and perhaps even "chatbot" would all be treated the same in an
NLP pipeline with lemmatization, even though they have different meanings.
Likewise "bank", "banked", and "banking" would be treated the same by a
stemming pipeline despite the river meaning of "bank", the motorcycle
meaning of "banked" and the finance meaning of "banking."

As you work through this section, think about words where lemmatization
would drastically alter the meaning of a word, perhaps even inverting its
meaning and producing the opposite of the intended response from your
pipeline. This scenario is called spoofing — when you try to elicit the wrong
response from a machine learning pipeline by cleverly constructing a difficult
input.

`<----------section---------->`

**A Naive Stemmer**

```python
def simple_stemmer(word):
    suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

words = ["running", "happily", "stopped", "curious", "cries", "effective", "runs", "management"]
print({simple_stemmer(word) for word in words})
# Output: {'cur', 'runn', 'stopp', 'run', 'effect', 'manage', 'happi', 'cr'}
```

Very basic example...

*   Doesn't handle exceptions, multiple suffixes, or words that require more complex modifications.  This simple stemmer illustrates the basic idea of suffix removal, but it's not very accurate.

NPL libraries include more accurate stemmers.

`<----------section---------->`

**Porter Stemmer**

The Porter Stemmer is a widely used algorithm with a set of rules for removing suffixes.
It includes these steps:

*   Step 1a: Remove s and es endings
    *   cats → cat, buses → bus
*   Step 1b: Remove ed, ing, and at endings
    *   hoped → hope, running → run
*   Step 1c: Change y to i if preceded by a consonant
    *   happy → happi, cry → cri
*   Step 2: Remove "nounifying" endings such as ational, tional, ence, and able
    *   relational → relate, dependence → depend

`<----------section---------->`

**Porter Stemmer (Continued)**

*   Step 3: Remove adjective endings such as icate, ful, and alize
    *   duplicate → duplic, hopeful → hope
*   Step 4: Remove adjective and noun endings such as ive, ible, ent, and ism
    *   effective → effect, responsible → respons
*   Step 5a: Remove stubborn e endings
    *   probate → probat, rate → rat
*   Step 5b: Reduce trailing double consonants ending in l to a single l
    *   controlling → controll → control, rolling → roll → rol

```python
import nltk

texts = [
    "I love machine learning.",
    "Deep learning is a subset of machine learning.",
    "Natural language processing is fun.",
    "Machine learning can be used for various tasks."
]

stemmed_texts = []
stemmer = nltk.stem.PorterStemmer() # Initialize the Porter Stemmer

for text in texts:
    tokens = nltk.tokenize.word_tokenize(text.lower())
    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]
    stemmed_texts.append(' '.join(stemmed_tokens))

for text in stemmed_texts:
    print(text)
```

```
i love machin learn
deep learn is a subset of machin learn
natur languag process is fun
machin learn can be use for variou task
```

The code demonstrates how to use the Porter Stemmer in NLTK. It first tokenizes the text, converts it to lowercase, and then applies the stemmer.

`<----------section---------->`

**Snowball Project**

Provides stemming algorithms for several languages.

*   <https://snowballstem.org/>

Italian stemming examples:

| Input           | Stem     |
|-----------------|----------|
| abbandonata     | abbandon |
| abbandonate     | abbandon |
| abbandonati     | abbandon |
| abbandonato     | abbandon |
| abbandonava     | abbandon |
| abbandonera     | abbandon |
| abbandoneranno  | abbandon |
| abbandonerà      | abbandon |
| abbandono        | abbandon |
| abbandono        | abbandon |
| abbaruffato      | abbaruff |
| abbassamento     | abbass   |
| abbassando       | abbass   |
| abbassandola     | abbass   |
| abbassandole     | abbass   |
| abbassare        | abbass   |
| abbassarono      | abbass   |

(Table truncated for brevity; original table was present in source material.)

Supported in NLTK.  This highlights that stemming algorithms exist for many languages.

```python
import nltk
nltk.download('punkt')
stemmer = nltk.stem.PorterStemmer()
```

Other example words:

| Input          | Stem      |
|----------------|-----------|
| pronto         | pront     |
| pronuncerà     | pronunc   |
| pronuncia      | pronunc   |
| pronunciamento | pronunc   |
| pronunciare    | pronunc   |
| pronunciarsi   | pronunc   |
| pronunciata    | pronunc   |
| pronunciate    | pronunc   |
| pronunciato    | pronunc   |
| pronunzia      | pronunz   |
| pronunziano    | pronunz   |
| pronunziare    | pronunz   |
| pronunziarle   | pronunz   |
| pronunziato    | pronunz   |
| pronunzio      | pronunz   |
| pronunzio      | pronunz   |
| propaga        | propag    |
| propagamento   | propag    |

(Table truncated for brevity; original table was present in source material.)

`<----------section---------->`

**Lemmatization**

Determines the dictionary form (lemma) of a word.

*   Considers the context of the word. This is a key difference from stemming.
*   Uses dictionaries and language rules (morphological analysis). This is why it's more accurate than stemming.
*   Requires to prior identify the part of speech (PoS) of the word (verb, noun, adjective, ...)

**Lemmatization vs Stemming:**

*   Lemmatization always produces a valid lemma; stemming may produce roots that are not actual words.
*   Lemmatization is slower; stemming is faster.

|           | Lemmatization        | Stemming           |
|-----------|----------------------|--------------------|
| English   | went → go            | went → went        |
|           | ate → eat            | ate → at           |
<----------section---------->

Natural Language Processing and Large Language Models. This material is from Lesson 20 of the Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering) on Retrieval Augmented Generation (RAG). It was prepared by Nicola Capuano and Antonio Greco, DIEM – University of Salerno.

**Outline**

This lesson will cover the following topics:

*   Introduction to RAG
*   Introduction to LangChain
*   Building a RAG with LangChain and HuggingFace

<----------section---------->

**Introduction to RAG**

**What is RAG?**

Large Language Models (LLMs) are capable of sophisticated reasoning across a broad spectrum of topics. However, they have inherent limitations:

*   **Limited Knowledge:** LLMs are constrained by the data they were trained on. Their understanding of the world is fixed at the point of their last training update.
*   **Inability to Access New Information:** They cannot inherently access or reason about information introduced after their training cutoff date. This means they are unable to provide insights on current events or recently published data.
*   **Lack of Access to Private Data:** LLMs, in their default configuration, cannot reason about private, proprietary, or sensitive data. This is a major limitation in enterprise environments where data security and privacy are paramount.

Retrieval Augmented Generation (RAG) is a technique designed to overcome these limitations. RAG enhances the knowledge of LLMs by providing them with access to additional data sources. RAG enables the creation of AI applications that can:

*   Reason about private or proprietary data in a secure and compliant manner.
*   Incorporate and reason about data introduced after a model’s cutoff date, ensuring up-to-date and relevant responses.
*   Effectively utilize external knowledge sources to enhance the quality and accuracy of LLM outputs.

<----------section---------->

**RAG Concepts**

A typical RAG application consists of two primary components:

*   **Indexing:** This is a pipeline responsible for ingesting data from various sources and indexing it for efficient retrieval. Indexing typically happens offline as a preparatory step. This process transforms raw data into a searchable format.
*   **Retrieval and Generation:** This component operates at runtime, processing user queries and generating responses. The retrieval and generation process consists of the following steps:
    *   **Query Input:** Receives the user's query as input.
    *   **Data Retrieval:** Retrieves the relevant data from the index based on the user's query. This step leverages the indexed data to find information pertinent to the query.
    *   **Prompt Generation:** Generates a prompt that combines the user query with the retrieved data. The prompt is carefully crafted to provide the LLM with the necessary context.
    *   **LLM Generation:** Uses an LLM to generate an answer based on the augmented prompt. The LLM leverages both the original query and the retrieved context to produce a well-informed response.

<----------section---------->

**Indexing**

The indexing process prepares external data for retrieval. The indexing pipeline typically involves the following stages:

*   **Load:** The initial step involves loading data from a variety of sources. Popular RAG frameworks provide document loaders to handle a wide range of formats, including:
    *   File Formats: PDF, CSV, HTML, JSON, etc.
    *   Data Sources: File systems, websites, databases, etc.
*   **Split:** Large documents are split into smaller, more manageable chunks. This splitting is done for two primary reasons:
    *   **Indexing Efficiency:** Smaller chunks are easier to search and index, resulting in faster retrieval times.
    *   **LLM Context Window:** Smaller chunks ensure that the relevant information fits within the LLM's finite context window, which limits the amount of text the model can process at once.
*   **Store:** The final step involves storing and indexing the split chunks. This is often done using a Vector Store, which allows for efficient semantic search.

<----------section---------->

**Indexing: Vector Stores and Embeddings**

Vector stores are used to represent text splits as vector representations (embeddings).

**Vector Stores**

These are specialized data stores that are designed to enable indexing and retrieving information based on embeddings.

*   **Recap: Embeddings:** Embeddings are vector representations that capture the semantic meaning of data. They encode the meaning of text in a numerical format.
*   **Advantage:** Information retrieval is based on semantic similarity rather than keyword matching. This allows for more nuanced and relevant search results.

<----------section---------->

**Retrieval and Generation**

Given a user input, relevant splits are retrieved from the vector store. An LLM then generates an answer using a prompt that includes both the question and the retrieved data. This allows the LLM to leverage external knowledge to provide accurate and contextually relevant responses.

<----------section---------->

**Introduction to LangChain**

**LangChain**

LangChain is a framework designed to simplify the development of applications that utilize LLMs.

*   It provides a set of building blocks to incorporate LLMs into applications, making it easier to integrate these models into complex workflows.
*   It connects to third-party LLMs (like OpenAI and HuggingFace), enabling access to a wide range of models. It also connects to various data sources (such as Slack or Notion), and external tools, allowing for seamless integration with existing systems.
*   It enables the chaining of different components to create sophisticated workflows, facilitating the creation of complex NLP pipelines.
*   It supports several use cases like chatbots, document search, RAG, Q&A, data processing, information extraction, etc., providing a versatile platform for various NLP tasks.
*   It has both open-source and commercial components, offering flexibility and scalability for different needs.

<----------section---------->

**Key Components of LangChain**

LangChain provides a rich set of components that can be combined to build powerful NLP applications:

*   **Prompt Templates:** These facilitate translating user input into language model instructions. They support both string and message list formats, allowing for flexible prompt engineering.
*   **LLMs:** These are third-party language models that accept strings or messages as input and return strings as output. LangChain provides interfaces to connect with various LLM providers.
*   **Chat Models:** These are third-party models designed for conversational applications. They use sequences of messages as input and output, supporting distinct roles within conversational messages to maintain context and coherence.
*   **Example Selectors:** These dynamically select and format concrete examples in prompts to enhance model performance. By providing relevant examples, they guide the model towards more accurate and contextually appropriate outputs.
*   **Output Parsers:** These convert model-generated text into structured formats (e.g., JSON, XML, CSV). They support error correction and advanced features, ensuring that the output is easily consumable by other applications.
*   **Document Loaders:** These load documents from various data sources, providing a unified interface for accessing data regardless of its source or format.
*   **Vector Stores:** These are systems for storing and retrieving unstructured documents and data using embedding vectors. They enable efficient semantic search and retrieval.
*   **Retrievers:** These are interfaces for document and data retrieval that are compatible with vector stores and other external sources. They provide a flexible way to access and retrieve relevant information.
*   **Agents:** These are systems leveraging LLMs for reasoning to decide actions based on user inputs. They can interact with external tools and data sources to accomplish complex tasks.

<----------section---------->

**Installation**

To get started with LangChain, you need to install the core library and any relevant extensions. Here are the basic installation steps:

*   Install the core LangChain library:

    ```bash
    pip install langchain
    ```
*   Install community-contributed extensions:

    ```bash
    pip install langchain_community
    ```
*   Installs Hugging Face integration for LangChain:

    ```bash
    pip install langchain_huggingface
    ```
*   Install a library to load, parse, and extract text from PDF:

    ```bash
    pip install pypdf
    ```
*   Install the FAISS vector store (CPU version):

    ```bash
    pip install faiss-cpu
    ```

<----------section---------->

**Preliminary Steps**

Before using the Hugging Face integration, you need to:

*   Obtain a Hugging Face access token. Copy your Access token. This token is required to authenticate with the Hugging Face Hub.
*   Gain access to the Mistral-7B-Instruct-v0.2 model by accepting the user license: [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) This step is necessary to ensure you have the rights to use the model.

<----------section---------->

**Query a LLM Model**

The following code demonstrates how to query a LLM model using LangChain and Hugging Face:

```python
from langchain_huggingface import HuggingFaceEndpoint
import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR_API_TOKEN"

llm = HuggingFaceEndpoint(
    repo_id = "mistralai/Mistral-7B-Instruct-v0.2",
    temperature = 0.1
)

query = "Who won the FIFA World Cup in the year 2006?"
print(llm.invoke(query))
```

This code will output:

```
The FIFA World Cup in the year 2006 was won by the Italian national football team. They defeated France in the final match held on July 9, 2006, at the Allianz Arena in Munich, Germany. The Italian team was coached by Marcello Lippi and was led by the legendary goalkeeper Gianluigi Buffon. The team's victory was significant as they had not won the World Cup since 1982. The final match ended in a 1-1 draw after extra time, and the Italians won the penalty shootout 5-3. The winning goal in the shootout was scored by Andrea Pirlo.
```

**NOTE:** It is recommended to save your API key in an environment variable for security and ease of use.

*   On macOS or Linux:

    ```bash
    export HUGGINGFACEHUB_API_TOKEN="api_token"
    ```
*   On Windows with PowerShell:

    ```powershell
    setx HUGGINGFACEHUB_API_TOKEN "api_token"
    ```

<----------section---------->

**Prompt Templates**

Prompt templates are predefined text structures used to create dynamic and reusable prompts for interacting with LLMs. They allow you to easily format user input and inject variables into the prompt.

```python
from langchain.prompts import PromptTemplate

template = "Who won the {competition} in the year {year}?"
prompt_template = PromptTemplate(
    template = template,
    input_variables = ["competition", "year"]
)

query = prompt_template.invoke({"competition": "Davis Cup", "year": "2018"})
answer = llm.invoke(query)

print(answer)
```

This code will output:

```
The Davis Cup in the year 2018 was won by Croatia. They defeated France in the final held in Lille, France. The Croatian team was led by Marin Cilic and Borna Coric, while the French team was led by Jo-Wilfried Tsonga and Lucas Pouille. Croatia won the tie 3-2. This was Croatia's first Davis Cup title.
```

<----------section---------->

**Introduction to Chains**

Chains combine multiple steps in an NLP pipeline, automating complex tasks and integrating external systems.

*   The output of each step becomes the input for the next, creating a seamless workflow.
*   Chains are useful to automate complex tasks and integrate external systems, enabling the creation of more sophisticated applications.

```python
chain = prompt_template | llm
answer = chain.invoke({"competition": "Davis Cup", "year": "2018"})

print(answer)
```

This code will output:

```
The Davis Cup in the year 2018 was won by Croatia. They defeated France in the final held in Lille, France. The Croatian team was led by Marin Cilic and Borna Coric, while the French team was led by Jo-Wilfried Tsonga and Lucas Pouille. Croatia won the tie 3-2. This was Croatia's first Davis Cup title.
```

<----------section---------->

**Chains: Refining LLM Output**

Chains can be used to refine the LLM output for future processing, allowing for more structured and usable results.

```python
followup_template = """
task: extract only the name of the winning team from the following text
output format: json without formatting

example: {{"winner": "Italy"}}

### {text} ###
"""

followup_prompt_template = PromptTemplate(
    template = followup_template,
    input_variables = ["text"]
)

followup_chain = followup_prompt_template | llm

print(followup_chain.invoke({"text": answer}))
```

This code will output:

```json
{"winner": "Croatia"}
```

<----------section---------->

**Chaining All Together**

LangChain allows you to create complex chains by combining multiple prompts and LLMs.

```python
from langchain_core.runnables import RunnablePassthrough

chain = (
    prompt_template
    | llm
    | {"text": RunnablePassthrough()}
    | followup_prompt_template
    | llm
)

print(chain.invoke({"competition": "Davis Cup", "year": "2018"}))
```

This code will output:

```json
{"winner": "Croatia"}
```

The `RunnablePassthrough` object forwards the output of the previous step to the next step associated with a specific dictionary key, allowing for flexible data flow within the chain.

<----------section---------->

**More on Chains: LCEL**

LCEL (LangChain Expression Language) is a syntax to create modular pipelines for chaining operations, making it easier to build and manage complex NLP workflows.

*   **Pipe Syntax:** The `|` operator allows for easy chaining of operations, creating a readable and intuitive syntax.
*   **Modular and Reusable Components:** LCEL supports modular and reusable components, promoting code reuse and simplifying maintenance.
*   **Branching Logic and Follow-Up Queries:** It can handle branching logic or follow-up queries, enabling the creation of dynamic and adaptive NLP pipelines.

LangChain includes Predefined Chains for common tasks like question answering, document summarization, conversational agents, etc. Documentation: [https://python.langchain.com/docs/](https://python.langchain.com/docs/)

<----------section---------->

**Building a RAG with LangChain and HuggingFace**

**Example Project**

This section demonstrates building a RAG system capable of answering queries on documents from the Census Bureau US, an agency responsible for collecting statistics about the nation, its people, and its economy. The first step is to download the documents to be indexed:

```python
from urllib.request import urlretrieve
import os

os.makedirs("us_census", exist_ok = True)

files = [
    "https://www.census.gov/content/dam/Census/library/publications/2022/demo/p70-178.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-017.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-016.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-015.pdf",
]

for url in files:
    file_path = os.path.join("us_census", url.split("/")[-1])
    urlretrieve(url, file_path)
```

This code downloads several PDF documents from the Census Bureau website and stores them in a directory named "us_census".

<----------section---------->

**Document Loaders**

LangChain provides components used to extract content from diverse data sources, simplifying the process of ingesting data into the RAG pipeline:

*   `TextLoader`: Handles plain text files, making it easy to load and process text-based data.
*   `PyPDFLoader`: Extracts content from PDF documents, enabling the ingestion of information stored in PDF format.
*   `CSVLoader`: Reads tabular data from CSV files, allowing for the incorporation of structured data into the RAG system.
*   `WebBaseLoader`: Extracts content from web pages, enabling the retrieval of information from online sources.
*   `WikipediaLoader`: Fetches Wikipedia pages, providing access to a vast repository of knowledge.
*   `SQLDatabaseLoader`: Queries SQL databases to fetch and load content, allowing for the integration of data stored in relational databases.
*   `MongoDBLoader`: Extracts data from MongoDB collections, enabling the use of NoSQL databases as a data source.
*   `IMAPEmailLoader`: Loads emails using the IMAP protocol from various providers, facilitating the processing of email content.
*   `HuggingFaceDatasetLoader`: Fetches datasets from the Hugging Face datasets library, providing access to a wide range of pre-processed datasets.
*   and many others…

<----------section---------->

**Extract Content from PDFs**

This section demonstrates how to extract text from PDF documents using LangChain's document loaders.

We can use `PyPDFLoader` to extract text from a single PDF document:

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("us_census/p70-178.pdf")
doc = loader.load()
print(doc[0].page_content[0:100]) # Print the first page (first 100 characters)
```

This code will output the first 100 characters of the first page of the specified PDF document:

```
Occupation, Earnings, and Job
Characteristics

July 2022

P70-178

Clayton Gumber and Briana Sullivan
```

Alternatively, we can use `PyPDFDirectoryLoader` to fetch a set of PDF documents from a folder:

```python
from langchain_community.document_loaders import PyPDFDirectoryLoader

loader = PyPDFDirectoryLoader("./us_census/")
docs = loader.load()

print(docs[60].page_content[0:100]) # The 61st page (documents are concatenated)
print('\n' + docs[60].metadata['source']) # The source of the page 61st page
```

This code will output the first 100 characters of the 61st page (documents are concatenated) and the source of that page:

```
U.S. Census Bureau 19

insurance, can exacerbate the dif-
ferences in monetary compensa -
tion and w

us_census/p70-178.pdf
```

<----------section---------->

**Text Splitters**

LangChain provides components used to break large text documents into smaller chunks, improving indexing and retrieval efficiency:

*   `CharacterTextSplitter`: Splits text into chunks based on a character count, providing a simple way to divide documents.
*   `RecursiveCharacterTextSplitter`: Attempts to split text intelligently by using hierarchical delimiters, such as paragraphs and sentences, to preserve semantic coherence.
*   `TokenTextSplitter`: Splits text based on token count rather than characters, ensuring that chunks are of a consistent size in terms of tokens.
*   `HTMLHeaderTextSplitter`: Splits HTML documents by focusing on headers (e.g., `<h1>`, `<h2>`) to create meaningful sections from structured web content.
*   `HTMLSectionSplitter`: Divides HTML content into sections based on logical groupings or structural markers, enabling more granular control over document splitting.
*   `NLPTextSplitter`: Uses NLP to split text into chunks based on semantic meaning, attempting to create chunks that represent complete ideas or concepts.
*   and many others…

<----------section---------->

**Split Text in Chunks**

This section demonstrates how to split text into chunks using LangChain's `RecursiveCharacterTextSplitter`.

We use `RecursiveCharacterTextSplitter` to obtain:

*   Chunks of about 700 characters, balancing the need for context with the limitations of the LLM's context window.
*   Overlapped for about 50 characters, ensuring that context is preserved across chunks.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 700,
    chunk_overlap = 50,
)

chunks = text_splitter.split_documents(docs)

print(chunks[0].page_content) # The first chunk
```

This code will output the content of the first chunk:

```
Health Insurance Coverage Status and Type
by Geography: 2021 and 2022

American Community Survey Briefs
ACSBR-015
```

<----------section---------->

**Split Text in Chunks: Analysis**

This section analyzes the effect of splitting the text into chunks, comparing the average document length before and after the splitting process.

```python
len_chunks = len(chunks)
avg_docs = sum([len(doc.page_content) for doc in docs]) // len(docs)
avg_chunks = sum([len(chunk.page_content) for chunk in chunks]) // len(chunks)

print(f'Before split: {len(docs)} pages, with {avg_docs} average characters.')
print(f'After split: {len(chunks)} chunks, with {avg_chunks} average characters.')
```

This code will output the number of pages and average character count before splitting, and the number of chunks and average character count after splitting:

```
Before split: 63 pages, with 3840 average characters.
After split: 398 chunks, with 624 average characters.
```

<----------section---------->

**Index Chunks: Embeddings**

Embeddings are used to index text chunks, enabling semantic search and retrieval. They transform text into numerical vectors that capture the meaning of the text.

*   We can use pre-trained embedding models from Hugging Face, leveraging existing models to avoid the need for training our own.
*   Bi-Directional Generative Embeddings (BGE) models excel at capturing relationships between text pairs, making them suitable for semantic search tasks.
*   `BAAI/bge-small-en-v1.5` is a lightweight embedding model from the Beijing Academy of Artificial Intelligence, providing a good balance between speed and accuracy.
*   Suitable for tasks requiring fast generation, making it ideal for real-time RAG applications.

[https://huggingface.co/BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)

```python
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
import numpy as np

embedding_model = HuggingFaceBgeEmbeddings(
    model_name = "BAAI/bge-small-en-v1.5",
    encode_kwargs = {'normalize_embeddings': True}  # useful for similarity tasks
)

# Embed the first document chunk
sample_embedding = np.array(embedding_model.embed_query(chunks[0].page_content))

print(sample_embedding[:5])
print("\nSize: ", sample_embedding.shape)
```

This code will output the first five elements of the embedding vector for the first chunk and the size of the embedding vector:

```
[-0.07508391 -0.01188472 -0.03148879  0.02940382  0.05034875]

Size:  (384,)
```

<----------section---------->

**Vector Stores: Storage and Retrieval**

Vector stores enable semantic search and retrieval by indexing and querying embeddings of documents or text chunks. They provide efficient methods for finding the most relevant information for a given query.

*   `FAISS`: Open-source library for efficient similarity search and clustering of dense vectors, ideal for local and small-to-medium datasets. FAISSDocumentStore in Haystack gives you three indexing approaches to choose from: 'Flat', 'HNSW', or 'IVF{num_clusters},Flat'.
*   `Chroma`: Lightweight and embedded vector store suitable for local applications with minimal setup.
*   `Qdrant`: Open-source vector database optimized for similarity searches and nearest-neighbor lookup.
*   `Pinecone`: Managed vector database offering real-time, high-performance semantic search with automatic scaling.
*   and many others…

This example uses the Facebook AI Similarity Search (FAISS).

```python
from langchain_community.vectorstores import FAISS

# Generate the vector store
vectorstore = FAISS.from_documents(chunks, embedding_model)

# Save the vector store for later use...
vectorstore.save_local("faiss_index")

# To load the vector store later...
# loaded_vectorstore = FAISS.load_local("faiss_index", embedding_model)
```

This code generates a FAISS vector store from the text chunks and the embedding model. The vector store is then saved locally for later use.

<----------section---------->

**Querying the Vector Store: Semantic Search**

This section demonstrates how to use the vector store to search for chunks relevant to a user query, leveraging the power of semantic search.

```python
query = """
What were the trends in median household income across
different states in the United States between 2021 and 2022.
"""

matches = vectorstore.similarity_search(query)

print(f'There are {len(matches)} relevant chunks.\nThe first one is:\n')
print(matches[0].page_content)
```

This code will output the number of relevant chunks found and the content of the first chunk:

```
There are 4 relevant chunks.
The first one is:

Comparisons
The U.S. median household income
```

<----------section---------->

**RAG Prompt Template: Integrating Context and Question**

A RAG prompt template integrates the retrieved context with a user question, providing the LLM with the necessary information to generate an accurate and contextually relevant answer.

*   A placeholder `{context}` is used to dynamically inject retrieved chunks, providing the LLM with the relevant information.
*   A placeholder `{question}` is used to specify the user query, ensuring that the LLM addresses the specific question being asked.
*   Explicit instructions are provided for handling the information, guiding the LLM on how to use the context and question to generate an answer.

```python
template = """
Use the following pieces of context to answer the question at the end.
Please follow the following rules:
1. If you don't know the answer, don't try to make up an answer. Just say "I can't find the final answer".
2. If you find the answer, write the answer in a concise way with five sentences maximum.

{context}

Question: {question}

Helpful Answer:
"""

from langchain.prompts import PromptTemplate
prompt_template = PromptTemplate(template = template, input_variables = ["context", "question"])
```

This code defines a prompt template that instructs the LLM to use the provided context to answer the question, avoid making up answers, and provide concise answers.

<----------section---------->

**Vector Store as a Retriever: Abstraction for Chains**

To be used in chains, a vector store must be wrapped within a retriever interface. A retriever takes a text query as input and provides the most relevant information as output, abstracting away the complexities of the vector store.

```python
# Create a retriever interface on top of the vector store
retriever = vectorstore.as_retriever(
    search_type = "similarity",  # use cosine similarity
    search_kwargs = {"k": 3}  # use the top 3 most relevant chunks
)
```

This code creates a retriever interface on top of the vector store, using cosine similarity as the search type and retrieving the top 3 most relevant chunks.

<----------section---------->

**Custom RAG Chain: Orchestrating the Workflow**

This section demonstrates how to define a custom RAG chain using LangChain, orchestrating the retrieval and generation steps.

```python
from langchain_core.runnables import RunnablePassthrough

# Helper function to concatenate the retrieved chunks
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

my_rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt_template
    | llm
)
```

This code builds a dictionary where each value is obtained as the result of a sub-chain, allowing for a modular and flexible RAG pipeline. The `format_docs` function concatenates the retrieved chunks into a single string, providing the LLM with a coherent context.

<----------section---------->

**Predefined Chains: Streamlining Development**

LangChain includes ready-to-use chains that handle common tasks with minimal setup, simplifying the development process.

*   `LLMChain`: Executes a single prompt using a language model and returns the output, providing a basic building block for NLP pipelines.
*   `RetrievalQAChain`: Combines a retriever and an LLM to answer questions based on retrieved context, providing a streamlined way to build RAG applications.
*   `AnalyzeDocumentChain`: Extracts insights, structured data, or key information from documents, enabling more sophisticated document processing workflows.
*   `SequentialChain`: Executes a series of chains sequentially, passing outputs from one as inputs to the next, allowing for the creation of complex, multi-step NLP pipelines.
*   `ConditionalChain`: Executes different chains based on conditions in the input or intermediate outputs, providing a way to create adaptive and dynamic NLP workflows.
*   and many others…

<----------section---------->

**Predefined RAG Chain: Simplifying RAG Implementation**

This section demonstrates how to use a predefined `RetrievalQA` chain instead of a custom chain, further simplifying the implementation of a RAG system.

```python
from langchain.chains import RetrievalQA

# Create a RetrievalQA chain
retrievalQA = RetrievalQA.from_chain_type(
    llm = llm,
    retriever = retriever,
    chain_type = "stuff",  # concatenate retrieved chunks
    chain_type_kwargs = {"prompt": prompt_template},
    return_source_documents = True
)
```

This code creates a `RetrievalQA` chain, specifying the LLM, retriever, chain type (stuff, which concatenates retrieved chunks), prompt template, and whether to return source documents.

<----------section---------->

**Querying the RAG: Generating Answers**

This section demonstrates how to use the RAG system for question answering, generating answers based on the retrieved context.

```python
query = """
What were the trends in median household income across
different states in the United States between 2021 and 2022.
"""

# Call the QA chain with our query.
result = retrievalQA.invoke({"query": query})
print(result['result'])
```

This code will output the answer generated by the RAG system:

```
Five states, including Alabama, Alaska, Delaware, Florida, and Utah, experienced a statistically
significant increase in real median household income from 2021 to 2022. Conversely, 17 states showed
a decrease. For 28 states, the District of Columbia, and Puerto Rico, there was no statistically

YY difference in real median household income between the two years.
```

<----------section---------->

**Querying the RAG: Identifying Sources**

This section demonstrates how to identify the chunks that have been used to generate the answer, providing transparency and traceability.

```python
sources = result['source_documents']
print(f'{len(sources)} chunks have been used to generate the answer.')

for doc in sources:
    print(f"\n------- from: {doc.metadata['source']}, page: {doc.metadata['page']}\n\n{doc.page_content}")
```

This code will output the number of chunks used and the content, source, and page number of each chunk:

```
3 chunks have been used to generate the answer.

------- from: us_census/acsbr-017.pdf, page: 1

Comparisons
The U.S. median household income

in 2022 was $74,755, according

Figure 1.
Median Household Income in the Past 12 Months in the United States: 2005-2022
```

<----------section---------->

**Additional Context from Haystack Documentation**

This section supplements the previous RAG workflow by referencing documentation from the Haystack framework, highlighting similarities and differences in implementation and concepts. This extra context can be used for further exploration of RAG implementations.

In Haystack, your document storage database is wrapped in a `DocumentStore` object. The `DocumentStore` class gives you a consistent interface to the database containing the documents you just downloaded in a CSV. For now the "documents" are just the lines of text for an early version of the ASCIIDoc manuscript for this book — really really short documents. The haystack `DocumentStore` class allows you to connect to different open source and commercial vector databases that you can host locally on your machine, such as Faiss, PineCone, Milvus, ElasticSearch or even just SQLLite. For now, use the `FAISSDocumentStore` and its default indexing algorithm (`'Flat'`).

```python
>>> from haystack.document_stores import FAISSDocumentStore
>>> document_store = FAISSDocumentStore(
...     return_embedding=True)  # #1
>>> document_store.write_documents(documents)
```

The FAISSDocumentStore in haystack gives you three of these indexing approaches to choose from. The default `'Flat'` index will give you the most accurate results (highest recall rate) but will use a lot of RAM and CPU. If you’re really constrained on RAM or CPU, like when you’re hosting your app on Hugging Face, you can experiment with two other FAISS options: `'HNSW'` or `f’IVF{num_clusters},Flat'`. The question-answering app you’ll see at the end of this section used the `'HNSW'` indexing approach to fit within a hugging face "free tier" server. See the Haystack documentation for details on how to tune your vector search index. You will need to balance speed, RAM, and recall for your needs. Like many NLP questions, there is no right answer to the question of the "best" vector database index. Hopefully, when you ask this question to your question-answering app, it will say something like "It depends…".

Now go to your working directory where you ran this Python code. You should see a file named `'faiss_document_store.db'`. That’s because FAISS automatically created an SQLite database to contain the text of all your documents. Your app will need that file whenever you use the vector index to do semantic search. It will give you the actual text associated with the embedding vectors for each document. However, this file is not enough in order to load your data store into another piece of code - for that, you’ll need to you the `save` method of the `DocumentStore` class. We’ll do that later in the code after we fill the document store with embeddings.

Now, it’s time to set up our indexing models! The semantic search process includes two main steps - retrieving documents that might be relevant to the query (semantic search), and processing those documents to create an answer. So you will need an EmbeddingRetriever semantic vector index and a generative transformer model.

In Chapter 9 you met BERT and learn how to use it to create general-purpose embeddings that represent the meaning of text. Now you’ll learn how to use an embedding-based retriever to overcome the curse of dimensionality and find the embeddings for text most likely to answer a user’s question. You can probably guess that you’ll get better results if both your retriever and your reader are fine-tuned for question-answering tasks. Luckily there are a lot of BERT-based models that have been pretrained on question-answering datasets like SQuAD.

Listing 10.17 Configuring the `reader` and `retriever` components of the question answering pipeline.

```python
>>> from haystack.nodes
<----------section---------->

# Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)

**Lesson 21: Reinforcement Learning from Human Feedback**

Nicola Capuano and Antonio Greco
DIEM – University of Salerno

<----------section---------->

## Outline

This lesson will cover the following topics:

*   Reinforcement Learning from Human Feedback (RLHF): An in-depth explanation of the RLHF technique.
*   Transformers trl library: An introduction to the Transformers Reinforcement Learning (TRL) library.
*   Try it yourself: Resources and guidance for implementing RLHF in your own projects.

<----------section---------->

## Reinforcement Learning from Human Feedback (RLHF)

### What is RLHF?

*   RLHF is a technique used to refine large language models (LLMs) by incorporating direct human feedback into the training process. This feedback acts as a crucial guide, steering the model toward desired behaviors and outputs.
*   It serves as a strategy to strike a balance between maximizing the LLM's raw performance (e.g., fluency, coherence) and aligning its responses with human values, ethical considerations, and individual user preferences. This ensures the model is not only capable but also responsible and useful.

### Why RLHF?

*   RLHF offers a potential method for "grounding" the LLM. Grounding refers to anchoring the model's focus and output in a more concrete, relevant domain or context.  Without grounding, LLMs can sometimes generate outputs that are factually incorrect, nonsensical, or irrelevant.
*   It enhances several key aspects of LLM performance:
    *   **Safety:**  Reduces the likelihood of the model generating harmful, offensive, or dangerous content.
    *   **Ethical Responses:** Encourages the model to provide responses that are aligned with ethical principles and avoid biases.
    *   **User Satisfaction:** Increases user satisfaction by tailoring responses to be more helpful, relevant, and aligned with their specific needs and expectations.

### Workflow of RLHF

The RLHF workflow typically consists of three primary stages:

1.  **Pre-trained Language Model:** This is the foundation of the entire process. It's a large language model (LLM) that has been trained on a massive dataset of text and code. Examples of such models include BERT, GPT, and T5.  These models possess a general understanding of language and the ability to generate text.
2.  **Reward Model:**  A separate, secondary model is trained to evaluate and score the outputs generated by the pre-trained LLM. This reward model learns to predict human preferences based on the human feedback it receives. It assigns a "reward" score to each LLM-generated output, reflecting its perceived quality and alignment with human expectations.
3.  **Fine-Tuning with Reinforcement Learning:** In this stage, the pre-trained LLM is further refined using reinforcement learning techniques.  The reward model provides the signal that guides the optimization process. The goal is to adjust the LLM's parameters so that it consistently generates outputs that receive high reward scores from the reward model.  This process is iterative, allowing the LLM to gradually improve its ability to produce desirable and human-aligned responses.

<----------section---------->

### Reward Model

*   The training data for the reward model is crucial for its success and consists of the following:
    *   **Multiple LLM-generated outputs for given prompts:** For a single input prompt, several different outputs are generated by the LLM. This creates a set of diverse responses for the reward model to evaluate.
    *   **Corresponding human rank responses according to their preferences:** Human evaluators then rank these different LLM outputs based on their subjective assessment of quality, helpfulness, safety, and alignment with the prompt.  These rankings represent human preferences.
*   The primary objective is to train the reward model to accurately predict human preferences. This means the model should learn to assign higher scores to outputs that humans would prefer and lower scores to outputs that humans would find less desirable.
*   The training methodology involves using a ranking loss function.  This type of loss function is specifically designed to teach the reward model to differentiate between preferred and less preferred outputs.  It penalizes the model when it assigns a higher score to an output that humans ranked lower, and vice versa.

<----------section---------->

### Fine-tuning with Proximal Policy Optimization (PPO)

*   The ultimate goal is to align the LLM's outputs with the quality metrics defined by humans. In other words, the LLM should learn to generate responses that are consistently considered high-quality by human evaluators.  Proximal Policy Optimization (PPO) is a reinforcement learning algorithm often used to achieve this.
    1.  **Generate responses using the LLM:**  Given a specific prompt, the LLM generates a response.
    2.  **Score responses with the reward model:** The reward model evaluates the generated response and assigns a score, reflecting how well it aligns with human preferences.
    3.  **Update the LLM to maximize reward scores:**  The PPO algorithm then adjusts the LLM's parameters to increase the likelihood of generating responses that receive higher reward scores in the future. This iterative process gradually improves the LLM's alignment with human-defined quality metrics.

<----------section---------->

### Pros and Cons of RLHF

**Pros:**

*   **Iterative Improvement:** The framework enables a continuous cycle of improvement. Human feedback can be collected as the model evolves, allowing for periodic updates to the reward model and subsequent fine-tuning of the LLM. This iterative approach ensures the model remains aligned with evolving human values and preferences.
*   **Improved Alignment:** RLHF leads to the generation of responses that are more closely aligned with human intent and expectations. This improves the usefulness and relevance of the model's outputs.
*   **Ethical Responses:** By incorporating human feedback, RLHF helps to reduce the generation of harmful, biased, or otherwise undesirable outputs. This enhances the safety and ethical considerations of the LLM.
*   **User-Centric Behavior:** The method facilitates the tailoring of interactions to individual user preferences. This customization leads to a more personalized and satisfying user experience.

**Cons:**

*   **Subjectivity:** Human feedback is inherently subjective and can vary widely depending on the individual evaluator. This variability can introduce noise into the training process and make it challenging to define a consistent objective for the model.
*   **Scalability:** Gathering sufficient, high-quality human feedback is a resource-intensive process, especially for very large language models. The cost and logistical challenges associated with collecting and processing this feedback can be significant.
*   **Reward Model Robustness:** A poorly trained or misaligned reward model can lead to suboptimal fine-tuning of the LLM. If the reward model does not accurately reflect human preferences, the LLM may learn to generate outputs that are technically "high-scoring" but are not actually desirable from a human perspective.

<----------section---------->

### Tasks to Enhance with RLHF

RLHF can be used to improve various NLP tasks:

*   **Text Generation:** Elevate the quality and coherence of the text generated by LLMs.
*   **Dialogue Systems:** Boost the performance of dialogue systems, resulting in more natural and engaging conversations.
*   **Language Translation:** Enhance the precision and accuracy of language translation, yielding more faithful and reliable translations.
*   **Summarization:** Increase the standard of summaries produced by LLMs, ensuring they are concise, informative, and representative of the original text.
*   **Question Answering:** Improve the accuracy and relevance of question answering systems, allowing them to provide more helpful and insightful answers.
*   **Sentiment Analysis:** Increase the precision of sentiment identification for specific domains or businesses. RLHF enables targeted analysis that reflects nuanced sentiment within particular contexts.
*   **Computer Programming:** Expedite and refine software development by using LLMs to generate code, documentation, and other programming-related outputs that align with human expectations.

<----------section---------->

### Case Study: GPT-3.5 and GPT-4

*   These pre-trained models have undergone fine-tuning using RLHF, among other techniques, to improve their overall performance and alignment.
*   OpenAI reports that RLHF has led to several key improvements:
    *   Enhanced alignment with human values and preferences.
    *   Reduced generation of unsafe or harmful outputs.
    *   More human-like and engaging interactions.
*   These models have been, or are currently, widely used in real-world applications such as ChatGPT.
*   The models are continuously refined with ongoing human feedback, enabling incremental improvements in their capabilities and alignment over time.

<----------section---------->

## Transformers trl library

### TRL: Transformer Reinforcement Learning

*   TRL (Transformer Reinforcement Learning) is a comprehensive library that provides a set of tools and modules for training transformer language models using reinforcement learning. It covers the entire training pipeline, from supervised fine-tuning (SFT) and reward modeling (RM) to proximal policy optimization (PPO).
*   The library is seamlessly integrated with the Hugging Face Transformers library, providing a user-friendly and efficient environment for RLHF training.

<----------section---------->

## Try it yourself

*   Explore the TRL library on Hugging Face: [https://huggingface.co/docs/trl/v0.7.8/index](https://huggingface.co/docs/trl/v0.7.8/index)
*   Examine the following components:
    *   `PPOTrainer`: [https://huggingface.co/docs/trl/v0.7.8/ppo_trainer](https://huggingface.co/docs/trl/v0.7.8/ppo_trainer)
    *   `RewardTrainer`: [https://huggingface.co/docs/trl/v0.7.8/reward_trainer](https://huggingface.co/docs/trl/v0.7.8/reward_trainer)
*   Investigate examples relevant to your project:
    *   Sentiment analysis tuning: [https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning](https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning)
    *   Detoxifying a Large Language Model with PPO: [https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm](https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm)
*   Implement RLHF in your project.

```python
# Step 1: Train your model on your favorite dataset using Supervised Fine-Tuning (SFT)
from trl import SFTTrainer

trainer = SFTTrainer(
    model_name="facebook/opt-350m",  # Specify the base model
    dataset_text_field="text",        # Indicate the text field in your dataset
    max_seq_length=512,               # Define the maximum sequence length
    train_dataset=dataset,             # Provide your training dataset
)
trainer.train()                       # Start the training process

# Step 2: Train a reward model
from trl import RewardTrainer

trainer = RewardTrainer(
    model=model,                     # Pass your model
    tokenizer=tokenizer,               # Provide your tokenizer
    train_dataset=dataset,             # Provide your training dataset
)
trainer.train()                       # Start the training process

# Step 3: Further optimize the SFT model using the rewards from the reward model and PPO algorithm
from trl import PPOConfig, PPOTrainer

config = PPOConfig()                   # Initialize the PPO configuration
trainer = PPOTrainer(
    config=config,                     # Pass the PPO configuration
    model=model,                     # Pass your model
    tokenizer=tokenizer,               # Provide your tokenizer
    query_dataloader=query_dataloader, # Provide a dataloader for queries
)

for query in query_dataloader:        # Iterate through the queries
    response = model.generate(query)  # Generate a response
    reward = reward_model(response)   # Get the reward score
    trainer.step(query, response, reward) # Perform a PPO training step
```

<----------section---------->

**Additional Context:**

**Popular sampling techniques:** To produce diverse and creative text, LLMs utilize sampling techniques during generation, deviating from simply selecting the most probable word at each step. Two common techniques are:

*   **Top-k sampling:** The model chooses from the K most likely words in its vocabulary. This parameter controls how creative the LLM will be.

*   **Nucleus sampling:** Instead of choosing among the K most likely words, the model looks at the smallest set of words whose cumulative probability is smaller than *p*. So if there are only a few candidates with large probabilities, the "nucleus" would be smaller, than in the case of larger group of candidates with smaller probabilities.

**Example of generating text using nucleus sampling method:**

```python
nucleus_sampling_args = {
    'do_sample': True,  # Enable sampling
    'max_length': 50,    # Maximum length of the generated text
    'top_p': 0.92       # Probability threshold for nucleus sampling
}
print(generate(prompt='NLP is a', **nucleus_sampling_args))
```

The output might be something like:

```
NLP is a multi-level network protocol, which is one of the most
well-documented protocols for managing data transfer protocols. This
is useful if one can perform network transfers using one data transfer
protocol and another protocol or protocol in the same chain.
```

This is better, but still not quite what you were looking for. Your output
still uses the same words too much (just count how many times "protocol"
was mentioned!) But more importantly, though NLP indeed can stand for
Network Layer Protocol, it’s not what you were looking for. To get generated
text that is domain-specific, you need to fine-tune our model - that means, to
train it on a dataset that is specific to our task.

**10.1.7 Fine-tuning your generative model:**

In your case, this dataset would be this very book, parsed into a database of lines. Let’s load it from `nlpia2` repository. In this case, we only need the book’s text, so we’ll ignore code, headers, and all other things that will not be helpful for our generative model.

Let’s also initialize a new version of our GPT-2 model for finetuning. We can reuse the tokenizer for GPT-2 we initialized before.

**Listing 10.8 Loading the NLPiA2 lines as training data for GPT-2:**

```python
import pandas as pd
DATASET_URL = (
    'https://gitlab.com/tangibleai/nlpia2/'
    '-/raw/main/src/nlpia2/data/nlpia_lines.csv'
)
df = pd.read_csv(DATASET_URL)
df = df[df['is_text']]
lines = df.line_text.copy()
```

This will read all the sentences of natural language text in the manuscript for this book. Each line or sentence will be a different "document" in your NLP pipeline, so your model will learn how to generate sentences rather than longer passages. You want to wrap your list of sentences with a PyTorch Dataset class so that your text will be structured in the way that our training pipeline expects.

**Listing 10.9 Creating a PyTorch Dataset for training:**

```python
from torch.utils.data import Dataset
from torch.utils.data import random_split

class NLPiADataset(Dataset):
    def __init__(self, txt_list, tokenizer, max_length=768):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.attn_masks = []
        for txt in txt_list:
            encodings_dict = tokenizer(txt, truncation=True,
                max_length=max_length, padding="max_length")
            self.input_ids.append(
                torch.tensor(encodings_dict['input_ids']))
    def __len__(self):
        return len(self.input_ids)
    def __getitem__(self, idx):
        return self.input_ids[idx]
```

Now, we want to set aside some samples for evaluating our loss mid-training. Usually, we would need to wrap them in the `DataLoader` wrapper, but luckily, the Transformers package simplifies things for us.

**Listing 10.10 Creating training and evaluation sets for fine-tuning:**

```python
dataset = NLPiADataset(lines, tokenizer, max_length=768)
train_size = int(0.9 * len(dataset))
eval_size = len(dataset) - train_size
train_dataset, eval_dataset = random_split(
    dataset, [train_size, eval_size])
```

Finally, you need one more Transformers library object - DataCollator. It dynamically builds batches out of our sample, doing some simple pre-prossesing (like padding) in the process. You’ll also define batch size - it will depend on the RAM of your GPU. We suggest starting from single-digit batch sizes and seeing if you run into out-of-memory errors.

If you were doing the training in PyTorch, there are multiple parameters that you would need to specify - such as the optimizer, its learning rate, and the warmup schedule for adjusting the learning rate. This is how you did it in the previous chapters. This time, we’ll show you how to use the presets that transformers package offers in order to train the model as a part of `Trainer` class. In this case, we only need to specify the batch size and number of epochs! Easy-peasy.

**Listing 10.11 Defining training arguments for GPT-2 fine-tuning:**

```python
from nlpia2.constants import DATA_DIR
from transformers import TrainingArguments
from transformers import DataCollatorForLanguageModeling
training_args = TrainingArguments(
   output_dir=DATA_DIR / 'ch10_checkpoints',
   per_device_train_batch_size=5,
   num_train_epochs=5,
   save_strategy='epoch'
)
collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)
```

Now you have the pieces that a HuggingFace training pipeline needs to know to start training (finetuning) your model. The `TrainingArguments` and `DataCollatorForLanguageModeling` classes help you comply with the Hugging Face API and best practices. It’s a good pattern to follow even if you do not plan to use Hugging Face to train your models. This pattern will force you to make all your pipelines maintain a consistent interface. This allows you to train, test, and upgrade your models quickly each time you want to try out a new base model. This will help you keep up with the fast-changing world of open-source transformer models. You need to move fast to compete with the chickenized reverse centaur algorithms that BigTech is using to try to enslave you.

The `mlm=False` (masked language model) setting is an especially tricky quirk of transformers. This is your way of declaring that the dataset used for training your model need only be given the tokens in the causal direction — left to right for English. You would need to set this to True if you are feeding the trainer a dataset that has random tokens masked. This is the kind of dataset used to train bidirectional language models such as BERT.

**tip:**
A causal language model is designed to work the way a neurotypical human brain model works when reading and writing text. In your mental model of the English language, each word is causally linked to the next one you speak or type as you move left to right. You can’t go back and revise a word you’ve already spoken … unless you’re speaking with a keyboard. And we use keyboards a lot. This has caused us to develop mental models where we can skip around left or right as we read or compose a sentence. Perhaps if we’d all been trained to predict masked-out words, like BERT was, we would have a different (possibly more efficient) mental model for reading and writing text. Speed reading training does this to some people as they learn to read and understand several words of text all at once, as fast as possible. People who learn their internal language models differently than the typical person might develop the ability to hop around from word to word in their mind, as they are reading or writing text. Perhaps the language model of someone with symptoms of dyslexia or autism is somehow related to how they learned the language. Perhaps the language models in neurodivergent brains (and speed readers) are more similar to BERT (bidirectional) rather than GPT (left-to-right).

Now you are ready for training! You can use your collator and training args to configure the training and turn it loose on your data.

**Listing 10.12 Fine-tuning GPT-2 with HuggingFace’s Trainer class:**

```python
from transformers import Trainer
ft_model = GPT2LMHeadModel.from_pretrained("gpt2")
trainer = Trainer(
       ft_model,
       training_args,
       data_collator=collator,
       train_dataset=train_dataset,
       eval_dataset=eval_dataset)
trainer.train()
```

This training run can take a couple of hours on a CPU. So if you have access to a GPU you might want to train your model there. The training should run about 100x faster on a GPU.

Of course, there is a trade-off in using off-the-shelf classes and presets — it gives you less visibility on how the training is done and makes it harder to tweak the parameters to improve performance. As a take-home task, see if you can train the model the old way, with a PyTorch routine.

Let’s see how well our model does now!

```python
generate(model=ft_model, tokenizer=tokenizer,
           prompt='NLP is')
```

NLP is not the only way to express ideas and understand ideas.

OK, that looks like a sentence you might find in this book. Take a look at the results of the two different models together to see how much your fine-tuning changed the text the LLM will generate.

```python
print(generate(prompt="Neural networks",
                   model=vanilla_gpt2,
                   tokenizer=tokenizer,
                   **nucleus_sampling_args))
```

Neural networks in our species rely heavily on these networks to understand
   their role in their environments, including the biological evolution of
   language and communication...

```python
print(generate(prompt="Neural networks",
                  model=ft_model,
                  tokenizer=tokenizer,
                  **nucleus_sampling_args))
```

Neural networks are often referred to as "neuromorphic" computing because
   they mimic or simulate the behavior of other human brains. footnote:[...

That looks like quite a difference! The vanilla model interprets the term 'neural networks' in its biological connotation, while the fine-tuned model realizes we’re more likely asking about artificial neural networks. Actually, the sentence that the fine-tuned model generated resembles closely a sentence from Chapter 7:

Neural networks are often referred to as "neuromorphic" computing because they mimic or simulate what happens in our brains.

There’s a slight difference though. Note the ending of "other human brains". It seems that our model doesn’t quite realize that it talks about artificial, as opposed to human, neural networks, so the ending doesn’t make sense. That shows once again that the generative model doesn’t really have a model of the world, or "understand" what it says. All it does is predict the next word in a sequence. Perhaps you can now see why even rather big language models like GPT-2 are not very smart and will often generate nonsense.

**10.1.8 Nonsense (hallucination):**

As language models get larger, they start to sound better. But even the largest LLMs generate a lot of nonsense. The lack of "common sense" should be no surprise to the experts who trained them. LLMs have not been trained to utilize sensors, such as cameras and microphones, to ground their language models in the reality of the physical world. An embodied robot might be able to ground itself by checking its language model with what it senses in the real world around it. It could correct its common sense logic rules whenever the real world contradicts those faulty rules. Even seemingly abstract logical concepts such as addition have an effect in the real world. One apple plus another apple always produces two apples in the real world. A grounded language model should be able to count and do addition much better.

Like a baby learning to walk and talk, LLMs could be forced to learn from their mistakes by allowing them to sense when their assumptions were incorrect. An embodied AI wouldn’t survive very long if it made the kinds of common sense mistakes that LLMs make. An LLM that only consumes and produces text on the Internet has no such opportunity to learn from mistakes in the physical world. An LLM "lives" in the world of social media, where fact and fantasy are often indistinguishable.

So even the largest of the large, trillion-parameter transformers will generate nonsense responses. Scaling up the nonsense training data won’t help. The largest and most famous LLMs were trained on virtually the entire Internet and this only improves their grammar and vocabulary, not their reasoning ability. Some engineers and researchers describe this nonsensical text as hallucinating. But that’s a misnomer that can lead you astray in your quest to get something consistently useful out of LLMs. An LLM can’t even hallucinate because it can’t think, much less reason or have a mental model of reality.

Hallucination happens when a human fails to separate imagined images or words from the reality of the world they live in. But an LLM has no sense of reality and has never lived in the real world. An LLM that you use on the Internet has never been embodied in a robot. It has never suffered from the consequences of mistakes. It can’t think, and it can’t reason. So it can’t hallucinate.

LLMs have no concept of truth, facts, correctness, or reality. LLMs that you interact with online "live" in the unreal world of the Internet. Engineers fed them texts from both fiction and nonfiction sources. If you spend a lot of time probing what an LLM knows you will quickly get a feel for just how ungrounded models like ChatGPT are. At first, you may be pleasantly surprised by how convincing and plausible the responses to your questions are. And this may lead you to anthropomorphize it. And you might claim that its ability to reason was an "emergent" property that researchers didn’t expect. And you would be right. The researchers at BigTech have not even begun to try to train LLMs to reason. They hoped the ability to reason would magically emerge if they gave LLMs enough computational power and text to read. Researchers hoped to shortcut the need for AI to interact with the physical world by giving LLMs enough descriptions of the real world to learn from. Unfortunately, they also gave LLMs an equal or larger dose of fantasy. Most of the text found online is either fiction or intentionally misleading.

So the researchers' hope for a shortcut was misguided. LLMs only learned what they were taught — to predict the most plausible next words in a sequence. By using the like button to nudge LLMs with reinforcement learning, BigTech has created a BS artist rather than the honest and transparent virtual assistant that they claimed to be building. Just as the like button on social media has turned many humans into sensational blow-hards, it has turned LLMs into "influencers" that command the attention of more than 100 million users. And yet LLMs have no ability or incentives (objective functions) to help them differentiate fact from fiction. To improve the machine’s answers' relevance and accuracy, you need to get better at grounding your models - have their answers based on relevant facts and knowledge.

Luckily, there are time-tested techniques for incentivizing generative models for correctness. Information extraction and logical inference on knowledge graphs are very mature technologies. And most of the biggest and best knowledge bases of facts are completely open source. BigTech can’t absorb and kill them all. Though the open source knowledge base FreeBase has been
different approaches; we show you techniques for both.

In addition, deep learning and data-driven programming (machine learning, or probabilistic language modeling) have rapidly diversified the possible applications for NLP and chatbots. This data-driven approach allows ever greater sophistication for an NLP pipeline by providing it with greater and greater amounts of data in the domain you want to apply it to. And when a new machine learning approach is discovered that makes even better use of this data, with more efficient model generalization or regularization, then large jumps in capability are possible.

The NLP pipeline for a chatbot shown in Figure 1.4 contains all the building blocks for most of the NLP applications that we described at the start of this chapter. As in `Taming Text`, we break out our pipeline into four main subsystems or stages. In addition, we have explicitly called out a database to record data required for each of these stages and persist their configuration and training sets over time. This can enable batch or online retraining of each of the stages as the chatbot interacts with the world. We have also shown a "feedback loop" on our generated text responses so that our responses can be processed using the same algorithms used to process the user statements. The response "scores" or features can then be combined in an objective function to evaluate and select the best possible response, depending on the chatbot’s plan or goals for the dialog. This book is focused on configuring this NLP pipeline for a chatbot, but you may also be able to see the analogy to the NLP problem of text retrieval or "search," perhaps the most common NLP application. And our chatbot pipeline is certainly appropriate for the question-answering application that was the focus of `Taming Text`.

The application of this pipeline to financial forecasting or business analytics may not be so obvious. But imagine the features generated by the analysis portion of your pipeline. These features of your analysis or feature generation can be optimized for your particular finance or business prediction. That way they can help you incorporate natural language data into a machine learning pipeline for forecasting. Despite focusing on building a chatbot, this book gives you the tools you need for a broad range of NLP applications, from search to financial forecasting.

One processing element in Figure 1.4 that is not typically employed in search, forecasting, or question-answering systems is natural language generation. For chatbots, this is their central feature. Nonetheless, the text generation step is often incorporated into a search engine NLP application and can give such an engine a large competitive advantage. The ability to consolidate or summarize search results is a winning feature for many popular search engines (DuckDuckGo, Bing, and Google). And you can imagine how valuable it is for a financial forecasting engine to be able to generate statements, tweets, or entire articles based on the business-actionable events it detects in natural language streams from social media networks and news feeds.

The next section shows how the layers of such a system can be combined to create greater sophistication and capability at each stage of the NLP pipeline.

**1.10 Processing in depth:**

The stages of a natural language processing pipeline can be thought of as layers, like the layers in a feed-forward neural network. Deep learning is all about creating more complex models and behavior by adding additional processing layers to the conventional two-layer machine learning model architecture of feature extraction followed by modeling. In Chapter 5 we explain how neural networks help spread the learning across layers by backpropagating model errors from the output layers back to the input layers. But here we talk about the top layers and what can be done by training each layer independently of the other layers.

The top four layers in Figure 1.8 correspond to the first two stages in the chatbot pipeline (feature extraction and feature analysis) in the previous section. For example, part-of-speech tagging (POS tagging), is one way to generate features within the Analyze stage of our chatbot pipeline. POS tags are generated automatically by the default SpaCY pipeline, which includes all the top four layers in this diagram. POS tagging is typically accomplished with a finite state transducer like the methods in the `nltk.tag` package.

The bottom two layers (Entity Relationships and a Knowledge Base) are used to populate a database containing information (knowledge) about a particular domain. And the information extracted from a particular statement or document using all six of these layers can then be used in combination with that database to make inferences. Inferences are logical extrapolations from a set of conditions detected in the environment, like the logic contained in the statement of a chatbot user. This kind of "inference engine" in the deeper layers of this diagram is considered the domain of artificial intelligence, where machines can make inferences about their world and use those inferences to make logical decisions. However, chatbots can make reasonable decisions without this knowledge database, using only the algorithms of the upper few layers. And these decisions can combine to produce surprisingly human-like behaviors.

Over the next few chapters, we dive down through the top few layers of NLP. The top three layers are all that is required to perform meaningful sentiment analysis and semantic search and to build human-mimicking chatbots. In fact, it’s possible to build a useful and interesting chatbot using only a single layer of processing, using the text (character sequences) directly as the features for a language model. A chatbot that only does string matching and search is capable of participating in a reasonably convincing conversation if given enough example statements and responses.

For example, the open source project `ChatterBot` simplifies this pipeline by merely computing the string "edit distance" (Levenshtein distance) between an input statement and the statements recorded in its database. If its database of statement-response pairs contains a matching statement, the corresponding reply (from a previously "learned" human or machine dialog) can be reused as the reply to the latest user statement. For this pipeline, all that is required is step 3 (Generate) of our chatbot pipeline. And within this stage, only a brute-force search algorithm is required to find the best response. With this simple technique (no tokenization or feature generation required), `ChatterBot` can maintain a convincing conversion as the dialog engine for Salvius, a mechanical robot built from salvaged parts by Gunther Cox.

`Will` is an open source Python chatbot framework by Steven Skoczen with a completely different approach. `Will` can only be trained to respond to statements by programming it with regular expressions. This is the labor-intensive and data-light approach to NLP. This grammar-based approach is especially effective for question-answering systems and task-execution assistant bots, like Lex, Siri, and Google Now. These kinds of systems overcome the "brittleness" of regular expressions by employing "fuzzy regular expressions."

harm your business, your reputation, or your users. You’ll need to do more than simply connect your users directly to the LLM.

There are three popular approaches to reducing an LLM’s toxicity and reasoning errors:

1. Scaling: Make it bigger (and hopefully smarter)
2. Guardrails: Monitoring it to detect and prevent it from saying bad things
3. Grounding: Augment an LLM with a knowledge base of real-world facts
4. Retrieval: Augment an LLM with a search engine to retrieve text used to generate responses.

The next two sections will explain the advantages and limitations of the scaling and guardrail approaches. You will learn about grounding and retrieval in chapter
**10.1.1 Scaling up:**

One of the attractive aspects of LLMs is that you only need to add data and neurons if you want to improve your bot. You don’t have to handcraft ever more complicated dialog trees and rules. OpenAI placed a billion-dollar bet on the idea that the ability to handle complex dialog and reason about the world would emerge once they added enough data and neurons. It was a good bet. Microsoft invested more than a billion dollars in ChatGPT’s emergent ability to respond plausibly to complex questions.

However many researchers question whether this overwhelming complexity in the model is merely hiding the flaws in ChatGPT’s reasoning. Many researchers believe that increasing the dataset does not create more generally intelligent behavior just more confident and intelligent-sounding text. The authors of this book are not alone in holding this opinion. Way back in 2021, in the paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" prominent researchers explained how the appearance of understanding in LLMs was an illusion. And they were fired for the sacrilege of questioning the ethics and reasonableness of OpenAI’s "spray and pray" approach to AI — relying exclusively on the hope that more data and neural network capacity would be enough to create intelligence.

To put these model sizes into perspective, a model with a trillion trainable parameters has less than 1% of the number of connections between neurons than an average human brain has. This is why researchers and large organizations have been investing millions of dollars in the compute resources required to train the largest language models.

Many researchers and their corporate backers are hopeful that increased size will unlock human-like capabilities. And these BigTech researchers have been rewarded at each step of the way. 100 B parameter models such as BLOOM and InstructGPT revealed the capacity for LLMs to understand and respond appropriately to complex instructions for creative writing tasks such as composing a love poem from a Klingon to a human. And then trillion parameter models such as GPT-4 can perform few-shot learning where the entire machine learning training set is contained within a single conversational prompt
<----------section---------->

**Natural Language Processing and Large Language Models**

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
Lesson 22
Guardrails for LLMs
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This lesson focuses on implementing guardrails for Large Language Models (LLMs), which are crucial for ensuring their safe, accurate, and ethical deployment in real-world applications. The lesson is presented by Nicola Capuano and Antonio Greco from DIEM at the University of Salerno.

Outline:
*   Adding guardrails to LLMs
*   Techniques for adding guardrails
*   Frameworks for implementing guardrails

<----------section---------->

**Adding Guardrails to LLMs**

Guardrails:

Guardrails are mechanisms or policies implemented to control and regulate the behavior of LLMs. These mechanisms are essential to ensure that the LLMs generate responses that are safe, accurate, and context-appropriate. They provide a safety net that prevents unintended or harmful outputs.

Key functions of guardrails:

*   **Prevent harmful, biased, or inaccurate outputs:** Guardrails help filter and block the generation of toxic, discriminatory, or factually incorrect content.
*   **Align responses with ethical and operational guidelines:** They ensure that LLMs adhere to predefined ethical standards and business rules.
*   **Build trust and reliability for real-world applications:** By ensuring consistent and reliable performance, guardrails foster user trust and confidence in the use of LLMs.

Examples of Guardrails:

*   **Blocking harmful content:** Implementing filters to prevent the generation of hate speech, offensive language, or other inappropriate material.
*   **Restricting outputs to specific domains:** Limiting the LLM's responses to predefined knowledge areas to avoid irrelevant or inaccurate information.

<----------section---------->

**Types of Guardrails:**

Various types of guardrails can be implemented to address specific concerns and objectives:

*   **Safety Guardrails:** These guardrails are designed to prevent the generation of harmful or offensive content. They focus on eliminating outputs that could be harmful, discriminatory, or otherwise inappropriate.
*   **Domain-Specific Guardrails:** These guardrails restrict the LLM’s responses to specific knowledge areas. They are useful in applications where the LLM needs to provide information only within a predefined scope, ensuring accuracy and relevance.
*   **Ethical Guardrails:** Ethical guardrails are aimed at avoiding bias, misinformation, and ensuring fairness in the LLM's outputs. These guardrails address ethical concerns such as fairness, transparency, and accountability.
*   **Operational Guardrails:** Operational guardrails limit the LLM's outputs to align with specific business or user objectives. These guardrails ensure that the LLM's responses are consistent with the intended application and user expectations.

<----------section---------->

**Techniques for Adding Guardrails**

Multiple techniques can be employed to implement effective guardrails for LLMs:

*   Rule-based filters
*   Fine-tuning with custom data
*   Prompt Engineering
*   External validation layers
*   Real-time monitoring and feedback

<----------section---------->

**Rule-based filters:**

Rule-based filters involve defining predefined rules to block or modify certain outputs.

*   These rules are typically based on patterns, keywords, or regular expressions that identify undesirable content.
*   Examples:
    *   **Keyword blocking:** Filtering offensive or prohibited terms.
    *   **Regex-based patterns:** Filtering sensitive information, such as personal identification numbers or credit card numbers.
*   Rule-based filters are simple and efficient for basic content filtering, providing a straightforward way to enforce content policies.

<----------section---------->

**Fine-tuning with custom data:**

Fine-tuning involves training the model on domain-specific, curated datasets.

*   This technique allows for adjusting the model's weights to produce outputs that are aligned with predefined guidelines.
*   Examples:
    *   Fine-tuning for medical advice: Restricting responses to accurate and safe recommendations based on established medical knowledge.
    *   Fine-tuning for question answering: Training the model on course-specific materials to enhance the relevance and accuracy of responses.

<----------section---------->

**Prompt Engineering:**

Prompt engineering involves crafting or refining prompts to guide the LLM's behavior within desired boundaries.

*   By carefully designing prompts, it is possible to influence the LLM's outputs and steer them toward safer and more appropriate responses.
*   Examples:
    *   "Respond only with factual, non-controversial information."
    *   "Avoid speculative or unverifiable statements."

<----------section---------->

**External validation layers:**

External validation layers involve additional systems or APIs that post-process the model's outputs.

*   These layers can perform various checks and validations to ensure that the responses meet predefined criteria.
*   Examples:
    *   Toxicity detection APIs: Identifying and flagging toxic or offensive content in the LLM’s responses.
    *   Fact-checking models: Verifying the accuracy and validity of the information provided by the LLM.
*   External validation layers allow for modular and scalable implementation of guardrails.

<----------section---------->

**Real-time monitoring and feedback:**

Real-time monitoring and feedback involve continuously monitoring outputs for unsafe or incorrect content.

*   This technique allows for immediate detection and flagging of problematic outputs.
*   Tools:
    *   Human-in-the-loop systems: Involving human reviewers to assess and validate the LLM's responses in real-time.
    *   Automated anomaly detection: Using algorithms to identify unusual or unexpected patterns in the LLM's outputs.

<----------section---------->

**Best practices:**

To ensure robust safeguards, it is recommended to combine multiple techniques.

*   Example:
    *   Rule-based filtering + External validation + Fine-tuning: Using a combination of rule-based filters, external validation layers, and fine-tuning to provide a comprehensive set of guardrails.

<----------section---------->

**Frameworks for Implementing Guardrails**

Existing frameworks for implementing guardrails offer several benefits:

*   Easy integration with LLM APIs.
*   Predefined and customizable rulesets.

Popular tools include:

*   Guardrails AI: A library for implementing safeguards.
*   LangChain: For chaining prompts and filtering outputs.
*   OpenAI Moderation: A prebuilt API to detect unsafe content.

<----------section---------->

**Guardrails AI:**

*   [https://www.guardrailsai.com/](https://www.guardrailsai.com/)
*   Key features:
    *   **Validation:** Ensures outputs are within specified guidelines.
    *   **Formatting:** Controls the output structure.
    *   **Filters:** Removes or blocks unsafe content.

Example using Guardrails AI:
```python
from guardrails import Guard

guard = Guard(rules="rules.yaml")
response = guard(llm("Provide medical advice"))
```

<----------section---------->

**Langchain:**

*   Chains prompts with checks and filters.
*   Verifies outputs against predefined criteria.
*   Integrable with Guardrails:
    *   [https://www.guardrailsai.com/docs/integrations/langchain](https://www.guardrailsai.com/docs/integrations/langchain)

Example using Langchain:
```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["question"],
    template="Answer safely and factually: {question}"
)
```

<----------section---------->

**Try it yourself:**

*   Evaluate which techniques to add guardrails are more suited for your purposes.
*   A possible suggestion may be to proceed by incrementally adding complexity to the guardrails if you are not able to achieve a satisfying result with a simpler approach.
*   Give a careful look to the documentation of the existing frameworks.
*   Study similar examples that are available in the documentation of existing frameworks.
*   Try to apply guardrails to your project.

<----------section---------->

**Additional Context and Best Practices for Hardening NLP Software and LLMs:**

The information provided in the original text emphasizes the importance of guardrails in ensuring the safety, accuracy, and ethical behavior of LLMs. However, additional context and best practices can further enhance the effectiveness of these guardrails. Below are several recommendations based on industry practices and expert insights.

1.  **Bug Bounties**: Consider implementing bug bounties to reward users who identify bugs or gaps in your LLM's guardrails. This proactive approach can incentivize users to help improve the system's security and reliability.
2.  **Open Source Filter Rules**: If using an open-source framework, allow users to submit filter rules. This collaborative approach can leverage community knowledge to enhance the robustness of the guardrails.
3.  **Real-Time Unit Tests**: Think of your filters as real-time unit tests. Guardrails-ai, for example, provides rule templates that can be configured for your specific needs.
4.  **Machine Learning Classifiers**: Implement conventional machine learning classifiers to detect malicious intent or inappropriate content in LLM outputs. These models can generalize from examples to provide high reliability.
5.  **Custom Machine Learning Models**: Use custom machine learning models to protect your LLM from prompt injection attacks and other adversarial techniques. This approach is particularly useful for preventing legal or medical advice generation, which is strictly regulated in many countries.
6.  **Fuzzy Regular Expressions**: Employ fuzzy regular expressions to detect bad messages, even with misspellings or transliterations. This can help avoid the "whack-a-mole" effect of manually creating individual statements for every possible attack vector. Tools like SpaCy's Matcher class, ReLM, Eleuther AI’s LM evaluation harness, and the Python fuzzy regular expression package can be beneficial.
7.  **Rule-Based Pipelines with SpaCy**: Use SpaCy as the primary tool for building NLP guardrails and rule-based pipelines. Its flexibility and power make it suitable for creating effective content filters.
8.  **Guardrails-AI Limitations**: Be aware that while Guardrails-AI can be useful, it may not completely prevent LLMs from going off the rails. Look for additional filters and rules to monitor LLM responses and prevent inappropriate content.
9.  **Python Templating Systems**: For expressive templating, use standard Python templating systems like f-strings or jinja2 templates instead of RAIL. LangChain also offers example LLM prompt templates.
10. **SpaCy Matcher for Taboo Words**: Configure a SpaCy Matcher to avoid taboo words or names. This can involve substituting more meaningful synonyms or euphemisms or avoiding competitor mentions. The ability to detect and replace specific terms ensures more controlled and appropriate content generation.
11. **Red Teaming**: Implement a red teaming approach to test and improve the reliability of your NLP pipeline. This involves having a team attempt to bypass or disable the guardrails, helping identify vulnerabilities and improve the system's robustness.
12. **Llama 2**: Use Llama 2, Vicuna, and Falcon for open-source models that are performant and can be downloaded and run in a reasonable amount of time. These open-source models allow for community-driven improvements and transparency.
13. **AI Ethics and Safety**: Recognize the difference between AI ethics and AI safety. Focus on both immediate harm caused by algorithms (AI ethics) and long-term existential risks posed by superintelligent machines (AI safety).
14. **Explainable AI**: Implement explainable AI to create algorithms that can explain how and why they make decisions. This can help create more ethical and safe AI by providing insights into the decision-making process.
15. **Guardrails-AI package**: Make sure you’ve installed the `guardrails-ai` package not the `guardrails` package. You can use `pip` or `conda` or your favorite Python package manager:

```bash
$ pip install guardrails-ai
```

<----------section---------->

# Natural Language Processing and Large Language Models

This lesson explores fundamental concepts in Natural Language Processing (NLP) and how they are applied, particularly concerning Large Language Models (LLMs). We will cover methods for representing text numerically, which is essential for enabling computers to process and understand human language.

Corso di Laurea Magistrale in Ingegneria Informatica. Lesson 3: Math with Words. Nicola Capuano and Antonio Greco. DIEM – University of Salerno.

## Outline

This lesson will cover the following topics:

*   **Term Frequency (TF):** A basic measure of how often a word appears in a document.
*   **Vector Space Model (VSM):** Representing documents as vectors in a multi-dimensional space, enabling mathematical operations.
*   **TF-IDF:** Term Frequency-Inverse Document Frequency, which refines TF by considering a word's importance across the entire corpus.
*   **Building a Search Engine:** Application of TF-IDF and vector space models to create a basic search engine.

<----------section---------->

## Term Frequency

Term Frequency (TF) is a foundational concept in NLP. It quantifies how many times a term (word) appears within a document. The underlying assumption is that the more often a word occurs in a document, the more relevant it is to the document's content.

### Bag of Words

The Bag of Words (BoW) model simplifies text into a numerical representation suitable for machine learning algorithms. The BoW model represents text as an unordered collection of words, disregarding grammar and word order but keeping track of word frequencies.

*   **One-Hot Encoding:** The initial step involves converting each word into a one-hot vector, a vector with all zero values except for a single one, indicating the presence of that word.

*   **Binary BoW:** In this version, the one-hot vectors are combined using the OR operation. The resulting vector indicates whether a word is present (1) or absent (0) in the text, without counting occurrences.

*   **Standard BoW:** Here, one-hot vectors are summed together. Each position in the resulting vector represents a word, and its value indicates the number of times that word appears in the text.

*   **Term Frequency (TF):** The number of times each word occurs in the text.

**Assumption:** The core idea is that the more a word appears in a document, the more meaning it contributes.

### Calculating TF

The following Python code demonstrates how to calculate term frequency using the `spaCy` library:

```python
# Extract tokens
sentence = "The faster Harry got to the store, the faster Harry, the faster, would get home."

import spacy
nlp = spacy.load("en_core_web_sm") # load the language model
doc = nlp(sentence)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct]

tokens
```

**Explanation:**

1.  **Import spaCy:** The `spaCy` library is imported for NLP tasks.
2.  **Load Language Model:** The `en_core_web_sm` language model is loaded. This model provides tokenization, part-of-speech tagging, and other NLP functionalities.
3.  **Tokenization:** The sentence is processed using `nlp(sentence)` to create a `Doc` object, which contains the tokenized words.
4.  **Token Extraction:** A list comprehension is used to extract the tokens. Each token is converted to lowercase (`tok.lower_`), and stop words (common words like "the", "is") and punctuation marks are excluded.

```
{'faster', 'harry', 'got', 'store', 'faster', 'harry', 'faster', 'home'}
```

This output shows the extracted tokens after removing stop words and punctuation.

```python
# Build BoW with word count
import collections

bag_of_words = collections.Counter(tokens) # counts the elements of a list
bag_of_words
```

**Explanation:**

1.  **Import `collections.Counter`:** The `Counter` class from the `collections` module is imported. It's used for counting the frequency of each token.
2.  **Build Bag of Words:** `collections.Counter(tokens)` creates a dictionary-like object that counts the occurrences of each token in the `tokens` list.

```
Counter({'faster': 3, 'harry': 2, 'got': 1, 'store': 1, 'home': 1})
```

This output shows the word counts. For example, "faster" appears 3 times, and "harry" appears 2 times.

```python
# Most common words
bag_of_words.most_common(2) # most common 2 words
```

**Explanation:**

1.  **`most_common(2)` Method:** The `most_common(2)` method returns the two most frequent words and their counts.

```
(('faster', 3), ('harry', 2))
```

This indicates that "faster" is the most common word (3 times), and "harry" is the second most common (2 times).

<----------section---------->

### Limits of TF

Term Frequency alone has limitations in determining the importance of a word:

**Example:**

*   In document A, the word "dog" appears 3 times.
*   In document B, the word "dog" appears 100 times.

Based on raw TF, it seems "dog" is more important for document B. But we need more context.

**Additional information:**

*   Document A is a 30-word email to a veterinarian.
*   Document B is the novel War & Peace (approx. 580,000 words).

Now, with the document lengths, we can see that "dog" is proportionally more important to the shorter document A.

**Analysis:**

The raw count of a word is not always indicative of its importance. Longer documents will naturally have higher word counts. To address this, we use normalized TF.

### Normalized TF

Normalized (weighted) TF is the word count divided by the total number of words in the document, giving a proportional measure:

*   TF (dog, document A) = 3/30 = 0.1
*   TF (dog, document B) = 100/580000 = 0.00017

This normalization reveals that "dog" is relatively more important in document A (0.1) than in document B (0.00017).

```python
import pandas as pd

counts = pd.Series(bag_of_words) # from dict to Pandas Series
counts / counts.sum() # calculate TF
```

**Explanation:**

1.  **Convert to Pandas Series:** The `bag_of_words` dictionary is converted into a Pandas Series for easier manipulation.
2.  **Calculate Normalized TF:** `counts / counts.sum()` divides each word count by the total number of words in the sentence, thus normalizing the term frequencies.

```
faster    0.375
harry     0.250
got       0.125
store     0.125
home      0.125
dtype: float64
```

This shows the normalized term frequencies for each word in the example sentence.  "faster" accounts for 37.5% of the words, and "harry" accounts for 25%.

<----------section---------->

## Vector Space Model

The Vector Space Model (VSM) provides a mathematical framework for representing documents as vectors in a high-dimensional space. Each dimension corresponds to a term (word), and the value in that dimension represents the term's importance in the document.

### NLTK Corpora

The Natural Language Toolkit (NLTK) is a Python library for working with human language data. It includes several text corpora:

*   **Purpose:** These corpora are used to train and test NLP algorithms.
*   **Accessibility:** NLTK provides a package to easily access corpus data.
*   **Documentation:** [https://www.nltk.org/howto/corpus.html](https://www.nltk.org/howto/corpus.html) offers instructions for using the NLTK corpora.

### Reuters 21578 corpus

The Reuters-21578 corpus is a widely used dataset for NLP and text classification:

*   **Application:** Used extensively for NLP research and text classification tasks.
*   **Content:** Contains thousands of news articles published by Reuters in 1986.
*   **Categories:** News articles are categorized into 90 different topics.

### Using Reuters 21578

The following code shows how to access and process the Reuters 21578 corpus using NLTK:

```python
import nltk

nltk.download('reuters') # download the reuters corpus
ids = nltk.corpus.reuters.fileids() # ids of the documents
sample = nltk.corpus.reuters.raw(ids[0]) # first document

print(len(ids), "samples.\n") # number of documents
print(sample)
```

**Explanation:**

1.  **Import NLTK:** The `nltk` library is imported.
2.  **Download Reuters Corpus:** `nltk.download('reuters')` downloads the Reuters corpus if it's not already present.
3.  **Get Document IDs:** `nltk.corpus.reuters.fileids()` retrieves a list of IDs for all documents in the corpus.
4.  **Access First Document:** `nltk.corpus.reuters.raw(ids[0])` accesses the raw text content of the first document in the corpus.
5.  **Print Information:** The number of documents and the content of the first document are printed.

```
Taiwan had a trade trade surplus of 15.6 billion dlrs last
year, 95 pct of it with the U.S.

The surplus helped swell Taiwan's foreign exchange reserves
to 53 billion dlrs, among the world's largest.

“We must quickly open our markets, remove trade barriers and
cut import tariffs to allow imports of U.S. Products, if we
want to defuse problems from possible U.S. Retaliation," said
Paul Sheen, chairman of textile exporters <Taiwan Safe Group>.

A senior official of South Korea's trade promotion
association said the trade dispute between the U.S. And Japan
might also lead to pressure on South Korea, whose chief exports
are similar to those of Japan.

Last year South Korea had a trade surplus of 7.1 billion
dirs with the U.S., Up from 4.9 billion dlrs in 1985.

In Malaysia, trade officers and businessmen said tough
curbs against Japan might allow hard-hit producers of
semiconductors in third countries to expand their sales to the
U.S.
```

This output is the raw text of the first news article in the Reuters corpus.

### Using Reuters 21578

The following code preprocesses the sample text and calculates term frequencies:

```python
doc = nlp(sample)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]

bag_of_words = collections.Counter(tokens)
counts = pd.Series(bag_of_words).sort_values(ascending=False) # sorted series
counts = counts / counts.sum() # calculate TF

print(counts.head(10))
```

**Explanation:**

1.  **Process with spaCy:** The sample text is processed using `nlp(sample)`.
2.  **Tokenization:** A list comprehension extracts tokens, converts them to lowercase, and removes stop words, punctuation, and whitespace.
3.  **Build Bag of Words:** `collections.Counter(tokens)` creates a Counter object to count token occurrences.
4.  **Calculate and Sort TF:** The token counts are converted to a Pandas Series, sorted in descending order, and then normalized to calculate term frequencies.
5.  **Print Top 10:** The top 10 most frequent words and their normalized frequencies are printed.

```
u.s.          0.039627
said         0.037296
trade        0.034965
japan        0.027972
dlrs         0.013986
exports      0.013986
tariffs      0.011655
imports      0.011655
billion      0.011655
electronics    0.009324
dtype: float64
```

This output shows the most relevant words in the first news article, based on normalized term frequency.

### Corpus Processing

The following code processes the first 100 documents of the Reuters corpus to create a term-document matrix:

```python
ids_subset = ids[:100] # to speed-up we consider only the first 100 elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    doc = nlp(sample)
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
```

**Explanation:**

1.  **Subset of Document IDs:** `ids[:100]` selects the first 100 document IDs for processing.
2.  **Iterate Through Documents:** A loop iterates through each document ID.
3.  **Process Each Document:**
    *   The raw text is retrieved using `nltk.corpus.reuters.raw(id)`.
    *   The text is processed using `nlp(sample)`.
    *   Tokens are extracted, converted to lowercase, and filtered to remove stop words, punctuation, and whitespace.
    *   A bag of words is created using `collections.Counter(tokens)`.
    *   Term frequencies are calculated and sorted.
    *   The term frequencies are appended to `counts_list`.
4.  **Create DataFrame:** `pd.DataFrame(counts_list)` converts the list of Series into a DataFrame, where each row represents a document and each column represents a term.
5.  **Fill NaN Values:** `df.fillna(0)` replaces NaN values with 0, ensuring a complete numerical representation.

```
Sample 100 of 100 processed.
```

This message confirms that all 100 documents have been processed.

```python
df
```

```
      u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[100 rows x 3089 columns]
```

The resulting DataFrame `df` represents the term-document matrix.  Each row is a document, each column is a term, and each value is the normalized term frequency.

<----------section---------->

### Corpus Processing

The initial corpus processing code is inefficient due to the full spaCy pipeline being run for each document. spaCy extracts a lot of information (POS tags, lemmas, etc.) that is unnecessary for simple tokenization.
```python
ids_subset = ids[:100] # to speed-up we consider only the first 100 elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    doc = nlp(sample)
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
df
```
#### spaCy Pipeline Components

When you call `nlp` on a text, spaCy performs several steps:

1.  **Tokenization:** The text is tokenized into a `Doc` object.
2.  **Pipeline Processing:** The `Doc` object is then processed through a pipeline of components.

```python
# Show the current pipeline components
print(nlp.pipe_names)
```

```
['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
```

The output shows the default spaCy pipeline components: `tok2vec`, `tagger`, `parser`, `attribute_ruler`, `lemmatizer`, and `ner`. Each of these performs a specific NLP task, but for simple tokenization, they are unnecessary.

<----------section---------->

### Corpus Processing (Optimization)

To optimize the process, we can disable the unnecessary pipeline components, focusing solely on tokenization:

```python
ids_subset = ids # we now consider all elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    # disable=["tok2vec", "tagger", “parser", “attribute_ruler", “lemmatizer", "ner"]
    doc = nlp(sample, disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
```

**Explanation:**

1.  **Disable Pipeline Components:** The `nlp(sample, disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])` line disables all components except the tokenizer.
2.  **Process All Documents:** `ids_subset = ids` now processes all documents in the corpus.
3. The rest of the code remains the same, but now it will execute much faster because it skips all the processing stages.

```
Sample 10788 of 10788 processed.
```
This message confirms that all documents have been processed.
```python
df
```

```
         u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[10788 rows x 49827 columns]
```

The resulting DataFrame `df` now contains the term-document matrix for all 10788 documents, significantly increasing the vocabulary size and showcasing the scalability of the optimized approach.

### Corpus Processing

#### Term-Document Matrix

The resulting `DataFrame` is a **Term-Document Matrix**:

*   **Rows:** Represent documents in the corpus.
*   **Columns:** Represent unique terms (words) from the vocabulary.
*   **Elements:** Contain the term frequency (TF) or other weighting schemes for each term in each document.

```python
df
```

```
         u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[10788 rows x 49827 columns]
```

<----------section---------->

### Vector Space Model

The Vector Space Model (VSM) represents documents as vectors in a multidimensional space.

*   **Dimensions:** Each dimension corresponds to a term from the vocabulary.
*   **Values:** The value in each dimension indicates the importance or frequency of that term in the document.
*   **Building:** It is constructed from a term-document matrix.

#### 2D Example: with normalized TF

Consider a simplified vocabulary with just two words, w1 and w2.  We can visualize the documents as points in a 2D space.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![2d_example_vector_space.png](2d_example_vector_space.png)

In this example, each document is a vector representing the normalized term frequencies of w1 and w2.

<----------section---------->

### Document Similarity

To compare documents, we need a measure of similarity between their vector representations.

#### Euclidean Distance

*   **Sensitivity:** Highly sensitive to the magnitude of the vectors. Documents with higher term frequencies will have larger vector lengths, skewing the distance calculation.
*   **Direction vs. Magnitude:** In text analysis, the direction of the vectors (relative importance of terms) is usually more informative than their magnitude.
*   **Usage:** Less commonly used in NLP due to its sensitivity to magnitude.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![euclidean_distance.png](euclidean_distance.png)

This illustrates how Euclidean distance measures the straight-line distance between document vectors, emphasizing magnitude differences.

<----------section---------->

#### Document Similarity

##### Cosine Similarity

*   **Definition:** Measures the cosine of the angle between two vectors.

*   **Focus:** Concentrates on the direction of the vectors, ignoring their magnitude. It considers the orientation of the vectors in the vector space.

*   **Effectiveness:** Effective for normalized text representations because it emphasizes the relative importance of terms.

*   **Usage:** Widely used in NLP because of its robustness to document length.

The formula for Cosine Similarity is:

```
sim(A, B) = cos(θ) = (A · B) / (|A| * |B|)
```

Where:

*   `A · B` is the dot product of vectors A and B.
*   `|A|` and `|B|` are the magnitudes (Euclidean norms) of vectors A and B.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![cosine_similarity.png](cosine_similarity.png)

This illustrates how Cosine Similarity focuses on the angle between vectors, disregarding magnitude.

#### Properties of Cosine Similarity

Cosine similarity produces a value between -1 and 1:

*   **1:** Vectors point in the same direction. The documents use the same words in similar proportions, indicating they likely discuss the same topic.
*   **0:** Vectors are orthogonal (perpendicular). Documents share no common words, suggesting completely different topics.
*   **-1:** Vectors point in opposite directions. This is impossible with TF (since word counts cannot be negative).  It can occur with other word representations (e.g., embeddings) where negative values are possible.

#### Calculate Cosine Similarity

```python
import numpy as np

def sim(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

# Example:
print(sim(df.iloc[0], df.iloc[1]))
print(sim(df.iloc[0], df.iloc[2]))
print(sim(df.iloc[0], df.iloc[0]))
```

**Explanation:**

1.  **Define Cosine Similarity Function:** The `sim(vec1, vec2)` function calculates the cosine similarity between two vectors.
2.  **Calculate Dot Product:** `np.dot(vec1, vec2)` computes the dot product of the two vectors.
3.  **Calculate Norms:** `np.linalg.norm(vec1)` and `np.linalg.norm(vec2)` compute the Euclidean norms (magnitudes) of the vectors.
4.  **Compute Cosine Similarity:** The cosine similarity is calculated as the dot product divided by the product of the norms.

```
0.14261893769917347
0.2365347461078053
1.0
```

These values represent the cosine similarity between different documents in the corpus. A value of 1.0 indicates that the first document is perfectly similar to itself, as expected.

```python
# Compare TF matrix subset (documents 0 and 1)
df.loc[[0, 1], (df.loc[0] > 0) & (df.loc[1] > 0)]
```

**Explanation:**

1.  **Select Documents:** `df.loc[[0, 1]]` selects rows corresponding to documents 0 and 1.
2.  **Select Common Terms:** `(df.loc[0] > 0) & (df.loc[1] > 0)` creates a boolean mask that identifies columns (terms) that have non-zero values in both documents 0 and 1.
3.  **Display Result:** The code displays the term frequencies for the common terms in the selected documents.

```
       said      min      year       pct  government
0  0.037296  0.002331  0.009324  0.004662    0.002331
1  0.042254  0.028169  0.014085  0.056338    0.014085
```

This output displays the term frequencies of common terms between documents 0 and 1, helping to understand their similarity.

```python
# Try also with other documents
```

This is a comment encouraging further exploration with different document pairs to gain a better understanding of cosine similarity.

<----------section---------->

## TF-IDF

Term Frequency-Inverse Document Frequency (TF-IDF) is a statistic that measures the importance of a term in a document relative to a collection of documents (corpus).

### Inverse Document Frequency

TF alone doesn't consider how unique a word is across the entire corpus.

*   **Common Words:** Words like "the," "is," and "and" appear frequently in many documents, contributing little to differentiating documents.
*   **Domain-Specific Terms:** Terms like "planet" or "star" might be common in a corpus about astronomy but still not useful for distinguishing different documents about astronomy.

Inverse Document Frequency (IDF) addresses this by scaling down the weight of common words and scaling up the weight of rare words.

### Inverse Document Frequency
IDF increases the weight of rare words and decreases the weight of common words.  The IDF is calculated as:

```
idf(t, D) = log(N / |{d ∈ D: t ∈ d}|)
```

Where:

*   N: total number of documents in the corpus (N = |D|).
*   `|{d ∈ D: t ∈ d}|`: the number of documents where the term t appears (i.e., tf(t, d) != 0).  This counts how many documents contain the term.

**Note:** If a term does not appear in any document in the corpus, the IDF calculation would result in division by zero, which is undefined. Therefore, a common adjustment is to add 1 to both the numerator and the denominator.
The adjusted IDF formula is:
```
idf(t, D) = log((1 + N) / (1 + |{d ∈ D: t ∈ d}|))
```

### TF-IDF
Term Frequency – Inverse Document Frequency combines TF and IDF to assess term importance.

*   **TF:** Gives higher importance to words frequent in a specific document.
*   **IDF:** Gives higher importance to words rare in the overall corpus.

The TF-IDF score is calculated as:

```
tfidf(t, d, D) = tf(t, d) * idf(t, D)
```

*   **High TF-IDF:** Indicates a term is frequent in a document but rare in the corpus, making it significant for that document.
*   **Low TF-IDF:** Indicates a term is infrequent in the document or common in the corpus, making it less significant.

<----------section---------->

### TF-IDF: Example

*   Document A: Jupiter is the largest planet
*   Document B: Mars is the fourth planet from the sun

| Term      | Document A | Document B | df | idf      | TF-IDF (A) | TF-IDF (B) |
| --------- | ---------- | ---------- | -- | -------- | ---------- | ---------- |
| jupiter   | 1          | 0          | 1  | ln(2/1)=0.69 | 0.138      | 0          |
| largest
<----------section---------->

**Natural Language Processing and Large Language Models**

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
Lesson 4: Text Classification

Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This lesson will cover the fundamental concepts of text classification, a core task in Natural Language Processing (NLP). It will explore different types of text classification and provide a practical example using the Reuters dataset, along with an exercise on sentiment analysis using the IMDB dataset.

<----------section---------->

**Outline**

This lesson will cover the following topics:

*   Text Classification: An introduction to the process of assigning categories to text documents.
*   Topic Labeling Example: A demonstration of classifying news articles using the Reuters-21578 dataset.
*   Sentiment Analysis Exercise: A practical task involving the classification of movie reviews based on their sentiment (positive or negative).

<----------section---------->

**Text Classification**

Text Classification is the process of assigning one or more predefined classes or categories to a given text document. This process relies primarily on the content of the text itself. It serves various purposes, including:

*   Topic Labeling: Identifying the main subject or theme of a document (e.g., 'sports', 'finance', 'technology').
*   Intent Detection: Determining the goal or purpose behind a user's input (e.g., 'book a flight', 'order food', 'get weather information').
*   Sentiment Analysis: Ascertaining the emotional tone or opinion expressed in a text (e.g., 'positive', 'negative', 'neutral').

**Important Distinctions:**

*   Text classification focuses solely on text content, disregarding other attributes or metadata associated with the document. This contrasts with *document classification*, which may incorporate metadata like author, date, or source.
*   The classes in text classification are predefined, meaning the categories are established *before* the classification process. This is different from *document clustering*, where the categories are not known in advance and the algorithm aims to group similar documents together based on inherent patterns.

<----------section---------->

**Definition**

Mathematically, text classification can be defined as follows:

Given:

*   A set of documents D = {d<sub>1</sub>, ..., d<sub>n</sub>}, where 'n' is the total number of documents.
*   A set of predefined classes C = {c<sub>1</sub>, ..., c<sub>m</sub>}, where 'm' is the total number of classes.

The goal of text classification is to find a classifier function:

f: D x C -> {True, False}

This function 'f' takes a document 'd<sub>i</sub>' from the set of documents 'D' and a class 'c<sub>j</sub>' from the set of classes 'C' as input. It then outputs a Boolean value:

*   True: indicates that the document 'd<sub>i</sub>' belongs to the class 'c<sub>j</sub>'.
*   False: indicates that the document 'd<sub>i</sub>' does not belong to the class 'c<sub>j</sub>'.

In essence, for every possible pair (d<sub>i</sub>, c<sub>j</sub>) ∈ D x C, the classifier determines whether the document 'd<sub>i</sub>' should be assigned to the class 'c<sub>j</sub>'.

<----------section---------->

**Types of Classification**

Text classification can be categorized based on the number of classes a document can be assigned to:

*   **Single-label:** Each document in the set D is assigned to *only one* class from the set C. For example, a news article can only be categorized as either 'sports' or 'politics' but not both.
*   **Binary:** This is a special case of single-label classification where the set C contains only two classes. This is a decision between a class and its complement. For example, classifying an email as either 'spam' or 'not spam'.
*   **Multi-label:** Each document can be assigned to *one or more* classes from the set C.  For example, a movie can be labeled as 'action', 'comedy', and 'adventure' simultaneously. Multi-label classification can be approached by breaking it down into a series of binary classification problems, one for each class.

<----------section---------->

**ML-Based Classification**

Machine learning (ML) provides powerful techniques for automating text classification.  The general process involves:

1.  **Training Data:** A machine learning model is trained on a set of annotated text documents. These annotated documents form the training set.
2.  **Labeling:** Each document in the training set is associated with one or more class labels, indicating its category or categories.
3.  **Model Training:** The machine learning algorithm learns patterns and relationships between the text content and the assigned labels during the training phase.
4.  **Prediction:** After training, the model can predict the category (or categories) for a new, unseen document.
5.  **Confidence Measure:** The classifier may also provide a confidence score or probability, indicating how certain it is about its prediction.
6.  **Vector Representation:** A crucial step is converting text documents into numerical vectors that machine learning models can process. Common techniques include:

    *   **TF-IDF (Term Frequency-Inverse Document Frequency):** Weighs words based on their frequency within a document and their rarity across the entire corpus.
    *   **Word Embeddings (Word2Vec, GloVe, FastText):** Represents words as dense vectors in a high-dimensional space, capturing semantic relationships between words.
    *   **BERT, transformer-based encoders:** Create contextualized word embeddings to better represent the semantic meaning of text

<----------section---------->

**Topic Labeling Example**

**Classifying Reuters News**

The Reuters-21578 dataset is a widely used benchmark dataset for text classification. It consists of news articles from Reuters newswire, categorized into various topics.

*   **Multi-class and Multi-label:** This dataset is both multi-class (articles can belong to one of many topics) and multi-label (articles can belong to multiple topics simultaneously).
*   **90 distinct classes:** The articles are categorized into 90 different topics.
*   **7,769 training documents, 3,019 test documents:** The dataset is split into a training set used to train the classification model, and a test set used to evaluate its performance.
*   **The number of words per document ranges from 93 to 1,263:** The length of the articles varies considerably.
*   **Skewness:** The distribution of documents across categories is uneven:

    *   Some classes have over 1,000 documents, indicating prevalent topics.
    *   Other classes have fewer than 5 documents, representing less common or niche topics.
    *   Most documents are assigned either one or two labels, but some documents are labeled with up to 15 categories. This makes it a complex multi-label classification problem.

More statistics on https://martin-thoma.com/nlp-reuters/

<----------section---------->

**Corpus Management**

The following Python code snippet demonstrates how to load and explore the Reuters dataset using the NLTK (Natural Language Toolkit) library.

```python
import nltk
from nltk.corpus import reuters

nltk.download('reuters')  # Download the Reuters dataset if not already present

ids = reuters.fileids()  # Get a list of all file IDs in the corpus

training_ids = [id for id in ids if id.startswith("training")]  # Get IDs of training documents
test_ids = [id for id in ids if id.startswith("test")]  # Get IDs of test documents
categories = reuters.categories()  # Get a list of all categories (topics)

print("{} training items:".format(len(training_ids)), training_ids)
print("{} test items:".format(len(test_ids)), test_ids)
print("{} categories:".format(len(categories)), categories)

print("\nCategories of '{}':".format(training_ids[0]), reuters.categories(training_ids[0]))
print("Categories of '{}':".format(test_ids[2]), reuters.categories(test_ids[2]))
print("Items within the category 'trade'", reuters.fileids('trade'))
```

The output of this code provides an overview of the dataset's structure:

```
[nltk_data] Downloading package reuters to /root/nltk_data...
7769 training items: ['training/1', 'training/10', 'training/100', 'training/1000', 'training/10000', ...]
3019 test items: ['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833', ...]
90 categories: ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', ...]

Categories of 'training/1': ['cocoa']
Categories of 'test/14829': ['crude', 'nat-gas']
Items within the category 'trade' ['test/14826', 'test/14832', 'test/14858', 'test/14862', ...]
```

This output shows the number of training and test documents, the list of categories, and examples of how documents are assigned to categories.  For example, the training document with ID 'training/1' belongs to the 'cocoa' category, while the test document 'test/14829' belongs to both 'crude' and 'nat-gas' categories. The code demonstrates the multi-label nature of the Reuters dataset.

<----------section---------->

**Process**

The typical process for training a text classifier on the Reuters dataset involves the following steps:

1.  **Data Extraction:** Extract the text content and associated labels for both the training and test sets.
2.  **Feature Engineering (TF-IDF):** Convert the text documents into numerical vectors using TF-IDF. This involves creating TF-IDF matrices for both the training and test sets.
3.  **Label Transformation (One-Hot Encoding):** Transform the list of labels (categories) into a binary matrix format using one-hot encoding. This is essential for training machine learning models on multi-label data.
4.  **Classifier Training:** Train a machine learning classifier (e.g., Multilayer Perceptron (MLP)) using the training data (TF-IDF vectors and corresponding binary labels).
5.  **Classifier Testing:** Evaluate the trained classifier on the test data to assess its performance.

<----------section---------->

**Pre-Processing**

The following Python code snippet demonstrates how to perform the pre-processing steps using scikit-learn.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer

# Generate training and test sets
training_corpus = [reuters.raw(id) for id in training_ids]
training_labels = [reuters.categories(id) for id in training_ids]
test_corpus = [reuters.raw(id) for id in test_ids]
test_labels = [reuters.categories(id) for id in test_ids]

# Create TF-IDF matrices
vectorizer = TfidfVectorizer(min_df=3)  # a word must appear in at least 3 documents
training_vectors = vectorizer.fit_transform(training_corpus) # Learn vocabulary and transform training data
test_vectors = vectorizer.transform(test_corpus) # Transform test data using learned vocabulary

# Transform a list of label lists in binary matrix
mlb = MultiLabelBinarizer() # Initialize MultiLabelBinarizer
training_mlb = mlb.fit_transform(training_labels) # Learn mapping from labels to binary vectors and transform training labels
test_mlb = mlb.transform(test_labels) # Transform test labels using learned mapping

len(vectorizer.vocabulary_)
# 11361
```

**Explanation:**

*   `TfidfVectorizer`: Converts the text corpus into a TF-IDF matrix. The `min_df=3` parameter specifies that a word must appear in at least 3 documents to be included in the vocabulary.
*   `fit_transform`:  This function combines two sequential steps, first applying the `fit` function to learn vocabulary and document frequencies from the training data, and then applying the `transform` function to create the TF-IDF matrix.  For the test data, only `transform` is used to ensure consistency in the feature space.
*   `MultiLabelBinarizer`: Transforms the lists of category labels into a binary matrix representation, where each column represents a category, and each row represents a document. A value of 1 indicates that the document belongs to that category, and 0 indicates that it does not. This is also known as one-hot encoding.

<----------section---------->

**MLP Classifier Training**

The following Python code snippet demonstrates how to train an MLP classifier using scikit-learn:

```python
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt

# Train an MLP classifier
classifier = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam',
                            max_iter=100, early_stopping=True, verbose=True)

classifier.fit(training_vectors, training_mlb)


# Plot loss and validation curves
def plot(ax, data, title, xlabel, ylabel):
    ax.plot(data, label=title, marker='o')
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_title(title)
    ax.legend()
    ax.grid()


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))  # create subplots

plot(ax1, classifier.loss_curve_, 'Training Loss', 'Iterations', 'Loss')
plot(ax2, classifier.validation_scores_, 'Validation Accuracy', 'Iterations', 'Accuracy')
plt.tight_layout()  # Adjust layout to prevent overlapping
plt.show()
```

**Explanation:**

*   `MLPClassifier`: This is a multi-layer perceptron classifier, a type of neural network.
    *   `hidden_layer_sizes=(128, 64)`: Defines the architecture of the neural network with two hidden layers, one with 128 neurons and the other with 64 neurons.
    *   `activation='relu'`: Specifies the Rectified Linear Unit activation function.
    *   `solver='adam'`: Uses the Adam optimization algorithm for training.
    *   `max_iter=100`: Sets the maximum number of iterations for training.
    *   `early_stopping=True`: Enables early stopping to prevent overfitting.
    *   `verbose=True`: Enables verbose output during training, showing the progress of each iteration.
*   `classifier.fit(training_vectors, training_mlb)`: Trains the MLP classifier using the TF-IDF vectors and the binary label matrix of the training data.
*   The `plot` function is defined to visualize training loss and validation accuracy curves. This helps monitor the training process and identify potential issues such as overfitting or underfitting.
*   `plt.subplots` creates a figure with two subplots for visualizing training loss and validation accuracy curves during the training process. `plt.tight_layout()` ensures labels don't overlap.

<----------section---------->

**MLP Classifier Training (Example plot)**

[The image of "Training Loss Validation Accuracy" should be displayed here.]
*This plot shows the training loss and validation accuracy over iterations. It allows for assessing the model's performance and identifying potential overfitting or underfitting.*

<----------section---------->

**Testing Metrics**

To evaluate the performance of the text classifier, several metrics are used:

*   **Micro Average:** Calculates the average metric across all classes by considering the total number of true positives, false negatives, and false positives. Useful when you want to give each instance equal weight, regardless of the class.
*   **Macro Average:** Calculates the metric independently for each class and then takes the average (unweighted mean) of these values. Useful when you want to give each class equal weight, regardless of its size.  Can be misleading if classes are imbalanced.
*   **Weighted Average:** Calculates the average of the metric, weighted by the support (the number of true instances) of each class. A more balanced metric than macro average in imbalanced datasets.
*   **Samples Average:** Computes the average of the metrics for each sample (instance) rather than for each class. This is particularly relevant in multi-label classification problems where each instance can belong to multiple classes.

<----------section---------->

**Testing Results**

The following Python code snippet demonstrates how to generate a classification report using scikit-learn:

```python
from sklearn.metrics import classification_report

# Predict the categories of the test set
predictions = classifier.predict(test_vectors)

# Print classification report
print(classification_report(test_mlb, predictions, target_names=mlb.classes_, zero_division=0))
```

**Example Output:**

```
              precision    recall  f1-score   support

         acq       0.98      0.91      0.94       719
        alum       1.00      0.30      0.47        23
      barley       0.89      0.57      0.70        14
         bop       1.00      0.43      0.60        30
     carcass       0.67      0.22      0.33        18
castor-oil       0.00      0.00      0.00         1
       cocoa       1.00      0.56      0.71        18
     coconut       0.00      0.00      0.00         2
 coconut-oil       0.00      0.00      0.00         3

   micro avg       0.97      0.79      0.87      3019
   macro avg       0.56      0.36      0.42      3019
weighted avg       0.95      0.79      0.85      3019
 samples avg       0.96      0.80      0.85      3019
```

**Explanation of Metrics:**

*   **Precision:** The ability of the classifier not to label an instance positive that is actually negative. For each class, it's the ratio of true positives to the sum of true positives and false positives.
*   **Recall:** The ability of the classifier to find all positive instances. For each class, it's the ratio of true positives to the sum of true positives and false negatives.
*   **F1-score:** The harmonic mean of precision and recall, providing a balanced measure of the classifier's accuracy.
*   **Support:** The number of actual occurrences of the class in the test set.

The classification report provides a detailed breakdown of the classifier's performance for each class, as well as overall performance metrics (micro, macro, weighted, and samples averages). The `zero_division=0` argument handles cases where a class has no predicted instances, preventing division-by-zero errors.

<----------section---------->

**Sentiment Analysis Exercise**

**Sentiment Analysis**

Sentiment Analysis is the process of identifying and categorizing opinions expressed in a piece of text, determining the writer's attitude towards a particular topic or product.

**Applications:**

*   **Business:** Analyzing customer feedback and product reviews to understand customer satisfaction and brand perception. This information can be used to improve products and services, and to tailor marketing campaigns.
*   **Finance:** Predicting market trends based on investor sentiment extracted from news articles and social media. This can be used to make informed investment decisions.
*   **Politics:** Analyzing public opinion during elections or policy changes. This can be used to understand voter preferences and to craft effective political messages.

Sentiment analysis can be viewed as a text classification problem: Given a text, the task is to classify it into one of several predefined sentiment categories, such as positive, negative, or neutral.

<----------section---------->

**IMDB Dataset**

The IMDB dataset consists of 50,000 highly polarized movie reviews from the Internet Movie Database. This dataset is often used for binary sentiment classification.

*   The dataset is balanced:
    *   50% negative reviews
    *   50% positive reviews

*   Download CSV from Kaggle: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

Example of the dataset:

| Review                                                        | Sentiment |
| :------------------------------------------------------------ | :-------- |
| One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The... | Positive  |
| A wonderful little production. <br/>  <br/>The filming technique is very unassuming. Very old-time-B... | Positive  |
| I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...     | Positive  |

`Review` and `Sentiment` are unique values.

<----------section---------->

**Exercise**

The exercise is to build a classifier for movie reviews:

*   Given a review, the classifier will determine its polarity (positive or negative).
*   Train the classifier on the IMDB Dataset.
*   Use 80% for training and 20% for testing.
*   Show and plot metrics and the confusion matrix.

<----------section---------->

**Suggestions**

*   One-hot encode the labels: (1,0) = negative, (0, 1) = positive using the ScikitLearn `OneHotEncoder` class.
*   To reduce the TF-IDF matrix, consider only words that appear at least in 5 documents.
*   Use the ScikitLearn `confusion_matrix` function to build the confusion matrix.
*   You can use the Seaborn `heatmap` to plot the confusion matrix: `pip install seaborn`.
    *   [https://seaborn.pydata.org/generated/seaborn.heatmap.html](https://seaborn.pydata.org/generated/seaborn.heatmap.html)

<----------section---------->

**Confusion Matrix Example**

[The image of a confusion matrix, with "Predicted Labels" on the x-axis (negative, positive) and count numbers (0 to 5000) on the y-axis, should be displayed here.]
*This is an example of a confusion matrix for a binary classification problem. It shows the number of true positives, true negatives, false positives, and false negatives.*

<----------section---------->

**Example Result**

**Training Loss Validation Accuracy**

[The image of "Training Loss Validation Accuracy", with values on the axis, should be displayed here.]
*This plot shows the training loss and validation accuracy during the training of the sentiment analysis model.*

Example Classification Report:

```
              precision    recall  f1-score   support

    negative       0.90      0.91      0.90      5013
    positive       0.90      0.90      0.90      4987

    accuracy                           0.90     10000
   macro avg       0.90      0.90      0.90     10000
weighted avg       0.90      0.90      0.90     10000
```

<----------section---------->

**Text Classification Applications**

Text classification has a wide range of applications across various domains:

*   Topic Labeling: Categorizing news articles, blog posts, or research papers into different topics.
*   Sentiment Analysis: Determining the emotional tone of customer reviews, social media posts, or survey responses.
*   Spam Filtering: Identifying and filtering out unwanted email messages.
*   Intent Detection: Understanding the user's intention behind a search query or a voice command.
*   Language Detection: Identifying the language of a given text.
*   Content Moderation: Identifying and flagging inappropriate or offensive content on online platforms.
*   Products Categorization: Automatically classifying products into different categories in e-commerce websites.
*   Author Attribution: Identifying the author of a given text based on their writing style.
*   Content Recommendation: Recommending relevant articles, videos, or products to users based on their interests.
*   Ad Click Prediction: Predicting whether a user will click on a given advertisement.
*   Job matching: Matching job seekers with relevant job postings based on their skills and experience.
*   Legal case classification: Categorizing legal documents and cases into different areas of law.

<----------section---------->

**Further Readings**

*   Pandas Docs: [https://pandas.pydata.org/docs/user_guide/](https://pandas.pydata.org/docs/user_guide/)
*   Scikit-Learn Docs: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)
*   Seaborn Docs: [https://seaborn.pydata.org/api.html](https://seaborn.pydata.org/api.html)

<----------section---------->

**Additional Context:**

The text also discusses alternative classifiers, such as Latent Discriminant Analysis (LDA), particularly in scenarios where the vocabulary size is significantly larger than the number of labeled examples. LDA aims to find a line or axis in the vector space that maximizes the separation between classes, projecting data points onto this axis for classification.  The text explains an approximation of LDA by finding the centroids of toxic and non-toxic comments and projecting comments onto the axis connecting them. The text also underscores the importance of splitting data into training and testing sets to avoid overfitting and accurately assess model performance. Finally, the text mentions challenges regarding the generalization of sentiment analysis models across different domains and the importance of addressing potential biases in training data. It emphasizes the value of techniques like oversampling and data augmentation to improve the robustness of language models.

<----------section---------->

# Natural Language Processing and Large Language Models

This lesson, Lesson 5 of the Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering) at the DIEM – University of Salerno, presented by Nicola Capuano and Antonio Greco, focuses on **Word Embeddings**. We will explore their purpose, how they are learned, and how they are used in various NLP tasks.

## Outline

The following topics will be covered:

*   Limitations of TF-IDF
*   Word Embeddings
*   Learning Word Embeddings
*   Word2Vec Alternatives
*   Working with Word Embeddings

`<----------section---------->`

## Limitations of TF-IDF

**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.  However, TF-IDF has inherent limitations.

TF-IDF counts terms according to their exact spelling. This means that TF-IDF treats words with similar meanings but different spellings as completely distinct terms. Consequently, texts with the same meaning can have drastically different TF-IDF vector representations if they employ different words, even if they are synonyms. This inability to capture semantic similarity is a major drawback.

**Examples:**

*   The movie was amazing and exciting.
*   The film was incredible and thrilling.

Although both sentences express a similar sentiment about a movie, TF-IDF would represent them with different vectors because they contain different words.

*   The team conducted a detailed analysis of the data and found significant correlations between variables.
*   The group performed an in-depth examination of the information and discovered notable relationships among factors.

Similarly, these sentences convey the same meaning, but the difference in wording would lead to distinct TF-IDF vectors.

`<----------section---------->`

### Term Normalization

**Term Normalization** techniques, such as stemming and lemmatization, are used to mitigate the limitations of TF-IDF by grouping words with similar spellings under a single token.

*   **Stemming** reduces words to their root form by removing suffixes (e.g., "running" becomes "run").
*   **Lemmatization** converts words to their base or dictionary form (lemma) considering the word's context (e.g., "better" becomes "good").

By applying stemming or lemmatization, we can reduce the dimensionality of the TF-IDF vectors and capture some degree of similarity between related words.

**Disadvantages:**

Despite their usefulness, stemming and lemmatization have limitations:

*   **Failure to group most synonyms:** These techniques primarily focus on morphological variations of words and often fail to group true synonyms that have different spellings (e.g., "good" and "excellent").
*   **Grouping words with similar/same spelling but different meanings:** Stemming and lemmatization may incorrectly group words that have the same spelling but different meanings (homonyms) or words with similar spelling but different meanings.

    *   **Example 1:** "She is leading the project" vs. "The plumber leads the pipe."  Stemming/Lemmatization might group "leading" and "leads" together, even though they have different contextual meanings.
    *   **Example 2:** "The bat flew out of the cave" vs. "He hit the ball with a bat."  Stemming/Lemmatization might group both uses of "bat," despite one referring to an animal and the other to a piece of sports equipment.

`<----------section---------->`

### TF-IDF Applications

TF-IDF is a valuable technique for many NLP applications, especially when semantic understanding is not crucial.

*   **Information Retrieval (Search Engines):**  TF-IDF helps search engines rank documents based on the relevance of search terms.
*   **Information Filtering (Document Recommendation):** TF-IDF can be used to recommend documents to users based on their past behavior or expressed interests.
*   **Text Classification:**  TF-IDF can be used as features for classifying documents into predefined categories.

However, other applications necessitate a deeper understanding of text semantics, where TF-IDF falls short.

*   **Text generation (Chatbot):**  Generating coherent and contextually appropriate text requires understanding the relationships between words and their meanings.
*   **Automatic Translation:**  Accurate translation depends on capturing the semantic meaning of the source text and conveying it in the target language.
*   **Question Answering:**  Providing accurate answers requires understanding the question's intent and retrieving relevant information from a knowledge base.
*   **Paraphrasing and Text Rewriting:** Generating paraphrases and rewriting text requires understanding the meaning of the original text and expressing it in a different way.

`<----------section---------->`

## Bag-of-Words (recap)

Before delving into word embeddings, it's useful to review the **Bag-of-Words (BoW)** model, a foundational concept in NLP.

In the BoW model, each word in the vocabulary is assigned a unique index representing its position in the vocabulary.

*   The 1st word (e.g., apple) has index 0
*   The 2nd word (e.g., banana) has index 1
*   The 3rd word (e.g., king) has index 2
*   ...

Each word is then represented by a **one-hot vector**, which is a vector of all zeros except for a single element that is set to 1, corresponding to the word's index in the vocabulary.

*   apple = (1,0,0,0,…,0)
*   banana = (0,1,0,0,…,0)
*   king = (0,0,1,0,…,0)

`<----------section---------->`

## Bag-of-Words (recap)

A major limitation of the BoW model is that, with this encoding, the distance between any pair of vectors is always the same. This means that the model cannot capture the semantics of words. For example, the vectors for "apple" and "banana" are as distant as the vectors for "apple" and "king," even though "apple" and "banana" are semantically related.

Furthermore, the BoW model is not efficient since it uses sparse vectors. The dimensionality of the vectors is equal to the vocabulary size, which can be very large. This results in high storage requirements and computational costs.

**Note:** The figure shows only three dimensions of a space where dimensions equals the cardinality of the vocabulary.  Imagine extending this to tens of thousands of dimensions or more for a real-world vocabulary.

`<----------section---------->`

## Word Embeddings

**Word Embeddings** are a technique for representing words with vectors (also known as Word Vectors) that address the limitations of BoW and TF-IDF.  These vectors are:

*   **Dense:** Unlike one-hot vectors, word embeddings are dense, meaning that most of their elements are non-zero. This allows them to capture more information in fewer dimensions.
*   **With dimensions much smaller than the vocabulary size:** Word embeddings have a much lower dimensionality than the vocabulary size, typically ranging from 100 to 300 dimensions.  This reduces storage requirements and computational costs.
*   **In a continuous vector space:** Word embeddings are embedded in a continuous vector space, where the position of a word vector reflects the semantic meaning of the word.

**Key feature:** Vectors are generated so that words with similar meanings are close to each other in the vector space. The position in the space represents the semantics of the word. This is a crucial distinction from BoW and TF-IDF, which treat words as independent entities.

Word Embeddings:

*   king and queen are close to each other
*   apple and banana are close to each other
The words of the first group are far from those of to the second group.

**Example:**

*   Apple = (0.25,0.16)
*   Banana = (0.33,0.10)
*   King = (0.29,0.68)
*   Queen = (0.51,0.71)

This example is, of course, a vast simplification. Real-world word embeddings have hundreds of dimensions.

`<----------section---------->`

### Word Embedding: Properties

Word embeddings enable semantic text reasoning based on vector arithmetic. This property allows us to perform operations on word vectors to infer relationships between words and concepts.

**Examples:**

*   Subtracting royal from king we arrive close to man: king – royal ≈ man
*   Subtracting royal from queen we arrive close to woman: queen – royal ≈ woman
*   Subtracting man from king and adding woman we arrive close to queen: king – man + woman ≈ queen

These examples demonstrate the ability of word embeddings to capture semantic relationships between words and perform vector arithmetic to solve analogies.

`<----------section---------->`

### Semantic Queries

Word embeddings allow for searching words or names by interpreting the semantic meaning of a query.  This is particularly useful when dealing with complex queries that involve multiple concepts.

**Examples:**

*   Query: "Famous European woman physicist"
    ```
    wv['famous'] + wv['European'] + wv['woman'] + wv['physicist'] ≈ wv['Marie_Curie'] ≈ wv['Lise_Meitner'] ≈ …
    ```

    By adding the vectors of the query terms, we can identify word vectors that are semantically similar to the query, such as "Marie_Curie" and "Lise_Meitner."
*   Query: “Popular American rock band”
    ```
    wv['popular'] + wv['American'] + wv['rock'] + wv['band'] ≈ wv['Nirvana'] ≈ wv['Pearl Jam'] ≈ …
    ```

    Similarly, this query can identify "Nirvana" and "Pearl Jam" as semantically relevant results.

`<----------section---------->`

### Analogies

Word embeddings enable answering analogy questions by leveraging their semantic relationships. This is one of the most compelling demonstrations of the power of word embeddings.

**Examples:**

*   Who is to physics what Louis Pasteur is to germs?
    ```
    wv['Louis_Pasteur'] – wv['germs'] + wv['physics'] ≈ wv['Marie_Curie']
    ```

    This analogy question can be solved by subtracting the vector for "germs" from the vector for "Louis_Pasteur" and adding the vector for "physics." The resulting vector will be closest to the vector for "Marie_Curie."
*   Marie Curie is to science as who is to music?
    ```
    wv['Marie_Curie'] – wv['science'] + wv['music'] ≈ wv['Ludwig_van_Beethoven']
    ```

    Similarly, this question can be solved by subtracting the vector for "science" from the vector for "Marie_Curie" and adding the vector for "music." The resulting vector will be closest to the vector for "Ludwig_van_Beethoven."
*   Legs is to walk as mouth is to what?
    ```
    wv['legs'] – wv['walk'] + wv['mouth'] ≈ wv['speak'] or wv['eat']
    ```
    This last analogy suggests that "speak" and "eat" have a similar vector relationship as "legs" and "walk".

`<----------section---------->`

## Visualizing Word Embeddings

Word embeddings are high-dimensional vectors, typically with hundreds of dimensions.  Visualizing these vectors directly is impossible. However, we can use dimensionality reduction techniques to project them onto a lower-dimensional space, such as 2D or 3D, for visualization purposes.

**Principal Component Analysis (PCA)** is a common technique for reducing the dimensionality of word embeddings. PCA identifies the principal components of the data, which are the directions of maximum variance. By projecting the word embeddings onto these principal components, we can create a 2D or 3D representation that captures most of the original data's variance.

Google News Word2vec 300-D vectors projected onto a 2D map using PCA demonstrates that semantic proximity approximates geographical proximity.

In a news corpus, cities that are similar in size and culture are clustered close together despite being far apart geographically, such as San Diego and San Jose, or vacation destinations such as Honolulu and Reno.

Fortunately you can use conventional algebra to add the vectors for cities to the vectors for states and state abbreviations. As you discovered in chapter 4, you can use tools such as principal components analysis to reduce the vector dimensions from your 300 dimensions to a human-understandable 2D representation. PCA enables you to see the projection or “shadow” of these 300-D vectors in a 2D plot. Best of all, the PCA algorithm ensures that this projection is the best possible view of your data, keeping the vectors as far apart as possible. PCA is like a good photographer that looks at something from every possible angle before composing the optimal photograph. You don’t even have to normalize the length of the vectors after summing the city + state + abbrev vectors, because PCA takes care of that for you.

We saved these augmented city word vectors in the nlpia package so you can load them to use in your application. In the following code, you use PCA to project them onto a 2D plot.

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
us_300D = get_data('cities_us_wordvectors')
us_2D = pca.fit_transform(us_300D.iloc[:, :300])
```

Figure 6.8 shows the 2D projection of all these 300-D word vectors for US cities:
Figure 6.8 Google News Word2vec 300-D vectors projected onto a 2D map using PCA

Listing 6.11 Bubble chart of US cities
The 2D vectors producted by PCA are for visualization. Retain the original 300-D Word2vec vectors for any vector reasoning you might want to do.
The last column of this DataFrame contains the city name, which is also stored in the DataFrame index.

Memphis, Nashville,
Charlotte, Raleigh, and Atlanta
Houston and Dallas
nearly coincide.
Ft. Worth
El Paso
San Diego
LA, SF, and San Jose
America/Los_…(0.9647851, –0.7217035)
Portland, OR
Honolulu, Reno,
Mesa, Tempe, and Phoenix

Size: population
Position: semantics
Color: time zone
America/Phoenix
America/New_York
America/Anchorage
America/Indiana/Indianapolis
America/Los_Angeles
America/Boise
America/Denver
America/Kentucky/Louisville
America/Chicago
Pacific/Honolulu

It is important to note that the 2D vectors produced by PCA are primarily for visualization purposes. For any vector reasoning or calculations, it is crucial to retain the original high-dimensional word2vec vectors, as the dimensionality reduction process can lead to information loss.

`<----------section---------->`

## Learning Word Embeddings

### Word2Vec

Word embeddings were introduced by Google in 2013 in the following paper:

*   T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space in 1st International Conference on Learning Representations, ICLR 2013

This paper defines **Word2Vec**, which is:

*   A methodology for the generation of word embeddings
*   Based on neural networks
*   Using unsupervised learning on a large unlabeled textual corpus

Word2Vec revolutionized NLP by providing an efficient and effective way to learn word embeddings from large amounts of text data.

`<----------section---------->`

### Word2Vec

The core idea behind Word2Vec is that words with similar meanings are often found in similar contexts. The "context" of a word refers to the surrounding words in a sentence.

*   Context: a sequence of words in a sentence

**Example:**

*   Consider the sentence "Apple juice is delicious."
*   Remove one word.
*   The remaining sentence is "____ juice is delicious."
*   Ask someone to guess the missing word.
*   Terms such as banana, pear or apple would probably be suggested.
*   These terms have similar meanings and used in similar contexts.

This example illustrates how the context of a word can provide clues about its meaning. Word2Vec leverages this principle to learn word embeddings by training neural networks to predict the surrounding words of a given word.

`<----------section---------->`

### Continuous Bag-of-Word (CBOW)

The **Continuous Bag-of-Word (CBOW)** model is one of the two main architectures in Word2Vec.  In CBOW, a neural network is trained to predict the central token of a context of *m* tokens.

*   **Input:** the bag of words composed of the sum of all one-hot vectors of the surrounding tokens.  This "bag" ignores the order of the words, hence the name.

*   **Output:** a probability distribution over the vocabulary with its maximum in the most probable missing word.  The network outputs the probability of each word being the central word, and the training aims to maximize the probability of the actual central word.

*Example:* Claude Monet painted the Grand Canal in Venice in 1806.

Given the surrounding words "Claude," "Monet," "the," and "Grand", the CBOW model tries to predict the center word "painted".

`<----------section---------->`

### Continuous Bag-of-Word (CBOW)

The CBOW neural network architecture consists of:

*   |V| input and output neurons where V is the vocabulary size. The input layer represents the surrounding words, and the output layer represents the probability distribution over the vocabulary for the central word.
*   *n* hidden neurons where *n* is the word embedding dimension. The hidden layer represents the word embeddings themselves.  The weights connecting the input layer to the hidden layer are the word embeddings.

`<----------section---------->`

### SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH

Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

The choice between CBOW and Skip-gram depends on the specific task and dataset.

`<----------section---------->`

### Continuous bag of words vs. bag of words

In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surrounding words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

**Example:** for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

The crucial difference is that CBOW considers the *context* of words (the surrounding words), while the regular BoW model treats each document as an independent bag of words without any regard to word order or proximity.

`<----------section---------->`

### Continuous Bag-of-Word (CBOW)

Ten 5-gram examples from the sentence about Monet:

CONTINUOUS BAG-OF-WORDS APPROACH

In the continuous bag-of-words approach, you’re trying to predict the center word based on the surrounding words. Instead of creating pairs of input and output tokens, you’ll create a multi-hot vector of all surrounding terms as an input vector. The multi-hot input vector is the sum of all one-hot vectors of the surrounding tokens to the center, target token.

Based on the training sets, you can create your multi-hot vectors as inputs and map them to the target word as output. The multi-hot vector is the sum of the one-hot vectors of the surrounding words’ training pairs wt-2 + wt-1 + wt+1 + wt+2 . You then build the training pairs with the multi-hot vector as the input and the target word wt as the output. During the training, the output is derived from the softmax of the output node with the highest probability.

| Input word wt-2 | Input word wt-1 | Input word wt+1 | Input word wt+2 | Expected output wt |
|-----------------|-----------------|-----------------|-----------------|--------------------|
| Monet           | painted         | Claude          | Monet             | Claude             |
| Claude          | painted         | the             | Monet             | painted            |
| Claude          | Monet           | the             | Grand             | painted            |
| Monet           | painted         | Grand           | Canal           | the                |
| painted         | the             | Canal           | of              | Grand              |
| the             | Grand           | of              | Venice            | Canal              |
| Grand           | Canal           | Venice          | in              | of               |
| Canal           | of              | in              | 1908            | Venice             |
| of              | Venice          | 1908            |                  | in               |
| Venice          | in              | 1908            |                  |                  |

target word w t = word to be predicted
surrounding words w t-2, w t-1 = input words
surrounding words w t+1, w t+2

painted the Grand Canal of Venice in 1908.

This table illustrates how the CBOW model constructs training examples by sliding a window across the sentence and using the surrounding words to predict the central word.

`<----------section---------->`

### Continuous Bag-of-Word (CBOW)

After the training is complete, the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words. These weights, connecting the input layer (surrounding words) to the hidden layer, capture the learned relationships between words and their contexts.

Similar words are found in similar contexts …

… their weights to the hidden layer adjust in similar ways

… this result in similar vector representations

Because the weights to the hidden layer are adjusted based on the surrounding words, the weights for similar words will be adjusted in similar ways.  This is what ultimately leads to semantically similar words having similar vector representations.

SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH

Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

Continuous bag of words vs. bag of words

In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the surround-ing words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

Example for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

the highest probability will be converted to 1, and all remaining terms will be set to 0.

This simplifies the loss calculation.

After training of the neural network is completed, you’ll notice that the weights have been trained to represent the semantic meaning. Thanks to the one-hot vector conversion of your tokens, each row in the weight matrix represents each word from the vocabulary for your corpus. After the training, semantically similar words will have similar vectors, because they were trained to predict similar surrounding words. This is purely magical!

After the training is complete and you decide not to train your word model any further, the output layer of the network can be ignored. Only the weights of the inputs to the hidden layer are used as the embeddings. Or in other words: the weight matrix is your word embedding. The dot product between the one-hot vector representing the input term and the weights then represents the word vector embedding.

Retrieving word vectors with linear algebra

The weights of a hidden layer in a neural network are often represented as a matrix: one column per input neuron, one row per output neuron. This allows the weight matrix to be multiplied by the column vector of inputs coming from the previous layer to generate a column vector of outputs going to the next layer . So if you multiply (dot product) a one-hot row vector by the trained weight matrix, you’ll get a vector that is one weight from each neuron (from each matrix column). This also works if you take the weight matrix and multiply it (dot product) by a one-hot column vector for the word you are interested in.

Of course, the one-hot vector dot product just selects that row from your weight matrix that contains the weights for that word, which is your word vector. So you could easily retrieve that row by just selecting it, using the word’s row number or index num-ber from your vocabulary.

WE of Monet

This explanation details how the weights of the input layer capture the semantic meaning and how to extract word vectors using linear algebra.

`<----------section---------->`

### Skip-Gram

The **Skip-Gram** model is the second main architecture in Word2Vec. It's an alternative training method for Word2Vec.

*   A neural network is trained to predict a context of *m* tokens based on the central token. The goal of Skip-Gram is the reverse of CBOW, instead of predicting the center word from the surrounding words, it tries to predict the surrounding words given a center word.
*   **Input:** the one-hot vector of the central token. The model takes a single word as input.
*   **Output:** the one-hot vector of a surrounding word (one training iteration for each surrounding word).  For each word in the surrounding context, there is a separate training iteration. This means for a given input word, the model tries to predict each of the words around it.

output example skip-grams are shown in figure 6.3. The predicted words for these skip-grams are the neighboring words “Claude,” “Monet,” “the,” and “Grand.”

WHAT IS A SKIP-GRAM? Skip-grams are n -grams that contain gaps because you skip over intervening tokens. In this example, you’re predicting “Claude” from the input token “painted,” and you skip over the token “Monet.”

The structure of the neural network used to predict the surrounding words is similar to the networks you learned about in chapter 5. As you can see in figure 6.4, the net-work consists of two layers of weights, where the hidden layer consists of n neurons; n is the number of vector dimensions used to represent a word. Both the input and out-put layers contain M neurons, where M is the number of words in the model’s vocabu-lary. The output layer activation function is a softmax, which is commonly used for classification problems.

WHAT IS SOFTMAX ?
The softmax function is often used as the activation function in the output layer of neural networks when the network’s goal is to learn classification problems. The softmax will squash the output results between 0 and 1, and the sum of all outputs will always add up to 1. That way, the results of an output layer with a softmax function can be considered as probabilities.
 For each of the K output nodes, the softmax output value can be calculated using the normalized exponential function:

```
σ(z)j = exp(z_j) / Σ_{k=1}^{K} exp(z_k)
```

**Example 3D vector:**
v = [0.5, 0.9, 0.2]

word w t = input word

painted the Grand Canal of Venice in 1908.
surrounding words w t-2 , w t-1 = words to be predicted
surrounding words w t+1 , w t+2

`<----------section---------->`

### Skip-Gram

|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

The “squashed” vector after the softmax activation would look like this:

**Example 3D vector after softmax:**
σ(v) = [0.309, 0.461, 0.229]

Notice that the sum of these values (rounded to three significant digits) is approximately 1.0, like a probability distribution.

Figure 6.4 shows the numerical network input and output for the first two surrounding words. In this case, the input word is “Monet,” and the expected output of the network is either “Claude” or “painted,” depending on the training pair.

`<----------section---------->`

### Skip-Gram

Ten 5-gram examples from the sentence about Monet

NOTE When you look at the structure of the neural network for word embedding, you’ll notice that the implementation looks similar to what you discovered in chapter 5.

How does the network learn the vector representations?
To train a Word2vec model, you’re using techniques from chapter 2. For example, in table 6.1, wt represents the one-hot vector for the token at position t. So if you want to train a Word2vec model using a skip-gram window size (radius) of two words, you’re considering the two words before and after each target word. You would then use your 5-gram tokenizer from chapter 2 to turn a sentence like this:

```python
sentence = "Claude Monet painted the Grand Canal of Venice in 1806."
```

into 10 5-grams with the input word at the center, one for each of the 10 words in the original sentence.

The training set consisting of the input word and the surrounding (output) words are now the basis for the training of the neural network. In the case of four surrounding words, you would use four training iterations, where each output word is being pre-dicted based on the input word.

Each of the words are represented as one-hot vectors before they are presented to the network (see chapter 2). The output vector for a neural network doing embedding is similar to a one-hot vector as well. The softmax activation of the output layer nodes (one for each token in the vocabulary) calculates the probability of an output word being found as a surrounding word of the input word. The output vector of word probabilities can then be converted into a one-hot vector where the word with the highest probability will be converted to 1, and all remaining terms will be set to 0.

| Input word wt | Expected output wt-2 | Expected output wt-1 | Expected output wt+1 | Expected output wt+2 |
|---------------|----------------------|----------------------|----------------------|----------------------|
| Claude        |                      |                      | Monet                |                      |
| Monet         |                      | Claude               | painted              |                      |
| painted       | Claude               | Monet                | the                  | Grand                |
| the           | Monet                | painted              | Grand                | Canal                |
| Grand         | painted              | the                  | Canal                | of                   |
| Canal         | the                  | Grand                | of                   | Venice               |
| of            | Grand                | Canal                | Venice               | in                   |
| Venice        | Canal                | of                   | in                   | 1908                 |
| in            | of                   | Venice               | 1908                 |                      |
| 1908          | Venice               | in                   |                      |                      |

This table showcases training data construction by establishing connections between the central input word and its surrounding words.

`<----------section---------->`

### Skip-Gram

After the training is complete, the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words. As with CBOW, the weights connecting the input layer (center word) to the hidden layer become the word embeddings.

Similar words are found in similar contexts …

… their weights to the hidden layer adjust in similar ways

… this result in similar vector representations

The training process ensures that words found in similar contexts have similar weight adjustments and thus, end up with similar vector representations.

`<----------section---------->`

### CBOW vs Skip-Gram

**CBOW**

*   Higher accuracies for frequent words, much faster to train, suitable for large datasets

    CBOW is generally preferred when dealing with large datasets and when the focus is on capturing the meaning of frequent words accurately.
**Skip-Gram**

*   Works well with small corpora and rare terms

    Skip-gram is more effective when the dataset is small, and the task involves understanding the meaning of rare or less frequent words. It's also effective at understanding relationships between words.
**Dimension of Embeddings (n)**

*   Large enough to capture the semantic meaning of tokens for the specific task
*   Not so large that it results in excessive computational expense

    Choosing the right dimensionality involves a trade-off. Higher dimensionality can capture more nuanced semantic information, but also increases computational complexity and the risk of overfitting.

`<----------section---------->`

### Improvements to Word2Vec

Several techniques have been developed to improve the performance of Word2Vec:

**Frequent Bigrams**

*   Some words often occur in combination
*   Elvis is often followed by Presley forming a bigram
*   Predicting Presley after Elvis doesn't add much value
*   To let the network focus on useful predictions frequent bigrams and trigrams are included as terms in the Word2vec vocabulary
*   Inclusion criteria: co-occurrence frequency greater than a threshold

```
score(wi, wj) = (count(wi, wj) - δ) / (count(wi) * count(wj))
```

*   Examples: Elvis\_Presley, New\_York, Chicago\_Bulls, Los\_Angeles\_Lakers, etc.

This technique involves treating frequently occurring pairs of words (bigrams) or triplets of words (trigrams) as single tokens, enabling the model to capture the meaning of these phrases more accurately. For instance, "New York" is more than just "New" and "York" individually; it's a specific place.

`<----------section---------->`

### Improvements to Word2Vec

**Subsampling Frequent Tokens**

*   Common words (like stop-words) often don’t carry significant information
*   Being frequent, they have a big influence on the training process

**To reduce their effect...**

*   During training (skip-gram method), words are sampled in inverse proportion to their frequency
*   **Probability of sampling:**

```
P(wi) = 1 - sqrt(t / f(wi))
```
Where ```f(wi)``` is the frequency of a word across the corpus, and ```t``` represents a frequency threshold above which you want to apply the subsampling probability.

*   The effect is like the IDF effect on TF-IDF vectors

Common words like "the," "a," and "is" occur very frequently but contribute little to the semantic meaning of a sentence. Subsampling reduces their influence on the training process by randomly discarding some occurrences of these words.

`<----------section---------->`

### Improvements to Word2Vec

**Negative Sampling**

*   Each training example causes the network to update all weights
*   With thousands or millions of words in the vocabulary, this makes the process computationally expensive

**Instead of updating all weights...**

*   Select 5 to 20 negative words (words not in the context)
*   Update weights only for the negative words and the target word
*   Negative words are selected based on their frequency
*   Common words are chosen more often than rare words
*   The quality of embeddings in maintained

This technique addresses the computational cost of training Word2Vec. Instead of updating all the weights in the network for each training example, negative sampling only updates the weights for a small number of "negative" words (words that are not in the context of the target word).

`<----------section---------->`

## Word2Vec Alternatives

While Word2Vec was a groundbreaking approach, other methods have emerged that offer improvements in certain aspects:

### GloVe

**Global Vectors for Word Representation**

*   Introduced by researchers from Stanford University in 2014
*   Uses classical optimization methods like Singular Value Decomposition instead of neural networks

**Advantages:**

*   Comparable precision to Word2Vec
*   Significantly faster training times
*   Effective on small corpora

[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)

GloVe is an alternative word embedding technique that leverages global word co-occurrence statistics to learn word vectors. It is based on matrix factorization techniques and often trains faster than Word2Vec.

`<----------section---------->`

### FastText

Introduced by Facebook researchers in 2017
<----------section---------->

# Natural Language Processing and Large Language Models

## Corso di Laurea Magistrale in Ingegneria Informatica Lesson 6: Neural Networks for NLP

This material is from Lesson 6, focusing on Neural Networks for Natural Language Processing (NLP), within the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree Course in Computer Engineering). The lecture is presented by Nicola Capuano and Antonio Greco from the DIEM (Department of Information Engineering and Mathematics) at the University of Salerno.

<----------section---------->

## Outline

This lesson covers the following topics:

*   **Recurrent Neural Networks (RNNs):** The foundational structure for processing sequential data in NLP.
*   **RNN Variants:** Exploration of different types of RNNs, each with specific architectural improvements to address limitations of basic RNNs.
*   **Building a Spam Detector:** A practical application of RNNs to classify text messages as spam or not spam, showcasing the model's utility in text classification.
*   **Introduction to Text Generation:** Overview of how neural networks can be used to generate new text, opening doors to various creative and practical applications.
*   **Building a Poetry Generator:** A hands-on project demonstrating text generation by training a model to create poetry.

<----------section---------->

## Recurrent Neural Networks

### Neural Networks and NLP

Neural networks are extensively employed in text processing due to their ability to learn complex patterns from data. However, traditional feedforward neural networks have an inherent limitation: they lack memory. This means each input is processed independently, without considering the context from previous inputs.

In the context of NLP, this poses a challenge. To process a text using a feedforward network, the entire sequence of words must be presented as a single, fixed-size input. Effectively, the entire text needs to be treated as one large data point. Approaches like Bag of Words (BoW) or TF-IDF (Term Frequency-Inverse Document Frequency) vectors, which transform text into fixed-size numerical representations, address this issue, as does averaging word vectors.

### Neural Networks with Memory

The core idea behind incorporating memory into neural networks for NLP is inspired by how humans read and understand text. When reading, we:

*   Process sentences and words sequentially, one by one.
*   Maintain a "memory" of what we've read previously.
*   Continuously update an internal model as new information arrives.

A Recurrent Neural Network (RNN) is designed to mimic this process. It operates on sequences of information by iterating through the elements of the sequence, such as the words in a text (often represented as word embeddings). Crucially, it maintains an internal "state" that stores information about what it has seen so far, enabling it to consider context.

### Recurrent Neural Networks

*   In visual representations of RNNs, circles typically represent feedforward network layers. These layers can be composed of one or more neurons.
*   The output of the hidden layer, as in a standard feedforward network, is passed to the output layer.
*   **Key Difference:** In addition to the regular output, the hidden layer's output is also fed back into the hidden layer as input in the *next* time step. This recurrent connection allows the network to maintain a state and process sequences.

<----------section---------->

**CHAPTER 8 Loopy (recurrent) neural networks (RNNs)** (Reference to external material)

One-dimensional convolutions offer one approach to address inter-token relationships by examining windows of words together. Pooling layers, discussed in a previous chapter, were designed to handle variations in word order. RNNs provide a different approach, introducing the concept of memory in a neural network. Rather than treating language as a static chunk of data, RNNs process it sequentially, token by token, over time.

### Remembering with Recurrent Networks

Words in a document are rarely independent of each other; their occurrence influences or is influenced by other words in the document. Consider these examples:

"The stolen car sped into the arena."
"The clown car sped into the arena."

The reader's emotional response to these sentences differs greatly because of a single adjective ("stolen" vs. "clown") early in the sentence. Even though the adjective doesn't directly modify "arena" or "sped," it profoundly affects the reader's interpretation of the entire sentence.

The goal is to model this relationship. To understand how "arena" and "sped" can take on slightly different connotations based on an adjective that appears earlier in the sentence, the network needs to "remember" what happened previously.

RNNs address this by enabling neural networks to remember past words within a sentence.

As illustrated in figure 8.3 (not included here), a recurrent neuron in the hidden layer incorporates a "recurrent loop" to recycle the output of the hidden layer at time *t*. This output at time *t* is added to the next input at time *t+1*. The network processes this new input at time step *t+1* to create the hidden layer's output at time *t+1*, which is then recycled back into the input at time step *t+2*, and so on.

Figure 8.3 Recurrent neural net (Note: This is a conceptual reference, not a displayed image)

<sup>1</sup> In finance, dynamics, and feedback control, this type of model is often called an auto-regressive moving average (ARMA) model: [https://en.wikipedia.org/wiki/Autoregressive\_model](https://en.wikipedia.org/wiki/Autoregressive_model). (Additional context and external reference)

Figure 8.4 Unrolled recurrent neural net (Note: This is a conceptual reference, not a displayed image)

**252 CHAPTER 8 Loopy (recurrent) neural networks (RNNs)**

During backpropagation, gradients are calculated to update the network's weights. Even though the unrolled network representation shows multiple copies of the network at different time steps, remember they are all different snapshots of the *same* network with a single set of weights.

Zooming in on the original representation of a recurrent neural network before it's "unrolled" to show the input-weight relationships clarifies the structure. The individual layers of this recurrent network look similar to figures 8.5 and 8.6 (not included).

Figure 8.5 Detailed recurrent neural net at time step t = 0 (Note: This is a conceptual reference, not a displayed image)

Figure 8.6 Detailed recurrent neural net at time step t = 1 (Note: This is a conceptual reference, not a displayed image)

Each neuron in the hidden state has a set of weights applied to each element of the input vector, as in a normal feedforward network. However, there's also an *additional* set of trainable weights applied to the output of the hidden neurons from the *previous* time step. The network can therefore learn how much weight or importance to give to the "past" events as the sequence is input token by token.

**TIP:** The first input in a sequence has no "past," so the hidden state at t=0 receives an input of 0 from its t-1 self. An alternative way of "filling" the initial state value is to first pass related but separate samples into the net one after the other. Each sample’s final output is used for the t=0 input of the next sample. You’ll learn how to preserve more of the information in your dataset using alternative "filling" approaches in the section on statefulness at the end of this chapter.

Consider a set of documents, each a labeled example. Instead of passing the collection of word vectors into a convolutional neural net all at once, as in the previous chapter (figure 8.7, not included), you take the sample one token at a time and pass the tokens individually into your RNN (figure 8.8, not included).

Figure 8.7 Data into convolutional network (Note: This is a conceptual reference, not a displayed image)

In your recurrent neural net, you pass in the word vector for the first token and get the network’s output. Then you pass in the second token, *along with the output from the first token!* You continue this process, passing in the third token along with the output from the second token, and so on. This allows the network to develop a concept of before and after, cause and effect, and a notion of time.

Figure 8.8 Data fed into a recurrent network (Note: This is a conceptual reference, not a displayed image)

Text: The clown car sped into the arena

### RNN Training

The network is now "remembering" something. However, the mechanism by which backpropagation works in this structure needs clarification.

### Backpropagation Through Time (BPTT)

All networks, including recurrent networks, aim to minimize a loss function based on a target variable (label). In RNNs, you don't typically have a separate label for *each* token; instead, you have a single label for the *entire* sample text (document).

That single label is sufficient.

… and that is enough.

Isadora Duncan

**TIP:** The concept of tokens as input to each time step applies equally to any sort of time series data, whether discrete or continuous. Your tokens can be weather station readings, musical notes, or characters in a sentence.

Initially, the error is defined by comparing the network's output at the *last* time step to the label. The network then tries to minimize this error. This differs from earlier approaches where you might deal with the output generated by each "subsample" directly; in RNNs, you feed the output back into the network.

You're only concerned with the *final* output for now. You input each token from the sequence into the network and calculate the loss based on the output from the last time step (token) in the sequence (see figure 8.9, not included).

Figure 8.9 Only last output matters here (Note: This is a conceptual reference, not a displayed image)

### Forward Pass

Figure 8.10 (Note: This is a conceptual reference, not a displayed image)

### Backpropagation

### What are RNNs Good For?

RNNs are versatile and can be applied in several ways:

The previous deep learning architectures are effective for processing short text segments, typically individual sentences. RNNs, however, are designed to overcome this length barrier and process infinitely long sequences of text. They can also *generate* text for as long as desired. This capability opens a new range of applications, such as generative conversational chatbots and text summarizers that synthesize information from various locations within a document.

| Type          | Description                                                                   | Applications                                                                                                                                                                |
| ------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| One to Many   | One input tensor used to generate a sequence of output tensors              | Generate chat messages, answer questions, describe images                                                                                                                   |
| Many to One   | Sequence of input tensors gathered up into a single output tensor             | Classify or tag text according to its language, intent, or other characteristics                                                                                           |
| Many to Many  | A sequence of input tensors used to generate a sequence of output tensors | Translate, tag, or anonymize the tokens within a sequence of tokens, answer questions, participate in a conversation                                                        |

The superpower of RNNs is their ability to process sequences of tokens or vectors. This eliminates the need to truncate or pad input text to fit a fixed-length vector. An RNN can generate text sequences of arbitrary length, stopping only when the code dynamically determines that enough text has been generated.

© Manning Publications Co. To comment go to liveBook Licensed to Nicola Capuano <nicola@capuano.biz> (Copyright notice)

### RNNs for Text Generation

When used for text generation:

*   The output of *each* time step is as important as the final output.
*   Error is captured and backpropagated at each step to adjust all network weights.

**258 CHAPTER 8 Loopy (recurrent) neural networks (RNNs)**

If you care only about the final output, the network will settle (assuming it converges) on the weight for that input to that neuron that best handles the overall task.

**BUT YOU DO CARE WHAT CAME OUT OF THE EARLIER STEPS**

Sometimes, the entire sequence generated by each of the intermediate time steps is important. Chapter 9 explores examples where the output at a given time step *t* is as important as the output at the final time step. Figure 8.11 (not included) shows a method for capturing the error at any given time step and carrying that backward to adjust all the weights of the network during backpropagation.

Figure 8.11 All outputs matter here (Note: This is a conceptual reference, not a displayed image)

This process resembles standard backpropagation through time for *n* time steps. However, you are now backpropagating the error from multiple sources simultaneously. Similar to the first example, the weight corrections are additive. You backpropagate from the last time step all the way to the first, summing up the changes for each weight. Then, you repeat the process with the error calculated at the second-to-last time step, summing up all the changes back to *t* = 0. This is repeated until you get all the way back to time step 0 and then backpropagate it as if it were the only one in the world. Finally, you apply the grand total of the updates to the corresponding hidden layer weights all at once.

As illustrated in figure 8.12 (not included), the error is backpropagated from each output all the way back to t=0 and aggregated before finally applying changes to the weights. This is a critical point. As with a standard feedforward network, you update the weights only after you have calculated the proposed change in the weights for the entire backpropagation step for that input (or set of inputs). In the case of a recurrent neural net, this backpropagation includes the updates all the way back to time *t* = 0.

<----------section---------->

## RNN Variants

### Bidirectional RNN

A Bidirectional RNN consists of two recurrent hidden layers:

*   One processes the input sequence forward (left to right).
*   The other processes the input sequence backward (right to left).
*   The outputs of these two layers are concatenated at each time step.

By processing a sequence in both directions, a bidirectional RNN can capture patterns that might be overlooked by a unidirectional RNN.

*   Example: "they wanted to pet the dog whose fur was brown" - Understanding "they" requires context from both the beginning and the end of the sentence.

Figure 8.13 Bidirectional recurrent neural net (Note: This is a conceptual reference, not a displayed image)

The core idea is to arrange two RNNs side by side, passing the input into one as normal and the same input backward into the other network (see figure 8.13). The outputs of those two RNNs are then concatenated at each time step to the related (same input token) time step in the other network. You combine the output of the final time step in the input and concatenate it with the output generated by the same input token at the first time step of the backward net.

**TIP:** Keras also has a `go_backwards` keyword argument. If this is set to `True`, Keras automatically flips the input sequences and inputs them into the network in reverse order. This is the second half of a bidirectional layer. If you’re not using a bidirectional wrapper, this keyword can be useful, because a recurrent neural network (due to the vanishing gradients problem) is more receptive to data at the end of the sample than at the beginning. If you have padded your samples with `<PAD>` tokens at the end, all the good, juicy stuff is buried deep in the input loop. `go_backwards` can be a quick way around this problem.

With these tools, you’re well on your way to not just predicting and classifying text but also modeling language itself and how it’s used. With that deeper algorithmic understanding, you can generate completely new statements instead of just parroting text your model has seen before!

#### What is this thing?

Ahead of the Dense layer, there's a vector of shape (number of neurons x 1) coming out of the last time step of the Recurrent layer for a given input sequence. This vector is the parallel to the thought vector you got out of the convolutional neural network.

**276 CHAPTER 9 Improving retention with long short-term memory networks**

In LSTMs, the rules that govern the information stored in the state (memory) are trained neural nets themselves—therein lies the magic. They can be trained to learn what to remember, while at the same time the rest of the recurrent net learns to predict the target label! With the introduction of a memory and state, you can begin to learn dependencies that stretch not just one or two tokens away, but across the entirety of each data sample. With those long-term dependencies in hand, you can start to see beyond the words themselves and into something deeper about language.

With LSTMs, patterns that humans take for granted and process on a subconscious level begin to be available to your model. And with those patterns, you can not only more accurately predict sample classifications, but you can start to generate novel text using those patterns. The state of the art in this field is still far from perfect, but the results you’ll see, even in your toy examples, are striking.

So how does this thing work (see figure 9.1, not included)?

Figure 9.1 LSTM network and its memory (Note: This is a conceptual reference, not a displayed image)

The memory state is affected by the input and also affects the layer output just as in a normal recurrent net. But that memory state persists across all the time steps of the time series (your sentence or document). So each input can have an effect on the memory state as well as an effect on the hidden layer output. The magic of the memory state is that it learns what to remember at the same time that it learns to reproduce the output, using standard backpropagation! So what does this look like?

### LSTM (Long Short-Term Memory)

RNNs should theoretically retain information from inputs seen many timesteps earlier, but:

*   They struggle to learn long-term dependencies.
*   *Vanishing Gradient Problem:* As more layers are added, the network becomes increasingly difficult to train due to gradients diminishing to near zero.

Long Short-Term Memory networks are designed to solve this problem:

*   They introduce a *state* that is updated with each training example.
*   The rules to decide *what* information to remember and *what* to forget are trained alongside the network.

Figure 9.2 Unrolled LSTM network and its memory (Note: This is a conceptual reference, not a displayed image)

First, let's unroll a standard recurrent neural net and add the memory unit. Figure 9.2 looks similar to a normal recurrent neural net. However, in addition to the activation output feeding into the next time-step version of the layer, you add a memory state that also passes through time steps of the network. At each time-step iteration, the hidden recurrent unit has access to the memory unit. The addition of this memory unit, and the mechanisms that interact with it, make this quite a bit different from a traditional neural network layer. However, you may like to know that it’s possible to design a set of traditional recurrent neural network layers (a computational graph) that accomplishes all the computations that exist within an LSTM layer. An LSTM layer is just a highly specialized recurrent neural network.

**TIP:** In much of the literature,<sup>6</sup> the “Memory State” block shown in figure 9.2 is referred to as an **LSTM cell** rather than an **LSTM neuron**, because it contains two additional neurons or gates just like a silicon computer memory cell.<sup>7</sup> When an LSTM memory cell is combined with a sigmoid activation function to output a value to the next LSTM cell, this structure, containing multiple interacting elements, is referred to as an **LSTM unit**. Multiple LSTM units are combined to form an **LSTM layer**. The horizontal line running across the unrolled recurrent neuron in figure 9.2 is the signal holding the memory or state. It becomes a vector with a dimension for each LSTM cell as the sequence of tokens is passed into a multi-unit LSTM layer.

<sup>6</sup> A good recent example of LSTM terminology usage is Alex Graves' 2012 Thesis “Supervised Sequence Labelling with Recurrent Neural Networks”: [https://mediatum.ub.tum.de/doc/673554/file.pdf](https://mediatum.ub.tum.de/doc/673554/file.pdf) . (Additional context and external reference)

<sup>7</sup> See the Wikipedia article “Memory cell” ([https://en.wikipedia.org/wiki/Memory\_cell\_(computing)](https://en.wikipedia.org/wiki/Memory_cell_(computing)) ). (Additional context and external reference)

LSTMs allow past information to be reinjected later, thus fighting the vanishing-gradient problem.

### GRU (Gated Recurrent Unit)

The Gated Recurrent Unit (GRU) is another RNN architecture designed to solve the vanishing gradient problem.

#### Main Features:

*   Like LSTM, but with a simpler architecture.
*   The GRU lacks a separate memory state, relying solely on the hidden state to store and transfer information across timesteps.
*   Fewer parameters than LSTM, making it faster to train and more computationally efficient.
*   The performance is comparable to LSTM, particularly in tasks with simpler temporal dependencies.

### Stacked LSTM

Layering (stacking multiple LSTM layers) enhances the model’s ability to capture complex relationships.

Note: The output at each timestep in one layer serves as the input for the corresponding timestep in the next layer.

**309 LSTM**

Those are just two of the RNN/LSTM derivatives out there. Experiments are ever ongoing, and we encourage you to join the fun. The tools are all readily available, so finding the next newest greatest iteration is in the reach of all.

#### Going Deeper

It's convenient to think of the memory unit as encoding a specific representation of noun/verb pairs or sentence-to-sentence verb tense references, but that isn't specifically what's going on. It's just a happy byproduct of the patterns that the network learns, assuming the training went well. Like in any neural network, layering allows the model to form more-complex representations of the patterns in the training data. And you can just as easily stack LSTM layers (see figure 9.13, not included).

Figure 9.13 Stacked LSTM (Note: This is a conceptual reference, not a displayed image)

Stacked layers are much more computationally expensive to train, but stacking them takes only a few seconds in Keras. See the following listing:

```python
from keras.models import Sequential
from keras.layers import LSTM

model = Sequential()
model.add(LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape))
model.add(LSTM(num_neurons_2, return_sequences=True))
```

Each LSTM layer is a cell with its own gates and state vector.

<----------section---------->

## Building a Spam Detector

### The Dataset

Download the dataset from: [https://archive.ics.uci.edu/dataset/228/sms+spam+collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

This dataset contains SMS messages labeled as either "spam" or "ham" (non-spam).

### Read the Dataset

```python
import pandas as pd

df = pd.read_csv("datasets/sms_spam.tsv", delimiter='\t', header=None, names=['label', 'text'])
print(df.head())
```

This code uses the pandas library to read the dataset from a tab-separated values (TSV) file. The data is assigned column names 'label' (for spam/ham) and 'text' (for the SMS message content). The `print(df.head())` statement displays the first few rows of the DataFrame to verify the data loading.

### Tokenize and Generate WEs (Word Embeddings)

```python
import spacy
import numpy as np

nlp = spacy.load('en_core_web_md')  # loads the medium model with 300-dimensional WEs

# Tokenize the text and save the WEs
corpus = []
for sample in df['text']:
    doc = nlp(sample, disable=["tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])  # only tok2vec
    corpus.append([token.vector for token in doc])

# Pad or truncate samples to a fixed length
maxlen = 50
zero_vec = [0] * len(corpus[0][0])
for i in range(len(corpus)):
    if len(corpus[i]) < maxlen:
        corpus[i] += [zero_vec] * (maxlen - len(corpus[i]))  # pad
    else:
        corpus[i] = corpus[i][:maxlen]  # truncate

corpus = np.array(corpus)
print(corpus.shape) # Expected output: (5572, 50, 300)
```

This code performs the following steps:

1.  **Load spaCy model:** Loads the `en_core_web_md` spaCy model, which includes pre-trained word embeddings of dimension 300.
2.  **Tokenization and Word Embedding Generation:** Iterates through each text message in the DataFrame and uses spaCy to tokenize the text. For each token (word), it retrieves its corresponding word vector (embedding) from the spaCy model. The disabled pipeline components ensure efficiency, focusing only on generating token vectors.
3.  **Padding/Truncation:** To handle variable-length text messages, the code pads or truncates each message to a fixed length (`maxlen = 50`). If a message is shorter than `maxlen`, it's padded with zero vectors. If it's longer, it's truncated.
4.  **Convert to NumPy Array:** The list of word embedding sequences is converted into a NumPy array for efficient processing in the neural network.

### Split the dataset

```python
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode the labels
encoder = LabelEncoder()
labels = encoder.fit_transform(df['label'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
# Expected output: ((4457, 50, 300), (1115, 50, 300), (4457,), (1115,))
```

This code:

1.  **Encodes Labels:** Converts the "label" column (containing "spam" and "ham" strings) into numerical labels (0 and 1) using LabelEncoder.
2.  **Splits Data:** Splits the data into training and testing sets using `train_test_split` from scikit-learn. `test_size=0.2` indicates that 20% of the data will be used for testing, and 80% for training.
3.  **Prints Shapes:** The code prints the shapes of the training and testing sets to confirm the split and dimensions of the data.

### Train an RRN (RNN) Model

```python
import keras

model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(keras.layers.SimpleRNN(64))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, batch_size=512, epochs=20, validation_data=(X_test, y_test))
```

This code defines and trains an RNN model using Keras:

1.  **Model Definition:**
    *   A sequential model is created.
    *   Input layer defines the expected input shape using the `Input` layer.
    *   A `SimpleRNN` layer with 64 units is added, representing the recurrent layer.
    *   A `Dropout` layer with a rate of 0.3 is added to prevent overfitting.
    *   A `Dense` output layer with a sigmoid activation function is added to perform binary classification (spam/ham).
2.  **Model Compilation:**
    *   The model is compiled using the `binary_crossentropy` loss function (appropriate for binary classification), the "adam" optimizer, and "accuracy" as the evaluation metric.
3.  **Model Summary:**
    *   `model.summary()` prints a summary of the model architecture, including the number of parameters.
4.  **Model Training:**
    *   `model.fit()` trains the model on the training data.
        *   `batch_size=512` sets the batch size.
        *   `epochs=20` trains the model for 20 epochs.
        *   `validation_data=(X_test, y_test)` uses the test data to evaluate the model's performance after each epoch.
    *   The training history (loss and accuracy) is stored in the `history` variable.

### Plot the Training History

```python
from matplotlib import pyplot as plt

def plot(history, metrics):
    fig, axes = plt.subplots(1, len(metrics), figsize=(15, 5))
    for i, metric in enumerate(metrics):
        ax = axes[i]
        ax.plot(history.history[metric], label='Train') # Corrected line
        ax.plot(history.history['val_' + metric], label='Validation') # Corrected line
        ax.set_title(f'Model {metric.capitalize()}')
        ax.set_ylabel(metric.capitalize())
        ax.set_xlabel('Epoch')
        ax.legend(loc='upper left')
        ax.grid()
    plt.tight_layout()
    plt.show()

plot(history, ['loss', 'accuracy'])
```

This code defines a `plot` function that visualizes the training history:

1.  **Create Subplots:** It creates subplots to display multiple metrics (loss and accuracy) side by side.
2.  **Plot Metrics:** For each metric, it plots the training and validation data over epochs.
3.  **Set Labels and Title:** The plots are labeled with titles, axis labels, and legends for clarity.
4.  **Add Grid:** Gridlines are added for better readability.
5.  **Display Plot:** The plot is displayed using `plt.show()`.

### Report and Confusion Matrix

```python
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def print_report(model, X_test, y_test, encoder):
    y_pred = model.predict(X_test).ravel()
    y_pred_class = (y_pred > 0.5).astype(int)  # convert probabilities to classes

    y_pred_lab = encoder.inverse_transform(y_pred_class)
    y_test_lab = encoder.inverse_transform(y_test)
    print(classification_report(y_test_lab, y_pred_lab, zero_division=0))

    cm = confusion_matrix(y_test_lab, y_pred_lab)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=encoder.classes_, yticklabels=encoder.classes_)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

print_report(model, X_test, y_test, encoder)
```

This code defines a function `print_report` to evaluate and visualize the model's performance:

1.  **Predictions:** `model.predict(X_test)` generates probability predictions for the test data.
2.  **Probability to Class Conversion:** The predicted probabilities are converted to class labels (0 or 1) based on a threshold of 0.5.
3.  **Inverse Transform:**  The numerical labels are transformed back to their original string values using the fitted `LabelEncoder`.
4.  **Classification Report:** A classification report (precision, recall, F1-score, support) is generated using `classification_report` from scikit-learn. The `zero_division=0` ensures that the report handles cases where a class has no predicted samples.
5.  **Confusion Matrix:** A confusion matrix is generated using `confusion_matrix` and visualized as a heatmap using Seaborn. The heatmap provides a clear representation of the model's performance on each class.

### Using RNN Variants

The code illustrates how to replace the basic `SimpleRNN` layer with other RNN variants in the model definition:

*   Bi-directional RRN: `model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(64)))`
*   LSTM: `model.add(keras.layers.LSTM(64))`
*   Bi-directional LSTM: `model.add(keras.layers.Bidirectional(keras.layers.LSTM(64)))`
*   GRU: `model.add(keras.layers.GRU(64))`
*   Bi-directional GRU: `model.add(keras.layers.Bidirectional(keras.layers.GRU(64)))`

These lines demonstrate how to easily switch between different RNN architectures by simply replacing the layer definition in the Keras model.

### Using Ragged Tensors

A Ragged Tensor is a tensor that allows rows to have variable lengths. This is particularly useful for NLP where sentences have different numbers of words.

*   Useful for handling data like text sequences, where each input can have a different number of elements (e.g., sentences with varying numbers of words).
*   Avoids the need for padding/truncating sequences to a fixed length.
*   Reduces overhead and improves computational efficiency by directly handling variable-length data.
*   Available in TensorFlow since version 2.0.
*   In PyTorch, similar functionality is provided by Packed Sequences.

```python
import tensorflow as tf

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)

# Convert sequences into RaggedTensors to handle variable-length inputs
X_train_ragged = tf.ragged.constant(X_train)
X_test_ragged = tf.ragged.constant(X_test)

# Build the model
model = keras.models.Sequential()
model.add(keras.layers.Input(shape=(None, 300), ragged=True))
model.add(keras.layers.SimpleRNN(64))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train the model using RaggedTensors
history = model.fit(X_train_ragged, y_train, batch_size=512, epochs=20,
                    validation_data=(X_test_ragged, y_test))
plot(history, ['loss', 'accuracy'])
print_report(model, X_test_ragged, y_test, encoder)
```

This code demonstrates how to use Ragged Tensors in TensorFlow to avoid padding sequences to a fixed length:

1.  **Convert to Ragged Tensors:**  The padded NumPy arrays (`X_train`, `X_test`) are converted into RaggedTensors using `tf.ragged.constant()`.
2.  **Specify `ragged=True` in Input Layer:** The `ragged=True` argument is added to the `Input` layer to indicate that the model will receive RaggedTensors.
3.  **Train with Ragged Tensors:** The model is trained directly with the RaggedTensors.

<----------section---------->

## Intro to Text Generation

### Generative Models

Generative models are a class of NLP models designed to generate *new* text.

*   This generated text is intended to be coherent and syntactically correct.
*   Generative models learn patterns and structures from training text corpora to achieve this.

#### Generative vs. Discriminative Models

*   Discriminative models are primarily used for classification or prediction of data categories (e.g., spam detection).
*   Generative models, conversely, can produce new and original data (e.g., writing a poem, translating a sentence
<----------section---------->

**Natural Language Processing and Large Language Models**
This material originates from the Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering) at the University of Salerno. Specifically, it's from Lesson 7, focusing on Dialog Engines. The content was prepared by Nicola Capuano and Antonio Greco from DIEM (Department of Information and Electrical Engineering and Applied Mathematics).

**Outline**

The topics covered in this lesson include:

*   Task-Oriented Dialogue Systems: An exploration of AI systems designed to help users achieve specific goals through conversation.
*   Introduction to Rasa: An overview of the Rasa framework, a popular open-source tool for building conversational AI applications.
*   Building a Chatbot with Rasa: A practical guide to creating a chatbot using the Rasa framework.
*   Custom Actions: Details on how to extend the functionality of a Rasa chatbot using custom Python code.

<----------section---------->

**Task-Oriented Dialogue Systems**

This section delves into the specifics of Task-Oriented Dialogue Systems (TOD), setting them apart from other types of Conversational AI.

**Types of Conversational AI**

Conversational AI can be broadly categorized into two main types:

*   **Chit-Chat:** These systems are designed for general conversation and aim to create engaging and natural-sounding interactions.
    *   They do not have a specific goal or task to accomplish.
    *   The primary focus is on generating human-like responses to maintain a smooth conversation flow.
    *   The success of a chit-chat system is often measured by the number of conversational turns it can sustain. The longer the conversation, the better the system is considered.
*   **Task-Oriented Dialogue Systems (TOD):** In contrast, TOD systems are built to assist users in achieving specific goals.
    *   Their primary focus is on accurately understanding user requests, maintaining the dialogue state, and determining the appropriate next action to take.
    *   Efficiency is key, and the systems aim to accomplish the user's goal in as few conversational turns as possible.
    *   Examples range from booking flights to scheduling meetings or providing recommendations.

<----------section---------->

**Task-Oriented Dialogue Examples:**

These illustrate the kinds of interactions a TOD system is designed to handle:

*   "I have a question." (Initiates a question-answering interaction)
    *   "Which room is the dialogue tutorial in?" (Specific question about location)
    *   "When is the IJCNLP 2017 conference?" (Specific question about time)
*   "I need to get this done." (Signals a task-oriented request)
    *   "Book me the flight from Seattle to Taipei." (Request to book a flight)
    *   "Schedule a meeting with Bill at 10:00 tomorrow." (Request to schedule a meeting)
*   "I need a recommendation." (Requests a recommendation)
    *   "Can you suggest me a restaurant?" (Request for restaurant suggestions)
    *   "Can you suggest me something to see near me?" (Request for nearby attractions)

<----------section---------->

**TOD System Architecture**

TOD systems typically consist of three main modules, each responsible for a distinct aspect of the conversation:

*   **Input Modules ("Ears"):** These modules are responsible for receiving and interpreting user input.
    *   **NLU (Natural Language Understanding):** Processes natural language text to extract intents and entities.
    *   **GUI elements:** Allow users to interact via graphical interfaces (e.g., buttons, forms).
    *   **Connector context, personal info:** Gathers contextual information from connected services or user profiles.
*   **Dialogue Management ("Brain"):** The core of the system, responsible for managing the conversation flow.
    *   **Conversational Modules:** Control the dialogue state, determine the next action, and manage context.
*   **Output Modules ("Mouth"):** Generate responses and interact with external systems.
    *   **NLG (Natural Language Generation):** Converts structured data into natural language responses.
    *   **GUI elements:** Display information to the user through a graphical interface.
    *   **Backend, Connector to any database, conversational API, etc.:** Connects the system to external databases, APIs, and other services to fulfill user requests.

    Rasa is identified as a prominent framework for building TOD systems. Further information is available at: [https://rasa.com/](https://rasa.com/)

<----------section---------->

**Natural Language Understanding**

The NLU module plays a critical role in TOD systems, performing two main tasks:

*   **Intent Classification:** Determining the user's intention behind their message. This is treated as a multi-label sentence classification problem, where a sentence can belong to one or more intent categories.
    *   Example:

    ```text
    What's the weather like tomorrow?
    {"intent": "request_weather"}
    ```

    In this case, the system identifies that the user's intent is to request weather information.
*   **Entity Recognition:** Identifying and extracting relevant pieces of information from the user's message. This is often approached using Named Entity Recognition (NER) techniques, which can be rule-based or machine learning-based.
    *   Example:

    ```text
    What's the weather like tomorrow?
    {"date": "tomorrow"}
    ```

    Here, the system extracts "tomorrow" as a date entity.

<----------section---------->

**Conversation Design**

Proper conversation design is crucial for creating effective and user-friendly chatbots. This involves:

*   Asking who your users are: Tailoring the chatbot's language and interaction style to the target audience.
*   Understanding the assistant’s purpose: Clearly defining what the chatbot is intended to do and the tasks it should handle.
*   Documenting the most typical conversations users will have with the assistant: Identifying common user scenarios and designing dialogue flows to address them.

It’s recognized that anticipating all possible user queries is challenging.

*   Relying on hypothetical conversations is helpful in the initial stages of development.
*   Training the assistant with real conversations as early as possible is essential for improving its performance and adaptability.

<----------section---------->

**Introduction to Rasa**

This segment introduces Rasa, a framework frequently used for building TOD systems.

**Rasa Intro**

*   Rasa is an Open-Source Conversational Framework, meaning its source code is publicly available and can be modified and distributed.
    *   It was launched in 2016.
    *   It is used globally for creating numerous chatbots in various languages.

<----------section---------->

**Rasa Basic Units**

Understanding the core components in Rasa is essential for building chatbots. These units are building blocks for conversations.

*   **Intents:** Represent what the user *wants* to achieve. An intent categorizes the purpose behind a user's message.
*   **Entities:** Represent terms or objects that are relevant or necessary for the intent. Entities provide the specific details needed to fulfill the user's intent.

    ```text
    Utterance: "Show me yesterday's financial news"
    Intent: showNews
    Entity: yesterday (time)
    Entity: financial news (news_type)
    ```

    In this example:
    *   The user's intention is to see news (`showNews` intent).
    *   The relevant details are the time frame (`yesterday`) and the type of news (`financial news`), which are the entities.

<----------section---------->

**Rasa Intro (continued)**

*   **Actions:** Define what the bot should *do* in response to the user's intents. Actions can range from simple responses to complex operations.
*   **Responses:** Predefined utterances, or canned responses, are the simplest form of action. These are the phrases the bot will say back to the user.
*   **Complex Actions:** These are custom Python code snippets that allow the bot to interact with external systems (e.g., databases, Web APIs). This is where more advanced logic and data retrieval are handled.
*   **Slots:** Variables used to store information extracted from user inputs during a conversation. Slots enable the bot to "remember" details and use them later in the conversation.
*   **Forms:** A structured way to collect multiple pieces of information from the user. Forms are sets of slots that need to be filled to complete a specific task (e.g., booking a flight requires origin, destination, and date).
*   **Stories:** Sequences of user intents and bot actions that pre-program dialog scenarios. Stories are used to train the bot's dialogue management model.

<----------section---------->

**Rasa Intro - Sample Story**

This demonstrates how a story is defined in Rasa, linking user input to bot actions:

```text
## explain nlu story name
* greet  // User expresses a greeting
  - utter_greet  // Bot responds with a greeting utterance
* explain_rasa_nlu  // User asks for an explanation of Rasa NLU
  - utter_explain_rasa_nlu  // Bot provides the explanation
```

*   The `*` indicates a user intent.
*   The `-` indicates a bot action.
*   This Story is named "explain nlu story name".

<----------section---------->

**Installing Rasa**

This section outlines the steps to install Rasa:

Create and activate a new virtual environment:

*   `python -m venv rasa.env` (Creates a virtual environment named `rasa.env`)
*   `source rasa.env/bin/activate` (Activates the virtual environment)

Install Rasa:

*   `pip install rasa` (Installs the Rasa package within the active virtual environment)

<----------section---------->

**Rasa Project**

*   In a Rasa project, most information is stored in YAML files (YAML Ain't Markup Language).
*   Python code is required for programming complex actions that go beyond simple responses.

<----------section---------->

**Create a New Project**

The command `rasa init` is used to create a new Rasa project, setting up the basic directory structure and files.

<----------section---------->

**Directory Structure**

This outlines the directory structure of a Rasa project, explaining the purpose of each directory and file:

*   `actions/`: Contains Python code for custom actions, allowing the bot to perform complex tasks and interact with external systems.
*   `data/nlu.yml`: Defines intents and entities, which are used to train the NLU model.
*   `data/rules.yml`: Defines short conversation paths that should always be followed. Rules provide deterministic behavior for specific scenarios.
*   `data/stories.yml`: Defines general stories, which are used to train the dialogue management model. Stories represent possible conversation flows.
*   `models/`: Stores the trained models, including the NLU model and the dialogue management model.
*   `tests/`: Contains bot test cases to ensure the bot is functioning correctly.
*   `config.yml`: Defines pipelines, policies, and components used to train the models. It's the main configuration file for the Rasa project.
*   `credentials.yml`: Stores credentials for external platforms (e.g., Facebook Messenger, Slack).
*   `domain.yml`: Serves as the main file, listing all intents, entities, slots, responses, forms, and actions. It defines the bot's knowledge and capabilities.
*   `endpoints.yml`: Lists the endpoints that the bot can use to connect to external services (e.g., action server).

<----------section---------->

**domain.yml - Session Configuration**

```yaml
session_config:
  session_expiration_time: 60  # minutes
  carry_over_slots_to_new_session: true
```

*   `session_expiration_time`: Specifies the duration (in minutes) after which a session is considered expired.
*   `carry_over_slots_to_new_session`: Determines whether data from the previous session should be transferred to a new session if the user starts a new interaction after the session expires. Setting this to `true` provides a more seamless user experience.

<----------section---------->

**nlu.yml**

This file contains the NLU training data.

*(See example content in later sections)*

*Note:* To effectively train the model to recognize intents, RASA needs at least 7-10 example utterances per intent. This helps the model learn the different ways users might express the same intent.

<----------section---------->

**stories.yml**

This file contains the stories used for training the dialogue management model.

*(See example content in later sections)*

<----------section---------->

**rules.yml**

This file defines the rules that dictate the bot's behavior in certain situations.

*(See example content in later sections)*

<----------section---------->

**Visualize Stories**

The command `rasa visualize` generates a visual representation of the stories defined in `stories.yml`, aiding in understanding and debugging the conversation flows.

<----------section---------->

**Other Commands**

These are essential Rasa CLI commands:

*   `rasa train`: Trains a model using your NLU data and stories, saving the trained model in the `./models` directory.
*   `rasa shell`: Loads your trained model and allows you to interact with your assistant on the command line for testing and debugging.
*   `rasa run`: Starts a server with your trained model, allowing external systems to connect to your assistant. For cross-origin calls (accessing the server from a different domain), use: `rasa run --cors "*"`
*   `rasa --h`: Displays a help message with a list of available commands and options.

<----------section---------->

**Rasa REST API**

Rasa provides a REST endpoint, enabling integration with external systems.

*   You can send messages to the bot via POST requests and receive responses in JSON format.
*   Add the REST channel to your `credentials.yml`:

    ```yaml
    # you don't need to provide anything here - this channel doesn't
    # require any credentials
    ```

*   After restarting your Rasa server, you can reach the bot at: `http://<host>:<port>/webhooks/rest/webhook`
*   Documentation: [https://rasa.com/docs/rasa/connectors/your-own-website/](https://rasa.com/docs/rasa/connectors/your-own-website/)

<----------section---------->

**Request and Response Format**

The REST API uses JSON for both requests and responses.

*   **Request Format:**

    ```json
    {
      "sender": "test_user",  // sender ID (unique identifier for the user)
      "message": "I'm sad!" // user's message
    }
    ```

*   **Response Format:**

    ```json
    [
      {
        "recipient_id": "test_user",
        "text": "Here is something to cheer you up:"
      },
      {
        "recipient_id": "test_user",
        "image": "https://i.imgur.com/nGF1K8f.jpg"
      },
      {
        "recipient_id": "test_user",
        "text": "Did that help you?"
      }
    ]
    ```

    The response is a list of bot responses, each with a `recipient_id` and content (text, image, etc.).

<----------section---------->

**Web-based Frontends**

You can integrate a Rasa bot into your website to enable user interaction:

*   **Custom Implementation:** Building a frontend using HTML/CSS/JavaScript provides full control over the user interface.
*   **Use a pre-built solution:** Using a pre-built widget or component simplifies the integration process.
    *   Rasa Widget (React-based):
        *   Clone from: [https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)
        *   Copy the `./dist` files to your web project to use the widget. This React-based widget provides a ready-to-use chat interface.

<----------section---------->

**More on Connectors**

*   You can enable authentication to secure the communication channel.
*   You can use web-socket for real-time interaction, providing a persistent connection between the client and the server.
*   Rasa provides built-in connectors for various messaging platforms:
    *   Facebook Messenger
    *   Slack
    *   Telegram
    *   Twilio
    *   Microsoft Bot Framework
        [https://rasa.com/docs/rasa/messaging-and-voice-channels/](https://rasa.com/docs/rasa/messaging-and-voice-channels/)
    *   Cisco Webex Teams
    *   RocketChat
    *   Mattermost
    *   Google Hangouts Chat

<----------section---------->

**Building a Chatbot with RASA**

This section describes how to construct a chatbot using the Rasa framework.

**Domain File**

The `domain.yml` file defines everything your assistant knows, essentially acting as its knowledge base:

*   **Responses:** The things the assistant can say to users. This includes text, images, buttons, and other content.
*   **Intents:** Categories of things users say. Intents classify the user's purpose in a message.
*   **Entities:** Pieces of information extracted from incoming text. Entities provide the specifics for fulfilling intents.
*   **Slots:** Variables remembered over the course of a conversation. Slots allow the assistant to maintain context and personalize interactions.
*   **Actions:** Add application logic and extend what your assistant can do. Actions can range from simple responses to complex API calls.

<----------section---------->

**Domain File - Basic Responses**

```yaml
responses:
  utter_greet:
    - text: "Hey there!"
  utter_goodbye:
    - text: "Goodbye :("
  utter_default:
    - text: "Sorry, I didn't get that, can you rephrase?"
  utter_youarewelcome:
    - text: "You're very welcome."
  utter_iamabot:
    - text: "I am a bot, powered by Rasa."
```

This example shows how to define simple text responses in the `domain.yml` file. Each response is given a unique name (e.g., `utter_greet`) and associated with a text message.

<----------section---------->

**Domain File - Multiple Responses**

```yaml
responses:
  utter_greet:
    - text: "Hey, {name}. How are you?"
    - text: "Hey, {name}. How is your day going?"
```

This example demonstrates how to include variables in responses using slots. `{name}` will be replaced with the value of the `name` slot (or "None" if the slot is not filled).

<----------section---------->

**Domain File - Responses: Buttons and Images**

```yaml
responses:
  utter_greet:
    - text: "Hey! How are you?"
      buttons:
        - title: "great"
          payload: "/mood_great"
        - title: "super sad"
          payload: "/mood_sad"
  utter_cheer_up:
    - text: "Here is something to cheer you up:"
      image: "https://i.imgur.com/nGF1K8f.jpg"
```

This shows how to include interactive elements like buttons and images in responses.

*   Buttons allow users to quickly select options. The `payload` defines the intent triggered when a button is pressed.
*   The `image` attribute displays an image in the response.

<----------section---------->

**Domain File - List of Intents**

```yaml
intents:
  - greet
  - goodbye
  - affirm
  - deny
  - thankyou
  - mood_great
  - mood_unhappy
  - bot_challenge
  - search_concerts
  - search_venues
  - compare_reviews
  - how_to_get_started
```

This section defines the list of intents that the assistant can recognize.

*   Each intent corresponds to intents defined in the NLU file (`nlu.yml`).
*   *Tip:* Start with the fewest intents possible and add more as needed based on user interactions.

<----------section---------->

**Domain File - List of Entities**

```yaml
entities:
  - PERSON
  - time
  - membership_type
  - priority
```

This section lists the entities that the assistant can extract from user messages.

*   Entities can be numbers, dates, names, or any other relevant information.
*   Standard entities can be extracted using pre-built models (e.g., using SpaCy). Specific modules must be included in the config file.
*   Custom entities can be extracted with regular expressions, lookup tables, or machine learning. The NLU file will specify how.

<----------section---------->

**NLU File**

The `nlu.yml` file trains the system to extract structured information from user messages, including intents and entities.

**Training data** consists of example user utterances categorized by intent.

**Extra information** can be included to improve entity recognition:

*   Regular Expressions: Patterns to capture specific types of entities (e.g., phone numbers, email addresses).
*   Lookup Tables: Comprehensive lists of possible values for entities (e.g., city names, product categories).
*   Synonyms: Define synonyms for common terms to ensure that variations in user input are understood correctly (e.g., "flight," "flight ticket," "plane ticket").

<----------section---------->

**NLU File - Sample Lists of Utterances**

```yaml
nlu:
  - intent: book_flight
    examples: |
      - I want to book a flight to [New York](city)
      - Book a flight from [Los Angeles](city) to [Chicago](city)
      - Can you help me book a flight to [San Francisco](city)?
      - I need a flight ticket to [Boston](city)
      - I'd like to fly from [Houston](city) to [Atlanta](city)

  - intent: check_flight_status
    examples: |
      - What is the status of flight [AA123](flight_number)?
      - Can you tell me if flight [UA456](flight_number) is delayed?
      - I want to check the status of flight number [DL789](flight_number)
      - Is flight [AA123](flight_number) on time?
```

This demonstrates how to define intents and provide example utterances in the `nlu.yml` file.

*   The `intent` field specifies the intent being defined.
*   The `examples` field contains a list of example utterances.
*   Entities are marked within the utterances using square brackets and parentheses: `[entity value](entity name)`.

<----------section---------->

**NLU File - Sample Extra Information**

```yaml
- lookup_table: city
  examples: |
    - New York
    - Los Angeles
    - Chicago
    - San Francisco
    - Boston
    - Houston
    - Atlanta

- synonym: flight
  examples: |
    - flight
    - flight ticket
    - plane ticket
    - air ticket

- regex:
  name: flight_number
  pattern: "\\b[A-Z0-9]{2,5}\\b" # Regex for flight numbers
```

This showcases how to include extra information in the `nlu.yml` file.

*   `lookup_table`: Defines a lookup table for the `city` entity, listing possible city names.
*   `synonym`: Defines synonyms for the word "flight."
*   `regex`: Defines a regular expression for the `flight_number` entity.

Without lookup tables and regex, custom entities are simply recognized based on machine learning, relying solely on the training data provided.

<----------section---------->

**NLU File - Entity Roles**

Entity Roles allow you to add more details to your entities, providing further context and disambiguation.

Example:

"I am looking for a flight from New York to Boston."

```yaml
- example: |
    I am looking for a flight from [New York]{"entity":"location", "role":"origin"} to [Boston] {"entity":"location", "role":"destination"}.
```

In this example, both "New York" and "Boston" are identified as `location` entities, but they are further distinguished by their roles: "origin" and "destination," respectively.

<----------section---------->

**NLU File - Good Practices**

*   Start with the smallest possible number of intents, focusing on core functionality.
*   Recognize that most users want to do the same things, so focus on the most common use cases.
*   Additional intents will emerge from user data as the bot interacts with real users.
*   Don’t use intents to store information; use entities instead. Intents should represent actions or goals, while entities should represent the details needed to fulfill those goals.

<----------section---------->

**Stories File**

Stories are training data that teach your assistant what it should do next in a conversation. They represent possible conversation flows.

Stories File - example

```yaml
stories:
  - story: happy path
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_great
      - action: utter_happy

  - story: sad path 1
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_unhappy
      - action: utter_cheer_up
      - action: utter_did_that_help
      - intent: affirm
      - action: utter_happy

  - story: sad path 2
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_unhappy
      - action: utter_cheer_up
      - action: utter_did_that_help
      - intent: deny
      - action: utter_goodbye
```

This example shows a few stories, representing different conversation flows.

*   Each `story` has a name (e.g., "happy path," "sad path 1").
*   Each `step` consists of a user `intent` and a bot `action`.

Example of a bad next action problem:

Hi, I need a hospital A

Next best action:

*   Sorry, I didn't quite understand. Can you rephrase?

utter\_ask\_name I am ill and would like to

utter\_greet\_user find the nearest hospital to

get a checkup.

*   Sure, let me do a search. Where are you based?

If the confidence is too low...

<----------section---------->

**Stories File - Good Practices**

*   Start with common flows (happy paths), representing the most straightforward and successful interactions.
*   Add common errors/digressions to handle unexpected user input or deviations from the happy path.
*   Use interactive learning to improve stories, allowing you to train the bot by conversing with it and correcting its behavior.
*   Once your model is in production, add more data from real user conversations to continuously improve its performance and adaptability.

<----------section---------->

**Stories File - OR Statements & Checkpoints**

```yaml
stories:
  - story: newsletter signup with OR
    steps:
      - intent: signup_newsletter
      - action: utter_ask_confirm_signup
      - or:
        - intent: affirm
        - intent: thanks
      - action: action_signup_newsletter

  - story: beginning of conversation
    steps:
      - intent: greet
      - action: utter_greet
      - checkpoint: ask_feedback

  - story: end conversation
    steps:
      - intent: goodbye
      - action: utter_goodbye
```

*   **OR Statements:** Allow for the same action to be taken in response to different intents, simplifying story creation.
*   **Checkpoints:** Link to other stories, creating modular and reusable conversation flows.

<----------section---------->

**Rules File**

Rules define short pieces of conversations that always follow the same path, providing deterministic behavior.

*   Not suitable for multi-turn interactions that require complex dialogue management.
*   Not used to train ML algorithms; rules are applied directly as defined.

```yaml
rules:
  - rule: Say goodbye anytime the user says goodbye
    steps:
      - intent: goodbye
      - action: utter_goodbye

  - rule: Say 'I am a bot' anytime the user challenges
    steps:
      - intent: bot_challenge
      - action: utter_iamabot
```

This shows how to define simple rules in the `rules.yml` file. Each rule specifies a condition (intent) and an action to take when that condition is met.

<----------section---------->

**Slots**

Slots are your assistant's memory, allowing it to store and recall important details during a conversation.

*   Enable your assistant to store important details and later use them in a specific context, creating more personalized and relevant interactions.
*   Can be configured to influence the flow of the conversation, guiding the dialogue based on the information stored in slots.

Examples of how slots influence conversation:

"I would like to book a flight to Sydney."
* Booking a ticket to Sydney! (The system remembers the destination)

"I would like to book a flight to New York."
* Sure! Looking for the options. (The system remembers the destination)

"I would like to book a flight ticket."
* What is your destination? (The system needs to ask for the missing information)

<----------section---------->

**Slots and Entities**

Slots are defined in the `domain.yml` file and are often connected to entities, allowing the assistant to automatically fill slots with extracted entity values.

```yaml
entities:
  - destination

slots:
  destination:
    type: text
    influence_conversation: true
    mappings:
      - type: from_entity
        entity: destination
```

*   `type` can be text, boolean, categorical, float, list, or any (allowing any data type).
*   `influence_conversation: true` indicates that this slot can influence the conversation flow, affecting which actions are taken.
*   The `mappings` section defines how the slot will be filled. In this case, it's filled with the value of the `destination` entity (if present in the user's message).

<----------section---------->

**Slot Mappings**

Slot mappings define how each slot will be filled in, applied after each user message.

```yaml
entities:
  - entity_name

slots:
  amount_of_money:
    type: any
    mappings:
      - type: from_entity
        entity: number
        intent: make_transaction
        not_intent: check_transaction
```

*   "Send $200 to Ben." - Intent: `make_transaction` - the slot is set because the intent matches.
*   "Did I receive the $1000 that Alice sent me yesterday?" - Intent: `check_transaction` - the slot is *not* set because the intent does not match.

<----------section---------->

**Slot Mappings - Parameters**

*   `intent`: Only applies the mapping when this specific intent is predicted, providing context-aware slot filling.
*   `not_intent`: Does not apply the mapping when this intent is predicted, preventing unintended slot filling.
*   `role`: Only applies the mapping if the extracted entity has this specific role, further refining slot filling based on entity context.

<----------section---------->

**Use Slots in Responses**

You can create more dynamic and personalized responses by including slots in the responses.

```yaml
slots:
  name:
    type: any

responses:
  utter_greet:
    - text: "Hello {name}! How are you?"
    - text: "Hello there :)"
    - text: "Hi. How can I help you today?"
```

*   If `name` is set, then its value will be used in the response.
*   If `name` is not set, then its value will be `None`, resulting in a less personalized response or a generic message.

<----------section---------->

**Pipeline Configuration**

The `config.yml` file defines the NLU pipeline and the dialogue policies used by Rasa, determining how the system processes user input and generates responses.

*   `language`: Defines the language of the bot (e.g., `en`, `fr`, `it`).
*   `pipeline`: Specifies the steps to process user messages (NLU pipeline) to extract intents and entities.
*   `policies`: Defines how the bot should handle dialogue and predict next actions.

```yaml
language: en

pipeline: null # The default pipeline is used to train your model.

policies: null # The default policies are used to train your model.
```

Using `null` here means Rasa uses its default configuration for each respectively.

<----------section---------->

**NLU Pipeline**

The pipeline defines the sequence of components that process user messages:

*   Tokenizers: Break down the text into tokens (words, subwords), preparing the text for further processing.
*   Featurizers: Convert tokens into numerical features that models can use, representing the text in a format that machine learning models can understand.
*   Classifiers: Determine the user’s intent, categorizing the purpose behind the user's message.
*   Entity Extractors: Identify named entities (e.g., names, dates), extracting relevant information from the text.

```yaml
pipeline:
  - name: WhitespaceTokenizer
  - name: CountVectorsFeaturizer
  - name: DIETClassifier
    epochs: 150
  - name: EntitySynonymMapper
```

<----------section---------->

**NLU Pipeline - Tokenizers**

*   `WhitespaceTokenizer`: Splits text into tokens based on whitespace, a simple and fast approach.
*   `SpacyTokenizer`: Leverages SpaCy’s tokenization, providing more advanced tokenization capabilities based on SpaCy's language models.

<----------section---------->

**NLU Pipeline - Featurizers**

*   `CountVectorsFeaturizer`: Converts text into a bag-of-words representation, counting the occurrences of each word.
*   `ConveRTFeaturizer`: Uses pre-trained ConveRT embeddings specialized for conversational data, capturing semantic relationships between words.
*   `SpacyFeaturizer`: Leverages SpaCy’s pre-trained word embeddings, providing rich semantic information about the tokens.

<----------section---------->

**NLU Pipeline - Classifiers**

*   `DIETClassifier`: A multi-task transformer-based classifier for both intent classification and entity extraction, offering state-of-the-art performance.
*   `SklearnIntentClassifier`: Uses scikit-learn algorithms (e.g., SVM or logistic regression) for intent classification, providing a simpler and more interpretable approach.

<----------section---------->

**NLU Pipeline - Entity Extractors**

*   `RegexEntityExtractor`: Extracts entities using regular expressions for pattern matching, suitable for well-defined entity formats.
*   `SpacyEntityExtractor`: Uses SpaCy’s pre-trained models to extract named entities based on SpaCy’s NER system, providing a general-purpose entity extraction solution.

<----------section---------->

**Training Policies**

Training policies are techniques your assistant uses to decide on how to respond back to the user. Policy priority defines how assistant makes decisions when multiple policies predict the next action with the same accuracy.

```yaml
config.yml
policies:
  - name: MemoizationPolicy
  - name: TEDPolicy
    max_history: 5
    epochs: 100
  - name: RulePolicy
```

<----------section---------->

**Training Policies - Rule Policy**

Assistant makes the decision on how to respond based on rules defined in `rules.yml`, ensuring deterministic behavior for specific scenarios.

```yaml
rules:
  - rule: Chitchat
    steps:
      - intent: chitchat
      - action: utter_chitchat
```

<----------section---------->

**Training Policies - Memoization Policy**

Assistant makes the decision on how to respond matching the real interaction with stories from `stories.yml`, learning from the provided example conversations.

Example:

Hello
Hey! How can I help you?

Can I check the balance of my card?
```yaml
stories:
 - story: check_balance
  steps:
   - intent: inform
   - action: check_balance
   - intent: thanks
   - action: utter_goodbye
```

<----------section---------->

**
<----------section---------->

**Original Text:**
Natural Language Processing and Large Language Models. Corso di Laurea Magistrale in Ingegneria Informatica. Lesson 8: Building a Chatbot. Nicola Capuano and Antonio Greco, DIEM – University of Salerno.

**Exercise: Building a Pizzeria Chatbot**

Develop a chatbot to assist with pizzeria operations. Users can:

*   Request the pizzeria's menu.
*   Order a pizza that is available on the menu (just one pizza, no beverage).
*   Upon order confirmation, the bot will log the date, the user ID, and the kind of ordered pizza (use a custom action).
*   The bot has a web-based GUI.

Hint: Start with a dummy bot.

Example conversation:

User: Can I have the menu?
Bot: What kind of pizza would you like? We have Parenti, Pepperoni, Vegetarian...
User: Pepperoni
Bot: You want a Pepperoni pizza. Is that correct?
User: Yes
Bot: Great! Your Pepperoni pizza is on its way!

**Hints**

Start with a dummy bot:

```bash
mkdir pizzaBot
cd pizzaBot
rasa init --no-prompt
```

Configure and run the REST and the Actions servers:

```bash
rasa run --cors "*"
rasa run actions
```

Use a Web frontend like:

[https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)

**Enhanced Text:**
This material comes from a lesson on Natural Language Processing (NLP) and Large Language Models (LLMs), part of the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree Course in Computer Engineering). This specific lesson, Lesson 8, focuses on building a chatbot. The lesson is presented by Nicola Capuano and Antonio Greco from DIEM (Department of Information Engineering and Mathematics) at the University of Salerno.

The core task presented is an:

**Exercise: Building a Pizzeria Chatbot**

The goal is to create a chatbot capable of assisting with basic pizzeria operations. The intended functionalities include:

*   **Menu Request:**  The chatbot should be able to provide the pizzeria's menu to the user upon request.
*   **Pizza Ordering:**  Users should be able to order a single pizza from the available menu options. Note that the bot is restricted to pizza orders only, without beverages.
*   **Order Logging:** Upon successful order confirmation, the chatbot needs to log specific details, including:
    *   Date of the order
    *   User ID (to identify the customer)
    *   Type of pizza ordered (e.g., Pepperoni, Vegetarian)

    This functionality requires the implementation of a custom action within the chatbot framework.  Custom actions allow developers to extend the bot's capabilities beyond simple pre-defined responses. They can involve database interactions, API calls, or, in this case, logging order details.
*   **Web-Based GUI:** The chatbot must have a web-based Graphical User Interface (GUI), making it accessible through a web browser.

A helpful suggestion for approaching this task is to:

*   **Hint: Start with a dummy bot:** Begin with a basic, non-functional chatbot to establish the fundamental structure and workflow before adding complex features.

To illustrate the interaction, an example conversation is provided:

User: Can I have the menu?
Bot: What kind of pizza would you like? We have Parenti, Pepperoni, Vegetarian...
User: Pepperoni
Bot: You want a Pepperoni pizza. Is that correct?
User: Yes
Bot: Great! Your Pepperoni pizza is on its way!

This example shows the bot's expected behavior in handling a menu request, taking a pizza order, and providing confirmation.

To aid in the development process, several hints and setup instructions are given:

*   **Dummy Bot Setup:**

    ```bash
    mkdir pizzaBot
    cd pizzaBot
    rasa init --no-prompt
    ```

    These commands will create a new directory named "pizzaBot," navigate into it, and initialize a new Rasa project without prompting for any user input.  Rasa is an open-source conversational AI framework used for building chatbots. The `--no-prompt` option bypasses interactive configuration, using default settings.
*   **Server Configuration:**

    ```bash
    rasa run --cors "*"
    rasa run actions
    ```

    These commands are crucial for running the chatbot.  `rasa run --cors "*"` starts the Rasa server, which handles the chatbot's core logic and communication with the user interface. The `--cors "*"` flag enables Cross-Origin Resource Sharing, allowing the web-based GUI to communicate with the Rasa server, regardless of the domain serving the GUI. `rasa run actions` starts the Actions server, which is responsible for executing custom actions, such as logging the order details to a database.
*   **Web Frontend Resource:**

    [https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)

    This link points to a GitHub repository containing a suitable web frontend widget that can be used to create the web-based GUI for the chatbot.  The specific branch "Widget2.0" is referenced.

<----------section---------->
**Additional Context:**
fly. When you need to inspire students and keep them engaged. Teachers do
this naturally, by adjusting what they say and how they say it based on
feedback on how well the student understand what they are saying. And
teachers think about more than just "delivering a message." They must think
up new ideas and approaches, on the fly as students pose interesting new
questions. Inspiring students' curiosity with Socratic questions and being
responsive to their changing needs is a full-time job.
It is virtually impossible to build a rule-based system that captures all the
things that teachers do to help students learn and grow. Students' needs are
too diverse and dynamic. This is why hybrid chatbots that integrate LLMs
have become the preferred way build production chatbots in virtually every
domain. An LLM can confidently and convincingly chat with your users on
virtually any topic. The key is to harness this power smartly, so that it doesn’t
mislead your users, or worse.
12.5 Chatbot frameworks
In each of the previous chapters, you’ve learned a new technique for
processing text to understand what the user is saying. And in this chapter,
you’ve learned four approaches to generating text for a chatbot to use in its
response to the user. You’ve already assembled a few chatbots from these
NLU and NLG algorithms to understand the advantages and disadvantages of
each of these algorithms. Now you have the knowledge you need to use a
chatbot framework
 smartly. A chatbot framework is an application and a
software library that abstracts away some of these detailed decisions you
need to make when building a dialog engine for your chatbot. A framework
gives you a way to specify your chatbot’s behavior in 
domain-specific
language
 that it can later interpret and 
run
 so that your chatbot replies the
way you intended.
Most chatbot frameworks use a declarative programming language to specify
a bot’s behavior and some even give you a graphical user interface to
program your bot. There are no-code chatbot frameworks that abstract the
declarative chatbot programming language with an interactive graphical
representation of the dialog graph or flow diagram that you can modify with
your mouse. These no-code frameworks usually include a dialog engine that
can execute your chatbot without you ever having to see or edit the
underlying data. In the impact world, an open source platform sponsored by
UNICEF, RapidPro,
[
34
]
 served as a core for several chatbot platforms, such as
Weni, Textit and Glific, that are all used for impact purposes. In RapidPro,
you can build your dialogs in a graphical user interface. You can also easily
import and export the content using open standard file formats which is
helpful when you want to translate the content from one natural language to
another for a multilingual chatbot. ManyChat and Landbot are two closed
source no-code chatbot builders that have similar functionality.
But if you’ve read this far, you probably have ideas for more sophisticated
chatbots than what’s possible in a no-code platform. So you will probably
need a chatbot programming language to make your vision a reality. Of
course, you can specify your bot "stack" in Python by directly employing the
skills you learned in this book. But if you want to build a scalable and
maintainable chatbot you’ll need a chatbot framework that uses a chatbot
design language or data structure that you understand. You want a language
that makes sense to you so that you can quickly get the conversation design
you have in your head embedded in a working chatbot. In this section, you
will learn of several different frameworks that can help you make your
chatbot dreams come true.
Using the tools described here, you can build a bot that can serve you (and
maybe a few friends, or even more people if you’re lucky) if deployed on a
server or in a cloud. However, if you want to build a chatbot that servers
hundreds or thousands of users, you need a more robust, scalable system.
Luckily, there are frameworks available that allow you to focus on building
your bot while taking care of the challenges that come with the need to build
a production-grade system. We will now discuss three popular open-source
Python chatbot frameworks for building chatbots with configurable NLP
capabilities: Rasa, LangChain, and qary.
12.5.1 Building an intent-based chatbot with Rasa
Rasa is an open-source conversational framework that started back in 2016
and today is used to create thousands of bots in various languages around the
world. Unlike many commercial frameworks, that create a drag-and-drop
interface to create the dialog trees we discussed in the previous section,
RASA took a radically different approach to organizing multi-step
conversations.
The basic units of a conversation in RASA are a user intent and a bot action -
which can be as simple as a pre-programmed utterance or a complex action
programmed in Python that results in interaction with other systems - such as
saving or retrieving data from a database, or invoking a Web API. By
chaining these building blocks into sequences - called Stories - RASA allows
you to pre-program dialog scenarios in a streamlined way. All this
information is stored in YAML files (YAML stands for Yet Another Markup
Language), each type of components in its own file.
But enough with the theoretical explanation - let’s get your hands dirty and
build your first RASA chatbot. First, let’s decide what dialog we want to
implement - based on our conversation diagram for the math tutor bot, let’s
implement the following short dialog:
USER: Hello
BOT: Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!
BOT: Are you a parent (or guardian) or are you a student?
USER: I'm a parent.
BOT: For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?
USER: I agree
BOT: Thank you for giving permission for your child to chat with Rori.
When your child is ready to start, please give them this phone and have them type "ready".
To create your bot, you will need to install 
rasa
 package (if you’re working
in 
nlpia2
 environment, it is already installed when you install the project).
Then, you can go to the directory you want to create the project in and run in
your command line:
$ rasa init
The installation wizard will guide you through creating a new project and
even offer you to train an initial model. Let it do that, and then you can even
chat with a simple chatbot the wizard initialized for you.
Let’s now dive into the structure of our project and understand how to build a
dialog like you’ve just had. Here is the directory structure you should see in
the project’s folder:
├───.rasa
│   └───cache
│       ├───...
├───actions
│   └───__pycache__
├───data
├───models
└───tests
The directory we are most interested in is the 
data
 directory. It contains the
files that define the data that is used to train the chatbot’s NLU model. First,
there’s the 
nlu.yml
 file, which contains the intents and examples of user
utterances that are used to train the intent recognition model. So let’s start
creating the intents that are used in our dialog. For every intent you want to
define, you need to provide a name and a list of examples of utterances that
belong to this intent.
For our short dialog, we need to understand the user’s greeting, their role
(parent or student), and their agreement to give permission to their child to
use the chatbot.
version
: 
"
3.1
"
nlu
:
- 
intent: greet
  
examples
: 
|
 - hey - hello - hi
- 
intent: parent
 - I am a parent - Parent - I'm a mom to 12 year old
- 
intent: agree
...
Pretty straightforward, right? RASA will warn if you have too few examples
for a particular intent, and recommends at least 7-10 utterance examples per
intent.
The next file you should look at is 
domain.yml
account. When text message chatbots came onto the scene most continued to
follow this dark pattern of non-cooperative conversation, trying your patience
and preventing you from creating cost for the business. The more advanced
NLP skills you have learned in this book now give you the power to build
chatbots that can simulate intelligent conversation and do useful work for you
and your organization.
The chatbot boom is not over yet. You are about to learn all the ways they
can be used to improve your life or your business.
12.1 Chatbots are everywhere
Chatbots are everywhere. Here are some examples to help you dream up your
own projects.
Virtual assistants
: Dicio (Google Assistant), Lyra (Siri) and MyCroft
(Alexa) can help you accomplish small tasks such as checking the
weather, calling a friend, launching an app, setting a reminder, playing
music, or turning on the lights.
Entertainment
: Chatbots in video games and websites promoting movies
are often used to keep you engaged in a fictional storyline. You can
measure how well an entertainment chatbot is doing by how long the
user is willing to interact with the chatbot and how often they suspend
their disbelief that they are interacting with a human.
Healthcare
: Depending on the regulations in your country, chatbots can
often answer your health-related questions, schedule an appointment for
you, or even give a preliminary diagnosis. Mental health chatbots, such
as Woebot 
[
4
]
 and Wysa,
[
5
]
 even provide therapeutic exercises that can
decrease depression and anxiety.
[
6
]
Impact
: Nonprofits and social businesses use chatbots to help people in
need. Often they leverage popular messaging channels like SMS and
WhatsApp to reach people in underserved communities where mobile
messaging is their main access to the Internet.
Operations (ChatOps)
: Businesses often use chatbots to increase team
productivity and job satisfaction. You can build chatbots that interact
with you on Telegram or WhatsApp to help you monitor and control
your software. And, if you’re lucky, your boss at work might use a
chatbot to onboard and train you, or even publicly recognize you when
you help a teammate learn something new.
Advertisement and Sales
: Search engines on corporate websites often
use chatbots to steer you towards advertisements and products they want
you to purchase or promote. Behind the scenes these bots are often used
to distract and 
engage
 you on (anti)social networks.
Customer (dis)service
: Machines have been replacing humans at
customer service call centers and chat message interfaces for decades.
Most large corporations do not allow you to interact with a human until
you first satisfy their chatbot gatekeepers.
The authors of this book founded Tangible AI to help nonprofits,
governments, and individual makers create impact chatbots.
[
7
]
 Impact
chatbots help people in underserved communities, from new immigrants in
the United States to teens in the Global South. We’ve built chatbots that help
people learn math, overcome imposter syndrome, learn new languages, evade
human traffickers, and even start a small business in a developing country. A
contributing author and Tangible AI volunteer, Vishvesh Bhat, has even
founded a startup of his own to build a chatbot that helps US college students
learn and reason about their course material.
[
8
]
Next, you will learn how to build your own chatbots to bring a positive
impact to your community or business.
[
9
]
 
[
10
]
12.1.1 Different chatbots, same tools
As diverse as the chatbot examples in this section seem to be, they all
leverage the same NLP tools and techniques that you have learned in this
book. All the previous chapters have been building up your skills and toolbox
so you can assemble a chatbot. Here are some of the NLP skills you’ve
learned that will help you build chatbots:
Chapter 6
: Embedding words and phrases into semantic vectors (from
Chapter 6) to recognize a chatbot user’s intent
Chapter 8
: Creating more meaningful embedding vectors of chat
messages using LSTMs and language models such as BERT.
Chapter 9
: Translating between languages to help your users interact
with your chatbot in their native language.
Chapter 10
: Semantic search and automatic text generation to respond to
chat messages without having to craft responses by hand.
Chapter 11
: Extracting relationships between real-world entities from
text to help your chatbot reason about a users' requests and maintain the
conversation context.
Figure 
Figure 12. 1
 shows how all these pieces fit together to create a
chatbot.
Figure 12.1 Chatbot flow diagram
Before you jump into assembling a chatbot system from all these tools and
libraries, you need to think about what you want your chatbot to talk about.
You need to design a conversation.
12.1.2 Conversation design
As chatbot technology gained more and more popularity in the last decade, so
did the field of conversation design. Conversation design is a branch of user
interaction (UI) design that deals specifically with designing engaging
dialogs. This section will help you get started, and when you’re ready to dive
deeper you can dig into more detailed resources wuch as as Andrew Freed’s
excellent 
Conversational AI
 book.
[
11
]
For every chatbot project you will work your way through four stages:
1
. 
Define your chatbot’s goal and the problem it solves. What does success
look like? How will you tell when your chatbot is doing a good job?
2
. 
Think about your users. Who will benefit from using your chatbot?
What do they need? Where will your users be when they use your
chatbot? What triggered them to engage in the conversation?
3
. 
Draft an imaginary conversation between the user and your chatbot. This
is called the "happy path" or "happy conversation." You might even go
as far as "act it out" with a colleague or a friend.
4
. 
Diagram a conversation tree. After drafting several happy conversations
with your chatbot, you will notice patterns that you can generalize from
to create a 
conversation diagram
 — a flow chart showing several
possible conversations between the user and the chatbot.
5
. 
Choose the NLP algorithms from Figure 
Figure 12. 1
 that you or your
teammates will need to implement in software in order for your chatbot
to generate responses at every branch in your dialog tree.
Think about the example of a math tutor bot. The goal is pretty clear, you
want to teach math to middle school children. However, when you start
thinking about the users in step 2, you realize that you cannot assume that the
child would be the person contacting your bot first. This is what the Rori
project experienced in low-income countries, where young children rarely
own a phone. Your younger users will often borrow someone else’s phone or
computer. So your chatbot may not be able to send homework reminders or
other push notifications to the users' phone.
Another important thing to consider when dealing with children is that you
need to obtain a parent or guardian’s consent before allowing your chatbot to
interact directly with a child. You will need to comply will all the child
protection laws in the countries where your chatbot will be used, including
mandatory reporting of 
safeguarding disclosures
 by your users. If a child
mentions that they are being abused or are considering self-harm you will
want to detect and report those disclosures. No matter what your chatbot’s
goals are, when your users indicate that they may be in danger, you will want
your chatbot to detect and report these interactions to you or the appropriate
authorities. So your math tutor chatbot will need an intent classifier that can
categorize the messages your users send to the chatbot. The open source
MathText
[
12
]
 and 
Maitag
[
13
]
 projects give you pretrained mulitlabel classifiers
and labeled datasets for intent recognition, including the intents required for
the Rori project.
expressions find the closest grammar matches among a list of possible
grammar rules (regular expressions) instead of exact matches by ignoring
some maximum number of insertion, deletion, and substitution errors.
However, expanding the breadth and complexity of behaviors for pattern-
matching chatbots requires a lot of difficult human development work. Even
the most advanced grammar-based chatbots, built and maintained by some of
the largest corporations on the planet (Google, Amazon, Apple, Microsoft),
remain in the middle of the pack for depth and breadth of chatbot IQ.
A lot of powerful things can be done with shallow NLP. And little, if any,
human supervision (labeling or curating of text) is required. Often a machine
can be left to learn perpetually from its environment (the stream of words it
can pull from Twitter or some other source).
[
67
]
 We show you how to do this
in Chapter 6.
1.11 Natural language IQ
Like human brainpower, the power of an NLP pipeline cannot be easily
gauged with a single IQ score without considering multiple "smarts"
dimensions. A common way to measure the capability of a robotic system is
along the dimensions of behavior complexity and the degree of human
supervision required. But for a natural language processing pipeline, the goal
is to build systems that fully automate the processing of natural language,
eliminating all human supervision (once the model is trained and deployed).
So a better pair of IQ dimensions should capture the breadth and depth of the
complexity of the natural language pipeline.
A consumer product chatbot or virtual assistant like Alexa or Allo is usually
designed to have extremely broad knowledge and capabilities. However, the
logic used to respond to requests tends to be shallow, often consisting of a set
of trigger phrases that all produce the same response with a single if-then
decision branch. Alexa (and the underlying Lex engine) behave like a single
layer, flat tree of (if, elif, elif, …) statements.
[
68
]
 Google Dialogflow (which
was developed independently of Google’s Allo and Google Assistant) has
similar capabilities to Amazon Lex, Contact Flow, and Lambda, but without
the drag-and-drop user interface for designing your dialog tree.
On the other hand, the Google Translate pipeline (or any similar machine
translation system) relies on a deep tree of feature extractors, decision trees,
and knowledge graphs connecting bits of knowledge about the world.
Sometimes these feature extractors, decision trees, and knowledge graphs are
explicitly programmed into the system, as in Figure 1.5. Another approach
rapidly overtaking this "hand-coded" pipeline is the deep learning data-driven
approach. Feature extractors for deep neural networks are learned rather than
hard-coded, but they often require much more training data to achieve the
same performance as intentionally designed algorithms.
You will use both approaches (neural networks and hand-coded algorithms)
as you incrementally build an NLP pipeline for a chatbot capable of
conversing within a focused knowledge domain. This will give you the skills
you need to accomplish the natural language processing tasks within your
industry or business domain. Along the way you will probably get ideas
about how to expand the breadth of things this NLP pipeline can do. Figure
1.6 puts the chatbot in its place among the natural language processing
systems that are already out there. Imagine the chatbots you have interacted
with. Where do you think they might fit in a plot like this? Have you
attempted to gauge their intelligence by probing them with difficult questions
or something like an IQ test? Try asking a chatbot something ambiguous that
requires common sense logic and the ability to ask clarifying questions, such
as "What’s larger, the sun or a nickel?"
[
69
]
 you will get a chance to do exactly
that in later chapters, to help you decide how your chatbot stacks up against
some of the others in this diagram.
Figure 1.9 2D IQ of some natural language processing systems
As you progress through this book, you will be building the elements of a
chatbot. Chatbots require all the tools of NLP to work well:
Feature extraction (usually to produce a vector space model)
Information extraction to be able to answer factual questions
Semantic search to learn from previously recorded natural language text
or dialog
Natural language generation to compose new, meaningful statements
Machine learning gives us a way to trick machines into behaving as if we had
spent a lifetime programming them with hundreds of complex regular
expressions or algorithms. We can teach a machine to respond to patterns
similar to the patterns defined in regular expressions by merely providing it
examples of user statements and the responses we want the chatbot to mimic.
And the "models" of language, the FSMs, produced by machine learning, are
much better. They are less picky about mispelings and typoz.
And machine learning NLP pipelines are easier to "program." We do not
have to anticipate every possible use of symbols in our language. We just
have to feed the training pipeline with examples of the phrases that match and
with example phrases that do not match. As long as we label the example
phrases during training so that the chatbot knows which is which, it will learn
to discriminate between them. And there are even machine learning
approaches that require little if any "labeled" data.
We have given you some exciting reasons to learn about natural language
processing. You want to help save the world, do you not? And we have
attempted to pique your interest with some practical NLP applications that
are revolutionizing the way we communicate, learn, do business, and even
think. It will not be long before you are able to build a system that
approaches human-like conversational behavior. And you should be able to
see in upcoming chapters how to train a chatbot or NLP pipeline with any
domain knowledge that interests you — from finance and sports to
psychology and literature. If you can find a corpus of writing about it, then
you can train a machine to understand it.
This book is about using machine learning to build smart text-reading
machines without you having to anticipate all the ways people can say things.
Each chapter incrementally improves on the basic NLP pipeline for the
chatbot introduced in this chapter. As you learn the tools of natural language
processing, you will be building an NLP pipeline that can not only carry on a
conversation but help you accomplish your goals in business and in life.
1.12 Test yourself
Chapter 1 review questions
Here are some review questions for you to test your understanding:
1
. 
Why is NLP considered to be a core enabling feature for AGI (human-
like AI)?
2
. 
Why do advanced NLP models tend to show significant discriminatory
biases?
3
. 
How is it possible to create a prosocial chatbot using training data from
sources that include antisocial examples?
4
. 
What are 4 different approaches or architectures for building a chatbot?
5
. 
How is NLP used within a search engine?
6
. 
Write a regular expression to recognize your name and all the variations
on its spelling (including nicknames) that you’ve seen.
7
. 
Write a regular expression to try to recognize a sentence boundary
(usually a period ("."), question mark "?", or exclamation mark "!")
tip
Active learning, quizzing yourself with questions such as these, is a fast way
to gain deep understanding of any new topic. It turns out, this same approach
is effective for machine learning and model evaluation as well.footnote:
[Suggested answers are provided within the Python packages 
nlpia
(
https://gitlab.com/tangibleai/nlpia
) and 
qary
(
https://gitlab.com/tangibleai/qary
) where they are used to evaluate advanced
NLP models for reading comprehension and question answering. Pooja Sethi
will share active learning NLP insights on Substack
(
https://activelearning.substack.com
) and github (
for a particular intent, and recommends at least 7-10 utterance examples per
intent.
The next file you should look at is 
domain.yml
 in the main directory. Its first
section is quite straightforward: it defines the intents from the 
nlu.yml
 file
that the chatbot should be able to understand. Let’s add the intents we just
defined to this part.
version
: 
"
3.1
"
intents
:
  - 
greet
  - 
parent
  - 
agree
...
The next section includes the action the chatbot can take - in this simplest
example, the pre-programmed utterances that the chatbot can use in the
conversation.
responses
:
  
utter_welcome
:
  - 
text: "Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!"
  
utter_parent_or_student
:
  - 
text: "Are you a parent (or guardian) or are you a student?"
  
utter_ask_permission
:
  - 
text: "For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?"
  
utter_permission_granted
:
  - 
text: "Thank you for giving permission for your child to chat with Rori."
  
utter_invite_child
:
  - 
text: "When your child is ready to start, please give them this phone and have them type *ready*."
The 
domain.yml
 file concludes with chatbot configuration parameters, that
we won’t deal with in this book. What’s more exciting, is the file 
config.yml
that allows you to configure all the components of your chatbot’s NLU
pipeline. Let’s look at the pipeline that RASA loads for you by default:
pipeline
:
  - 
name: WhitespaceTokenizer
  - 
name: RegexFeaturizer
  - 
name: LexicalSyntacticFeaturizer
  - 
name: CountVectorsFeaturizer
  - 
name: CountVectorsFeaturizer
    
analyzer
: 
char_wb
    
min_ngram
: 
1
    
max_ngram
: 
4
  - 
name: DIETClassifier
    
epochs
: 
100
    
constrain_similarities
: 
true
  - 
name: EntitySynonymMapper
  - 
name: ResponseSelector
    
epochs
: 
100
    
constrain_similarities
: 
true
  - 
name: FallbackClassifier
    
threshold
: 
0.3
    
ambiguity_threshold
: 
0.1
You can see that your NLU pipeline uses a tokenizer based on whitespaces,
and quite a few different algorithms (featurizers) to turn the user’s utterance
into a vector to be classified by the model. The CountVectorsFeaturizes is our
old friend Bag of Words vectorizer, while others are additional enhancements
helping the intent recognition (like RegexFeaturizer) or entity detection (like
LexicalSyntacticFeaturizer).
[
35
]
 Finally, the main classifier RASA uses is
DIETClassifier, which is a neural network model that combines intent
recognition and entity detection in a single model.
Of course, you don’t have to stick with the default components of the
pipeline. For example, if you want to replace the BoW embeddings, RASA
also offers to use pretrained embeddings from libraries like spaCy or
HuggingFace Transformers. You can change single components inside the
pipeline, or build your own completely from scratch - RASA documentation
even provides recommendations on how to create a pipeline based on your
use case and training set.
[
36
]
Finally, the last important file we haven’t covered yet is the 
stories.yml
 file
in the 
data
 folder. In this file, you can actually define a conversation
scenario, by chaining intents and actions together. Let’s combine a simple
story for the dialog we created above:
- 
story: onboarding parent
  
steps
:
  - 
intent: greet
  - 
action: utter_welcome
  - 
action: utter_parent_or_student
  - 
intent: parent
  - 
action: utter_ask_permission
  - 
intent: agree
  - 
action: utter_permission_granted
  - 
action: utter_invite_child
This story defines one possible conversational sequence between the chatbot
and the user. If you want the conversation to follow a different route (for
example, if the user of the phone is a child), you can define another story and
add it to the 
stories.yml
 file. You can also interactively train your bot by
running 
rasa interactive
 command in your shell. That would open a
training interface that allows you to chat with your bot and define new
intents, actions, and stories on the fly.
One question you might be asking yourself - given all the ways people say
things, how does the conversation engine decide what action to take at every
turn? And how can you anticipate in advance all the ways that your users will
use your chatbot? In chapter 10 you learned how LLMs can chat about
virtually anything. But it’s not good enough to just redirect your users to
some other corporation’s LLM interface. You will need to be able to integrate
the chatbot into your existing NLP pipeline, such as the block diagram in
Figure 12. 1
. The LangChain package gives you a way to do exactly that.
12.5.2 Adding LLMs to your chatbot with LangChain
This is especially useful in education when you need to inspire students and
keep them engaged. Teachers do this naturally, by adjusting what they say
and how they say it based on feedback on how well the student understands
what they are saying. And teachers think about more than just "delivering a
message." They must think up new ideas and approaches, on the fly as
students pose interesting new questions. Inspiring students' curiosity with
Socratic questions and being responsive to their changing needs is a full-time
job.
It is virtually impossible to build a rule-based system that captures all the
things that teachers do to help students learn and grow. Students' needs are
too diverse and dynamic. This is why hybrid chatbots that integrate LLMs
have become the preferred way build production chatbots in virtually every
domain. An LLM can confidently and convincingly chat with your users on
virtually any topic. The key is to harness this power smartly so that it doesn’t
mislead your users, or worse.
Let’s build a bot with one of the popular tools for creating generative
chatbots - LangChain.
[
37
]
 Langchain is not quite a chatbot framework as are
Rasa or Rapidpro. Rather, it’s a library that abstracts away the particular API
of the LLM you want to use, allowing you to quickly experiment with
different models and different approaches to using them. It also uses As there
is currently no leading open-source framework leveraging LLMs, we
<----------section---------->


**FAISS**

### Introduzione
FAISS (Facebook AI Similarity Search) è una libreria open-source sviluppata da Facebook AI Research per l'indicizzazione e la ricerca veloce di vettori ad alta dimensione. Il suo obiettivo principale è rendere più efficiente la ricerca della similarità tra vettori, un'operazione cruciale in molte applicazioni di intelligenza artificiale, come il recupero di informazioni, il riconoscimento delle immagini e i sistemi di raccomandazione.

### Perché FAISS è importante?
Nei moderni sistemi di AI, si lavora spesso con enormi dataset di vettori, specialmente nell'ambito del deep learning. La ricerca esatta della similarità in uno spazio vettoriale può essere computazionalmente proibitiva. FAISS fornisce algoritmi e strutture dati ottimizzate per eseguire ricerche approssimate in modo molto più veloce rispetto a un approccio naïve basato su ricerche brute-force.

### Embedding e Similarità nell'NLP
FAISS lavora principalmente con **embedding**, rappresentazioni vettoriali di oggetti come testi, immagini o segnali audio. Nel contesto del **Natural Language Processing (NLP)**, gli embedding sono vettori numerici che catturano la semantica delle parole, frasi o documenti. 

#### Generazione degli embedding
Nell'NLP, gli embedding sono generati attraverso modelli di apprendimento automatico come:
- **Word2Vec**: rappresenta le parole in base al loro contesto, utilizzando tecniche come Continuous Bag of Words (CBOW) e Skip-gram.
- **GloVe**: costruisce embedding sulla base della co-occorrenza delle parole in un grande corpus testuale.
- **FastText**: un'estensione di Word2Vec che considera anche i sottotokens delle parole, utile per lingue con morfologia complessa.
- **BERT e Transformer-based models**: generano embedding contestualizzati che variano in base alla frase in cui la parola appare.
- **Sentence Transformers**: creano embedding per intere frasi, migliorando la ricerca semantica e la similarità testuale.

#### Metriche di Similarità
Una volta ottenuti gli embedding, FAISS permette di confrontarli utilizzando diverse metriche di similarità:
- **Similarità coseno**: misura l'angolo tra due vettori e viene ampiamente utilizzata per valutare la similarità semantica tra testi.
- **Distanza euclidea (L2)**: meno comune nell'NLP, utile quando gli embedding hanno distribuzioni spaziali significative.
- **Prodotto scalare (dot product)**: spesso usato nei modelli neurali per valutare l'affinità tra vettori.

#### Applicazioni nell'NLP
FAISS è particolarmente utile in molteplici applicazioni NLP, tra cui:
- **Recupero di informazioni**: aiuta a trovare documenti o frasi simili in grandi dataset, come nella ricerca semantica.
- **Risoluzione della coreferenza**: identifica entità simili in un testo, associando riferimenti diversi a uno stesso concetto.
- **Sistemi di raccomandazione di testi**: suggerisce articoli, post o libri basandosi sulla similarità degli embedding.
- **Clustering e analisi dei topic**: raggruppa documenti con contenuti simili, utile per la categorizzazione automatica.
- **Traduzione automatica e allineamento testuale**: confronta frasi in lingue diverse per trovare corrispondenze tra segmenti di testo.

### Creazione e uso di un FAISS Index nell'NLP
FAISS fornisce API in Python per la gestione efficiente degli embedding testuali. Ecco un esempio per indicizzare e cercare frasi simili:

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Modello per generare embedding testuali
model = SentenceTransformer('all-MiniLM-L6-v2')

documents = [
    "Il gatto salta sul tavolo",
    "Un felino è balzato sulla superficie",
    "Oggi il tempo è splendido",
    "La partita di calcio è stata emozionante"
]

# Creazione degli embedding
d = 384  # Dimensione del modello MiniLM
embeddings = np.array(model.encode(documents), dtype='float32')

# Creazione e popolamento dell'indice FAISS
index = faiss.IndexFlatL2(d)
index.add(embeddings)

# Query per trovare frasi simili
query_text = "Il micio è saltato sul mobile"
query_embedding = np.array([model.encode(query_text)], dtype='float32')
k = 2  # Numero di risultati

distances, indices = index.search(query_embedding, k)

# Output dei risultati
print("Frasi più simili:")
for i in indices[0]:
    print(documents[i])
```

### Vantaggi di FAISS nell'NLP
- **Velocità di ricerca**: consente di trovare frasi simili in dataset di milioni di documenti in pochi millisecondi.
- **Scalabilità**: supporta grandi volumi di dati, ideale per motori di ricerca semantica e assistenti virtuali.
- **Flessibilità**: utilizzabile con vari modelli di embedding, adattandosi a diversi casi d'uso.

### Conclusione
FAISS è un potente strumento per la ricerca di similarità in embedding testuali, rendendo possibili applicazioni avanzate nell'NLP. Grazie alla sua efficienza, è usato in motori di ricerca semantica, chatbot e sistemi di raccomandazione testuale. L'integrazione con modelli come BERT e Sentence Transformers lo rende una scelta eccellente per chi lavora con grandi corpus di testo.

<----------section---------->
<----------section---------->

**NATURAL LANGUAGE PROCESSING AND LARGE LANGUAGE MODELS**

### General Information
- **Program of Study:** Computer Engineering  
- **Track:** Artificial Intelligence and Intelligent Robotics  
- **Course Type:** Master’s Degree  
- **Academic Year:** 2024/2025  
- **Course Year:** 2nd Year  
- **Educational Activity Type:** Elective  
- **Field:** Elective  
- **Language:** English  
- **Credits:** 6 CFU  
- **Teaching Activity Type:** Lecture, Laboratory  
- **Exam Type:** Written and oral exam with a single grade  
- **Assessment:** Final Grade  
- **Teaching Period:** First Semester (01/10/2024 – 15/12/2024)  
- **Instructors:** Nicola Capuano, Antonio Greco  
- **Duration:** 48 hours (24 hours lectures, 24 hours lab sessions)  
- **Scientific-Disciplinary Sector:** ING-INF/05  
- **Location:** University of Salerno - Fisciano  

### Learning Objectives
The course provides theoretical, methodological, technological, and practical knowledge on natural language understanding and text processing. It introduces the innovative paradigms of Large Language Models (LLMs) within the general framework of Natural Language Processing (NLP), highlighting their numerous modern applications.  

### Knowledge and Understanding
- Fundamental concepts of NLP systems  
- Standard language models  
- Transformer-based LLMs  
- NLP applications using LLMs  
- Prompt engineering and fine-tuning LLMs  

### Applying Knowledge and Understanding
- Design and implementation of NLP systems using LLMs, effectively integrating existing technologies and optimizing configuration parameters  

### Prerequisites
- **Prerequisite Exam:** Machine Learning  

## Course Content

### **Module 1: Fundamentals of Natural Language Processing** (10 hours lecture, 6 hours exercises)
1. **Introduction to NLP:** Basic concepts, tasks, evolution, and applications (2 hours lecture)  
2. **Text Representation:** Tokenization, stemming, lemmatization, bag of words, n-grams, similarity measures, word embeddings (2 hours lecture)  
3. **TF-IDF and Classification:** TF-IDF vectors, text classification, and clustering (2 hours lecture)  
4. **Neural Networks for Text Analysis:** CNNs, recurrent networks, LSTMs (2 hours lecture)  
5. **Implementation:** Developing a text classifier (2 hours exercises)  
6. **Information Extraction:** Named Entity Recognition (NER), Question Answering (2 hours lecture)  
7. **Chatbot Development:** Using Python and SpaCy/RASA (4 hours exercises)  

### **Module 2: Transformers** (6 hours lecture, 10 hours exercises)
1. **Core Concepts:** Self-attention, multi-head attention, positional encoding, masking (2 hours lecture)  
2. **Transformer Architectures:** Encoder and decoder (2 hours lecture)  
3. **Practical Implementation:** Introduction to Hugging Face (2 hours exercises)  
4. **Applications:** Encoder-decoder models for translation and summarization (2 hours exercises)  
5. **Encoder-only Models:** Sentence classification, Named Entity Recognition (NER) (2 hours exercises)  
6. **Decoder-only Models:** Text generation (2 hours exercises)  
7. **LLM Definition:** Defining and training an LLM (2 hours lecture)  
8. **LLM Training:** Hands-on training (2 hours exercises)  

### **Module 3: Prompt Engineering** (2 hours lecture, 4 hours exercises)
1. **Techniques:** Zero-shot, few-shot, chain-of-thought prompting, self-consistency, generated knowledge, prompt chaining, ReAct, Retrieval-Augmented Generation (RAG) (2 hours lecture)  
2. **Exercises:** Basic prompting techniques (2 hours)  
3. **Advanced Techniques:** Exercises on RAG and LangChain (2 hours)  

### **Module 4: Fine-Tuning LLMs** (4 hours lecture, 4 hours exercises, 2 hours lab)
1. **Fine-Tuning Methods:** Feature-based tuning, updating output layers, Parameter-Efficient Tuning (PEFT), Low-Rank Adaptation (LoRA) (2 hours lecture)  
2. **Hands-on Fine-Tuning:** Practical exercises (2 hours exercises)  
3. **Reinforcement Learning with Human Feedback (RLHF):** Theoretical overview (2 hours lecture)  
4. **RLHF Practical Exercises:** Application and experimentation (2 hours exercises)  
5. **Final Project:** Application of learned techniques (2 hours lab)  

## Teaching Methods
The course includes lectures and in-class exercises. Lectures provide fundamental knowledge on advanced text representation, analysis, and classification techniques using LLMs. Exercises focus on applying these techniques to develop tools for text classification, analysis, and question answering. Attendance is mandatory, with a minimum requirement of 70% to access the exam. Attendance is tracked through the university's EasyBadge system.  

## Assessment
The exam consists of a group project and an oral test:  
- **Project:** Students will critically apply methodologies learned during the course to a practical case.  
- **Oral Test:** Evaluation of theoretical knowledge, project design choices, and answers to specific topics covered in the lectures.  
The final grade is the average of both components.  

## Recommended Texts
- **Reference Book:**  
  H. Lane, C. Howard, H. M. Hapke: *Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python*, Manning.  
- **Supplementary Materials:** Available on the university's e-learning platform ([https://elearning.unisa.it](https://elearning.unisa.it)), accessible to course students using their university credentials.


<----------section---------->

NICOLA CAPUANO 
Nicola Capuano is an Associate Professor at the Department of Information and Electrical 
Engineering and Applied Mathematics (DIEM) at the University of Salerno. He obtained his 
degree in Computer Science and his Ph.D. in Computer Science and Computer Engineering 
from the University of Salerno. At the same university, he also held a four-year research 
fellowship on the topic of "Artificial Intelligence." Before pursuing an academic career, he 
collaborated with private research institutes, including the Center for Research in Pure and 
Applied Mathematics and the Center of Excellence for Software Technology. He also served as a 
researcher at the School of Engineering of the University of Basilicata. In 2021, he achieved the 
National Scientific Qualification as a Full Professor in the field 09/H1: Information Processing 
Systems. 
His research focuses on Natural Language Processing, Machine Learning, Knowledge 
Representation, Fuzzy Systems, and Artificial Intelligence in Education. He is the author of more 
than 120 publications, including journal articles, conference proceedings, and book chapters. 
He serves as an Associate Editor for the Journal of Ambient Intelligence and Humanized 
Computing by Springer Nature and Frontiers in Artificial Intelligence by Frontiers Media. He has 
been a Guest Editor for several journals, including the International Journal of Educational 
Technology in Higher Education and the International Journal of Emerging Technologies in 
Learning. He is a member of editorial boards and a reviewer for numerous journals, a track 
chair, and a program committee member for international conferences and workshops. He is 
also a member of the executive committee for the international conference The Learning Ideas. 
He edited the volume The Learning Grid Handbook, published by IOS Press. 
Nicola Capuano acts as an independent evaluator of projects and proposals for the European 
Commission under the Horizon Europe program and for the European Institute of Innovation 
and Technology. He coordinated the projects Diogene (A Training Web Broker for ICT 
Professionals) and InTraServ (Intelligent Training Service for Management Training in SMEs), both 
funded by the European Commission under the Fifth Framework Programme. He was a member 
of the European Network of Excellence Kaleidoscope (Concepts and Methods for Exploring the 
Future of e-Learning with Digital Technologies), where he coordinated the Special Interest 
Group on Learning Grid. He led the research line “Intelligent Features for Learning” at the Center 
of Excellence in Methods and Systems for Learning and Knowledge at the University of Salerno. 
He has held scientific and coordination roles in several other research and innovation projects. 
He is a Project Management Professional (PMP) certified by the Project Management Institute. 

<----------section---------->

ANTONIO GRECO 
Antonio Greco graduated with honors in Computer Engineering in 2014 from the University of 
Salerno (Italy). In March 2018, he earned a Ph.D. in Computer Science and Information 
Engineering from the same university. In March 2020, he became an RTD/A Researcher (SSD 
ING-INF/05 "Information Processing Systems") at the Department of Information and Electrical 
Engineering and Applied Mathematics (DIEM) at the University of Salerno, where he has been an 
RTD/B Researcher since November 2022. Within the same department, he has served as 
Delegate for Student Orientation since November 2022 and has been a member of the Ph.D. 
Board in Information Engineering since May 2024. 
For the department, he has taught various courses, including Logic Networks (SSD ING-INF/05, 
Bachelor's Degree in Computer Engineering), Autonomous Vehicle Driving (SSD ING-INF/05, 
Master's Degree in Computer Engineering), Web Software Technologies (SSD ING-INF/05, 
Bachelor's Degree in Computer Engineering), Artificial Intelligence for Cybersecurity (SSD ING
INF/05, Master's Degree in Computer Engineering), Robotics for E-Health (SSD ING-INF/05, 
Master's Degree in Digital Health and Bioinformatics Engineering), System and Network Security 
(SSD ING-INF/05, Master's Degree in Computer Engineering), Artificial Vision (SSD ING-INF/05, 
Master's Degree in Computer Engineering), and Natural Language Processing and Large 
Language Models (SSD ING-INF/05, Master's Degree in Computer Engineering). Since the 
2021/2022 academic year, he has also taught the course Advanced Machine Learning as part of 
the accredited Ph.D. program in Information Engineering (D.M. 226/2021). 
Since 2014, he has been a member of the MIVIA Lab (Machines for Intelligent Video, Image, and 
Audio recognition) at the University of Salerno’s DIEM, a group with a strong focus on 
international collaborations. His research primarily focuses on Computer Vision and Pattern 
Recognition, specifically on the design, implementation, and optimization of real-time 
computer vision and deep learning algorithms (e.g., gender recognition, age estimation, 
ethnicity recognition, emotion analysis, fire detection, anomaly detection, people counting, 
object tracking, and audio event recognition) for data acquired from static devices (smart 
cameras, microphones) or moving devices (drones, robots, autonomous vehicles). These 
activities are often conducted in collaboration with European research groups, particularly at 
the University of Malta, the University of Groningen (Netherlands), and the University of Twente 
(Netherlands), where he has spent a total of nine months as a Visiting Researcher (January
April 2020 and August 2021–February 2022). At Twente, he collaborated with the Data 
Management and Biometrics Group within the Faculty of Electrical Engineering, Mathematics, 
and Computer Science. 
In recent years, he has organized several Special Issues for international journals, including an 
issue of the Journal of Ambient Intelligence and Humanized Computing titled “Ambient 
Understanding for Mobile Autonomous Robots (AutoRob)," an issue of Pattern Recognition on 
“From Bench to the Wild: Recent Advances in Computer Vision Methods (WILD-VISION)," and 
an issue of Pattern Analysis and Applications on “Pedestrian Attribute Recognition and Person 
Re-Identification." As of March 2024, he is an Associate Editor for Pattern Analysis and 
Applications and serves as a reviewer for over 25 international journals. 
Antonio Greco has presented his scientific work at more than ten international conferences and 
has served as Contest Chair at various events. Notably, he organized the Guess the Age (GTA) 
Contest 2021 at the International Conference on Computer Analysis of Images and Patterns 
(CAIP), co-organized the ONFIRE 2023 contest at the International Conference on Image 
Analysis and Processing (ICIAP), and co-organized the Pedestrian Attributes Recognition (PAR) 
Contest 2023 at CAIP. Additionally, he served on the Local Committee for the International 
Workshop on Graph-based Representations (GBR) in Capri, Italy (May 16–18), the International 
Conference on Computer Analysis of Images and Patterns (CAIP) in Salerno, Italy (September 2
6, 2019), the IEEE Conference on Cognitive and Computational Aspects of Situation 
Management (CogSIMA) in Salerno, Italy (June 6–10, 2022), and the GBR Workshop in Salerno 
(September 6–8). In 2021, he was an Invited Speaker at the CogSIMA Challenge Problems 
Workshop at the IEEE CogSIMA Conference, delivering a talk titled “Adding Awareness to AI 
Systems.” 
In national and international research projects, he has served as the scientific lead of the local 
research unit at DIEM for the PON ARS01_01226 project PerMedNet – Personalized Medicine for 
Innovative Strategies in Neuropsychiatric and Vascular Diseases. He also led work packages 
WP4 and WP5 in the European research project Flexible Assembly Manufacturing with Human
Robot Collaboration and Digital Twin Models (FELICE) (Grant Agreement ID: 101017151, funded 
under H2020-EU.2.1.1). Additionally, he was the scientific lead for DIEM in a research contract 
with the company RED&BLUE for the project Development of a Platform for Evaluating AI 
System Safety. 
In the field of technology transfer, Antonio Greco co-founded AI-READY, a spin-off of the 
University of Salerno specializing in AI applications for cognitive robotics, mobility, and 
autonomous vehicles, in December 2019.

<----------section---------->

