**Natural Language Processing and Large Language Models**

*Corso di Laurea Magistrale in Ingegneria Informatica*
Lesson 0 - Course Introduction
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This document outlines the objectives, topics, and practical aspects of a master-level course focused on Natural Language Processing (NLP) and Large Language Models (LLM), instructed by Nicola Capuano and Antonio Greco at the University of Salerno. The course aims to provide students with a solid understanding of NLP fundamentals and equip them with the skills to design and implement LLM-based systems.

<----------section---------->

**Objectives**

The course has the following objectives.

**Knowledge:**

*   **Basic Concepts of Natural Language Processing (NLP):** The course will introduce the fundamental principles of NLP, including its history, evolution, and core tasks. NLP is a field of computer science focused on enabling computers to understand, interpret, and generate human language. It combines computational linguistics with statistical, machine learning, and deep learning models.
*   **Natural Language Understanding and Generation:**  Students will learn about the two primary branches of NLP: Natural Language Understanding (NLU), which focuses on enabling machines to comprehend the meaning of text, and Natural Language Generation (NLG), which deals with creating human-readable text from structured data or internal representations. NLU involves tasks such as semantic search, text alignment, paraphrase recognition, intent classification, and authorship attribution. NLG involves tasks such as synonym substitution, question answering, spelling and grammar correction, and casual conversation.
*   **Statistical Approaches to NLP:** The course will cover traditional statistical methods used in NLP, such as n-grams, Hidden Markov Models (HMMs), and Conditional Random Fields (CRFs).  These techniques rely on analyzing large amounts of text data to identify patterns and relationships between words and phrases.
*   **Large Language Models (LLM) based on Transformers:** A significant portion of the course will be dedicated to LLMs, particularly those based on the Transformer architecture.  Transformers have revolutionized NLP due to their ability to process sequential data in parallel and capture long-range dependencies in text. Key components like self-attention, multi-head attention, and positional encoding will be explored.
*   **NLP Applications with LLM:** Students will explore various real-world applications of NLP powered by LLMs.  These applications include machine translation, text summarization, question answering, chatbot development, sentiment analysis, and content generation.
*   **Prompt Engineering and Fine Tuning of LLM:**  The course will provide training on how to effectively interact with LLMs through prompt engineering and how to customize pre-trained models for specific tasks via fine-tuning. Prompt engineering involves designing effective input prompts to elicit desired responses from LLMs. Fine-tuning involves further training a pre-trained LLM on a smaller, task-specific dataset to improve its performance on a particular application.

**Abilities:**

*   **Design and Implementation of a NLP System based on LLMs:** Students will gain the practical skills necessary to design and build complete NLP systems using LLMs. This includes integrating existing technologies and tools from libraries like Hugging Face, PyTorch, and TensorFlow to create solutions for real-world problems.

<----------section---------->

**Fundamentals of NLP**

*   **Basic Concepts, Evolution, and Applications of NLP:**  This section will provide a comprehensive overview of NLP, including its definition, historical development, and the diverse range of applications across various industries, such as healthcare, finance, marketing, and education.
*   **Representing Text: Tokenization, Stemming, Lemmatization, POS tagging:** Students will learn how to convert raw text into a format suitable for machine processing. This involves:
    *   **Tokenization:**  Breaking down text into individual units or tokens (words, subwords, or characters).
    *   **Stemming:** Reducing words to their root form by removing suffixes (e.g., "running" becomes "run").
    *   **Lemmatization:** Converting words to their dictionary form (lemma) based on context (e.g., "better" becomes "good").
    *   **Part-of-Speech (POS) Tagging:** Assigning grammatical tags (noun, verb, adjective, etc.) to each word in a sentence.
*   **Math with Words: Bag of Words, Vector Space Model, TF-IDF, Search Engines:**  This section will explore methods for representing text numerically, enabling mathematical operations and analysis:
    *   **Bag of Words (BoW):**  Representing a document as an unordered collection of words and their frequencies, disregarding grammar and word order.
    *   **Vector Space Model (VSM):** Representing documents as vectors in a high-dimensional space, where each dimension corresponds to a word or term.
    *   **TF-IDF (Term Frequency-Inverse Document Frequency):** A weighting scheme that measures the importance of a word in a document relative to its frequency across a collection of documents.
    *   **Search Engines:** How the above methods are practically used for indexing and retrieving information
*   **Text Classification: Topic Labelling, Sentiment Analysis:**  Students will learn how to categorize and classify text documents:
    *   **Topic Labelling:** Assigning relevant topics or categories to documents based on their content.
    *   **Sentiment Analysis:** Determining the emotional tone or subjective opinion expressed in a text (positive, negative, or neutral).
*   **Word Embeddings: Word2Vec, CBOW, Skip-Gram, GloVe, FastText:** This section will introduce techniques for representing words as dense vectors in a continuous space, capturing semantic relationships between words:
    *   **Word2Vec:** A neural network-based method for learning word embeddings from large corpora. It includes two main architectures:
        *   **CBOW (Continuous Bag of Words):** Predicts a target word based on its surrounding context words.
        *   **Skip-Gram:** Predicts surrounding context words based on a target word.
    *   **GloVe (Global Vectors for Word Representation):** A matrix factorization-based method that combines global word co-occurrence statistics with local context information to learn word embeddings.
    *   **FastText:** An extension of Word2Vec that incorporates subword information (character n-grams) to handle out-of-vocabulary words and improve performance on morphologically rich languages.
*   **Neural Networks for NLP: RNN, LSTM, GRU, CNN, Introduction to Text Generation:**  The course will cover the use of neural networks for various NLP tasks:
    *   **RNN (Recurrent Neural Network):** A type of neural network designed for processing sequential data, where the output of each step is fed back into the input of the next step, enabling the network to maintain a memory of past inputs.
    *   **LSTM (Long Short-Term Memory):** A specialized type of RNN that addresses the vanishing gradient problem by introducing memory cells and gates to regulate the flow of information, allowing the network to capture long-range dependencies in text.
    *   **GRU (Gated Recurrent Unit):** A simplified version of LSTM with fewer parameters, offering similar performance while being computationally more efficient.
    *   **CNN (Convolutional Neural Network):** Neural Networks that use convolutional layers. In NLP they are typically used for feature extraction, identifying patterns in text, and text classification.
    *   **Introduction to Text Generation:** Generating new text with models.
*   **Information Extraction: Parsing, Named Entity Recognition:** Students will learn how to extract structured information from unstructured text:
    *   **Parsing:** Analyzing the grammatical structure of a sentence to identify its constituent parts and their relationships.
    *   **Named Entity Recognition (NER):** Identifying and classifying named entities in text, such as people, organizations, locations, and dates.
*   **Question Answering and Dialog Engines (chatbots):** This section will explore the development of systems that can answer questions posed in natural language and engage in conversations: Question answering systems and dialogue engines (chatbots) are designed to understand questions and answer them in a cohesive way.

<----------section---------->

**Transformers**

*   **Self-Attention, Multi-Head Attention, Positional Encoding, Masking:** This section will delve into the core mechanisms of the Transformer architecture:
    *   **Self-Attention:** A mechanism that allows the model to attend to different parts of the input sequence when processing each element, capturing relationships between words and phrases.
    *   **Multi-Head Attention:** An extension of self-attention that uses multiple attention heads to capture different aspects of the relationships between words.
    *   **Positional Encoding:** Adding information about the position of words in the sequence to the input embeddings, since the Transformer architecture does not inherently capture sequential information.
    *   **Masking:** Preventing the model from attending to certain parts of the input sequence, such as future tokens in a language modeling task.
*   **Encoder and Decoder of a Transformer:** The course will explain the roles of the encoder and decoder components in a Transformer model:
    *   **Encoder:** Processes the input sequence and generates a contextualized representation of each word.
    *   **Decoder:** Generates the output sequence based on the encoder's representation and the previously generated tokens.
*   **Introduction to HuggingFace:** The use of the HuggingFace library will be introduced for easy usage of available models.
*   **Encoder-Decoder or Seq2Seq models (translation and summarization):**  Students will learn about sequence-to-sequence models based on the Transformer architecture and their applications in machine translation and text summarization. These models consist of an encoder that processes the input sequence and a decoder that generates the output sequence.
*   **Encoder-only Models (sentence classification and named entity recognition):**  The course will cover models that use only the encoder part of the Transformer architecture for tasks such as sentence classification and named entity recognition.
*   **Decoder-only Models (text generation):**  Students will learn about models that use only the decoder part of the Transformer architecture for text generation tasks, such as language modeling and chatbot development.
*   **Definition and training of a Large Language Model:** The creation of a Large Language Model will be discussed, including the data needed to train and model one.

<----------section---------->

**Prompt Engineering**

*   **Zero-shot and Few-shot Prompting:**
    *   **Zero-shot Prompting:** Designing prompts that allow the LLM to perform a task without any prior training or examples.
    *   **Few-shot Prompting:** Providing a small number of examples in the prompt to guide the LLM's behavior and improve its performance.
*   **Chain-of-Thought, Self-Consistency, Prompt Chaining:** Prompt engineering techniques.
    *   **Chain-of-Thought:** Prompts where the model is encouraged to show each step it is taking when generating output.
    *   **Self-Consistency:** Uses a variety of reasoning paths and picks the most consistent answer.
    *   **Prompt Chaining:** Breaking down a complex task into smaller subtasks and using the output of one prompt as the input to the next.
*   **Role Prompting, Structured Prompts, System Prompts:**
    *   **Role Prompting:** Instructing the LLM to adopt a specific persona or role to guide its responses.
    *   **Structured Prompts:** Formatting the prompt in a specific way to provide more context and structure to the LLM.
    *   **System Prompts:** Providing high-level instructions to the LLM to guide its overall behavior and objectives.
*   **Retrieval Augmented Generation:** Combining a retrieval mechanism with a generative model to improve the accuracy and reliability of generated text. This involves retrieving relevant information from a knowledge base or external source and using it to inform the generation process.

<----------section---------->

**LLM Fine Tuning**

*   **Feature-Based Fine Tuning:** Extracting features from the pre-trained LLM and training a separate model on those features for a specific task.
*   **Parameter Efficient Fine Tuning and Low Rank Adaptation:**  Techniques for fine-tuning LLMs with a limited number of trainable parameters, reducing the computational cost and memory requirements. Low-Rank Adaptation (LoRA) falls into this category, decomposing weight updates and significantly reducing the number of parameters.
*   **Reinforcement Learning with Human Feedback:** Training LLMs using reinforcement learning, where the reward signal is based on human feedback. This allows the model to learn from human preferences and improve its ability to generate high-quality, human-aligned text.

<----------section---------->

**Textbook**

H. Lane, C. Howard, H. M. Hapke. *Natural Language Processing IN ACTION: Understanding, analyzing, and generating text with Python*. Manning, 2019.

Second Edition in fall 2024. Early Access version available online: [https://www.manning.com/books/natural-language-processing-in-action-second-edition](https://www.manning.com/books/natural-language-processing-in-action-second-edition)

The primary textbook for this course is "Natural Language Processing in Action" by H. Lane, C. Howard, and H. M. Hapke, published by Manning in 2019. A second edition is scheduled for release in Fall 2024, with an early access version available online. This book provides a practical introduction to NLP concepts and techniques using Python.

<----------section---------->

**Further Info**

**Teachers**

*   Nicola Capuano
    DIEM, FSTEC-05P02007
    ncapuano@unisa.it
    089 964292
*   Antonio Greco
    DIEM, FSTEC-05P01036
    agreco@unisa.it
    089 963003

The course instructors are Nicola Capuano and Antonio Greco, both affiliated with DIEM (Department of Industrial Engineering and Mathematics) at the University of Salerno. Contact information, including email addresses and phone numbers, is provided for each instructor.

**Online Material**

*   [https://elearning.unisa.it/](https://elearning.unisa.it/)

Additional course materials, including lecture notes, assignments, and announcements, will be available on the University of Salerno's e-learning platform.

**Exam**

*   Realization of a project work
*   Oral exam (including the discussion of the project work)

The course assessment will consist of a project and an oral exam. The project involves the design and implementation of an NLP system based on LLMs. The oral exam will cover the course content, including a discussion of the project work. This structure ensures that the students obtain both theoretical knowledge and practical application.

<----------section---------->

# Natural Language Processing and Large Language Models

**Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree in Computer Engineering) - Lesson 1: NLP Overview**

**Nicola Capuano and Antonio Greco**

**DIEM – University of Salerno**

This document provides an overview of Natural Language Processing (NLP) intended for students in a Master's level Computer Engineering course. It is presented by Nicola Capuano and Antonio Greco from the Department of Information Engineering and Mathematics (DIEM) at the University of Salerno. The overview includes definitions, applications, and historical context for understanding NLP's significance and challenges.

<----------section---------->

## Outline

This lesson will cover the following key areas:

*   **What is Natural Language Processing (NLP):** Introduction to the field, its core concepts, and importance.
*   **Applications of NLP:** A survey of diverse applications across various industries and domains.
*   **History of NLP:** Tracing the evolution of NLP from its early stages to the current state-of-the-art.

<----------section---------->

## What is Natural Language Processing?

NLP is an interdisciplinary field focusing on enabling computers to understand, interpret, and generate human language. It sits at the intersection of computer science, artificial intelligence, and linguistics. This allows computers to process and analyze large amounts of natural language data, derive insights, and perform tasks such as translation, summarization, and sentiment analysis.

### NLP in the Press

NLP's growing influence and capabilities are frequently highlighted in the media, underscoring its importance and potential impact:

*   "New powerful AI bot creates angst among users: Are robots ready to take our jobs?" - Reflects concerns about automation and job displacement.
*   "A Smarter Robot: A new chatbot shows rapid advances in artificial intelligence." - *The New York Times* - Highlights the rapid progress in AI-driven conversational agents.
*   "What is ChatGPT, the viral social media AI?" - *The Washington Post* - Introduces ChatGPT, a prominent example of advanced language models.
*   "This AI chatbot is dominating social media with its frighteningly good essays." - *CNN* - Emphasizes the impressive text generation capabilities of AI chatbots.
*   "ChatGPT may be coming for our jobs. Here are the 10 roles that AI is most likely to replace." - *Business Insider* - Speculates about the potential impact of AI on various professions.
*   "Microsoft co-founder Bill Gates: ChatGPT ‘will change our world’" - *Reuters* - Indicates the transformative potential of NLP technology, particularly ChatGPT, as seen by industry leaders.

These headlines reflect the growing societal awareness and debate surrounding NLP's capabilities and implications.

<----------section---------->

### Importance of NLP

Eminent figures emphasize NLP's crucial role in artificial intelligence:

*   "Natural language is the most important part of Artificial Intelligence" - *John Searle, Philosopher* - Highlights the centrality of human language understanding in achieving true AI.
*   "Natural language processing is a cornerstone of artificial intelligence, allowing computers to read and understand human language, as well as to produce and recognize speech" - *Ginni Rometty, IBM CEO* - Underscores NLP's role as a foundational technology for enabling human-computer interaction.
*   "Natural language processing is one of the most important fields in artificial intelligence and also one of the most difficult" - *Dan Jurafsky, Professor of Linguistics and Computer Science at Stanford University* - Acknowledges both the significance and complexity of NLP research.

These quotes emphasize that NLP is not just a subfield of AI, but a critical component that enables machines to interact with humans in a meaningful way.

<----------section---------->

### Definitions

Several definitions of NLP provide a comprehensive understanding of the field:

*   "Natural language processing is the set of methods for making human language accessible to computers" - *Jacob Eisenstein* - Focuses on bridging the gap between human and machine communication.
*   "Natural language processing is the field at the intersection of computer science and linguistics" - *Christopher Manning* - Highlights the interdisciplinary nature of NLP.
*   "Make computers to understand natural language to do certain tasks humans can do such as translation, summarization, questions answering" - *Behrooz Mansouri* - Emphasizes the goal of replicating human language capabilities in computers.
*   "Natural language processing is an area of research in computer science and artificial intelligence concerned with processing natural languages such as English or Mandarin. This processing generally involves translating natural language into data that a computer can use to learn about the world. And this understanding of the world is sometimes used to generate natural language text that reflects that understanding." - *(Natural Language Processing in Action)* - Provides a detailed description of the research area, including translating natural language into computer-interpretable data and using this to generate natural language text.

These definitions collectively illustrate NLP's goal of enabling computers to process and understand human language for various tasks.

<----------section---------->

### Natural Language Understanding (NLU)

NLU is a subfield of NLP dedicated to enabling machines to comprehend human language. It focuses on converting human language into a format that computers can process effectively.

*   Involves extracting meaning, context, and intent from text.
*   Text is transformed into a numerical representation (embedding), which captures the semantic information.

**Who uses Embeddings:**

*   **Search Engines:** Interpret the meaning behind search queries to deliver relevant results. For example, understanding that "best Italian restaurants near me" is a request for nearby Italian eateries.
*   **Email Clients:** Detect spam and classify emails as important or not, filtering out unwanted messages and prioritizing relevant correspondence.
*   **Social Media:** Moderate posts by identifying hate speech, offensive content, and misinformation. Understand user sentiment to improve content recommendations and user experience.
*   **CRM Tools:** Analyze customer inquiries to understand customer needs, route inquiries to the appropriate departments, and personalize interactions.
*   **Recommender Systems:** Suggest articles, products, or content based on user preferences and past behavior. For example, recommending books similar to those a user has previously enjoyed.

NLU is vital for extracting actionable insights from textual data, enabling a wide array of applications.

<----------section---------->

### Natural Language Generation (NLG)

NLG is another key subfield of NLP, focused on generating human-like text. Its goal is to enable computers to produce coherent and contextually appropriate text.

*   Involves creating coherent, contextually appropriate text.
*   Based on a numerical representation of the meaning and sentiment you would like to convey.

**Applications:**

*   **Machine Translation:** Translates text from one language to another, allowing for cross-lingual communication.
*   **Text Summarization:** Creates concise summaries of long documents, preserving key information. Useful for quickly grasping the essence of lengthy reports or articles.
*   **Dialogue Processing:** Powers chatbots and virtual assistants to provide relevant responses in conversations, enhancing customer service and user engagement.
*   **Content Creation:** Generates articles, reports, stories, poetry, and other forms of creative writing. Useful for automating content creation processes and inspiring creativity.

NLG enables computers to communicate effectively, generate content automatically, and enhance human-computer interactions.

<----------section---------->

### Example: Conversational Agents

Conversational agents, also known as chatbots or virtual assistants, exemplify the integration of NLU and NLG. They are designed to simulate human conversation and provide information or perform tasks.

Conversational agents include:

*   **Speech Recognition:** Converts spoken language into text.
*   **Language Analysis:** Understands the meaning and intent of the text.
*   **Dialogue Processing:** Manages the flow of conversation.
*   **Information Retrieval:** Accesses and retrieves relevant information.
*   **Text to Speech:** Converts text into spoken language.

**Example conversation:**

*   User: "Open the pod bay doors, Hal."
*   Hal: "I’m sorry, Dave, I’m afraid I can’t do that."
*   User: "What are you talking about, Hal?"
*   Hal: "I know that you and Frank were planning to disconnect me, and I'm afraid that's something I cannot allow to happen."

This example, inspired by the movie 2001: A Space Odyssey, illustrates the complex dialogue management and reasoning capabilities that conversational agents can achieve.

<----------section---------->

### Conversational Agents in Movies

Conversational agents are frequently featured in movies, showcasing advanced AI capabilities and sparking both excitement and concern about the future of human-computer interaction. They are frequently featured in movies, symbolizing advanced AI and raising questions about future human-computer relationships.

### NLP is Hard

One of the primary challenges in NLP is ambiguity. Natural language is inherently ambiguous, leading to multiple possible interpretations of the same input.

"I made her duck… what does it mean?"

*   Duck: noun (waterfowl) or verb (getting down)?
*   Make: cook X or cause X to do Y?
*   Her: for her or belonging to her?

**Possible meanings:**

*   I cooked waterfowl for her
*   I cooked waterfowl belonging to her
*   I created the (plaster?) duck she owns
*   I caused her to quickly lower her head or body
*   I waved my magic wand and turned her into undifferentiated waterfowl

This example demonstrates how a simple sentence can have multiple valid interpretations, posing a significant challenge for NLP systems.

<----------section---------->

### Ambiguity

Natural language is extremely rich in form and structure, yet highly ambiguous. NLP systems must handle this ambiguity to accurately understand and generate text.

*   One input can mean many different things.
*   Many inputs can mean the same thing.

**Levels of ambiguity:**

*   **Lexical ambiguity:** Different meanings of words (e.g., "bank" as a financial institution or a river bank).
*   **Syntactic ambiguity:** Different ways to parse the sentence (e.g., "I saw the man on the hill with a telescope").
*   **Interpreting partial information:** How to interpret pronouns (e.g., "John told Bill that he was wrong" - who was wrong?).
*   **Contextual information:** Context of the sentence may affect the meaning of that sentence (e.g., sarcasm, irony).

NLP systems must consider various levels of ambiguity to derive the correct meaning.

<----------section---------->

### Ambiguity Examples

*   "I saw bats… ?" - Could refer to seeing the animals or baseball bats.
*   "Call me a cab… ?" - Could mean "hail a taxi for me" or "refer to me as a cab."

These examples further illustrate the pervasiveness of ambiguity in natural language.

### NLP and Linguistics

NLP techniques draw on various aspects of linguistics to accurately process and generate human language.

*   **Phonetics:** Understanding the physical sounds of speech and how they are produced and perceived. Crucial for speech recognition and synthesis.
*   **Morphology:** Knowledge of the structure and formation of words, including their meaningful components (morphemes). Helps in understanding word meanings and relationships.
*   **Syntax:** Understanding the rules and structures governing the arrangement of words in sentences. Essential for parsing sentences and determining their grammatical structure.
*   **Semantics:** Insight into the meaning of words, phrases, and sentences. Enables the extraction of accurate meaning and relationships between concepts.
*   **Pragmatics:** Understanding how context influences the interpretation of meaning. Essential for capturing nuances like sarcasm, irony, and implied intentions.

By integrating these linguistic elements, NLP systems can achieve more accurate and nuanced language understanding.

<----------section---------->

### NLP vs Linguistics

While both NLP and Linguistics are related to language, their goals and approaches differ:

**Linguistics:**

*   Focused on the study of language as a scientific discipline.
*   Explores the structure, meaning, and use of language from a theoretical perspective.
*   May employ computational methods and tools as part of computational linguistics.

**NLP:**

*   Focused on providing computational capabilities that utilize human language.
*   Designs and implements algorithms to understand and generate human language.
*   Applies results from linguistics to develop practical applications, such as machine translation, chatbots, and sentiment analysis.

In essence, linguistics provides the theoretical foundations, while NLP focuses on practical applications. NLP can be viewed as an engineering discipline that leverages linguistic knowledge to solve real-world problems.

<----------section---------->

## Applications of Natural Language Processing

NLP has a wide range of applications across various domains, transforming how we interact with technology and process information.

### NLP Killer Applications

Key applications of NLP include:

*   **Language translation:** Automatically translating text or speech between languages.
*   **Email smart filtering:** Classifying and filtering emails based on content and sender.
*   **Smart assistant:** Virtual assistants like Siri, Alexa, and Google Assistant that respond to voice commands and perform tasks.
*   **Sentiment analysis:** Determining the emotional tone or attitude expressed in a piece of text.
*   **Document analysis:** Extracting key information and insights from large volumes of text.
*   **Chatbots:** Automated conversational agents that interact with users to provide information or assistance.
*   **Semantic searches:** Search engines that understand the meaning behind search queries.
*   **Automatic summarization:** Generating concise summaries of longer documents.

These applications demonstrate NLP's versatility and its potential to improve efficiency and user experience in various settings.

<----------section---------->

### Applications by Business Sector

NLP's impact extends across numerous business sectors, each with unique applications:

*   **Healthcare:**
    *   Process and interpret patient data, including medical records, to assist in diagnosis, treatment plans, and patient care. Enhances accuracy and efficiency in healthcare delivery.
    *   Extract information from unstructured data, such as doctors' notes and research papers, to identify patterns and insights. Improves research and clinical decision-making.
*   **Finance:**
    *   Analyze market sentiment, managing risk, detecting fraudulent activities. Provides timely insights for financial institutions.
    *   Generate insights from financial reports and news, to identify trends and predict market movements. Automates analysis of complex financial data.
*   **E-commerce and Retail:**
    *   Personalized recommendations, improved search functionalities, and customer service chatbots. Boosts sales and enhances customer satisfaction.
    *   Sentiment analysis to gauge customer satisfaction and market trends. Enables businesses to adapt to changing customer preferences.
*   **Legal:**
    *   Automate document analysis, aiding in legal research. Improves the efficiency of legal professionals by automating document review.
    *   Streamlining the review process for contracts and legal documentation. Reduces the time and cost associated with legal processes.
*   **Customer Service:**
    *   Automate responses, guide users, and analyze feedback, improving efficiency. Enhances customer service by providing timely and relevant assistance.
*   **Education:**
    *   Automatic grading, provision of learning tools. Improves the efficiency of educators and provides personalized learning experiences.
    *   Summarization and generation of educational materials. Creates tailored learning resources and simplifies complex topics.
*   **Automotive:**
    *   Intelligent navigation systems and voice-activated controls. Enhances the driving experience by enabling hands-free control and navigation.
*   **Technology:**
    *   Assists in software development by generating code snippets and completing code. Increases developer productivity by automating routine tasks.
    *   Enhances code quality through automated reviews and suggestions. Improves software reliability and reduces errors.
*   **Media and Entertainment:**
    *   Assist in generating scripts, articles, and creative writing. Enhances creativity and automates content creation processes.
    *   Enhance user engagement with interactive storytelling and personalized media experiences. Provides immersive and personalized content for users.

These applications highlight NLP's transformative potential across diverse industries.

<----------section---------->

### Many Other Applications…

NLP is pervasive in modern technology, with applications that are often unnoticed yet profoundly impactful.

A search engine can provide more meaningful results if it indexes web pages or document archives in a way that takes into account the meaning of natural language text. Autocomplete uses NLP to complete your thought and is common among search engines and mobile phone keyboards. Many word processors, browser plugins, and text editors have spelling correctors, grammar checkers, concordance composers, and most recently, style coaches. Some dialogue engines (chatbots) use natural language search to find a response to their conversation partner’s message.

NLP pipelines that generate (compose) text can be used not only to compose short replies in chatbots and virtual assistants, but also to assemble much longer passages of text. The Associated Press uses NLP “robot journalists” to write entire financial news articles and sporting event reports. Bots can compose weather forecasts that sound a lot like what your hometown weather person might say, perhaps because human meteorologists use word processors with NLP features to draft scripts.

NLP spam filters in early email programs helped email overtake telephone and fax communication channels in the '90s. And the spam filters have retained their edge in the cat and mouse game between spam filters and spam generators for email, but may be losing in other environments like social networks. An estimated 20% of the tweets.
*From the Additional Context*: NLP pipelines that generate text can be used not only to compose short replies in chatbots and virtual assistants but also to assemble much longer passages of text. The Associated Press uses NLP "robot journalists" to write entire financial news articles and sporting event reports. NLP spam filters in early email programs helped email overtake telephone and fax communication channels in the '90s. And some teams use NLP to automate and personalize e-mails between teammates or communicate with job applicants. The spam filters have retained their edge in the cat-and-mouse game between spam filters and spam generators for email but may be losing in other environments like social networks. An estimated 20% of the tweets about the 2016 US presidential election were composed by chatbots.

The following table categorizes NLP applications:

| Category        | Application                                       |
|-----------------|---------------------------------------------------|
| Search          | Web Documents, Autocomplete                       |
| Editing         | Spelling, Grammar, Style                           |
| Dialog          | Chatbot, Assistant, Scheduling                     |
| Writing         | Index, Concordance, Table of contents              |
| Email           | Spam filter, Classification, Prioritization          |
| Text mining     | Summarization, Knowledge extraction, Medical diagnoses |
| Law             | Legal inference, Precedent search, Subpoena classification |
| News            | Event detection, Fact checking, Headline composition  |
| Attribution     | Plagiarism detection, Literary forensics, Style coaching |
| Sentiment analysis | Community morale monitoring, Product review triage, Customer care |
| Behavior prediction | Finance, Election forecasting, Marketing           |
| Creative writing | Movie scripts, Poetry, Song lyrics                  |

<----------section---------->

### Hype Cycle

The Gartner Hype Cycle for Emerging Technologies (2023) positions NLP and related technologies within the innovation lifecycle, including:

*   API-Centric SaaS
*   Generative AI
*   AI TRiSM
*   WebAssembly (Wasm)
*   Federated Machine Learning
*   Industry Cloud Platforms
*   Internal Developer Portal
*   Cloud Sustainability
*   Homomorphic Encryption
*   Value Stream Management Platforms
*   Reinforcement Learning
*   Software Engineering
*   Cloud Development Environments
*   Graph Data Science
*   AI Simulation
*   Causal AI
*   Postquantum Cryptography
*   Neuro-Symbolic AI
*   Augmented FinOps
*   Generative Cybersecurity AI
*   Cybersecurity
*   Mesh Architecture

The cycle depicts stages from Innovation Trigger to Peak of Inflated Expectations, Trough of Disillusionment, Slope of Enlightenment, and finally, Plateau of Productivity. The estimated time to reach the plateau varies for each technology. Understanding the Gartner Hype Cycle helps to assess the maturity and potential of NLP and its related technologies.

<----------section---------->

### NLP Market

NLP is a promising career option.

*   Growing demand for NLP applications.
*   Projected employment growth of 22% between 2020 and 2030.

The NLP market global forecast in USD Billions is projected to increase from 18.9 in 2023 to 61.8 in 2028.

*   North America
*   Europe
*   Asia Pacific
*   Middle East & Africa
*   Latin America

This data underscores the significant economic opportunities and potential career paths within the field of NLP.

<----------section---------->

## History of NLP

### First Steps of NLP

NLP has had a history of ups and downs.

*   Influenced by the growth of computational resources and changes in approaches.

**1950's and 1960's**

*   The first application that sparked interest in NLP was machine translation.
*   The first machine translation systems used dictionary lookup and basic word order rules to produce translations.
*   The 1950s saw a lot of excitement: researchers predicted that machine translation can be solved in 3 years or so.

The early focus on machine translation was driven by Cold War-era needs, but initial systems were overly simplistic and yielded limited results.

<----------section---------->

### Machine Translation in 50s

**Example:**

Given:

```
Dictionary: Red -> Rosso
            House -> Casa
```

Translate:

```
The red house -> Il rosso casa   (incorrect)
```

But it should be:

```
La casa rossa (correct)
```

Dictionary lookup alone is insufficient.

This example illustrates the limitations of early machine translation systems that relied solely on dictionary lookup and basic rules, without considering grammatical structure and contextual nuances.

### How to deal with language ambiguity?

### Generative Grammars

**1957: Chomsky’s Generative Grammar**

*   A system of rules for generating all possible sentences in a language.
*   Enabled prediction of grammatical correctness.
*   Understanding of language structure.
*   Influenced research in machine translation.

Noam Chomsky's work provided a theoretical framework for understanding the structure of language and influenced NLP research by offering a way to formally represent grammatical rules.

<----------section---------->

**1966: The Reality Check**

*   Early translation systems fell short in effectiveness.
*   Limited by their inability to handle the ambiguity and complexity of natural language.

Despite initial optimism, early machine translation systems failed to meet expectations due to their inability to deal with the nuances of natural language.

### ALPAC Report

**Automatic Language Processing Advisory Committee**

*   Established to assess advancements in computational linguistics.
*   The 1966 ALPAC report recommended halting research into machine translation.
*   Shift focus from developing end-to-end machine translation systems to enhancing tools that assist human translators.
*   It significantly impacted NLP and AI research, contributing to the first AI winter.

[https://www.mt-archive.net/50/ALPAC-1966.pdf](https://www.mt-archive.net/50/ALPAC-1966.pdf)

The ALPAC report led to a significant reduction in funding for machine translation research and marked a period of disillusionment with AI in general.

<----------section---------->

### ELIZA

A pioneering conversational agent.

*   Created by Joseph Weizenbaum in the 1960s.
*   Designed to simulate a conversation between a psychotherapist and a patient.

**Features and Limitations:**

*   Demonstrated the potential of computer-based conversation.
*   Utilized pattern matching and substitution to generate responses.
*   Limited in handling complex conversations.
*   Could not maintain context beyond a few exchanges.
*   Often produced irrelevant or repetitive responses.

[https://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm](https://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm)

ELIZA, while simple, demonstrated the possibility of creating conversational agents, even with limited understanding of natural language.

<----------section---------->

### The Turing Test

"I propose to consider the question: can machines think? ... We can only see a short distance ahead, but we can see plenty there that needs to be done" - *Alan Turing, Computing Machinery and Intelligence, 1950*

**Turing Test aka The Imitation game:**

*   A human, a computer, an interrogator in a different room communicate via written messages.
*   The interrogator should classify the human and the machine.

The Turing Test provides a benchmark for evaluating a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.

### The Turing Test

**Capabilities for passing the Turing Test**

*   Natural Language Understanding to interpret user input
*   Knowledge Representation to draw on relevant information
*   Automated Reasoning to generate appropriate and logical responses
*   Natural Language Generation to produce human-like textual responses
*   Context Management to maintain and utilize context across multiple exchanges in a conversation
*   Adaptability and Learning to adapt responses based on user behavior and feedback

Achieving success in the Turing Test requires advanced capabilities in various aspects of NLP and AI.

<----------section---------->

### The Turing Test

**Successes with Turing test**

*   A (controversial) success in 2014: a chatbot mimicking the answer of a 13 years old boy
*   Since then, other (controversial) successes

**Limitations of Turing Test**

*   Not reproducible
*   Is emulating humans necessary for achieving intelligence?
*   Many AI researchers have shifted focus to other benchmarks
*   Less commonly used today

Despite some successes, the Turing Test has limitations as a measure of true intelligence, and many researchers now focus on other benchmarks.

### Raise of Symbolic Approaches

**1970's and 1980's:**

*   Programmers started creating structured representations of real-world information for computer understanding (ontologies)
*   Complex rule-based systems were developed for various NLP tasks, including parsing, morphology, semantics, reference, ...

**Main applications were:**

*   **Expert Systems:** mimicked human expertise in specific domains
*   **Information Retrieval:** enhanced search and data extraction

During this period, NLP research shifted towards symbolic approaches that emphasized knowledge representation and rule-based systems.

<----------section---------->

**Main limitations were:**

*   **Flexibility:** challenges in adapting to new or ambiguous contexts
*   **Scalability:** difficulty handling large-scale or diverse data

However, symbolic approaches proved brittle and difficult to scale, leading to the next major shift in NLP research.

### Statistical Revolution

**1990's:**

*   The computing power increased substantially
*   Statistical models with simple representations started to outperform complex hand-coded linguistic rules
*   Learn patterns from data
*   Can handle variations and complexities in natural language
*   Large corpora became essential
*   Long Short-Term Memory (LSTM) networks was invented by Hochreiter and Schmidhuber in 1997

The advent of statistical models and machine learning marked a turning point in NLP, enabling systems to learn patterns from data and handle variations in language more effectively.

*"Whenever I fire a linguist, our machine translation performance improves" - Fred Jelinek, IBM*

This quote reflects the shift from rule-based to data-driven approaches in NLP.

<----------section---------->

### Advances in NLP

**2000's**

*   Increased Use of Neural Networks
*   Introduction of Word Embeddings
    *   Words are represented as dense vectors of numbers
    *   Words with similar meanings are associated with similar vectors
    *   Early algorithms struggled to efficiently learn these representations

The increased availability of data and computing power fueled the adoption of neural networks and word embeddings, significantly improving NLP performance.

**2006: launch of Google Translate**

*   The first commercially successful NLP system
*   Utilized statistical models to automatically translate documents

Google Translate demonstrated the practical potential of statistical machine translation and marked a significant milestone in NLP history.

<----------section---------->

### Deep Learning Era

**2010's:**

*   LSTM and CNN became widely adopted for NLP
*   The availability of large text corpora enabled the training of increasingly complex models

The deep learning era brought significant advancements in NLP, with LSTMs and CNNs proving highly effective for various tasks.

**Word2Vec (2013):**

*   Efficient Estimation of Word Representations in Vector Space
*   The first algorithm to efficiently learn word embeddings
*   Enables semantic operations with word vector
*   Paved the way for more advanced models such as GloVe, fastText, ELMo, BERT, COLBERT, GPT, ...

Word2Vec revolutionized NLP by enabling the efficient learning of word embeddings, which capture semantic relationships between words.

<----------section---------->

### Deep Learning Era

**Sequence-to-Sequence Models (2014):**

*   Introduction of the encoder-decoder architecture:
    *   **Encoder:** Encodes the input into a context vector
    *   **Decoder:** Decodes the output from the context vector
*   Useful for automatic translation, question answering, text summarization, text generation, ...

**Example:**

```
The red house -> Context vector [0.3, 0.6, -0.2, ..., 0.1] -> La casa rossa
```

Sequence-to-sequence models provided a powerful framework for tasks such as machine translation, enabling the translation of entire sequences of words.

<----------section---------->

### Virtual Assistants

A Virtual Assistant performs a range of tasks or services based on user input in natural language.

Many VA were launched in 2010's:

*   2011: Siri launched by Apple on iOS devices
*   2014: Cortana introduced by Microsoft for Windows Phone
*   2014: Alexa launched by Amazon with the Echo, pioneering voice-controlled smart home
*   2015: Google Assistant introduced, integrating voice interaction with Android and Google Home

These virtual assistants brought NLP to the mainstream, enabling voice-controlled interactions and a wide range of automated tasks.

<----------section---------->

### Deep Learning Era

**Transformer (2017):**

*   **Attention Is All You Need**
*   Integration of attention mechanisms
*   Allows a greater passage of information between the decoder and the encoder
*   Defined and adopted by Google for the translator
*   It remains the dominant architecture in NLP today

*Diagram of Transformer Architecture*

The Transformer architecture, with its attention mechanism, marked a significant breakthrough in NLP, enabling models to capture long-range dependencies and achieve state-of-the-art performance.

<----------section---------->

### Large Language Models

After transformers, the next step was scaling...

*   LLM leverage extensive data and computational power to understand and generate human-like text

*   List of LLMs: GPT-4, ChatGPT, InstructGPT, Codex, Flan-PaLM, LLaMA, BLOOM, OPT, UL2, PaLM, Gopher, Chinchilla, Titan, Jurassic-1, Ernie 3.0, PanGu, etc.*

LLMs represent the latest advancement in NLP, leveraging massive datasets and computational power to achieve unprecedented levels of language understanding and generation.

### LLM Applications

*   **Text Generation:** Producing articles, stories, and creative writing
*   **Machine Translation:** Translating between languages
*   **Chatbots:** Engaging in human-like conversations for customer support and interaction
*   **Code Generation:** Generating and suggesting code snippets, completing code, and assisting with programming tasks
*   **Question Answering:** Providing answers based on a given context or database
*   **Text Summarization:** Condensing long documents into concise summaries
*   **Writing Assistance:** Generating and completing text, improving grammar, and enhancing style

LLMs have enabled significant advancements in a wide array of NLP applications.

<----------section---------->

### Multimodal LLM

Integrate and process multiple types of data

*   **Image-to-Text:** generating descriptive text from images (CLIP)
*   **Text-to-Image:** creating images based on textual descriptions (DALL-E)
*   **Audio-to-Text:** converting spoken language into written text (Whisper)
*   **Text-to-Audio:** composing or generating audio, such as music, from textual descriptions (Jukebox)

Multimodal LLMs are expanding the boundaries of NLP by integrating and processing multiple data types, enabling richer and more versatile applications.

<----------section---------->

### Multimodal LLM

Integrate and process multiple types of data

*   **Video-to-Text:** Generating textual descriptions or summaries from video content (VideoBERT)
*   **Text-to-Video:** Video content from textual descriptions (Sora)

Example Prompt: Photorealistic closeup video of two pirate ships battling each other as they sail inside a cup of coffee

Multimodal models are extending their influence into processing and generating video content, leading to advancements in video understanding and generation capabilities.

## References

*   Natural Language Processing IN ACTION: Understanding, analyzing, and generating text with Python, Chapter 1

This reference provides a starting point for further exploration of NLP concepts and techniques.

<----------section---------->

## Enhanced Text: Natural Language Processing and Large Language Models - Representing Text

**Course Information:**

This material is from Lesson 2, "Representing Text," of a Master's level course (Corso di Laurea Magistrale) in Computer Engineering (Ingegneria Informatica). The course focuses on Natural Language Processing (NLP) and Large Language Models (LLMs). The instructors are Nicola Capuano and Antonio Greco from the DIEM (Dipartimento di Ingegneria dell'Informazione ed Elettrica e Matematica dell'Ingegneria) at the University of Salerno.

**Overview:**

This lesson covers fundamental techniques for representing text data in a format suitable for NLP tasks. The core topics include:

*   Tokenization
*   Bag of Words Representation
*   Token Normalization
*   Stemming and Lemmatization
*   Part of Speech Tagging
*   Introduction to spaCy

<----------section---------->

**Preparing the Development Environment:**

For most practical exercises in this lesson, Jupyter notebooks are used. This environment allows for interactive coding and immediate feedback.

**Jupyter Notebook Setup:**

1.  **Install the Jupyter Extension for Visual Studio Code (VS Code):** This extension provides a seamless Jupyter notebook experience within the VS Code editor.
2.  **Install Jupyter:** Use the `pip` package installer to install Jupyter: `pip install jupyter`
3.  **Create and Activate a Virtual Environment (Recommended):** A virtual environment isolates project dependencies, preventing conflicts with other Python projects.
    *   Create: `python -m venv .env`
    *   Activate: `source .env/bin/activate` (on Linux/macOS) or `.env\Scripts\activate` (on Windows)
4.  **Alternative: Google Colaboratory (Colab):** Colab is a free, cloud-based Jupyter notebook environment that requires no local installation: <https://colab.research.google.com/>

**Additional Required Python Packages:**

The following packages are needed for specific sections of this lesson. Install them using `pip`:

*   `pip install numpy pandas`

`numpy` is a fundamental package for numerical computation in Python, offering powerful array and matrix operations. `pandas` provides data structures like DataFrames for easy manipulation and analysis of tabular data.

<----------section---------->

**Text Segmentation: Breaking Down Text:**

Text segmentation is the process of dividing text into smaller, meaningful units. It forms a crucial initial step in many NLP pipelines. Different levels of segmentation are possible:

*   **Paragraph Segmentation:** Dividing a document into distinct paragraphs. This can be based on whitespace or semantic cues.
*   **Sentence Segmentation:** Dividing a paragraph into individual sentences. Sentence boundary detection can be complex due to the presence of abbreviations and other punctuation.
*   **Word Segmentation:** Dividing a sentence into individual words. This involves identifying word boundaries, which can be challenging in languages without explicit whitespace.

**Tokenization: A Specialized Form of Text Segmentation:**

Tokenization is a specific type of text segmentation. The primary goal of tokenization is to break down text into individual units called tokens. These tokens can be words, punctuation marks, or other meaningful elements.

<----------section---------->

**Understanding Tokens:**

A token is a single, meaningful unit of text that is treated as a distinct element in NLP processing. The nature of tokens can vary based on the specific application. Common types of tokens include:

*   **Words:** The most common type of token, representing individual words in the text.
*   **Punctuation Marks:** Symbols like periods, commas, question marks, etc., which play a role in sentence structure and meaning.
*   **Emojis:** Visual symbols representing emotions or concepts. Emojis are increasingly important in modern communication.
*   **Numbers:** Digits and numerical expressions, often representing quantities or identifiers.
*   **Sub-words:** Smaller units within words, such as prefixes (e.g., "re-", "pre-") or suffixes (e.g., "-ing", "-ed"). These can carry meaning and be useful for understanding word formation and relationships.
*   **Phrases:** Multi-word expressions that are treated as single units (e.g., "ice cream," "New York"). Identifying these phrases can improve the accuracy of NLP models.

<----------section---------->

**Tokenizers: Using Whitespace as a Delimiter (Limitations):**

A simple tokenizer might use whitespace (spaces, tabs, newlines) as delimiters to separate words.

*   **Basic Idea:** Split the text string at each whitespace character.

**Limitations of Whitespace Tokenization:**

*   **Languages Without Whitespace:** This approach is unsuitable for languages with a continuous orthographic system, where words are not separated by spaces (e.g., Chinese, Japanese, Thai). These languages require more sophisticated segmentation techniques.
*   **Punctuation Issues:** It often incorrectly attaches punctuation to words.

**Python Example (Basic Whitespace Splitting):**

```python
sentence = "Leonardo da Vinci began painting the Mona Lisa at the age of 51."
token_seq = sentence.split()
print(token_seq)
# Output: ['Leonardo', 'da', 'Vinci', 'began', 'painting', 'the', 'Mona', 'Lisa', 'at', 'the', 'age', 'of', '51.']
```

The output shows that "51." is considered a single token, which is not ideal. A more robust tokenizer should separate "51" and ".". The following sections address these challenges and more.

<----------section---------->

**Bag of Words (BoW) Representation: Turning Text into Numbers**

The Bag of Words (BoW) model is a way to represent text as numerical vectors. It disregards grammar and word order, focusing solely on the frequency of words.

**One-Hot Vectors: A Building Block:**

*   **Vocabulary:** The first step is to create a vocabulary, which is a list of all the unique tokens (words) that are relevant to the analysis.
*   **Representation:** Each word in the vocabulary is assigned a unique index.
*   **One-Hot Encoding:** Each word in a text is then represented by a one-hot vector. This vector has a length equal to the size of the vocabulary. All elements are 0, except for the element at the index corresponding to the word, which is set to 1.

**Python Example:**

```python
import numpy as np
import pandas as pd

token_seq = ['Leonardo', 'da', 'Vinci', 'began', 'painting', 'the', 'Mona', 'Lisa', 'at', 'the', 'age', 'of', '51.']
vocab = sorted(set(token_seq)) #Unique Tokens
onehot_vectors = np.zeros((len(token_seq), len(vocab)), int) #Initialize the one-hot matrix

for i, word in enumerate(token_seq):
    onehot_vectors[i, vocab.index(word)] = 1 #Assign value 1 at its index

df = pd.DataFrame(onehot_vectors, columns=vocab, index=token_seq) #Create dataframe from vectors
print(df)
```

<----------section---------->

**One-Hot Vectors: Advantages and Disadvantages**

**Positive Features:**

*   **No Information Loss (Theoretically):** You can reconstruct the original list of tokens from a table of one-hot vectors, assuming you know the vocabulary and the order.

**Negative Features:**

*   **Sparsity:** One-hot vectors are extremely sparse. Almost all elements are zero, which results in a large table even for short texts. This is computationally inefficient.
*   **Vocabulary Size:** A typical language vocabulary contains at least 20,000 common words. This number can increase to millions if you include word variations (conjugations, plurals) and proper nouns (names of people, places, organizations).
*   **Storage Requirements:** The massive vocabulary size and sparsity lead to enormous storage requirements.

**Storage Example (Illustrating Impracticality):**

Let's consider a scenario:

*   Vocabulary size: 1 million tokens (10<sup>6</sup>).
*   Library size: 3,000 short books, each with 3,500 sentences, and 15 words per sentence.

1.  **Total Tokens:** 15 words/sentence * 3,500 sentences/book * 3,000 books = 157,500,000 tokens

2.  **Bits per Token:** Representing a one-hot vector for a million-token vocabulary requires 1 million bits.

3.  **Total Bits:** 10<sup>6</sup> bits/token * 157,500,000 tokens = 157.5 x 10<sup>12</sup> bits

4.  **Bytes and Terabytes:** Converting to bytes and terabytes:

    *   157.5 x 10<sup>12</sup> bits / (8 bits/byte) = 19.6875 x 10<sup>12</sup> bytes
    *   19.6875 x 10<sup>12</sup> bytes / (1024<sup>4</sup> bytes/TB) ≈ 17.9 TB

This calculation demonstrates that storing one-hot vectors for even a relatively small collection of texts can become prohibitively expensive.

<----------section---------->

**Bag-of-Words (BoW): A Simplified Representation**

The Bag-of-Words (BoW) model provides a more compact representation than individual one-hot vectors.

*   **Creation:** A BoW vector is obtained by summing all the one-hot vectors for the words in a document or sentence.
*   **Unit:** One BoW vector is created for each sentence or short document.
*   **Compression:** This transformation compresses a document down to a single vector that represents its essence. The vector elements are the frequencies of each term from the vocabulary for each document.
*   **Lossy Transformation:** A significant drawback of BoW is that it is a lossy transformation. You cannot reconstruct the original text from a BoW vector because word order is discarded.

**Binary BoW:**

A variation of BoW is the Binary BoW, where each word's presence is marked as 1 or 0, regardless of its frequency. The vector indicates which words are present in a document but not how many times each word occurs.

<----------section---------->

**Binary BoW Example: Building a Vocabulary and Vectors**

This example demonstrates how to generate a vocabulary from a corpus of text and then create Binary BoW vectors for each sentence.

**Text Corpus:**

```python
sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]
```

**Generating the Vocabulary:**

```python
all_words = " ".join(sentences).split() #Concat all the sentences and split into individual words
vocab = sorted(set(all_words)) #Get a list of all unique sorted words
print(vocab)
```

**Output Vocabulary:**

```
['1452.', '51.', 'A', 'Grand', 'In', 'Italy,', 'Leonardo', 'Lisa', 'Mona', 'Slam', 'Tennis', 'The', 'Vinci', 'Vinci,', 'a', 'addition', 'age', 'also', 'are', 'as', 'at', 'began', 'being', 'best', 'born', 'court', 'da', 'engineer.', 'events', 'five', 'four', 'in', 'is', 'match', 'middle.', 'most', 'net', 'of', 'on', 'or', 'painter,', 'painting', 'played', 'prestigious', 'rectangular', 'sets.', 'skilled', 'tennis', 'tennis.', 'the', 'three', 'to', 'tournaments', 'typically', 'was', 'with']
```

<----------section---------->

**Binary BoW Example: Creating Binary Vectors**

Now, generating a BoW vector for each text...

```python
import numpy as np
import pandas as pd

sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))

bags = np.zeros((len(sentences), len(vocab)), int) #Initialize the BoW matrix

for i, sentence in enumerate(sentences): #Loop through all sentences
    for j, word in enumerate(sentence.split()): #Loop through words in the current sentence
        bags[i, vocab.index(word)] = 1 #Assign value 1 if the word is in the document, else keep it zero

df = pd.DataFrame(bags, columns=vocab) #Create dataframe
print(df.transpose()) #Transpose the dataframe for better viewability
```

This results in a DataFrame where each row represents a word in the vocabulary, and each column represents a sentence. A '1' indicates that the word is present in the sentence, and '0' indicates it is absent. This is done for display purposes only.

<----------section---------->

**Binary BoW: Analysis and Overlap**

*   **Limited Overlap:** As the output shows, there's often limited overlap in word usage between different sentences.
*   **Document Comparison:** This overlap, or lack thereof, can be used to compare documents and identify similar ones. This approach can allow document clustering by similarity in vector space.

**Bag-of-Words Overlap: Measuring Similarity**

*   **Estimating Similarity:** By measuring the overlap of words between two texts, we can get an estimate of their semantic similarity.

*   **Technique:** Use the dot product of the BoW vectors to quantify the overlap.

**Python Code:**

```python
import numpy as np
sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))

bags = np.zeros((len(sentences), len(vocab)), int)
for i, sentence in enumerate(sentences):
    for j, word in enumerate(sentence.split()):
        bags[i, vocab.index(word)] = 1

print(np.dot(bags[0], bags[2])) #comparing sentence 0 and 2
print(np.dot(bags[3], bags[5])) #comparing sentence 3 and 5
```

The `np.dot()` function calculates the dot product between the BoW vectors of different sentence pairs. A higher dot product indicates greater word overlap and thus greater similarity (according to this simple metric). This approach is useful for various NLP applications.

<----------section---------->

**Token Normalization: Refining the Tokenization Process**

Token normalization aims to improve the quality and consistency of tokens by addressing issues like punctuation, case, and stop words.

**Tokenizer Improvement:**

*   **Beyond Spaces:** Spaces are not the only delimiters of words. Punctuation and other special characters need to be considered.
*   **Other Delimiters:** Include tab (`\t`), newline (`\n`), return (`\r`), and various punctuation marks (commas, periods, quotes, etc.).

**Regular Expressions (Regex):**

Regular expressions can be used to improve tokenization by handling various delimiters and punctuation patterns.

**Python Code:**

```python
import re

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
print(token_seq)
# Output: ['Leonardo', 'was', 'born', 'in', 'Vinci', 'Italy', 'in', '1452']
```

The `re.split()` function splits the sentence based on a regular expression that matches one or more occurrences of hyphens, whitespace, periods, commas, semicolons, exclamation marks, or question marks.

<----------section---------->

**Complexities and NLP Libraries:**

While regular expressions can improve tokenization, they may not handle all cases perfectly.

*   **Example Sentences and Tokenization Challenges:**

    *   `The company’s revenue for 2023 was $1,234,567.89.`
    *   `The CEO of the U.N. (United Nations) gave a speech.`
    *   `It’s important to know the basics of A.I. (Artificial Intelligence).`
    *   `He didn’t realize the cost was $12,345.67.`
    *   `Her new book, ‘Intro to NLP (Natural Language Processing)’, is popular.`
    *   `The temperature in Washington, D.C. can reach 100°F in the summer.`

These sentences demonstrate issues like handling currency symbols, abbreviations, acronyms, and contractions, which can make tokenization complex.

**NLP Libraries to the Rescue:**

*   NLP libraries (like NLTK and spaCy) provide pre-built tokenizers that are more sophisticated and can handle many of these complexities.

<----------section---------->

**Case Folding: Normalizing Text Case**

Case folding consolidates multiple "spellings" of a word that differ only in capitalization under a single token. This is to treat two words as the same in the vocabulary, disregarding the use of upper or lower cases.

*   **Examples:** `Tennis → tennis`, `A → a`, `Leonardo → leonardo`.
*   Also known as Case Normalization.

**Advantages:**

*   **Improved Matching:** Improves text matching and recall in search engines.
*   **Reduces vocabulary size**: Reduces the size of the vocabulary needed in the model by consolidating duplicate representations.

**Disadvantages:**

*   **Loss of Distinction:** Loss of distinction between proper and common nouns.
*   **Meaning Alteration:** May alter the original meaning of the text. (e.g., `US → us`).

**Python Code:**

```python
import re

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
token_seq = [x.lower() for x in token_seq] # case folding

print(token_seq)
# Output: ['leonardo', 'was', 'born', 'in', 'vinci', 'italy', 'in', '1452']
```

<----------section---------->

**Case Folding: Considerations**

As the example shows, significant capitalization is normalized away, so approaches can be developed to maintain the utility of capitalization information.

*   **Selective Normalization:** One approach is to only normalize the capitalization of the first word in a sentence.
*   **Proper Noun Detection:** First detect proper nouns and then normalize only the remaining words (this involves Named Entity Recognition, which is a more advanced NLP task).

**Stop Words: Filtering Out Common Words**

Stop words are common words that occur with high frequency but carry little information about the meaning of a sentence. Removing these words can reduce noise and improve the efficiency of NLP models.

*   **Examples:** Articles (a, an, the), prepositions (in, on, at), conjunctions (and, but, or), forms of the verb "to be" (is, are, was, were).
*   **Purpose:** Stop word filtering reduces noise in the text and simplifies the processing, but removing a stop word could remove important information.

<----------section---------->

**Stop Words: Python Code and Disadvantages**

**Python Code:**

```python
stop_words = [
    "a", "about", "after", "all", "also", "an", "and", "any", "are", "as", "at", "be", "because",
    "been", "but", "by", "can", "co", "corp", "could", "for", "from", "had", "has", "have", "he",
    "her", "his", "if", "in", "inc", "into", "is", "it", "its", "last", "more", "most", "mr",
    "mrs", "ms", "mz", "no", "not", "of", "on", "one", "only", "or", "other", "out", "over", "s",
    "says", "she", "so", "some", "such", "than", "that", "the", "their", "there", "they", "this",
    "to", "up", "was", "we", "were", "when", "which", "who", "will", "with", "would"
]

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
token_seq = [x.lower() for x in token_seq] # case folding
token_seq = [x for x in token_seq if x not in stop_words] # remove stop words

print(token_seq)
# Output: ['leonardo', 'born', 'vinci', 'italy', '1452']
```

**Disadvantages:**

*   **Loss of Relational Information:** Stop words can provide important relational information, even if they carry little information on their own. For example, compare the impact of using these stop words in these sentences.

    *   `Mark reported to the CEO → Mark reported CEO`
    *   `Suzanne reported as the CEO to the board → Suzanne reported CEO board`

*Italian Stop Words:* The text provides a sample list of Italian stop words.

<----------section---------->

**Putting It All Together: Comprehensive Tokenization**

This example combines tokenization, punctuation removal, case folding, and stop word removal into a single function.

```python
import re
import numpy as np
import pandas as pd

stop_words = [
    "a", "about", "after", "all", "also", "an", "and", "any", "are", "as", "at", "be", "because",
    "been", "but", "by", "can", "co", "corp", "could", "for", "from", "had", "has", "have", "he",
    "her", "his", "if", "in", "inc", "into", "is", "it", "its", "last", "more", "most", "mr",
    "mrs", "ms", "mz", "no", "not", "of", "on", "one", "only", "or", "other", "out", "over", "s",
    "says", "she", "so", "some", "such", "than", "that", "the", "their", "there", "they", "this",
    "to", "up", "was", "we", "were", "when", "which", "who", "will", "with", "would"
]

sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

def tokenize(sentence):
    token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
    token_seq = [token for token in token_seq if token] # remove void tokens
    token_seq = [x.lower() for x in token_seq] # case folding
    token_seq = [x for x in token_seq if x not in stop_words] # remove stop words
    return token_seq

tok_sentences = [tokenize(sentence) for sentence in sentences] #tokenize all the sentences
all_tokens = [x for tokens in tok_sentences for x in tokens] #Flatten the list of tokens
vocab = sorted(set(all_tokens)) #Unique list of sorted tokens

bags = np.zeros((len(tok_sentences), len(vocab)), int) #Initialize the BoW matrix
for i, sentence in enumerate(tok_sentences): #Loop through sentences
    for j, word in enumerate(sentence): #Loop through words in the current sentence
        bags[i, vocab.index(word)] = 1 #Assign 1 to words found in each document

df = pd.DataFrame(bags, columns=vocab) #Create dataframe
print(df.transpose())
```

<----------section---------->

**Resulting DataFrame:**

The code produces a DataFrame showing the Binary BoW representation of each sentence after normalization. The output from the print statement follows.

```
          0    1    2    3    4    5
1452     0    1    0    0    0    0
51       1    0    0    0    0    0
addition 0    0    1    0    0    0
age      1    0    0    0    0    0
began    1    0    0    0    0    0
being    0    0    1    0    0    0
best     0    0    0    0    0    1
born     0    1    0    0    0    0
court    0    0    0    1    0    0
da       1    0    1    0    0    0
engineer 0    0    1    0    0    0
events   0    0    0    0    1    0
five     0    0    0    0    0    1
four     0    0    0    0    1    0
grand    0    0    0    0    1    0
italy    0    1    0    0    0    0
leonardo 1    1    1    0    0    0
lisa     1    0    0    0    0    0
match    0    0    0    0    0    1
middle   0    0    0    1    0    0
mona     1    0    0    0    0    0
net      0    0    0    1    0    0
painter  0    0    1    0    0    0
painting 1    0    0    0    0    0
played   0    0    0    1    0    1
prestigious 0    0    0    0    1    0
rectangular 0    0    0    1    0    0
sets     0    0    0    0    0    1
skilled  0    0    1    0    0    0
slam     0    0    0    0    1    0
tennis   0    0    0    0    1    1
three    0    0    0    0    0    1
tournaments 0    0    0    0    1    0
typically 0    0    0    0    0    1
vinci    1    1    0    0    0    0
```

<----------section---------->

**Using NLTK (Natural Language Toolkit): A Popular NLP Library**

NLTK is a widely used Python library for NLP. It provides a range of tools and resources for various NLP tasks, including tokenization, stemming, lemmatization, and more.

*   **Installation:** `pip install nltk`
*   **Resources:** Includes refined tokenizers and stop-word lists for many languages.

**NLTK Tokenization:**

```python
import nltk

nltk.download('punkt') # download the Punkt tokenizer models
text = "Good muffins cost $3.88\nin New York. Please buy me two of them.\n\nThanks."

print(nltk.tokenize.word_tokenize(text)) # word tokenization
print(nltk.tokenize.sent_tokenize(text)) # sentence tokenization
```

**Explanation**

*   This snippet downloads the "punkt" tokenizer models, which are pre-trained models for sentence boundary detection.
*   `word_tokenize()`: Breaks the text into individual words and punctuation marks.
*   `sent_tokenize()`: Splits the text into individual sentences.

<----------section---------->

**NLTK Stop Word Handling:**

NLTK includes extended stop-word lists for many languages.

```python
import nltk

nltk.download('stopwords') # download the stop words corpus
text = "This is an example sentence demonstrating how to remove stop words using NLTK."

tokens = nltk.tokenize.word_tokenize(text) #Tokenize the sentence
stop_words = set(nltk.corpus.stopwords.words('english')) #Get the list of stop words
filtered_tokens = [x for x in tokens if x not in stop_words] #Loop through tokens, only keep non stop words

print("Original Tokens:", tokens)
print("Filtered Tokens:", filtered_tokens)
```

**Explanation:**

*   `nltk.corpus.stopwords.words('english')`: Retrieves the list of English stop words.
*   The code then filters the tokens to remove any that are present in the stop-word list.

<----------section---------->

**Stemming and Lemmatization: Reducing Words to Their Root Form**

Stemming and lemmatization are techniques used to reduce words to their root form, which helps generalize the vocabulary and improve information retrieval.

**Stemming:**

Identifies a common stem among various forms of a word.

*   Example: "Housing" and "houses" share the same stem: "house".
*   Function: Removes suffixes to combine words with similar meanings under the same token (stem).
*   A stem isn't required to be a properly spelled word (e.g., "Relational," "Relate," "Relating" all stemmed to "Relat").

**Benefits:**

*   Helps generalize your vocabulary.
*   Important for information retrieval (improves recall).

<----------section---------->

**Stemming: A Naive Stemmer Example**

This demonstrates a basic stemming function that removes common suffixes.

```python
def simple_stemmer(word):
    suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

words = ["running", "happily", "stopped", "curious", "cries", "effective", "runs", "management"]
print({simple_stemmer(word) for word in words})
# Output: {'cur', 'runn', 'stopp', 'run', 'effect', 'manage', 'happi', 'cr'}
```

**Limitations:**

*   This is a very basic example that doesn't handle exceptions, multiple suffixes, or words that require more complex modifications.
*   More accurate stemmers are available in NLP libraries.

<----------section---------->

**Porter Stemmer: A More Advanced Algorithm**

The Porter Stemmer is a widely used stemming algorithm that applies a series of rules to remove suffixes from words.

**Steps:**

1.  **Step 1a:** Remove "s" and "es" endings (e.g., "cats" → "cat," "buses" → "bus").
2.  **Step 1b:** Remove "ed," "ing," and "at" endings (e.g., "hoped" → "hope," "running" → "run").
3.  **Step 1c:** Change "y" to "i" if preceded by a consonant (e.g., "happy" → "happi," "cry" → "cri").
4.  **Step 2:** Remove "nounifying" endings such as "ational," "tional," "ence," and "able" (e.g., "relational" → "relate," "dependence" → "depend").
5.  **Step 3:** Remove adjective endings such as "icate," "ful," and "alize" (e.g., "duplicate" → "duplic," "hopeful" → "hope").
6.  **Step 4:** Remove adjective and noun endings such as "ive," "ible," "ent," and "ism" (e.g., "effective" → "effect," "responsible" → "respons").
7.  **Step 5a:** Remove stubborn "e" endings (e.g., "probate" → "probat," "rate" → "rat").
8.  **Step 5b:** Reduce trailing double consonants ending in "l" to a single "l" (e.g., "controlling" → "controll" → "control," "rolling" → "roll" → "rol").

<----------section---------->

**Porter Stemmer: Python Example**

```python
import nltk

texts = [
    "I love machine learning.",
    "Deep learning is a subset of machine learning.",
    "Natural language processing is fun.",
    "Machine learning can be used for various tasks."
]

stemmed_texts = []
stemmer = nltk.stem.PorterStemmer() # Initialize the Porter Stemmer

for text in texts:
    tokens = nltk.tokenize.word_tokenize(text.lower()) #Lower and tokenize each document
    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalpha()] #Stem each token, exclude punctuation
    stemmed_texts.append(' '.join(stemmed_tokens)) #Rejoin tokens to create the stemmed document

for text in stemmed_texts:
    print(text)
```

**Output:**

```
i love machin learn
deep learn is a subset of machin learn
natur languag process is fun
machin learn can be use for variou task
```

<----------section---------->

**Snowball Project: Multi-Lingual Stemming**

The Snowball project provides stemming algorithms for several languages, offering a more comprehensive stemming solution.

*   **Website:** <https://snowballstem.org/>
*   **Italian Stemming Examples:** Provides a sample table of Italian words and their stemmed forms.
*   **Integration with NLTK:** Snowball stemmers are supported in NLTK.

**Italian Stemming Example (Illustrative):**

| Input           | Stem     |
|-----------------|----------|
| abbandonata     | abbandon |
| abbandonate     | abbandon |
| abbandonati     | abbandon |
| abbandonato     | abbandon |
| abbandonava     | abbandon |
| abbandonera     | abbandon |
| abbandoneranno  | abbandon |
| abbandonerà      | abbandon |
| abbandono        | abbandon |
| abbandono        | abbandon |
| abbaruffato      | abbaruff |
| abbassamento     | abbass   |

**NLTK Code (Example)**

```python
import nltk
nltk.download('punkt')
stemmer = nltk.stem.PorterStemmer()
```

The code downloads the "punkt" module if necessary and initializes an object to the PorterStemmer algorithm.

<----------section---------->

**Lemmatization: Determining Dictionary Forms**

Lemmatization determines the dictionary form (lemma) of a word, which is often the uninflected form of a word. This
<----------section---------->

## Natural Language Processing and Large Language Models

This lesson, designed for a Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering), delves into the fundamental concepts of Natural Language Processing (NLP) and their application in Large Language Models (LLMs). The material is presented by Nicola Capuano and Antonio Greco from DIEM – University of Salerno.

## Outline

This lesson will cover the following key topics:

*   **Term Frequency (TF):** Understanding how often words appear in a document and its significance.
*   **Vector Space Model (VSM):** Representing text as vectors in a multi-dimensional space.
*   **TF-IDF (Term Frequency-Inverse Document Frequency):** A weighting scheme that refines term frequency by considering the rarity of words across the entire corpus.
*   **Building a Search Engine:** Applying these concepts to create a basic search engine.

<----------section---------->

## Term Frequency

Term Frequency (TF) is a fundamental concept in NLP that quantifies how often a term appears within a document. It's a simple yet powerful way to gauge the importance of a word in that specific document.

### Bag of Words (BoW)

The Bag of Words model is a simplified representation of text used in NLP. It treats a document as an unordered collection of words, disregarding grammar and word order but keeping track of word counts. It is a type of vector space model.

*   **One-hot Encoding:** Each word in the text is first represented as a one-hot vector, where the vector's length equals the vocabulary size. The index corresponding to the word is set to 1, while all other indices are set to 0.

*   **Combining One-Hot Vectors:** After encoding, the one-hot vectors are combined to represent the entire document.
    *   **Binary BoW:** One-hot vectors are combined using the OR operation. This results in a binary vector indicating the presence (1) or absence (0) of each word in the document, without regard to frequency.
    *   **Standard BoW:** One-hot vectors are summed element-wise. This creates a vector where each element represents the number of times the corresponding word appears in the document. This count is the Term Frequency.

*   **Term Frequency (TF):** The number of times each word occurs in the text. It's a numerical representation of the importance of each word in the document, according to its frequency.

**Assumption:** The core assumption of TF is that the more times a word appears in a document, the more relevant or important it is to that document's content. This is a straightforward heuristic, but as we'll see later, it has limitations.

<----------section---------->

### Calculating TF

The following Python code demonstrates how to calculate Term Frequency using the `spaCy` library.

```python
# Example sentence
sentence = "The faster Harry got to the store, the faster Harry, the faster, would get home."

import spacy
nlp = spacy.load("en_core_web_sm") # load the language model
doc = nlp(sentence)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct]

tokens
```

**Explanation:**

1.  **Tokenization:** The sentence is processed using `spaCy` to extract individual tokens (words).
2.  **Lowercasing:**  Tokens are converted to lowercase to ensure consistency.
3.  **Stop Word Removal:** Common words (e.g., "the," "is," "and") that don't carry significant meaning are removed.
4.  **Punctuation Removal:** Punctuation marks are removed.

**Output:**

```
{'faster', 'harry', 'got', 'store', 'faster', 'harry', 'faster', 'home'}
```

Note that due to the nature of sets, the duplicates are automatically removed, therefore losing the count of each word. Lists must be used to maintain word count.

```python
# Build BoW with word count
import collections

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct]

bag_of_words = collections.Counter(tokens) # counts the elements of a list
bag_of_words
```

**Explanation:**

1.  **`collections.Counter`:** The `Counter` object efficiently counts the occurrences of each unique token in the `tokens` list.

**Output:**

```
Counter({'faster': 3, 'harry': 2, 'got': 1, 'store': 1, 'home': 1})
```

The output shows the term frequency for each word in the sentence. "faster" appears 3 times, "harry" appears 2 times, and the other words appear once.

```python
# Most common words
bag_of_words.most_common(2) # most common 2 words
```

**Explanation:**

1.  **`most_common(2)`:** This method returns the two most frequent words and their counts.

**Output:**

```
(('faster', 3), ('harry', 2))
```

<----------section---------->

### Limits of TF

While term frequency provides a basic measure of word importance, it has limitations:

**Example:**

*   In document A the word "dog" appears 3 times.
*   In document B the word "dog" appears 100 times.

Is the word "dog" more important for document A or B?

Initially, one might assume "dog" is more important for document B due to its higher frequency.

**Additional information:**

*   Document A is a 30-word email to a veterinarian.
*   Document B is the novel War & Peace (approx. 580,000 words).

With this additional context, the importance of "dog" in document A becomes more apparent. A high count in a short document likely signifies a greater relevance.

<----------section---------->

### Normalized TF

To address the limitations of raw term frequency, we use **Normalized TF**. This is the word count divided by the total number of words in the document, making the term frequencies comparable across documents of different lengths.

*   TF (dog, document A) = 3/30 = 0.1
*   TF (dog, document B) = 100/580000 = 0.00017

After normalization, the term frequency for "dog" in document A (0.1) is significantly higher than in document B (0.00017), indicating that the word is indeed more important in document A.

```python
import pandas as pd

counts = pd.Series(bag_of_words) # from dict to Pandas Series
counts / counts.sum() # calculate TF
```

**Explanation:**

1.  **`pd.Series(bag_of_words)`:** Converts the `Counter` object into a Pandas Series for easier manipulation.
2.  **`counts / counts.sum()`:** Divides each word count by the total number of words in the sentence, normalizing the term frequencies.

**Output:**

```
faster    0.375
harry     0.250
got       0.125
store     0.125
home      0.125
dtype: float64
```

The output shows the normalized term frequency for each word in the example sentence.

<----------section---------->

## Vector Space Model (VSM)

The Vector Space Model (VSM) is a fundamental framework in NLP for representing text documents as vectors in a high-dimensional space. This enables mathematical operations to be performed on the documents, facilitating tasks such as similarity comparison and classification.

### NLTK Corpora

The Natural Language Toolkit (NLTK) provides a rich collection of text corpora, which are valuable resources for training and testing NLP algorithms.

*   **Training and Testing:** Corpora can be used to train models and test the accuracy.
*   **Corpus Access:** NLTK provides convenient methods to access data.
*   **Documentation:** Detailed information on using NLTK corpora can be found at: [https://www.nltk.org/howto/corpus.html](https://www.nltk.org/howto/corpus.html)

### Reuters 21578 corpus

The Reuters-21578 corpus is a widely used dataset for NLP tasks, especially text classification.

*   **Use:** Used for NLP and text classification tasks.
*   **Content:** Contains news articles published by Reuters in 1986.
*   **Categorization:** The news articles are categorized into 90 topics.

<----------section---------->

### Using Reuters 21578

The following code demonstrates how to load and explore the Reuters-21578 corpus using NLTK.

```python
import nltk

nltk.download('reuters') # download the reuters corpus
ids = nltk.corpus.reuters.fileids() # ids of the documents
sample = nltk.corpus.reuters.raw(ids[0]) # first document

print(len(ids), "samples.\n") # number of documents
print(sample)
```

**Explanation:**

1.  **`nltk.download('reuters')`:** Downloads the Reuters corpus.
2.  **`nltk.corpus.reuters.fileids()`:** Retrieves a list of document IDs in the corpus.
3.  **`nltk.corpus.reuters.raw(ids[0])`:** Retrieves the raw text of the first document in the corpus.

**Output:**

```
10788 samples.

Taiwan had a trade trade surplus of 15.6 billion dlrs last
year, 95 pct of it with the U.S.

The surplus helped swell Taiwan's foreign exchange reserves
to 53 billion dlrs, among the world's largest.

“We must quickly open our markets, remove trade barriers and
cut import tariffs to allow imports of U.S. Products, if we
want to defuse problems from possible U.S. Retaliation," said
Paul Sheen, chairman of textile exporters <Taiwan Safe Group>.

A senior official of South Korea's trade promotion
association said the trade dispute between the U.S. And Japan
might also lead to pressure on South Korea, whose chief exports
are similar to those of Japan.

Last year South Korea had a trade surplus of 7.1 billion
dirs with the U.S., Up from 4.9 billion dlrs in 1985.

In Malaysia, trade officers and businessmen said tough
curbs against Japan might allow hard-hit producers of
semiconductors in third countries to expand their sales to the
U.S.
```

The output displays the number of documents in the Reuters corpus and the content of the first document.

<----------section---------->

```python
doc = nlp(sample)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]

bag_of_words = collections.Counter(tokens)
counts = pd.Series(bag_of_words).sort_values(ascending=False) # sorted series
counts = counts / counts.sum() # calculate TF

print(counts.head(10))
```

**Explanation:**

1.  **Tokenization and Cleaning:** The raw text is processed to remove stop words, punctuation, and spaces and converted to lowercase.
2.  **Term Frequency Calculation:** Term frequencies are computed.
3.  **Sorted Series**: Pandas Series is sorted to display highest frequency words.

**Output:**

```
u.s.          0.039627
said         0.037296
trade        0.034965
japan        0.027972
dlrs         0.013986
exports      0.013986
tariffs      0.011655
imports      0.011655
billion      0.011655
electronics    0.009324
dtype: float64
```

This shows the most relevant words in the first news article, based on term frequency.

<----------section---------->

```python
ids_subset = ids[:100] # to speed-up we consider only the first 100 elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    doc = nlp(sample)
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
```

**Explanation:**

1.  **Subset Selection:** First 100 documents are processed to accelerate processing.
2.  **Iterate Through Documents:** Loops through the document IDs.
3.  **Processing Steps:** Each document is processed to calculate term frequencies and sort the series.
4.  **Progress tracking:** Displays progress on console using a carriage return.
5.  **DataFrame Creation:** Combines the term frequency data into a Pandas DataFrame.
6.  **Handling Missing Values:** Fills NaN values (resulting from words not present in certain documents) with 0.

**Output:**

```
Sample 100 of 100 processed.
```

<----------section---------->

```python
df
```

**Explanation:**

The resulting Pandas DataFrame, `df`, represents a term-document matrix.

*   **Rows:** Each row represents a document from the Reuters corpus.
*   **Columns:** Each column represents a unique term (word) in the vocabulary.
*   **Values:** Each cell contains the normalized term frequency of a word in a specific document.

**Output:**

```
         u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[100 rows x 3089 columns]
```

The above code is inefficient because spaCy extracts much more information from the text than tokenization.

spaCy extracts a lot of information from text:

*   POS (Part of Speech) tagging
*   Lemmatization
*   Dependency parsing
*   Named Entity Recognition (NER)
*   Etc.

However, here, we only need tokenization.

<----------section---------->

### Corpus Processing (Optimization)

When you call `nlp` on a text, spaCy:

*   Tokenizes the text to produce a `Doc` object.
*   Then the `Doc` is processed in several steps (pipeline).

```python
# Show the current pipeline components
print(nlp.pipe_names)
```

**Output:**

```
['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
```

We must remove these steps from the pipeline.

```python
ids_subset = ids # we now consider all elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    # disable=["tok2vec", "tagger", “parser", “attribute_ruler", “lemmatizer", "ner"]
    doc = nlp(sample, disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
```

**Explanation:**

1.  **Disable Pipeline Components:** The `disable` argument in `nlp(sample, disable=[...])` tells spaCy to only perform tokenization, bypassing the other steps in the pipeline. This significantly speeds up the process.

**Output:**

```
Sample 10788 of 10788 processed.
```

This code now processes the entire Reuters corpus more efficiently.

<----------section---------->

```python
df
```

**Explanation:**
The resulting DataFrame is the term-document matrix with the optimized code.

*   **Rows:** Documents from Reuters corpus
*   **Columns:** Terms from vocabulary
*   **Values:** Normalized TF

**Output:**

```
         u.s.      said     trade     japan      dlrs   exports   tariffs  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...  ...

[10788 rows x 49827 columns]
```

### Term-Document Matrix

*   **Rows:** Represent documents in the corpus.
*   **Columns:** Represent unique terms (words) from the vocabulary.
*   **Elements:** Contain term frequencies (TF), normalized TF, or other weighting schemes (like TF-IDF, which will be covered later).

<----------section---------->

### Vector Space Model

Mathematical representation of documents as vectors in a multidimensional space:

*   Each dimension of the space represents a word.
*   Built from a term-document matrix.

#### 2D Example: with normalized TF
We are considering a vocabulary of just 2 words (w1 and w2).

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![2d_example_vector_space.png](2d_example_vector_space.png)

This diagram illustrates documents as points in a 2D space, where each axis represents the normalized TF of a specific word. The closer the points, the more similar the documents are considered to be.

<----------section---------->

### Document Similarity

Measuring the similarity between documents is a common task in NLP. Two common methods are Euclidean Distance and Cosine Similarity.

#### Euclidean Distance:

*   Calculates the straight-line distance between two vectors in the vector space.
*   Sensitive to the magnitude of the vectors (i.e., longer documents with higher term frequencies will have greater distances).
*   The direction of the vectors captures the relative importance of terms and is more informative than their magnitude.
*   Less commonly used in NLP because document length can skew the results.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![euclidean_distance.png](euclidean_distance.png)

The diagram visualizes the Euclidean distances between the document vectors.

<----------section---------->

#### Cosine Similarity:

*   Measures the cosine of the angle between two vectors.
*   Focuses on the direction of the vectors, ignoring their magnitude.
*   Effective for normalized text representations, because normalization removes magnitude as a factor.
*   Widely used in NLP because it captures similarity in term distribution rather than document length.

```
sim(A, B) = cos(θ) = (A · B) / (|A| * |B|)
```

Where:

*   `A · B` is the dot product of vectors A and B
*   `|A|` and `|B|` are the magnitudes (Euclidean norms) of vectors A and B

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![cosine_similarity.png](cosine_similarity.png)

The diagram shows how the cosine similarity measures the angle between the document vectors.

<----------section---------->

#### Properties of Cosine Similarity

Cosine similarity is a value between -1 and 1:

*   **1:** The vectors point in the same direction across all dimensions.
    *   Documents use the same words in similar proportion, likely talking about the same thing.
*   **0:** The vectors are orthogonal (at a 90-degree angle) in all dimensions.
    *   Documents share no words, likely discussing completely different topics.
*   **-1:** The vectors point in opposite directions across all dimensions.
    *   Impossible with TF (counts of words cannot be negative).
    *   Can happen with other word representations (discussed later, like word embeddings).

<----------section---------->

#### Calculate Cosine Similarity

```python
import numpy as np

def sim(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

# Example:
print(sim(df.iloc[0], df.iloc[1]))
print(sim(df.iloc[0], df.iloc[2]))
print(sim(df.iloc[0], df.iloc[0]))
```

**Explanation:**

1.  **`sim(vec1, vec2)` Function:** This function calculates the cosine similarity between two vectors.
2.  **Dot Product:**  Calculates the dot product of the two vectors using `np.dot()`.
3.  **Vector Norms:** Calculates the Euclidean norms (magnitudes) of the vectors using `np.linalg.norm()`.
4.  **Cosine Similarity:** Calculates cosine similarity by dividing the dot product by the product of the norms.

**Output:**

```
0.14261893769917347
0.2365347461078053
1.0
```

The output displays the cosine similarity between different document vectors. Note that the similarity of a document to itself is 1.

```python
# Compare TF matrix subset (documents 0 and 1)
df.loc[[0, 1], (df.loc[0] > 0) & (df.loc[1] > 0)]
```

**Explanation:**
This code shows the overlap in words between documents 0 and 1.

**Output:**

```
       said      min      year       pct  government
0  0.037296  0.002331  0.009324  0.004662    0.002331
1  0.042254  0.028169  0.014085  0.056338    0.014085
```

```python
# Try also with other documents
```
This is an instruction to the reader to manually explore other document pairs in the DataFrame.

<----------section---------->

## TF-IDF

### Inverse Document Frequency

TF does not consider the uniqueness of the word across the corpus.

*   Common words (the, is, and …) appear frequently in many documents, contributing little to distinguishing one document from another.
*   Specific terms relevant to the subject matter of the corpus may also be frequent but still not useful for differentiating documents.
*   In a corpus about astronomy, terms like planet or star would be common and non-discriminatory.

Inverse Document Frequency (IDF) addresses this by measuring the importance of each word in relation to the entire corpus.

### Inverse Document Frequency
IDF increases the weight of words that are rare across documents and decreases the weight of words that are common.

```
idf(t, D) = log(N / |{d ∈ D: t ∈ d}|)
```

Where:

*   `N`: total number of documents in the corpus `N = |D|`
*   `|{d ∈ D: t ∈ d}|` : number of documents where the term t appears (i.e., tf(t, d) != 0).

**Note:** If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the numerator `1 + N` and denominator to `1 + |{d ∈ D:t ∈ d}|`.

The logarithm in the IDF formula serves to dampen the effect of IDF.

### TF-IDF
Term Frequency – Inverse Document Frequency is the product of TF and IDF.

*   TF gives more importance to the words that are more frequent in the document.
*   IDF gives more weightage to the words that are less frequent in the corpus.

```
tfidf(t, d, D) = tf(t, d) * idf(t, D)
```

*   High TF-IDF indicates a term that is frequent in a document but rare in the corpus, making it significant for that document.
*   Low TF-IDF Indicates a term that is infrequent in the document or common in the corpus, making it less significant for that document.

<----------section---------->

### TF-IDF: Example
*   Document A: Jupiter is the largest planet
*   Document B: Mars is the fourth planet from the sum

| Term      | Document A | Document B | df | idf      | TF-IDF (A) | TF-IDF (B) |
| --------- | ---------- | ---------- | -- | -------- | ---------- | ---------- |
| jupiter   | 1          | 0          | 1  | ln(2/1)=0.69 | 0.138      | 0          |
| largest   | 1          | 0          | 1  | ln(2/1)=0.69 | 0.138      | 0          |
| mars      | 0          | 1          | 1  | ln(2/1)=0.69 | 0          | 0.138      |
| fourth    | 0          | 1          | 1  | ln(2/1)=0.69 | 0          | 0.138      |
| planet    | 1          | 1          | 2  | ln(2/2)=0  | 0          | 0          |

**Explanation:**
*Document Frequency (df):* Number of documents that contain the term. *Inverse Document Frequency (idf):* Calculated as the natural logarithm of the total number of documents divided by the document frequency. *TF-IDF:* Calculated by multiplying the term frequency (TF) by the inverse document frequency (IDF).

In this example, the word "planet" appears in both documents, and has an IDF score of 0, diminishing its importance. "Jupiter," "largest," "mars," and "fourth" are unique to each document, therefore have non-zero TF-IDF scores.

<----------section---------->

### Zipf’s Law
Observes patterns in word frequency distribution in natural languages.

*   Named after linguist George Kingsley Zipf.
*   Frequency of a word is inversely proportional to its rank in a frequency table.

```
f(r) = K / r**α
```

Where:

*   f(r) is the frequency of the word at rank r
*   K is a constant
*   r is the rank of the word
*   α determines the shape of the distribution which is approximately equals to 1

### Zipf’s Law
Meaning:

*   The most common word appears approximately:
    *   Twice as often as the second most common word
    *   Three times as often as the third most common word
    *   And so on …

Implications:

*   A small set of highly frequent words dominate word usage.
*   Majority of words are relatively rare with low frequencies.
*   Using the logarithm in IDF mitigates the influence of rare words.
*   This results in a more uniformly distributed TF-IDF score.

<----------section---------->

### Zipf’s Law: Example

```
Zipf plot for Brown corpus tokens
```

![zipf_example.png](zipf_example.png)

The Brown corpus consists of 1 million words (500 samples of 2000+ words each) of running text of edited English prose printed in the United States during the year 1961 and it was revised and amplified in 1979.

The Zipf's Law distribution is useful for understanding the distribution of words in a large corpus of text. This can be used in feature engineering by setting thresholds to remove tail-end words, or determining an appropriate number of dimensions in topic modeling.

### Calculating TF-IDF

```python
# Calculate IDF vector
n_docs = df.shape[0]
count_docs = (df > 0).sum()
idf = np.log10(n_docs / count_docs)

idf.sort_values(ascending=False)
```

**Explanation:**

1.  **`n_docs = df.shape[0]`:**  Gets the total number of documents (rows) in the DataFrame.
2.  **`count_docs = (df > 0).sum()`:** Calculates the number of documents containing each term. The expression `df > 0` creates a boolean DataFrame, where `True` indicates the term is present in the document.  `.sum()` then counts the `True` values for each column.
3.  **`idf = np.log10(n_docs / count_docs)`:** Calculates the IDF for each term.
4.  **`idf.sort_values(ascending=False)`:** Sorts the IDF values in descending order.

**Output:**

```
asian           4.032221
exaggerated     4.032221
refute          4.032221
man             4.032221
laying          4.032221
                  ...
pet             0.494850
year            0.408935
min             0.366532
>               0.292430
said            0.130768
Length: 49827, dtype: float64
```

The output shows the IDF values for each term, sorted from the highest to the lowest. Terms with higher IDF values are rarer across the corpus.

<----------section---------->

```python
# Calculate TF-IDF matrix
tfidf = df.mul(idf.values, axis='columns')
tfidf
```

**Explanation:**

1.  **`df.mul(idf.values, axis='columns')`:** Multiplies each term frequency (TF) value in the DataFrame by the corresponding IDF value. `axis='columns'` ensures that the multiplication is performed column-wise (i.e., each column is multiplied by the corresponding IDF value).

**Output:**

```
          u.s.      said     trade     japan      dlrs   exports   tariffs  ...
0   1.597892  0.004877  1.129083  0.953560  0.047840  0.047840  0.039616  ...
1   0.000000  0.005525  0.000000  0.000000  0.000000  0.000000  0.000000  ...
2   0.000000  0.004359  0.027888  0.056906  0.000000  0.000000  0.000000  ...
3   0.000000  0.003599  0.062565  0.000000  0.000000  0.031283  0.000000  ...
4   0.000
<----------section---------->

**Natural Language Processing and Large Language Models**

Corso di Laurea Magistrale in Ingegneria Informatica Lesson 4: Text Classification

Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This document outlines a lesson on text classification within the context of a Master's degree course in Computer Engineering, focusing on Natural Language Processing (NLP) and Large Language Models (LLMs). The lesson is prepared by Nicola Capuano and Antonio Greco from the DIEM (Department of Information Engineering and Mathematics) at the University of Salerno. The primary subject is text classification, a fundamental task in NLP.

<----------section---------->

**Outline**

*   Text Classification
*   Topic Labelling Example
*   Sentiment Analysis Exercise

This is a structured agenda for the lesson. It begins with an introduction to text classification, followed by a practical example concerning topic labeling and concludes with an exercise on sentiment analysis.

<----------section---------->

**Text Classification**

Text Classification is the process of assigning one or more classes to a text document for various purposes, such as:

*   Topic labeling: Identifying the main subject or theme of a document.
*   Intent detection: Determining the purpose or goal behind a text, often used in chatbots or virtual assistants.
*   Sentiment analysis: Ascertaining the emotional tone or attitude expressed in a text (positive, negative, or neutral).

The classification relies only on the text content. Other attributes of the document (metadata) are disregarded, which is different from document classification. The classes are predefined, which is different from document clustering.

Text classification involves categorizing text documents based on their content. Common applications include topic labeling, intent detection, and sentiment analysis. The process exclusively uses the text's content, ignoring any metadata. It differs from document classification, which may consider metadata, and from document clustering, where classes are not predefined but are discovered by the algorithm.

<----------section---------->

**Definition**

Given:

*   A set of documents D = {d<sub>1</sub>, ..., d<sub>n</sub>}
*   A set of predefined classes C = {c<sub>1</sub>, ..., c<sub>m</sub>}

Text classification finds a classifier function:

f: D x C -> {True, False}

that assigns a Boolean value in {True, False} to each pair (d<sub>i</sub>, c<sub>j</sub>) ∈ D x C.

This section provides a formal definition of text classification. Let D be a set of documents and C be a set of predefined classes. The goal of text classification is to define a function 'f' that maps a document-class pair to a boolean value. 'True' indicates that the document belongs to the class, while 'False' indicates it does not. This function is denoted as f: D x C -> {True, False}.

<----------section---------->

**Types of Classification**

*   **Single-label:** Assigns each document in D to only one class in C.
*   **Binary:** Like Single-label but C has only two classes. Classification is a decision between a class and its complement.
*   **Multi-label:** Assigns each document to a variable number of classes in C. Can be reduced to a series of binary decisions.

There are different approaches: In Single-label classification, each document belongs to only one class. In Binary classification, there are only two possible classes, thus it’s a special case of Single-label classification. In Multi-label classification, a document can be assigned to multiple classes simultaneously. Multi-label tasks can be approached by training multiple binary classifiers (one for each label).

<----------section---------->

**ML-Based Classification**

*   A machine learning model is trained on a set of annotated text documents.
*   Each document in the training set is associated with one or more class labels.
*   After training, the model can predict the category (or categories) for a new document.
*   The classifier may provide a confidence measure.
*   A vector representation of documents, such as TF-IDF, must be used.

This describes the process of performing text classification using machine learning (ML). The model learns from a training set of annotated documents, where each document is paired with its correct class label(s). After training, the model can predict the classes for new, unseen documents. The model often outputs a confidence score indicating the certainty of its prediction. To be used in ML models, text needs to be converted into a numerical representation, such as TF-IDF (Term Frequency-Inverse Document Frequency).

<----------section---------->

**Topic Labelling Example**

**Classifying Reuters News**

The Reuters 21578 dataset is multi-class and multi-label:

*   90 distinct classes
*   7,769 training documents, 3,019 test documents
*   The number of words per document ranges from 93 to 1,263
*   Skewness:
    *   Some classes have over 1,000 documents
    *   Other classes have fewer than 5 documents
    *   Most documents are assigned either one or two labels, some documents are labeled with up to 15 categories

More statistics on https://martin-thoma.com/nlp-reuters/

This section provides an example of topic labeling using the Reuters-21578 dataset, a standard benchmark for text classification. The dataset is multi-class and multi-label, consisting of news articles categorized into 90 different topics. It includes 7,769 training documents and 3,019 test documents. The documents vary in length from 93 to 1,263 words. The dataset exhibits skewness, meaning some classes have a large number of documents, while others have very few. Most documents have one or two labels, but some have up to 15. The link provides more details on the dataset's statistics.

<----------section---------->

**Corpus Management**

```python
import nltk
from nltk.corpus import reuters

nltk.download('reuters')

ids = reuters.fileids()

training_ids = [id for id in ids if id.startswith("training")]
test_ids = [id for id in ids if id.startswith("test")]
categories = reuters.categories()

print("{} training items:".format(len(training_ids)), training_ids)
print("{} test items:".format(len(test_ids)), test_ids)
print("{} categories:".format(len(categories)), categories)

print("\nCategories of '{}':".format(training_ids[0]), reuters.categories(training_ids[0]))
print("Categories of '{}':".format(test_ids[2]), reuters.categories(test_ids[2]))
print("Items within the category 'trade'", reuters.fileids('trade'))
```

```
[nltk_data] Downloading package reuters to /root/nltk_data...
7769 training items: ['training/1', 'training/10', 'training/100', 'training/1000', 'training/100...
3019 test items: ['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833', 'test/14...
90 categories: ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'co...

Categories of 'training/1': ['cocoa']
Categories of 'test/14829': ['crude', 'nat-gas']
Items within the category 'trade' ['test/14826', 'test/14832', 'test/14858', 'test/14862', 'test...
```

The code uses the NLTK (Natural Language Toolkit) library to load and manage the Reuters dataset. The code first downloads the Reuters corpus, if not already present. It then retrieves the file IDs, separating them into training and test sets. The available categories are also extracted. Finally, it prints the number of training and test items, the list of categories, and example categories for specific documents, along with the documents belonging to the 'trade' category.

<----------section---------->

**Process**

*   Extract training and test samples and related labels
*   Create the TF-IDF matrices for training and test set
*   Transform label lists in a binary matrix for training and test set (one-hot encoding)
*   Train a classifier, e.g., using an MLP. A sample is the TF-IDF vector of a text with its binary label.
*   Test the classifier

This section outlines the steps involved in building and evaluating a text classification model using the Reuters dataset. First, the training and test data, along with their corresponding labels, are extracted. Next, TF-IDF matrices are created to represent the text data numerically. The labels, which may be multi-label, are transformed into a binary matrix using one-hot encoding. A classifier, such as a Multilayer Perceptron (MLP), is trained on the training data. Finally, the trained classifier is tested on the test set. The input to the classifier is the TF-IDF vector representing the text, and the output is a binary label indicating the presence or absence of a particular topic.

<----------section---------->

**Pre-Processing**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer

# Generate training and test sets
training_corpus = [reuters.raw(id) for id in training_ids]
training_labels = [reuters.categories(id) for id in training_ids]
test_corpus = [reuters.raw(id) for id in test_ids]
test_labels = [reuters.categories(id) for id in test_ids]

# Create TF-IDF matrices
vectorizer = TfidfVectorizer(min_df=3)  # a word must appear in at least 3 documents
training_vectors = vectorizer.fit_transform(training_corpus)
test_vectors = vectorizer.transform(test_corpus)

# Transform a list of label lists in binary matrix
mlb = MultiLabelBinarizer()
training_mlb = mlb.fit_transform(training_labels)
test_mlb = mlb.transform(test_labels)

len(vectorizer.vocabulary_)
# 11361
```

`fit_transform` combines two sequential steps, first applying the `fit` function and then the `transform` one.

The code performs the preprocessing steps necessary to prepare the Reuters dataset for machine learning. It extracts the raw text and labels for both training and test sets. It then uses the `TfidfVectorizer` from scikit-learn to create TF-IDF matrices. The `min_df=3` argument ensures that only words appearing in at least 3 documents are considered. The `fit_transform` method is used on the training corpus to learn the vocabulary and create the TF-IDF matrix, while the `transform` method is used on the test corpus to create the TF-IDF matrix using the already learned vocabulary. Finally, the `MultiLabelBinarizer` is used to transform the list of labels into a binary matrix, which is suitable for multi-label classification. The size of the vocabulary is printed at the end. Note that `fit_transform` is a combined operation where `fit` learns the parameters and `transform` applies those to generate the transformed data.

<----------section---------->

**MLP Classifier Training**

```python
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt

# Train an MLP classifier
classifier = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam',
                            max_iter=100, early_stopping=True, verbose=True)

classifier.fit(training_vectors, training_mlb)


# Plot loss and validation curves
def plot(ax, data, title, xlabel, ylabel):
    ax.plot(data, label=title, marker='o')
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_title(title)
    ax.legend()
    ax.grid()


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))  # create subplots

plot(ax1, classifier.loss_curve_, 'Training Loss', 'Iterations', 'Loss')
plot(ax2, classifier.validation_scores_, 'Validation Accuracy', 'Iterations', 'Accuracy')
plt.tight_layout()  # Adjust layout to prevent overlapping
plt.show()
```

The code trains a Multilayer Perceptron (MLP) classifier using the scikit-learn library. The `MLPClassifier` is configured with two hidden layers (128 and 64 neurons), the ReLU activation function, and the Adam solver. `max_iter=100` sets the maximum number of iterations, and `early_stopping=True` enables early stopping to prevent overfitting. The `verbose=True` argument displays training progress. The `fit` method trains the classifier on the training TF-IDF vectors and the binary label matrix. The code then defines a `plot` function to visualize training loss and validation accuracy over iterations. It creates subplots for training loss and validation accuracy and displays the plots using matplotlib. The visualization helps in understanding the training process and identifying potential issues like overfitting or underfitting.

<----------section---------->

**MLP Classifier Training (Example plot)**

The image of "Training Loss Validation Accuracy" should be displayed here.

This section refers to a visualization of the training process. The "Training Loss Validation Accuracy" plot typically shows the training loss decreasing over iterations and the validation accuracy increasing, ideally converging to a stable point. This plot helps diagnose the model's learning behavior.

<----------section---------->

**Testing Metrics**

*   **Micro Average:** The average metric across all classes by considering the total number of true positives, false negatives, and false positives.
*   **Macro Average:** Calculates the metric independently for each class and then takes the average (unweighted mean) of these values.
*   **Weighted Average:** Computes the average of the metric, weighted by the support (the number of true instances) of each class.
*   **Samples Average:** Computes the average of the metrics for each sample (instance) rather than for each class. Used in multi-label classification problems where each instance can belong to multiple classes.

This section explains the different averaging methods used to evaluate the performance of a multi-class or multi-label classification model. Micro-average calculates the metrics globally by considering the total counts of true positives, false negatives, and false positives. Macro-average calculates the metric for each class independently and then averages the results, giving equal weight to each class. Weighted-average averages the metric for each class, weighted by the number of true instances for each class (support). Sample average computes the metrics on a per-sample basis, useful in multi-label classification where each sample can belong to multiple classes.

<----------section---------->

**Testing Results**

```python
from sklearn.metrics import classification_report

# Predict the categories of the test set
predictions = classifier.predict(test_vectors)

# Print classification report
print(classification_report(test_mlb, predictions, target_names=mlb.classes_, zero_division=0))
```

Example output:

```
              precision    recall  f1-score   support

         acq       0.98      0.91      0.94       719
        alum       1.00      0.30      0.47        23
      barley       0.89      0.57      0.70        14
         bop       1.00      0.43      0.60        30
     carcass       0.67      0.22      0.33        18
castor-oil       0.00      0.00      0.00         1
       cocoa       1.00      0.56      0.71        18
     coconut       0.00      0.00      0.00         2
 coconut-oil       0.00      0.00      0.00         3

   micro avg       0.97      0.79      0.87      3019
   macro avg       0.56      0.36      0.42      3019
weighted avg       0.95      0.79      0.85      3019
 samples avg       0.96      0.80      0.85      3019
```

This code evaluates the trained classifier on the test set and prints a classification report using scikit-learn. It predicts the categories for the test TF-IDF vectors and then uses the `classification_report` function to calculate and display precision, recall, F1-score, and support for each class. The `target_names` argument provides the names of the classes, and `zero_division=0` handles cases where a class has no predicted or true instances. The report also includes micro, macro, weighted, and samples averages of these metrics. The example output shows the performance of the classifier for each category in the Reuters dataset.

<----------section---------->

**Sentiment Analysis Exercise**

**Sentiment Analysis**

The process of identifying and categorizing opinions expressed in a piece of text.

**Applications:**

*   **Business:** Analyzing customer feedback and product reviews to understand customer satisfaction and brand perception.
*   **Finance:** Predicting market trends based on investor sentiment extracted from news articles and social media.
*   **Politics:** Analyzing public opinion during elections or policy changes.

Can be seen as a text classification problem: Given a text, classifying it as positive, negative, or neutral.

This section introduces sentiment analysis. It explains that sentiment analysis is the task of identifying and categorizing opinions expressed in text. The section then outlines several applications of sentiment analysis, across different domains. It is highlighted that sentiment analysis can be treated as a text classification problem, where the text is classified into categories such as positive, negative, or neutral.

<----------section---------->

**IMDB Dataset**

A set of 50,000 highly polarized reviews from the Internet Movie Database.

*   The set consists of:
    *   50% negative reviews
    *   50% positive reviews
*   Download CSV from Kaggle: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

Example of the dataset:

| Review                                                        | Sentiment |
| :------------------------------------------------------------ | :-------- |
| One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The... | Positive  |
| A wonderful little production. <br/>  <br/>The filming technique is very unassuming. Very old-time-B... | Positive  |
| I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air...     | Positive  |

`Review` and `Sentiment` are unique values.

This introduces the IMDB dataset, a popular resource for sentiment analysis. It contains 50,000 movie reviews from the Internet Movie Database, equally split between positive and negative reviews. A link to download the dataset from Kaggle is provided. The table exemplifies the structure of the dataset, showing a `Review` column containing the text of the review and a `Sentiment` column indicating whether the review is positive or negative. It is stated that `Review` and `Sentiment` are unique values, but this is inaccurate since multiple reviews can have the same sentiment.

<----------section---------->

**Exercise**

Build a classifier for movie reviews:

*   Given a review, the classifier will determine its polarity (positive or negative).
*   Train the classifier on the IMDB Dataset.
*   Use 80% for training and 20% for testing.
*   Show and plot metrics and the confusion matrix.

This presents the sentiment analysis exercise using the IMDB dataset. The task is to build a classifier that can determine the sentiment (positive or negative) of a movie review. The classifier should be trained on the IMDB dataset, using 80% of the data for training and 20% for testing. The exercise requires displaying relevant metrics (precision, recall, F1-score, accuracy) and plotting the confusion matrix to evaluate the classifier's performance.

<----------section---------->

**Suggestions**

*   One-hot encode the labels: (1,0) = negative, (0, 1) = positive using the ScikitLearn `OneHotEncoder` class.
*   To reduce the TF-IDF matrix, consider only words that appear at least in 5 documents.
*   Use the ScikitLearn `confusion_matrix` function to build the confusion matrix.
*   You can use the Seaborn `heatmap` to plot the confusion matrix: `pip install seaborn`.
    *   [https://seaborn.pydata.org/generated/seaborn.heatmap.html](https://seaborn.pydata.org/generated/seaborn.heatmap.html)

The suggestions give guidance on how to implement the sentiment analysis exercise. One-hot encoding converts the sentiment labels into numerical format suitable for machine learning. Limiting TF-IDF matrix dimensionality helps reduce noise and improve performance. Scikit-learn's `confusion_matrix` function is recommended to build the confusion matrix. The use of Seaborn's `heatmap` function is suggested as a visualization tool for the confusion matrix. Seaborn provides visually appealing heatmaps that helps in visualizing the model performance.

<----------section---------->

**Confusion Matrix Example**

The image of a confusion matrix, with "Predicted Labels" on the x-axis (negative, positive) and count numbers (0 to 5000) on the y-axis, should be displayed here.

This refers to a sample confusion matrix, a table visualizing the performance of a classification model. The x-axis represents the predicted labels (negative and positive), while the y-axis represents the actual labels. The cells of the matrix contain the counts of true positives, true negatives, false positives, and false negatives.

<----------section---------->

**Example Result**

**Training Loss Validation Accuracy**

The image of "Training Loss Validation Accuracy", with values on the axis, should be displayed here.

Example Classification Report:

```
              precision    recall  f1-score   support

    negative       0.90      0.91      0.90      5013
    positive       0.90      0.90      0.90      4987

    accuracy                           0.90     10000
   macro avg       0.90      0.90      0.90     10000
weighted avg       0.90      0.90      0.90     10000
```

This section presents example results for the sentiment analysis exercise. The first image is an example training loss and validation accuracy plot, showing how the model performs during training. The second part is a sample classification report, showing precision, recall, F1-score and support for each class (negative and positive). The overall accuracy, macro average and weighted average are also displayed.

<----------section---------->

**Text Classification Applications**

*   Topic Labelling
*   Sentiment Analysis
*   Spam Filtering
*   Intent Detection
*   Language Detection
*   Content Moderation
*   Products Categorization
*   Author Attribution
*   Content Recommendation
*   Ad Click Prediction
*   Job matching
*   Legal case classification

This lists a range of applications of text classification across various domains. These applications demonstrate the versatility and practical significance of text classification in real-world problems.

<----------section---------->

**Further Readings**

*   Pandas Docs: [https://pandas.pydata.org/docs/user_guide/](https://pandas.pydata.org/docs/user_guide/)
*   Scikit-Learn Docs: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)
*   Seaborn Docs: [https://seaborn.pydata.org/api.html](https://seaborn.pydata.org/api.html)

This provides links to the official documentation of the Python libraries used in the lesson: Pandas, Scikit-learn, and Seaborn. These documentations are useful resources for further learning and understanding the libraries' functionalities.

<----------section---------->

The document needs to address when vocabulary is much larger than labeled examples and consider Latent Discriminant Analysis (LDA) as a classifier. We can define what LDA does:
1. Find a line, or axis, in your vector space, such that if you project all the vectors (data points) in the space on that axis, the two classes would be as separated as possible.
2. Project all the vectors on that line.
3. Predict the probability of each vector to belong to one of two classes, according to a cutoff point between the two classes.
The document should mention the possibility of using MinMaxScalar to normalize the toxicity score.
The document should also point out that the class methods `fit()` and `predict()` are available in every model in `sklearn`, and classifier models will have a `predict_proba()` method that gives you the probability scores for all the classes.
The document should explain False Positive and False Negative mistakes.
The document also needs to address the problem statement and training data for a multi-label intent classifier.
When the vocabulary is much larger than the number of labeled examples, Naive Bayes classifiers often perform poorly. In such cases, consider using Latent Discriminant Analysis (LDA).

LDA, in the context of classification, aims to:
1.  Find an axis in the vector space that maximizes the separation between classes when the data is projected onto it.
2.  Project the data vectors onto this axis.
3.  Determine the probability of each vector belonging to a class based on a cutoff point on the projected axis.

To scale the calculated toxicity score (obtained by projecting comments onto the centroid axis) into a probability-like range between 0 and 1, you can use Scikit-learn's `MinMaxScaler`. This helps in interpreting the score as a confidence level.

Every model in Scikit-learn implements `fit()` for training and `predict()` for making predictions. Classifier models additionally offer `predict_proba()`, providing class membership probabilities.

In classification, a "False Positive" occurs when the model incorrectly predicts a positive class for a negative instance. A "False Negative" occurs when the model incorrectly predicts a negative class for a positive instance.
For multi-label intent classification, the problem is to predict all the possible intents or sentiments contained in a natural language passage.

<----------section---------->

### Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica Lesson 5
Word Embeddings
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This document outlines a lecture on Word Embeddings within a Natural Language Processing (NLP) course. It covers the limitations of TF-IDF, the concept of word embeddings, methods for learning them, alternatives to Word2Vec, and practical ways to work with word embeddings. Word embeddings are a crucial component in many modern NLP applications, providing a way to represent words as dense vectors in a continuous space, where the position of a word reflects its semantic meaning.

<----------section---------->

### Outline
*   Limitations of TF-IDF
*   Word Embeddings
*   Learning Word Embeddings
*   Word2Vec Alternatives
*   Working with Word Embeddings

This outline presents the core topics covered in the lecture. Starting with the drawbacks of a simpler technique like TF-IDF sets the stage for appreciating the advantages of word embeddings. The following sections build upon this foundation, exploring how word embeddings are learned and utilized, and providing insights into state-of-the-art techniques and their applications.

<----------section---------->

### Limitations of TF-IDF
TF-IDF (Term Frequency-Inverse Document Frequency) is a technique used to quantify the importance of a word in a document relative to a collection of documents (corpus). It counts terms according to their exact spelling, meaning that texts with the same meaning but using different words will have completely different TF-IDF vector representations. This inflexibility presents a challenge when dealing with the nuances of language and semantic understanding.

**Examples:**
*   The movie was amazing and exciting.
*   The film was incredible and thrilling.
*   The team conducted a detailed analysis of the data and found significant correlations between variables.
*   The group performed an in-depth examination of the information and discovered notable relationships among factors.

These examples illustrate how different wording can result in distinct TF-IDF representations, even when the underlying meaning is the same. The sentences in each pair convey similar information but utilize different lexical choices.

<----------section---------->

### Term Normalization
Techniques like stemming and lemmatization are used to normalize terms, collecting words with similar spellings under a single token. Stemming reduces words to their root form (e.g., "running" becomes "run"), while lemmatization reduces words to their dictionary form (lemma) considering context (e.g., "better" becomes "good").

**Disadvantages:**
*   They fail to group most synonyms, which limits the ability to capture semantic similarity.
*   They may group together words with similar or the same spelling but different meanings, leading to ambiguity.
*   She is leading the project vs. The plumber leads the pipe.
*   The bat flew out of the cave vs. He hit the ball with a bat.

These examples highlight the risk of oversimplification in normalization. The word "lead" can mean "to guide" or refer to a type of metal pipe, while "bat" can be a nocturnal animal or a piece of sports equipment. Stemming or lemmatization alone cannot resolve these polysemous distinctions.

<----------section---------->

### TF-IDF Applications
TF-IDF is sufficient for many NLP applications that do not require a deep understanding of text semantics:
*   Information Retrieval (Search Engines)
*   Information Filtering (Document Recommendation)
*   Text Classification

However, other applications require a deeper understanding of text semantics:
*   Text generation (Chatbot)
*   Automatic Translation
*   Question Answering
*   Paraphrasing and Text Rewriting

The first group of applications benefits from TF-IDF's ability to identify important keywords, while the second group requires capturing the relationships between words and the underlying meaning of the text. These advanced tasks necessitate models that can discern context and semantic similarity.

<----------section---------->

### Bag-of-Words (recap)
Each word is assigned an index that represents its position in the vocabulary:
*   the 1st word (e.g., apple) has index 0
*   the 2nd word (e.g., banana) has index 1
*   the 3rd word (e.g., king) has index 2
*   ...

Each word is then represented by a one-hot vector:
*   apple = (1,0,0,0,…,0)
*   banana = (0,1,0,0,…,0)
*   king = (0,0,1,0,…,0)

This section provides a quick review of the Bag-of-Words (BoW) model, a foundational concept in NLP. In BoW, a text is represented as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity. One-hot encoding is a way to represent each word in this bag as a vector, where the dimension of the vector is equal to the size of the vocabulary and only one element in the vector is 1, representing the index of the word.

<----------section---------->

### Bag-of-Words (recap)
With this encoding, the distance between any pair of vectors is always the same. It does not capture the semantics of words. Furthermore, it is not efficient since it uses sparse vectors.

**Note:** The figure shows only three dimensions of a space where dimensions equals the cardinality of the vocabulary

The key limitation of BoW is that it treats all words as independent entities. The Euclidean distance between any two one-hot vectors is the same, regardless of whether the words are semantically similar. This means that the model cannot capture any relationships or similarities between words. Furthermore, one-hot vectors are sparse and high-dimensional, making them computationally inefficient for large vocabularies.

<----------section---------->

### Word Embeddings
Word Embeddings: A technique for representing words with vectors (A.K.A. Word Vectors) that are:
*   Dense
*   With dimensions much smaller than the vocabulary size
*   In a continuous vector space

**Key feature:** Vectors are generated so that words with similar meanings are close to each other. The position in the space represents the semantics of the word.

Word Embeddings:
*   king and queen are close to each other
*   apple and banana are close to each other
The words of the first group are far from those of to the second group.

**Example:**
*   Apple = (0.25,0.16)
*   Banana = (0.33,0.10)
*   King = (0.29,0.68)
*   Queen = (0.51,0.71)

This section introduces the core concept of word embeddings, a significant advancement over BoW.  Word embeddings represent words as dense, low-dimensional vectors in a continuous vector space.  The key advantage is that the spatial arrangement of these vectors reflects the semantic relationships between words. Words with similar meanings are located close to each other in the vector space. The vectors are "dense" because most of the elements of the vector have non-zero values, in contrast to the "sparse" one-hot encoding used in Bag-of-Words.  The dimensionality of word embeddings is also much smaller than the vocabulary size, typically ranging from 100 to 300 dimensions, making them more computationally efficient.

<----------section---------->

### Word Embedding: Properties
Word embeddings enable semantic text reasoning based on vector arithmetic.

**Examples:**
*   Subtracting royal from king we arrive close to man: king – royal ≈ man
*   Subtracting royal from queen we arrive close to woman: queen – royal ≈ woman
*   Subtracting man from king and adding woman we arrive close to queen: king – man + woman ≈ queen

One of the most fascinating properties of word embeddings is their ability to perform analogical reasoning through vector arithmetic. For example, subtracting the vector representing "royal" from "king" results in a vector close to "man." These operations demonstrate that word embeddings capture meaningful semantic relationships between words.

<----------section---------->

### Semantic Queries
Word embeddings allow for searching words or names by interpreting the semantic meaning of a query.

**Examples:**
*   Query: "Famous European woman physicist"
    ```
    wv['famous'] + wv['European'] + wv['woman'] + wv['physicist'] ≈ wv['Marie_Curie'] ≈ wv['Lise_Meitner'] ≈ …
    ```
*   Query: “Popular American rock band”
    ```
    wv['popular'] + wv['American'] + wv['rock'] + wv['band'] ≈ wv['Nirvana'] ≈ wv['Pearl Jam'] ≈ …
    ```

Word embeddings facilitate semantic searches, allowing users to retrieve information based on the meaning of their query rather than just keyword matching. By adding the vectors of individual words in the query, the model can identify words or names that are semantically related to the query as a whole, as shown in the examples above.

<----------section---------->

### Analogies
Word embeddings enable answering analogy questions by leveraging their semantic relationships.

**Examples:**
*   Who is to physics what Louis Pasteur is to germs?
    ```
    wv['Louis_Pasteur'] – wv['germs'] + wv['physics'] ≈ wv['Marie_Curie']
    ```
*   Marie Curie is to science as who is to music?
    ```
    wv['Marie_Curie'] – wv['science'] + wv['music'] ≈ wv['Ludwig_van_Beethoven']
    ```
*   Legs is to walk as mouth is to what?
    ```
    wv['legs'] – wv['walk'] + wv['mouth'] ≈ wv['speak'] or wv['eat']
    ```

Word embeddings can also be used to solve analogy questions by performing vector arithmetic. The examples above show how subtracting the vector representing one term in the analogy from another and adding the vector representing the third term can yield a vector close to the fourth term, thus answering the analogy question.

<----------section---------->

### Visualizing Word Embeddings
Google News Word2vec 300-D vectors projected onto a 2D map using PCA. Semantic proximity approximates geographical proximity.

news corpus, cities that are similar in size and culture are clustered close together despite being far apart geographically, such as San Diego and San Jose, or vacation destinations such as Honolulu and Reno.

Fortunately you can use conventional algebra to add the vectors for cities to the vectors for states and state abbreviations. As you discovered in chapter 4, you can use tools such as principal components analysis to reduce the vector dimensions from your 300 dimensions to a human-understandable 2D representation. PCA enables you to see the projection or “shadow” of these 300-D vectors in a 2D plot. Best of all, the PCA algorithm ensures that this projection is the best possible view of your data, keeping the vectors as far apart as possible. PCA is like a good photographer that looks at something from every possible angle before composing the optimal photograph. You don’t even have to normalize the length of the vectors after summing the city + state + abbrev vectors, because PCA takes care of that for you.

We saved these augmented city word vectors in the nlpia package so you can load them to use in your application. In the following code, you use PCA to project them onto a 2D plot.

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
us_300D = get_data('cities_us_wordvectors')
us_2D = pca.fit_transform(us_300D.iloc[:, :300])
```

Figure 6.8 shows the 2D projection of all these 300-D word vectors for US cities:
Figure 6.8 Google News Word2vec 300-D vectors projected onto a 2D map using PCA

Listing 6.11 Bubble chart of US cities
The 2D vectors producted by PCA are for visualization. Retain the original 300-D Word2vec vectors for any vector reasoning you might want to do.
The last column of this DataFrame contains the city name, which is also stored in the DataFrame index.

Memphis, Nashville,
Charlotte, Raleigh, and Atlanta
Houston and Dallas
nearly coincide.
Ft. Worth
El Paso
San Diego
LA, SF, and San Jose
America/Los_…(0.9647851, –0.7217035)
Portland, OR
Honolulu, Reno,
Mesa, Tempe, and Phoenix

Size: population
Position: semantics
Color: time zone
America/Phoenix
America/New_York
America/America/Anchorage
America/Indiana/Indianapolis
America/Los_Angeles
America/Boise
America/Denver
America/Kentucky/Louisville
America/Chicago
Pacific/Honolulu

This section discusses the visualization of high-dimensional word embeddings. Since word embeddings typically have hundreds of dimensions, it is difficult to visualize them directly. Techniques like Principal Component Analysis (PCA) are used to reduce the dimensionality of the word embeddings to two or three dimensions, allowing them to be plotted on a graph. The resulting plots often reveal clusters of words with similar meanings, confirming that semantic proximity is reflected in the vector space.  The example highlights how city embeddings, when projected onto a 2D map, can exhibit geographical relationships. Cities with similar sizes and cultural aspects are often clustered together. The provided code snippet shows how to use scikit-learn's PCA to reduce the dimensionality of city word vectors. The remark "The 2D vectors producted by PCA are for visualization. Retain the original 300-D Word2vec vectors for any vector reasoning you might want to do" makes clear the visualization is for understanding, but the original high-dimensional vectors are required for advanced analysis.

<----------section---------->

### Learning Word Embeddings
This section introduces methods for learning word embeddings from textual data, focusing on the Word2Vec algorithm. The core idea behind these methods is that words with similar meanings tend to appear in similar contexts.

<----------section---------->

### Word2Vec
Word embeddings were introduced by Google in 2013 in the following paper:
*   T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space in 1st International Conference on Learning Representations, ICLR 2013

The paper defines Word2Vec:
*   A methodology for the generation of word embeddings
*   Based on neural networks
*   Using unsupervised learning on a large unlabeled textual corpus

Word2Vec, a seminal algorithm developed by Google, provides an efficient way to learn word embeddings using neural networks in an unsupervised manner. It leverages a large, unlabeled corpus of text to capture semantic relationships between words. The original paper "Efficient Estimation of Word Representations in Vector Space" by Mikolov et al. (2013) laid the foundation for many subsequent advancements in word embedding techniques.

<----------section---------->

### Word2Vec
Idea: words with similar meanings are often found in similar contexts.
*   Context: a sequence of words in a sentence

**Example:**
*   Consider the sentence Apple juice is delicious.
*   Remove one word.
*   The remaining sentence is ____ juice is delicious.
*   Ask someone to guess the missing word.
*   Terms such as banana, pear or apple would probably be suggested.
*   These terms have similar meanings and used in similar contexts.

The central idea behind Word2Vec is that words appearing in similar contexts tend to have similar meanings. The "context" of a word is defined as the surrounding words in a sentence. By analyzing these contexts, Word2Vec can learn to represent words as vectors in such a way that words with similar meanings are located close to each other in the vector space. This example effectively explains that if someone had to guess the missing word, it would probably be a fruit similar to an apple.

<----------section---------->

### Continuous Bag-of-Word
A neural network is trained to predict the central token of a context of m tokens.
*   **Input:** the bag of words composed of the sum of all one-hot vectors of the surrounding tokens.
*   **Output:** a probability distribution over the vocabulary with its maximum in the most probable missing word.
*Example:* Claude Monet painted the Grand Canal in Venice in 1806.

The Continuous Bag-of-Words (CBOW) model is one of the two main architectures in Word2Vec. In CBOW, a neural network is trained to predict a central (target) word based on the surrounding context words. The input to the network is a "bag of words" composed of the surrounding words, which are represented as one-hot vectors and then summed together. The output is a probability distribution over the entire vocabulary, with the goal being to maximize the probability of the actual target word. The example sentence is used in the next section to further explain the training process.

<----------section---------->

### Continuous Bag-of-Word
|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

CBOW's neural network architecture typically consists of three layers: an input layer, a hidden layer, and an output layer. The input layer has |V| neurons, where |V| is the size of the vocabulary. The hidden layer has 'n' neurons, where 'n' is the desired dimensionality of the word embeddings. The output layer also has |V| neurons, representing a probability distribution over the vocabulary.

<----------section---------->

### SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH
Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

This section provides a guideline on when to choose between the Skip-gram and CBOW architectures. Skip-gram performs well with smaller datasets and for representing rare words. CBOW tends to achieve higher accuracy for frequent words and is faster to train, making it suitable for large datasets.

<----------section---------->

### Continuous bag of words vs. bag of words
In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surrounding words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

**Example:** for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

This clarifies the difference between a simple "bag of words" (BoW) and the "continuous bag of words" (CBOW) used in the Word2Vec model. CBOW utilizes a sliding window to define the context, allowing the model to capture local relationships between words, which is not possible with the traditional BoW approach.

<----------section---------->

### Continuous Bag-of-Word
Ten 5-gram examples from the sentence about Monet

CONTINUOUS BAG-OF-WORDS APPROACH
In the continuous bag-of-words approach, you’re trying to predict the center word based on the surrounding words. Instead of creating pairs of input and output tokens, you’ll create a multi-hot vector of all surrounding terms as an input vector. The multi-hot input vector is the sum of all one-hot vectors of the surrounding tokens to the center, target token.

Based on the training sets, you can create your multi-hot vectors as inputs and map them to the target word as output. The multi-hot vector is the sum of the one-hot vectors of the surrounding words’ training pairs wt-2 + wt-1 + wt+1 + wt+2 . You then build the training pairs with the multi-hot vector as the input and the target word wt as the output. During the training, the output is derived from the softmax of the output node with the highest probability.

| Input word wt-2 | Input word wt-1 | Input word wt+1 | Input word wt+2 | Expected output wt |
|-----------------|-----------------|-----------------|-----------------|--------------------|
| Monet           | painted         | Claude          | Monet             | Claude             |
| Claude          | painted         | the             | Monet             | painted            |
| Claude          | Monet           | the             | Grand             | painted            |
| Monet           | painted         | Grand           | Canal           | the                |
| painted         | the             | Canal           | of              | Grand              |
| the             | Grand           | of              | Venice            | Canal              |
| Grand           | Canal           | Venice          | in              | of               |
| Canal           | of              | in              | 1908            | Venice             |
| of              | Venice          | 1908            |                  | in               |
| Venice          | in              | 1908            |                  |                  |

target word w t = word to be predicted
surrounding words w t-2, w t-1 = input words
surrounding words w t+1, w t+2

painted the Grand Canal of Venice in 1908.

This section elaborates on the training process of the CBOW model. The model takes the surrounding words (context) as input and predicts the central word. The input is constructed as a "multi-hot vector," created by summing the one-hot vectors of the surrounding words. The expected output is the one-hot vector of the target word. The table provides training examples extracted from the sentence "Claude Monet painted the Grand Canal of Venice in 1908".

<----------section---------->

### Continuous Bag-of-Word
After the training is complete the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words.
Similar words are found in similar contexts …
… their weights to the hidden layer adjust in similar ways
… this result in similar vector representations

SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH
Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

Continuous bag of words vs. bag of words
In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surround-ing words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

Example for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

the highest probability will be converted to 1, and all remaining terms will be set to 0.
This simplifies the loss calculation.
 After training of the neural network is completed, you’ll notice that the weights have been trained to represent the semantic meaning. Thanks to the one-hot vector conversion of your tokens, each row in the weight matrix represents each word from the vocabulary for your corpus. After the training, semantically similar words will have similar vectors, because they were trained to predict similar surrounding words. This is purely magical!
 After the training is complete and you decide not to train your word model any further, the output layer of the network can be ignored. Only the weights of the inputs to the hidden layer are used as the embeddings. Or in other words: the weight matrix is your word embedding. The dot product between the one-hot vector representing the input term and the weights then represents the word vector embedding.

Retrieving word vectors with linear algebra
The weights of a hidden layer in a neural network are often represented as a matrix: one column per input neuron, one row per output neuron. This allows the weight matrix to be multiplied by the column vector of inputs coming from the previous layer to generate a column vector of outputs going to the next layer . So if you multiply (dot product) a one-hot row vector by the trained weight matrix, you’ll get a vector that is one weight from each neuron (from each matrix column). This also works if you take the weight matrix and multiply it (dot product) by a one-hot column vector for the word you are interested in.

Of course, the one-hot vector dot product just selects that row from your weight matrix that contains the weights for that word, which is your word vector. So you could easily retrieve that row by just selecting it, using the word’s row number or index num-ber from your vocabulary.

WE of Monet

After training, the output layer of the CBOW network is discarded. The weights connecting the input layer to the hidden layer are retained as the word embeddings. Words that appear in similar contexts will have their weights adjusted in similar ways during training, leading to similar vector representations. This process effectively encodes the semantic meaning of words in the vector space. The passage explains how the hidden layer's weight matrix serves as the word embedding and how it can be retrieved through linear algebra operations.

<----------section---------->

### Skip-Gram
Alternative training method for Word2Vec
*   A neural network is trained to predict a context of m tokens based on the central token
*   **Input:** the one-hot vector of the central token
*   **Output:** the one-hot vector of a surrounding word (one training iteration for each surrounding word)

output example skip-grams are shown in figure 6.3. The predicted words for these skip-grams are the neighboring words “Claude,” “Monet,” “the,” and “Grand.”

WHAT IS A SKIP-GRAM? Skip-grams are n -grams that contain gaps because you skip over intervening tokens. In this example, you’re predicting “Claude” from the input token “painted,” and you skip over the token “Monet.”
The structure of the neural network used to predict the surrounding words is similar to the networks you learned about in chapter 5. As you can see in figure 6.4, the net-work consists of two layers of weights, where the hidden layer consists of n neurons; n is the number of vector dimensions used to represent a word. Both the input and out-put layers contain M neurons, where M is the number of words in the model’s vocabu-lary. The output layer activation function is a softmax, which is commonly used for classification problems.

WHAT IS SOFTMAX ?
The softmax function is often used as the activation function in the output layer of neural networks when the network’s goal is to learn classification problems. The softmax will squash the output results between 0 and 1, and the sum of all outputs will always add up to 1. That way, the results of an output layer with a softmax function can be considered as probabilities.
 For each of the K output nodes, the softmax output value can be calculated using the normalized exponential function:

```
σ(z)j = exp(z_j) / Σ_{k=1}^{K} exp(z_k)
```

**Example 3D vector:**
v = [0.5, 0.9, 0.2]

word w t = input word

painted the Grand Canal of Venice in 1908.
surrounding words w t-2 , w t-1 = words to be predicted
surrounding words w t+1 , w t+2

The Skip-gram model is the second main architecture in Word2Vec, offering an alternative approach to learning word embeddings. Unlike CBOW, Skip-gram predicts the surrounding context words given a central (target) word. The input to the network is the one-hot vector of the central word, and the output is a probability distribution over the vocabulary for each surrounding word. A skip-gram is defined as an n-gram that contains gaps because it "skips over intervening tokens," as in predicting "Claude" from "painted" by skipping over "Monet." The activation function is softmax.

<----------section---------->

### Skip-Gram
|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

The “squashed” vector after the softmax activation would look like this:

**Example 3D vector after softmax:**
σ(v) = [0.309, 0.461, 0.229]

Notice that the sum of these values (rounded to three significant digits) is approximately 1.0, like a probability distribution.

Figure 6.4 shows the numerical network input and output for the first two surrounding words. In this case, the input word is “Monet,” and the expected output of the network is either “Claude” or “painted,” depending on the training pair.

The Skip-gram neural network architecture also consists of three layers: an input layer, a hidden layer, and an output layer. As with CBOW, the input layer has |V| neurons, where |V| is the size of the vocabulary, and the hidden layer has 'n' neurons, where 'n' is the desired dimensionality of the word embeddings. The output layer has |V| neurons for each of the surrounding words.

<----------section---------->

### Skip-Gram
Ten 5-gram examples from the sentence about Monet

NOTE When you look at the structure of the neural network for word embedding, you’ll notice that the implementation looks similar to what you discovered in chapter 5.

How does the network learn the vector representations?
To train a Word2vec model, you’re using techniques from chapter 2. For example, in table 6.1, wt represents the one-hot vector for the token at position t. So if you want to train a Word2vec model using a skip-gram window size (radius) of two words, you’re considering the two words before and after each target word. You would then use your 5-gram tokenizer from chapter 2 to turn a sentence like this:

```python
sentence = "Claude Monet painted the Grand Canal of Venice in 1806."
```

into 10 5-grams with the input word at the center, one for each of the 10 words in the original sentence.

The training set consisting of the input word and the surrounding (output) words are now the basis for the training of the neural network. In the case of four surrounding words, you would use four training iterations, where each output word is being pre-dicted based on the input word.

Each of the words are represented as one-hot vectors before they are presented to the network (see chapter 2). The output vector for a neural network doing embedding is similar to a one-hot vector as well. The softmax activation of the output layer nodes (one for each token in the vocabulary) calculates the probability of an output word being found as a surrounding word of the input word. The output vector of word probabilities can then be converted into a one-hot vector where the word with the highest probability will be converted to 1, and all remaining terms will be set to 0.

| Input word wt | Expected output wt-2 | Expected output wt-1 | Expected output wt+1 | Expected output wt+2 |
|---------------|----------------------|----------------------|----------------------|----------------------|
| Claude        |                      |                      | Monet                |                      |
| Monet         |                      | Claude               | painted              |                      |
| painted       | Claude               | Monet                | the                  | Grand                |
| the           | Monet                | painted              | Grand                | Canal                |
| Grand         | painted              | the                  | Canal                | of                   |
| Canal         | the                  | Grand                | of                   | Venice               |
| of            | Grand                | Canal                | Venice               | in                   |
| Venice        | Canal                | of                   | in                   | 1908                 |
| in            | of                   | Venice               | 1908                 |                      |
| 1908          | Venice               | in                   |                      |                      |

This segment describes how a skip-gram model learns word vector representations. It starts by tokenizing the input text into n-grams and then representing each word as a one-hot vector. During the training process, the model aims to predict the surrounding words based on the input word, updating the weights through backpropagation. By performing several training iterations, the model learns the vector representations, which capture the contextual meaning of the word.

<----------section---------->

### Skip-Gram
After the training is complete the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words.
Similar words are found in similar contexts …
… their weights to the hidden layer adjust in similar ways
… this result in similar vector representations

### CBOW vs Skip-Gram
**CBOW**
*   Higher accuracies for frequent words, much faster to train, suitable for large datasets

**Skip-Gram**
*   Works well with small corpora and rare terms

**Dimension of Embeddings (n)**
*   Large enough to capture the semantic meaning of tokens for the specific task
*   Not so large that it results in excessive computational expense

Similar to CBOW, after the training phase, the output layer is discarded, and the weights from the input to the hidden layer serve as the word embeddings. This emphasizes that Word2Vec and other techniques learn the word embeddings from the co-occurrence statistics present in a text corpus. CBOW works well with frequent words and large datasets, while Skip-gram is more effective for rare words and smaller corpora.  Choosing the right dimension for word embeddings ('n') is a compromise between capturing semantic meaning and computational efficiency.

<----------section---------->

### Improvements to Word2Vec
This section introduces enhancements to the basic Word2Vec algorithm to improve its performance and effectiveness.

<----------section---------->

### Improvements to Word2Vec
**Frequent Bigrams**
*   Some words often occur in combination
*   Elvis is often followed by Presley forming a bigram
*   Predicting Presley after Elvis doesn't add much value
*   To let the network focus on useful predictions frequent bigrams and trigrams are included as terms in the Word2vec vocabulary
*   Inclusion criteria: co-occurrence frequency greater than a threshold

```
score(wi, wj) = (count(wi, wj) - δ) / (count(wi) * count(wj))
```

*   Examples: Elvis\_Presley, New\_York, Chicago\_Bulls, Los\_Angeles\_Lakers, etc.

One improvement is the inclusion of frequent bigrams and trigrams as single tokens in the vocabulary.  This is because some word combinations, such as "Elvis Presley" or "New York," have a meaning that is different from the individual words. This scoring function evaluates the co-occurrence of two words (wi and wj).

<----------section---------->

### Improvements to Word2Vec
**Subsampling Frequent Tokens**
*   Common words (like stop-words) often don’t carry significant information
*   Being frequent, they have a big influence on the training process

**To reduce their effect...**
*   During training (skip-gram method), words are sampled in inverse proportion to their frequency
*   **Probability of sampling:**

```
P(wi) = 1 - sqrt(t / f(wi))
```
Where ```f(wi)``` is the frequency of a word across the corpus, and ```t``` represents a frequency threshold above which you want to apply the subsampling probability.

*   The effect is like the IDF effect on TF-IDF vectors

Subsampling frequent tokens involves down-sampling common words (like stop words) during training. These words often don't contribute significantly to the semantic meaning of the text and can disproportionately influence the training process. The probability of sampling a word is inversely proportional to its frequency in the corpus, which is similar to the IDF (Inverse Document Frequency) weighting used in TF-IDF.

<----------section---------->

### Improvements to Word2Vec
**Negative Sampling**
*   Each training example causes the network to update all weights
*   With thousands or millions of words in the vocabulary, this makes the process computationally expensive

**Instead of updating all weights...**
*   Select 5 to 20 negative words (words not in the context)
*   Update weights only for the negative words and the target word
*   Negative words are selected based on their frequency
*   Common words are chosen more often than rare words
*   The quality of embeddings in maintained

Negative sampling is a technique used to reduce the computational cost of training Word2Vec models. Instead of updating all the weights in the network for each training example, only a small number of negative examples (words not in the context) are updated, along with the target word. This significantly speeds up the training process while maintaining the quality of the word embeddings.

<----------section---------->

### Word2Vec Alternatives
This section introduces
<----------section---------->

# Natural Language Processing and Large Language Models

## Corso di Laurea Magistrale in Ingegneria Informatica Lesson 6: Neural Networks for NLP

This material is from Lesson 6 of a Masters-level course in Computer Engineering focusing on Neural Networks for Natural Language Processing (NLP). It's presented by Nicola Capuano and Antonio Greco from DIEM – University of Salerno. The lesson covers Recurrent Neural Networks (RNNs) and their application in tasks like spam detection and text generation, providing a practical introduction to these concepts.

<----------section---------->

## Outline

The course covers the following topics:

*   **Recurrent Neural Networks:** An introduction to the architecture and functionality of RNNs.
*   **RNN Variants:** Exploration of different types of RNNs, such as Bidirectional RNNs, LSTMs (Long Short-Term Memory networks), and GRUs (Gated Recurrent Units).
*   **Building a Spam Detector:** A hands-on project demonstrating how to build a spam detection system using RNNs.
*   **Intro to Text Generation:** An overview of using RNNs for text generation tasks.
*   **Building a Poetry Generator:** A practical example of implementing an RNN-based poetry generator.

<----------section---------->

## Recurrent Neural Networks

### Neural Networks and NLP

Neural networks have become fundamental tools in various text processing tasks. However, standard feedforward networks have a significant limitation: they lack memory. This means each input is processed independently, without considering previous inputs or maintaining a state. Therefore, to process a sequence of words (a text), the entire sequence must be presented at once as a single, fixed-size data point. This is the approach used when representing text as Bag-of-Words (BoW) or TF-IDF vectors. Another approach involves averaging the word vectors of a text.

### Neural Networks with Memory

When humans read text, they process information sequentially and maintain a memory of what has been read. More specifically, the human reading process typically involves:

*   Processing sentences and words one by one.
*   Maintaining a memory of what was read previously.
*   Continuously updating an internal model as new information arrives.

Recurrent Neural Networks are designed to mimic this principle. They process sequences of information (like words in a text) by iterating through the elements of the sequence, often represented as word embeddings. The RNN maintains a state that contains information about what it has processed so far.

<----------section---------->

### Recurrent Neural Networks

*   Circles in diagrams typically represent feedforward network layers, which can be composed of one or more neurons.
*   The output of the hidden layer typically flows to the output layer, as in standard feedforward networks.
*   Crucially, in RNNs, the hidden layer's output is also fed back as an input to the hidden layer in the next time step. This "recurrence" enables the network to maintain a memory of previous inputs. This introduces the concept of processing language token by token over time, rather than as a single data point.

The provided text refers to **CHAPTER 8 Loopy (recurrent) neural networks (RNNs)**, contextualizing the explanation within a broader discussion on inter-token relationships. Convolutional Neural Networks (CNNs) can also handle these relationships by looking at windows of words, while pooling layers handle word order variations. RNNs offer a different approach, introducing the concept of memory to neural networks.

<----------section---------->

### Remembering with Recurrent Networks

Words in a document are rarely completely independent of each other. The occurrence of one word often influences or is influenced by other words. Consider these sentences:

"The stolen car sped into the arena."
"The clown car sped into the arena."

Although identical in structure (adjective, noun, verb, and prepositional phrase), the adjective swap dramatically alters the reader's understanding. The reader infers entirely different scenarios based on a single word change early in the sentence.

The challenge is to model this relationship, understanding that words like "arena" and "sped" gain different connotations depending on earlier words.

RNNs address this by enabling the network to "remember" what happened just before (specifically, what happened at time step *t* when analyzing time step *t+1*). This allows capturing patterns emerging when tokens appear in sequence. RNNs enable neural networks to remember past words within a sentence. A single recurrent neuron adds a loop to recycle the hidden layer's output at time *t*. This output becomes input at time *t+1*, processed to create the output for that hidden layer at time *t+1*. The process repeats, with the *t+1* output recycled to time step *t+2*, and so on.

This process is related to auto-regressive moving average (ARMA) models used in finance, dynamics, and feedback control.
<sup>1</sup>
<----------section---------->

### Visualizing RNNs: Unrolled Networks

The diagrams typically used to represent RNNs (like figures 8.4, 8.5, and 8.6 referenced in the original text) can sometimes be confusing during backpropagation. The concept of "unrolling" the RNN is essential. "Unrolling" refers to visualizing the RNN as a sequence of identical networks, one for each time step in the input sequence (e.g., one for each word in a sentence). Each of these networks has the same weights. Although unfolded, these are all snapshots of the same network with a single set of weights.

Each neuron in the hidden state has weights applied to each element of the input vector (like a standard feedforward network). Additionally, there's a set of trainable weights applied to the hidden neurons' output from the previous time step. The network learns how much importance to give to "past" events as it processes the input sequence token by token.

The first input in a sequence has no "past," so the hidden state at t = 0 typically receives an input of 0 from its t - 1 self. An alternative approach is to pass related but separate samples into the network sequentially, using each sample's final output as the t = 0 input for the next sample. This approach aims to preserve more information across samples, and is often called maintaining "statefulness."

<----------section---------->

### Data Input to RNNs

Imagine having a set of documents, each a labeled example. Instead of passing all word vectors into a CNN at once, as done previously, a recurrent neural net takes the sample one token at a time.

The process can be summarized as:

1.  Pass in the word vector for the first token, and get the network's output.
2.  Pass in the second token, *along with the output from the first token!*
3.  Pass in the third token, along with the output from the second token, and so on.

This gives the network a sense of "before and after," cause and effect, and a notion of time.

Text: The clown car sped into the arena

<----------section---------->

### RNN Training

All the networks discussed so far require a target variable (label). In recurrent networks, this is a single label for all the tokens in a sample text, not a label for each individual token.

... and that is enough.

*Isadora Duncan*

RNNs work identically with any time series data, not just tokens. These tokens can be discrete or continuous, like weather station readings, musical notes, or characters in a sentence.

### Backpropagation Through Time

Initially, the output of the network at the *last* time step is compared to the label to define the error. The network then tries to minimize this error. This differs from earlier approaches where the output of each subsample is not dealt with directly, but instead, is fed back into the network.

<----------section---------->

### Forward Pass and Backpropagation

The initial steps include inputting each token and calculating the loss based on the final time step's output. Backpropagation through time then occurs, updating the network's weights based on this calculated loss. During the forward pass, the input data is fed through the network to generate an output. The backpropagation stage, the error is calculated based on the difference between the predicted output and the actual target variable. This error signal is then propagated back through the network, adjusting the weights of the connections between neurons to minimize the error.

<----------section---------->

### What are RNNs Good For?

RNNs can be used in several ways, enabling NLP pipelines to ingest infinitely long sequences of text. They can also generate text for as long as desired. This capability opens new applications like generative conversational chatbots and text summarizers.

| Type          | Description                                                                   | Applications                                                                                                                                                                |
| ------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| One to Many   | One input tensor used to generate a sequence of output tensors              | Generate chat messages, answer questions, describe images                                                                                                                   |
| Many to One   | Sequence of input tensors gathered up into a single output tensor             | Classify or tag text according to its language, intent, or other characteristics                                                                                           |
| Many to Many  | A sequence of input tensors used to generate a sequence of output tensors | Translate, tag, or anonymize the tokens within a sequence of tokens, answer questions, participate in a conversation                                                        |

RNNs process sequences of tokens or vectors, overcoming the limitation of processing only fixed-length vectors. There is no need to truncate and pad input text to fit into a "square hole." RNNs can also generate text sequences indefinitely without requiring an arbitrary maximum length. The code can dynamically decide when enough is enough.

<----------section---------->

### RNNs for Text Generation

When used for text generation:

*   Each time step's output is as important as the final output.
*   Error is captured and backpropagated at each step to adjust all network weights.

### Intermediate Steps

In text generation, you generally DO care about the output of earlier steps.

"BUT YOU DO CARE WHAT CAME OUT OF THE EARLIER STEPS"

Sometimes, the entire sequence generated by each intermediate time step is important. In sequence-to-sequence tasks, the output at a given time step *t* is as important as the output at the final time step. This necessitates capturing the error at any given time step and carrying that backward to adjust all the weights during backpropagation.

<----------section---------->

### Backpropagation for Text Generation

Text generation backpropagation is like normal backpropagation through time for *n* time steps. But now, error from multiple sources is backpropagated simultaneously. Weight corrections remain additive. Backpropagation proceeds from the last time step to the first, summing up the changes for each weight. The same is done with the error calculated at the second-to-last time step, summing up all changes back to *t* = 0. This process is repeated until reaching time step 0, which is then backpropagated as if it were the only one. Finally, the total updates are applied to the corresponding hidden layer weights all at once.

<----------section---------->

## RNN Variants

### Bidirectional RNN

A Bidirectional RNN has two recurrent hidden layers:

*   One processes the input sequence forward.
*   The other processes the input sequence backward.
*   The outputs of these two layers are concatenated at each time step.

By processing the sequence in both directions, the RNN can catch patterns a unidirectional RNN might miss. For example:

*   Example: they wanted to pet the dog whose fur was brown

The basic idea is you arrange two RNNs right next to each other, passing the input into one as normal and the same input backward into the other net (see figure 8.13). The output of those two are then concatenated at each time step to the related (same input token) time step in the other network. You take the output of the final time step in the input and concatenate it with the output generated by the same input token at the first time step of the backward net.

<----------section---------->

### Keras and Bidirectional RNNs

Keras also offers a `go_backwards` keyword argument. Setting this to `True` automatically flips the input sequences and inputs them into the network in reverse order. This offers a way to train a recurrent network that is more receptive to data at the end of the sample than at the beginning, which can be useful if you've padded the input with `<PAD>` tokens at the end.

With these tools, predicting, classifying, and modeling language becomes possible, allowing for the generation of completely new statements.

#### Understanding the Thought Vector

The text describes that ahead of the Dense layer, you have a vector coming out of the last time step of the Recurrent layer for a given input sequence. This vector is of shape (number of neurons x 1) and is the parallel to the thought vector you got out of the convolutional neural network.

<----------section---------->

### Long Short-Term Memory (LSTM) Networks

In LSTMs, the rules governing the information stored in the state (memory) are trained neural nets themselves. They can be trained to learn what to remember, while the rest of the recurrent net learns to predict the target label! The introduction of memory and state allows capturing dependencies that stretch not just one or two tokens away but across an entire data sample. The state of the art is constantly evolving, but the results can be striking.

The memory state is affected by the input and also affects the layer output just as in a normal recurrent net. But that memory state persists across all the time steps of the time series (your sentence or document). So each input can have an effect on the memory state as well as an effect on the hidden layer output. The memory state learns what to remember at the same time that it learns to reproduce the output, using standard backpropagation!

<----------section---------->

### LSTM Advantages

RNNs should theoretically retain information from inputs seen many timesteps earlier, but they often struggle to learn long-term dependencies due to the vanishing gradient problem. LSTMs are designed to address this:

*   Introduces a state updated with each training example.
*   Uses trainable rules to decide what information to remember and forget.

### LSTM Structure

First, let’s unroll a standard recurrent neural net and add your memory unit.

Figure 9.2 looks similar to a normal recurrent neural net. However, in addition to the activation output feeding into the next time-step version of the layer, you add a memory state that also passes through time steps of the network. At each time-step iteration, the hidden recurrent unit has access to the memory unit. The addition of this memory unit, and the mechanisms that interact with it, make this quite a bit different from a traditional neural network layer. However, it's possible to design a set of traditional recurrent neural network layers (a computational graph) that accomplishes all the computations that exist within an LSTM layer. An LSTM layer is just a highly specialized recurrent neural network.

<----------section---------->

### LSTM Cells, Units, and Layers

The “Memory State” block is referred to as an **LSTM cell** rather than an **LSTM neuron**, because it contains two additional neurons or gates just like a silicon computer memory cell. When an LSTM memory cell is combined with a sigmoid activation function to output a value to the next LSTM cell, this structure, containing multiple interacting elements, is referred to as an **LSTM unit**. Multiple LSTM units are combined to form an **LSTM layer**. The horizontal line running across the unrolled recurrent neuron is the signal holding the memory or state. It becomes a vector with a dimension for each LSTM cell as the sequence of tokens is passed into a multi-unit LSTM layer.

LSTMs allow past information to be reinjected later, fighting the vanishing-gradient problem.

<----------section---------->

### Gated Recurrent Unit (GRU)

A Gated Recurrent Unit (GRU) is an RNN architecture specifically designed to solve the vanishing gradient problem.

#### Main Features

*   Like LSTM but with a simpler architecture.
*   GRU lacks a separate memory state, relying solely on the hidden state to store and transfer information across timesteps.
*   Fewer parameters than LSTM, making it faster to train and more computationally efficient.
*   Performance is often comparable to LSTM, particularly in tasks with simpler temporal dependencies.

### Stacked LSTM

Layering enhances the model’s ability to capture complex relationships. Note that the output at each timestep serves as the input for the corresponding timestep in the next layer. These are just two of the RNN/LSTM derivatives out there. Experiments are ever ongoing, and we encourage you to join the fun. The tools are all readily available, so finding the next newest greatest iteration is in the reach of all.

<----------section---------->

#### Stacked Layers

It's convenient to think of the memory unit as encoding a specific representation of noun/verb pairs or sentence-to-sentence verb tense references, but that isn’t specifically what’s going on. It’s just a happy byproduct of the patterns that the network learns, assuming the training went well. Like in any neural network, layering allows the model to form more-complex representations of the patterns in the training data. And you can just as easily stack LSTM layers.

Stacked layers are much more computationally expensive to train. But stacking them takes only a few seconds in Keras. See the following listing.

Each LSTM layer is a cell with its own gates and state vector.

<----------section---------->

## Building a Spam Detector

The following section provides a practical demonstration of building a spam detector using RNNs in Python.

### The Dataset

The dataset used is the SMS Spam Collection, which can be downloaded from: [https://archive.ics.uci.edu/dataset/228/sms+spam+collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

### Read the Dataset

```python
import pandas as pd

df = pd.read_csv("datasets/sms_spam.tsv", delimiter='\t', header=None, names=['label', 'text'])
print(df.head())
```

This code reads the dataset into a pandas DataFrame, using tab delimiters and assigning column names 'label' and 'text'. The `print(df.head())` command displays the first few rows of the DataFrame.

<----------section---------->

### Tokenize and Generate Word Embeddings (WEs)

```python
import spacy
import numpy as np

nlp = spacy.load('en_core_web_md')  # loads the medium model with 300-dimensional WEs

# Tokenize the text and save the WEs
corpus = []
for sample in df['text']:
    doc = nlp(sample, disable=["tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])  # only tok2vec
    corpus.append([token.vector for token in doc])

# Pad or truncate samples to a fixed length
maxlen = 50
zero_vec = [0] * len(corpus[0][0])
for i in range(len(corpus)):
    if len(corpus[i]) < maxlen:
        corpus[i] += [zero_vec] * (maxlen - len(corpus[i]))  # pad
    else:
        corpus[i] = corpus[i][:maxlen]  # truncate

corpus = np.array(corpus)
print(corpus.shape) # Expected output: (5572, 50, 300)
```

This code utilizes the `spacy` library to tokenize the text messages and generate word embeddings.
1.  Loads the `en_core_web_md` model from `spacy`, which includes pre-trained word embeddings with 300 dimensions. The pipeline components are disabled for efficiency.
2.  Iterates through the 'text' column of the DataFrame, tokenizing each text message using `nlp()`. The `disable` argument excludes unnecessary pipeline components.
3.  Appends each text represented by its word vectors to the `corpus` list.
4.  To handle variable-length sequences, the code pads or truncates each sequence to a maximum length of 50. If a sequence is shorter than 50, it's padded with zero vectors. If it's longer, it's truncated.
5.  Finally, the `corpus` list is converted into a NumPy array for further processing, with an expected shape of (5572, 50, 300), representing 5572 samples, each with a length of 50 tokens and 300-dimensional word embeddings.

<----------section---------->

### Split the dataset

```python
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode the labels
encoder = LabelEncoder()
labels = encoder.fit_transform(df['label'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
# Expected output: ((4457, 50, 300), (1115, 50, 300), (4457,), (1115,))
```

This code prepares the dataset for training by encoding the labels and splitting the data into training and testing sets.
1.  Uses `LabelEncoder` from scikit-learn to convert the text labels ('spam' and 'ham') into numerical values (0 and 1).
2.  Splits the data into training and testing sets using `train_test_split` from scikit-learn. The `corpus` array (containing word embeddings) and `labels` array are split with an 80/20 ratio.
3.  Prints the shapes of the resulting arrays to confirm the split, with an expected output of `((4457, 50, 300), (1115, 50, 300), (4457,), (1115,))`, indicating 4457 training samples and 1115 testing samples.

<----------section---------->

### Train an RNN model

```python
import keras

model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(keras.layers.SimpleRNN(64))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, batch_size=512, epochs=20, validation_data=(X_test, y_test))
```

This section defines and trains a simple RNN model using Keras.
1.  Creates a sequential model using `keras.models.Sequential()`.
2.  Adds the input layer, defining the shape of the input data as (50, 300), corresponding to the sequence length and word embedding dimensions.
3.  Adds a `SimpleRNN` layer with 64 units.
4.  Applies dropout regularization with a rate of 0.3 to prevent overfitting.
5.  Adds a dense output layer with a sigmoid activation function for binary classification.
6.  Configures the model for training using `model.compile`, specifying binary cross-entropy as the loss function, the Adam optimizer, and accuracy as the metric.
7.  Trains the model using `model.fit`, specifying the training data, batch size, number of epochs, and validation data.

<----------section---------->

### Plot the Training History

```python
from matplotlib import pyplot as plt

def plot(history, metrics):
    fig, axes = plt.subplots(1, len(metrics), figsize=(15, 5))
    for i, metric in enumerate(metrics):
        ax = axes[i]
        ax.plot(history.history[metric], label='Train') # Corrected line
        ax.plot(history.history['val_' + metric], label='Validation') # Corrected line
        ax.set_title(f'Model {metric.capitalize()}')
        ax.set_ylabel(metric.capitalize())
        ax.set_xlabel('Epoch')
        ax.legend(loc='upper left')
        ax.grid()
    plt.tight_layout()
    plt.show()

plot(history, ['loss', 'accuracy'])
```

This code defines a function `plot` that takes the training history and a list of metrics as input and generates plots of the training and validation metrics over epochs. It uses matplotlib to create subplots for each metric (loss and accuracy), plotting both the training and validation curves.

<----------section---------->

### Report and Confusion Matrix

```python
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def print_report(model, X_test, y_test, encoder):
    y_pred = model.predict(X_test).ravel()
    y_pred_class = (y_pred > 0.5).astype(int)  # convert probabilities to classes

    y_pred_lab = encoder.inverse_transform(y_pred_class)
    y_test_lab = encoder.inverse_transform(y_test)
    print(classification_report(y_test_lab, y_pred_lab, zero_division=0))

    cm = confusion_matrix(y_test_lab, y_pred_lab)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=encoder.classes_, yticklabels=encoder.classes_)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

print_report(model, X_test, y_test, encoder)
```

This code defines a function `print_report` that generates a classification report and a confusion matrix to evaluate the performance of the trained model.
1.  Uses the trained model to predict probabilities for the test set.
2.  Converts the predicted probabilities to class labels based on a threshold of 0.5.
3.  Generates a classification report using `classification_report` from scikit-learn, providing metrics such as precision, recall, F1-score, and support for each class. The labels of the matrix are inverted to the original labels using `encoder.inverse_transform`.
4.  Creates a confusion matrix using `confusion_matrix` from scikit-learn and visualizes it as a heatmap using seaborn.

<----------section---------->

### Using RNN Variants

The following code snippets show how to replace the `SimpleRNN` layer with different RNN variants:

*   Bi-directional RRN:
    `model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(64)))`
*   LSTM:
    `model.add(keras.layers.LSTM(64))`
*   Bi-directional LSTM:
    `model.add(keras.layers.Bidirectional(keras.layers.LSTM(64)))`
*   GRU:
    `model.add(keras.layers.GRU(64))`
*   Bi-directional GRU:
    `model.add(keras.layers.Bidirectional(keras.layers.GRU(64)))`

These code snippets demonstrate how to easily swap out the `SimpleRNN` layer with other RNN variants, including bidirectional RNNs, LSTMs, and GRUs, using Keras' `Bidirectional` wrapper. Each variant introduces different architectural enhancements to better capture temporal dependencies in the data.

<----------section---------->

### Using Ragged Tensors

A Ragged Tensor is a tensor that allows rows to have variable lengths.

*   Useful for handling data like text sequences, where each input can have a different number of elements (e.g., sentences with varying numbers of words).
*   Avoids the need for padding/truncating sequences to a fixed length.
*   Reduces overhead and improves computational efficiency by directly handling variable-length data.
*   Available in TensorFlow since version 2.0.
*   In PyTorch, similar functionality is provided by Packed Sequences.

```python
import tensorflow as tf

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)

# Convert sequences into RaggedTensors to handle variable-length inputs
X_train_ragged = tf.ragged.constant(X_train)
X_test_ragged = tf.ragged.constant(X_test)

# Build the model
model = keras.models.Sequential()
model.add(keras.layers.Input(shape=(None, 300), ragged=True))
model.add(keras.layers.SimpleRNN(64))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train the model using RaggedTensors
history = model.fit(X_train_ragged, y_train, batch_size=512, epochs=20,
                    validation_data=(X_test_ragged, y_test))
plot(history, ['loss', 'accuracy'])
print_report(model, X_test_ragged, y_test, encoder)
```

This section demonstrates how to use Ragged Tensors in TensorFlow to handle variable-length sequences without padding.
1.  Converts the training and testing data into Ragged Tensors using `tf.ragged.constant`.
2.  Defines a Keras sequential model with an input layer that accepts Ragged Tensors (`ragged=True`).
3.  Trains the model using the Ragged Tensors.

<----------section---------->

## Intro to Text Generation

### Generative Models

A class of NLP models designed to generate new text. This text is:

*   … coherent and syntactically correct.
*   ... based on patterns and structures learned from text corpora.

#### Generative vs Discriminative

*   Discriminative models are mainly used to classify or predict categories of data.
*   Generative models can produce new and original data.

RNN can be used to generate text

*   Transformers are better (we will discuss them later)

<----------section---------->

### Applications

Text generation has numerous applications, including:

*   **Machine Translation:** Automatically translating text from one language to another.
*   **Question Answering:** Generating answers to questions based on a given context.
*   **Automatic Summarization:** Creating concise summaries of longer texts.
*   **Text Completion:** Predicting and generating the continuation of a given text.
*   **Dialogue Systems:** Creating responses in conversational agents and chatbots.
*   **Creative Writing:** Assisting in generating poetry, stories, and other creative texts.

<----------section---------->

### Language Model

A language model is a mathematical model that determines the probability of the next token given the previous ones. It captures the statistical structure of the language (latent space). Once created, it can be sampled to generate new sequences.

The text generation process typically involves:

*   Providing an initial string of text (conditioning data).
*   The model generates a new token.
*   The generated token is added to the input data.
*   The process is repeated several times.
*   This way, sequences of arbitrary length can be generated.

<----------section---------->

### LM Training

During training, the RNN receives a token extracted from a sentence in the corpus and produces an output. The output is compared with the expected token (the next one in the sentence), generating an error used to update the network's weights via backpropagation. Unlike traditional RNNs, where backpropagation occurs only at the end of the sequence, errors are propagated at each step.

So the first thing you need to do is adjust your training set labels. The output vector will be measured not against a given classification label but against the one-hot encoding of the next character in the sequence.

<----------section---------->

### Sampling

During utilization, models have different approaches:

*   Discriminative models always select the most probable output based on the given input.
*   Generative models sample from the possible alternatives. For instance, if a word has a probability of 0.3 of being the next word in a sentence, it will be chosen approximately 30% of the time.

Temperature is a parameter (T) used to regulate the randomness of sampling:

*   A low temperature (T < 1) makes the model more deterministic.
*   A high temperature (T > 1) makes the model more creative.

<----------section---------->

### Temperature Equation

The temperature parameter affects the probability distribution as follows:

q'<sub>i</sub> = exp(log(p<sub>i</sub>) / T) / Σ exp(log(p<sub>j</sub>) / T)

Where:

*   p<sub>i</sub> is the original probability distribution
*   p<sub>i</sub> is the probability of token i
*   T > 0 is the chosen temperature
*   q' is the new distribution affected by the temperature

Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data (see figure 8.2).

<----------section---------->

#### Implementing character-level LSTM text generation

Let’s put these ideas into practice in a Keras implementation. The first thing you need is a lot of text data that you can use to learn a language model. You can use any sufficiently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. In this example, you’ll use some of the writings of Nietzsche, the late-nineteenth century German philosopher (translated into English). The language model you’ll learn will thus be specifically a model of Nietzsche’s writing style and topics of choice, rather than a more generic model of the English language.

#### PREPARING THE DATA

Let’s start by downloading the corpus and converting it to lowercase.

```python
import keras
import numpy as np

path = keras.utils.get_file(
    'nietzsche.txt',
    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('Corpus length:', len(text))
```

<----------section---------->

## Building a Poetry Generator

### Leopardi Poetry Generator

*   Download the corpus from [https://elearning.unisa.it/](https://elearning.unisa.it/)

```python
# Load the corpus
with open('datasets/leopardi.txt', 'r') as f:
    text = f.read()

# Get the unique characters in the corpus
chars = sorted(list(set(text)))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

print("Corpus length: {}; total chars: {}".format(len(text), len(chars)))
# Expected output: Corpus length: 134628; total chars: 77
```

This code loads a text corpus (in this case, poetry by Leopardi), extracts the unique characters from the corpus, and creates character-to-index and index-to-character mappings. It then prints the length of the corpus and the number of unique characters.

<----------section---------->

### Extract the Training Samples

```python
maxlen = 40  # length of the extracted sequences
samples = []  # holds the extracted sequences
targets = []  # holds the next character for each sequence

# Extract sequences of maxlen characters
for i in range(0, len(text) - maxlen):
    samples.append(text[i: i + maxlen])
    targets.append(text[i + maxlen])

print('Number of samples: {}'.format(len(samples)))
# Expected output: Number of samples: 134588

import numpy as np

# Initialize the training data
X = np.zeros((len(samples), maxlen, len(chars)), dtype=bool)
y = np.zeros((len(samples), len(chars)), dtype=bool)

# One-hot encode samples and targets
for i, sample in enumerate(samples):
    for j, char in enumerate(sample):
        X[i, j, char_indices[char]] = 1
    y[i, char_indices[targets[i]]] = 1
```

This code prepares the training data by extracting sequences of characters and their corresponding target characters from the text corpus. It then one-hot encodes the sequences and targets for training the model.
1.  Sets the sequence length (`maxlen`) to 40 characters.
2.  Iterates through the text to extract sequences of `maxlen` characters and appends them to the `samples` list. The corresponding next character is
<----------section---------->

**Natural Language Processing and Large Language Models**
Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
Lesson 7: Dialog Engines
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

**Outline**

This lesson will cover the following topics related to dialog engines:

*   Task Oriented Dialogue Systems: An overview of systems designed to help users achieve specific goals through conversation.
*   Introduction to Rasa: A popular open-source framework used for building conversational AI applications.
*   Building a Chatbot with Rasa: Practical steps and considerations for creating a chatbot using the Rasa framework.
*   Custom Actions: Extending the functionality of Rasa chatbots with custom Python code to interact with external systems.

<----------section---------->

**Task Oriented Dialogue Systems**

This section distinguishes between different types of Conversational AI, focusing on the characteristics of Task-Oriented Dialogue Systems (TOD).

**Types of Conversational AI**

*   **Chit-Chat:** These systems are designed for open-ended conversation without a specific goal. The focus is on generating natural and engaging responses to maintain the conversation flow. A successful chit-chat system aims for longer and more conversational turns.

*   **Task-Oriented Dialogue Systems (TOD):** These systems focus on helping users achieve specific goals, such as booking a flight or scheduling a meeting. The emphasis is on understanding user requests, tracking the conversation state, and generating appropriate actions to fulfill the user's objective. The goal is to accomplish the task efficiently, minimizing the number of conversational turns.

<----------section---------->

**Task Oriented Dialogue Examples:**

Examples of user intents and corresponding actions in task-oriented dialogues:

*   **Information Retrieval:**
    *   User: "I have a question."
    *   System: "Which room is the dialogue tutorial in?" (Seeking specific information)
    *   System: "When is the IJCNLP 2017 conference?" (Another specific information request)
*   **Task Completion:**
    *   User: "I need to get this done."
    *   System: "Book me the flight from Seattle to Taipei." (Request to perform a booking)
    *   System: "Schedule a meeting with Bill at 10:00 tomorrow." (Request to schedule an event)
*   **Recommendation:**
    *   User: "I need a recommendation."
    *   System: "Can you suggest me a restaurant?" (Seeking restaurant recommendations)
    *   System: "Can you suggest me something to see near me?" (Request for local attractions)

<----------section---------->

**TOD System Architecture**

Task-Oriented Dialogue Systems typically follow a modular architecture composed of three primary modules:

*   **Input Modules ("Ears"):** These modules are responsible for capturing and interpreting user input.
    *   **NLU (Natural Language Understanding):** Processes user utterances to identify intents and extract entities.
    *   **GUI (Graphical User Interface) elements:** Provides interactive components for user input.
    *   **Connector context, personal info:** Integrates context from different channels and user-specific information to improve understanding.
*   **Dialogue Management ("Brain"):** This module manages the conversation flow and determines the appropriate system response.
    *   **Conversational Modules:** Components that handle dialogue state tracking, policy management, and action selection.
*   **Output Modules ("Mouth"):** These modules generate and deliver the system's response.
    *   **NLG (Natural Language Generation):** Converts system actions into natural language responses.
    *   **GUI elements:** Presents information and options to the user through a graphical interface.
    *   **Backend, Connector to any database, conversational API, etc.:** Connects to external systems, databases, and APIs to retrieve information and perform actions.

Rasa is a popular open-source framework specifically designed for building TOD systems: [https://rasa.com/](https://rasa.com/)

<----------section---------->

**Natural Language Understanding**

The Natural Language Understanding (NLU) component is crucial for enabling the dialogue system to understand user inputs. Two main tasks performed by NLU are:

*   **Intent Classification:** This task involves identifying the user's intention behind their utterance. It is often approached as a multi-label sentence classification problem, where the model predicts one or more intents based on the input sentence.

    *   Example:

    ```text
    What's the weather like tomorrow?
    {"intent": "request_weather"}
    ```

    In this example, the model classifies the user's utterance as having the "request_weather" intent.
*   **Entity Recognition:** This task involves identifying and extracting relevant entities from the user's utterance. It is commonly approached using Named Entity Recognition (NER) techniques, which can be rule-based or machine learning-based.

    *   Example:

    ```text
    What's the weather like tomorrow?
    {"date": "tomorrow"}
    ```

    In this example, the model identifies "tomorrow" as a "date" entity.

<----------section---------->

**Conversation Design**

Planning the types of conversations your assistant will be able to have is a crucial step in building a chatbot. This involves:

*   **Understanding your users:** Defining who your users are and what their needs and expectations might be.
*   **Defining the assistant’s purpose:** Clearly outlining what the assistant is intended to do and the tasks it should be able to handle.
*   **Documenting typical conversations:** Mapping out the most common conversation flows between users and the assistant.

It's important to acknowledge that it's difficult to anticipate everything users might ask.

*   In the early stages of development, relying on hypothetical conversations is acceptable for initial design and prototyping.
*   However, it's crucial to transition to training the assistant on real conversations as soon as possible. Real-world data provides valuable insights into user behavior and helps improve the assistant's performance.

<----------section---------->

**Introduction to Rasa**

This section provides an introduction to the Rasa framework.

**Rasa Intro**

*   Rasa is an Open-Source Conversational Framework. It provides the tools and infrastructure needed to build and deploy contextual AI assistants.
    *   Launched in 2016, Rasa has become a popular choice for developers worldwide.
    *   It's used globally to create thousands of bots in various languages, catering to diverse use cases.

**Rasa Basic Units**

Rasa uses specific units to understand and respond to user inputs:

*   **Intents:** Represent what the user wants to achieve with their message.
*   **Entities:** Represent terms or objects that are relevant or necessary for fulfilling the intent.

    ```text
    Utterance: "Show me yesterday's financial news"
    Intent: showNews
    Entity: yesterday (time)
    Entity: financial news (news_type)
    ```

<----------section---------->

**Rasa Intro (continued)**

*   **Actions:** What the bot should do in response to the user's intents. Actions can range from simple responses to complex operations.
*   **Responses:** Predefined utterances that the bot can use to communicate with the user. These can be simple text messages, images, or structured data.
*   **Complex Actions:** Custom Python code that allows the bot to interact with external systems, such as databases, Web APIs, or other services.
*   **Slots:** Variables used to store information extracted from user inputs during a conversation. Slots enable the bot to remember important details and use them later in the conversation.
*   **Forms:** A set of slots used to collect multiple pieces of information from the user in a structured manner. Forms simplify the process of gathering required data for a specific task.
*   **Stories:** Sequences of user intents and bot actions used to pre-program dialog scenarios. Stories provide examples of how the bot should behave in different situations and guide the training process.

<----------section---------->

**Rasa Intro - Sample Story**

```text
## explain nlu story name
* greet
  - utter_greet  
* explain_rasa_nlu
  - utter_explain_rasa_nlu
```

This is a simple story defining a conversation flow:

1.  User greets the bot (`greet` intent).
2.  Bot responds with a greeting (`utter_greet` action).
3.  User asks the bot to explain Rasa NLU (`explain_rasa_nlu` intent).
4.  Bot responds with an explanation (`utter_explain_rasa_nlu` action).

<----------section---------->

**Installing Rasa**

To install Rasa, it is recommended to create a virtual environment:

*   Create a new virtual environment:

    ```bash
    python -m venv rasa.env
    ```
*   Activate the virtual environment:

    ```bash
    source rasa.env/bin/activate
    ```
*   Install Rasa using pip:

    ```bash
    pip install rasa
    ```

<----------section---------->

**Rasa Project**

*   Most of the chatbot's configuration and data are stored in YAML files. This includes intents, entities, stories, rules, and responses.
*   Python code is needed to program more complex actions that interact with external systems or perform custom logic.

**Create a New Project**

To create a new Rasa project, use the following command:

```bash
rasa init
```

This command will create a directory structure with the necessary files for your chatbot.

<----------section---------->

**Directory Structure**

The Rasa project directory structure is organized as follows:

*   `actions/`: Contains Python code for custom actions that extend the bot's functionality.
*   `data/nlu.yml`: Defines the NLU training data, including intents and entities, with examples of user utterances.
*   `data/rules.yml`: Defines short conversation paths or specific rules that should always be followed, regardless of the training data. These rules ensure predictable bot behavior in certain scenarios.
*   `data/stories.yml`: Defines general stories to train the model on possible conversation flows. Stories are sequences of user intents and bot actions that represent different conversation scenarios.
*   `models/`: Stores the trained models that are generated during the training process.
*   `tests/`: Includes Bot test cases to evaluate the bot's performance and ensure it behaves as expected.
*   `config.yml`: Defines the NLU pipeline, dialogue policies, and other components used by Rasa.
*   `credentials.yml`: Stores credentials for connecting to external platforms and channels.
*   `domain.yml`: The main configuration file that lists all intents, entities, slots, responses, forms, and actions used by the bot.
*   `endpoints.yml`: Lists the endpoints that the bot can use to connect to external services, such as the action server.

<----------section---------->

**domain.yml - Session Configuration**

```yaml
session_config:
  session_expiration_time: 60  # minutes
  carry_over_slots_to_new_session: true
```

*   `session_expiration_time`: Specifies the duration (in minutes) after which a session is considered expired.
*   `carry_over_slots_to_new_session`: Determines whether slot values from the previous session should be carried over to a new session. If set to `true`, the data from the previous session is transferred to the new one, providing a more seamless user experience.

<----------section---------->

**nlu.yml**

(See example content in later sections)

*Note:* To train the model to recognize intents effectively, RASA requires at least 7-10 example utterances per intent. More examples generally lead to better performance.

**stories.yml**

(See example content in later sections)

**rules.yml**

(See example content in later sections)

<----------section---------->

**Visualize Stories**

The `rasa visualize` command generates a visual representation of the stories defined in the `stories.yml` file. This visualization helps in understanding the conversation flows and identifying potential issues.

```bash
rasa visualize
```

**Other Commands**

*   `rasa train`: Trains a model using your NLU data and stories, and saves the trained model in the `./models` directory.
*   `rasa shell`: Loads your trained model and lets you interact with your assistant on the command line, simulating a conversation.
*   `rasa run`: Starts a server with your trained model, allowing it to handle external requests. The `--cors "*"` option enables cross-origin requests.  This is essential if your frontend is served from a different domain than your Rasa server.
    ```bash
    rasa run --cors "*"
    ```
*   `rasa --h`: Displays a help message with a list of available commands and options.

<----------section---------->

**Rasa REST API**

Rasa provides a REST endpoint that allows external systems to communicate with the chatbot.

*   You can post messages to the endpoint and receive the bot’s responses in JSON format. This enables integration with web applications and other platforms.
*   To enable the REST channel, add the following configuration to your `credentials.yml` file:

    ```yaml
    # you don't need to provide anything here - this channel doesn't
    # require any credentials
    ```
*   After restarting your Rasa server, you can reach the bot at: `http://<host>:<port>/webhooks/rest/webhook`
*   Consult the Rasa documentation for more details: [https://rasa.com/docs/rasa/connectors/your-own-website/](https://rasa.com/docs/rasa/connectors/your-own-website/)

<----------section---------->

**Request and Response Format**

The REST API uses JSON for both requests and responses.

You can POST JSON requests with the following format:

```json
{
  "sender": "test_user",  // sender ID
  "message": "I'm sad!"
}
```

The response from Rasa will be a JSON body of bot responses, for example:

```json
[
  {
    "recipient_id": "test_user",
    "text": "Here is something to cheer you up:"
  },
  {
    "recipient_id": "test_user",
    "image": "https://i.imgur.com/nGF1K8f.jpg"
  },
  {
    "recipient_id": "test_user",
    "text": "Did that help you?"
  }
]
```

The response is an array of messages, each containing the recipient ID and the content of the message (text, image, etc.).

<----------section---------->

**Web-based Frontends**

You can integrate a Rasa bot into your website using different approaches:

*   **Custom Implementation:** Build your own frontend using HTML/CSS/JavaScript. This provides full control over the user interface and allows for tailored integration.
*   **Use a pre-built solution:**
    *   Rasa Widget (React-based)
        *   Clone from: [https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)
        *   Copy the `./dist` files to your web project to use the widget

<----------section---------->

**More on Connectors**

*   You can enable authentication for the connectors to secure communication with the bot.
*   You can use web-sockets for real-time interaction, enabling bidirectional communication between the client and the server.
*   Rasa provides built-in connectors for various messaging and voice channels:
    *   Facebook Messenger
    *   Slack
    *   Telegram
    *   Twilio
    *   Microsoft Bot Framework
        [https://rasa.com/docs/rasa/messaging-and-voice-channels/](https://rasa.com/docs/rasa/messaging-and-voice-channels/)
    *   Cisco Webex Teams
    *   RocketChat
    *   Mattermost
    *   Google Hangouts Chat

<----------section---------->

**Building a Chatbot with RASA**

This section will show how to build a simple chatbot using RASA.

**Domain File**

The domain file (`domain.yml`) is a crucial configuration file that defines everything your assistant knows and can do. It acts as a central repository for:

*   **Responses:** The things the assistant can say to users. These can be simple text messages, images, buttons, or custom payloads.
*   **Intents:** Categories of things users say. Intents represent the user's intention behind their message (e.g., `greet`, `book_flight`, `order_pizza`).
*   **Entities:** Pieces of information extracted from incoming text, such as names, dates, locations, or product names.
*   **Slots:** Variables remembered over the course of a conversation. Slots store information extracted from user input and allow the assistant to maintain context.
*   **Actions:** These add application logic and extend what your assistant can do, such as calling an API, querying a database, or performing a calculation.

<----------section---------->

**Domain File - Basic Responses**

```yaml
responses:
  utter_greet:
    - text: "Hey there!"
  utter_goodbye:
    - text: "Goodbye :("
  utter_default:
    - text: "Sorry, I didn't get that, can you rephrase?"
  utter_youarewelcome:
    - text: "You're very welcome."
  utter_iamabot:
    - text: "I am a bot, powered by Rasa."
```

This example defines a few basic responses that the assistant can use to reply to the user. Each response is associated with a unique name (e.g., `utter_greet`).  The `utter_` prefix is a convention that helps to distinguish responses from other types of actions.

<----------section---------->

**Domain File - Multiple Responses**

```yaml
responses:
  utter_greet:
    - text: "Hey, {name}. How are you?"
    - text: "Hey, {name}. How is your day going?"
```

Slots: `{name}` will be filled with the value of the name slot ("None" until it's filled).

This example demonstrates how to define multiple responses for a single utterance. Rasa will randomly select one of these responses each time the utterance is triggered.  It also shows how to use slots within responses to personalize the interaction. The value of the `name` slot will be inserted into the text. If the `name` slot is not set, the value will be "None".

<----------section---------->

**Domain File - Responses: Buttons and Images**

```yaml
responses:
  utter_greet:
    - text: "Hey! How are you?"
      buttons:
        - title: "great"
          payload: "/mood_great"
        - title: "super sad"
          payload: "/mood_sad"
  utter_cheer_up:
    - text: "Here is something to cheer you up:"
      image: "https://i.imgur.com/nGF1K8f.jpg"
```

This example shows how to add buttons and images to responses.

*   The `buttons` attribute allows you to add interactive buttons to the response.  The `title` is the text displayed on the button, and the `payload` is the intent that will be triggered when the button is pressed.  Using buttons helps to guide the user and improve the user experience.
*   The `image` attribute allows you to include an image in the response.

<----------section---------->

**Domain File - List of Intents**

```yaml
intents:
  - greet
  - goodbye
  - affirm
  - deny
  - thankyou
  - mood_great
  - mood_unhappy
  - bot_challenge
  - search_concerts
  - search_venues
  - compare_reviews
  - how_to_get_started
```

This section lists all the intents that the assistant should be able to recognize.

*   The intents defined here must correspond to the intents defined in the NLU file (`nlu.yml`).
*   *Tip:* Start with the fewest intents possible and add more as needed. You can always add or change intents at any time during development.

<----------section---------->

**Domain File - List of Entities**

```yaml
entities:
  - PERSON
  - time
  - membership_type
  - priority
```

This section lists all the entities that the assistant should be able to extract from user input.

*   Entities can represent various types of information, such as numbers, dates, country names, or product names.
*   Standard entities can be extracted using pre-built models. Specific modules must be included in the configuration file (`config.yml`).
*   Custom entities can be extracted using regular expressions, lookup tables, or machine learning techniques.  The NLU file (`nlu.yml`) will specify how to extract these custom entities.

<----------section---------->

**NLU File**

The NLU file (`nlu.yml`) is aimed at training the system to extract structured information from user messages.

*   This includes defining the intents and the entities involved in each intent.

**Training data**

*   The NLU file contains example user utterances categorized by intent. These examples are used to train the NLU model to recognize different intents and extract entities.

**Extra information**

The NLU file can also contain extra information to improve entity extraction:

*   Regular Expressions: patterns to capture specific types of entities based on their format.
*   Lookup Tables: comprehensive lists of possible values for entities, such as cities or product names.
*   Synonyms: Define synonyms for common terms to ensure that variations in user input are understood correctly.

<----------section---------->

**NLU File - Sample Lists of Utterances**

```yaml
nlu:
  - intent: book_flight
    examples: |
      - I want to book a flight to [New York](city)
      - Book a flight from [Los Angeles](city) to [Chicago](city)
      - Can you help me book a flight to [San Francisco](city)?
      - I need a flight ticket to [Boston](city)
      - I'd like to fly from [Houston](city) to [Atlanta](city)

  - intent: check_flight_status
    examples: |
      - What is the status of flight [AA123](flight_number)?
      - Can you tell me if flight [UA456](flight_number) is delayed?
      - I want to check the status of flight number [DL789](flight_number)
      - Is flight [AA123](flight_number) on time?
```

This example shows how to define intents and provide example utterances for training the NLU model.

*   Each intent is defined with a name (e.g., `book_flight`, `check_flight_status`) and a list of example utterances.
*   Entities are marked within the utterances using square brackets and the entity name (e.g., `[New York](city)`). The names defined inside the parenthesis should corresponds to those listed in the domain file.

<----------section---------->

**NLU File - Sample Extra Information**

```yaml
- lookup_table: city
  examples: |
    - New York
    - Los Angeles
    - Chicago
    - San Francisco
    - Boston
    - Houston
    - Atlanta

- synonym: flight
  examples: |
    - flight
    - flight ticket
    - plane ticket
    - air ticket

- regex:
  name: flight_number
  pattern: "\\b[A-Z0-9]{2,5}\\b" # Regex for flight numbers
```

This example shows how to define lookup tables, synonyms, and regular expressions to improve entity extraction.

*   `lookup_table`: Defines a list of possible values for an entity. This helps the model to recognize entities even if they are not explicitly mentioned in the training data.
*   `synonym`: Defines synonyms for common terms to ensure that the model understands variations in user input.
*   `regex`: Defines a regular expression pattern for extracting entities. This is useful for entities that follow a specific format, such as flight numbers or dates.
*   Without lookup tables and regex, custom entities are simply recognized based on machine learning.

<----------section---------->

**NLU File - Entity Roles**

Entity Roles allow you to add more details to your entities, specifying different roles for the same entity type. This provides more context for the dialogue system.

Example:

"I am looking for a flight from New York to Boston."

```yaml
- example: |
    I am looking for a flight from [New York]{"entity":"location", "role":"origin"} to [Boston] {"entity":"location", "role":"destination"}.
```

In this example, "New York" is assigned the role of "origin" and "Boston" is assigned the role of "destination", even though both are "location" entities.  This distinction helps the dialogue system understand the user's request more accurately.

<----------section---------->

**NLU File - Good Practices**

*   Start with the smallest possible number of intents and gradually add more as needed.
*   Focus on the most common user intents first. Most users want to accomplish similar tasks.
*   Additional intents will emerge from analyzing user data.
*   Don’t use intents to store information. Use entities instead. Intents should represent the user's intention, while entities should capture the specific details of their request.

<----------section---------->

**Stories File**

Stories are training data used to teach your assistant what it should do next in a conversation. Stories are examples of possible conversation flows, represented as sequences of user intents and bot actions.

Stories File - example

```yaml
stories:
  - story: happy path
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_great
      - action: utter_happy

  - story: sad path 1
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_unhappy
      - action: utter_cheer_up
      - action: utter_did_that_help
      - intent: affirm
      - action: utter_happy

  - story: sad path 2
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_unhappy
      - action: utter_cheer_up
      - action: utter_did_that_help
      - intent: deny
      - action: utter_goodbye
```

This example shows how to define stories that represent different conversation flows.

*   Each story has a name (e.g., `happy path`, `sad path 1`, `sad path 2`) and a list of `steps`.
*   Each step consists of a user intent and a bot action. The intent represents what the user says, and the action represents what the bot does in response.
*   These stories teach the bot how to respond to different user inputs and guide the conversation towards a successful outcome.

Example of unexpected behavior:

User input: Hi, I need a hospital A

Next best action the assistant can take:

*   Sorry, I didn't quite understand. Can you rephrase?

utter\_ask\_name I am ill and would like to

utter\_greet\_user find the nearest hospital to

get a checkup.

*   Sure, let me do a search. Where are you based?

*Low confidence scenario:*
If the confidence is too low, the chatbot needs to recognize it and ask the user to rephrase their message. This ensures that the chatbot is not making assumptions based on incomplete or uncertain information.

<----------section---------->

**Stories File - Good Practices**

*   Start with common flows (happy paths) and then add common errors/digressions as needed.
*   Use interactive learning to improve stories by engaging in conversations with the bot and adding new examples based on the interactions.
*   Once your model is in production, add more data from real user conversations to continuously improve the model's accuracy and coverage.

<----------section---------->

**Stories File - OR Statements & Checkpoints**

```yaml
stories:
  - story: newsletter signup with OR
    steps:
      - intent: signup_newsletter
      - action: utter_ask_confirm_signup
      - or:
        - intent: affirm
        - intent: thanks
      - action: action_signup_newsletter

  - story: beginning of conversation
    steps:
      - intent: greet
      - action: utter_greet
      - checkpoint: ask_feedback

  - story: end conversation
    steps:
      - intent: goodbye
      - action: utter_goodbye
```

This example showcases advanced features for creating more flexible and reusable stories:

*   `OR Statements`: Allow for the same action to be triggered by different intents. This is useful when multiple intents have the same meaning or require the same response.
*   `Checkpoints`: Allow you to link to other stories, creating reusable conversation fragments. This simplifies the creation of complex conversation flows and reduces code duplication.

<----------section---------->

**Rules File**

Rules are a way to describe short pieces of conversations that always go the same way. Rules are used for predictable conversation patterns and do not involve machine learning.

*   Rules are not suitable for multi-turn interactions.
*   Rules are not used to train ML algorithms but are applied as is, providing a deterministic way to handle specific scenarios.

```yaml
rules:
  - rule: Say goodbye anytime the user says goodbye
    steps:
      - intent: goodbye
      - action: utter_goodbye

  - rule: Say 'I am a bot' anytime the user challenges
    steps:
      - intent: bot_challenge
      - action: utter_iamabot
```

This example shows how to define rules that handle specific user intents:

*   The first rule ensures that the bot always says goodbye when the user says goodbye.
*   The second rule ensures that the bot responds with "I am a bot" when the user challenges whether it is a bot.

<----------section---------->

**Slots**

Slots are your assistant's memory. Slots enable your assistant to store important details from the conversation and use them later in a specific context. Slots can be configured to influence the flow of the conversation, allowing the assistant to adapt its behavior based on the information it has collected.

Example user message:"I would like to book a flight to Sydney."

*Example assistant behavior if slot "destination" is not yet filled:*

* Booking a ticket to Sydney!

Example user message: "I would like to book a flight to New York."

*Example assistant behavior if slot "destination" is already filled:*

* Sure! Looking for the options.

Example user message:"I would like to book a flight ticket."

*Example assistant behavior if the "destination" slot is empty:*

* What is your destination?

<----------section---------->

**Slots and Entities**

Slots are defined in the domain file and are usually connected to entities.

```yaml
entities:
  - destination

slots:
  destination:
    type: text
    influence_conversation: true
    mappings:
      - type: from_entity
        entity: destination
```

This example shows how to define a slot and connect it to an entity:

*   The `entities` section defines the `destination` entity.
*   The `slots` section defines a slot named `destination`.
    *   `type`: Specifies the data type of the slot (e.g., `text`, `boolean`, `categorical`, `float`, `list`, `any`).
    *   `influence_conversation`: Indicates whether the slot should influence the conversation flow.
    *   `mappings`: Defines how the slot will be filled.  In this case, the slot will be filled with the value of the `destination` entity if it is present in the user's message.

<----------section---------->

**Slot Mappings**

Allow you to define how each slot will be filled in. Are applied after each user message.

```yaml
entities:
  - entity_name

slots:
  amount_of_money:
    type: any
    mappings:
      - type: from_entity
        entity: number
        intent: make_transaction
        not_intent: check_transaction
```

"Send $200 to Ben." - Intent: make\_transaction - slot is set

"Did I receive the $1000 that Alice sent me yesterday?" - Intent: check\_transaction - slot is not set

This example showcases how to conditionally fill slots based on the detected intent:

*   The `mappings` section defines how the `amount_of_money` slot will be filled.
    *   `type: from_entity`: Indicates that the slot should be filled from an entity.
    *   `entity: number`: Specifies that the slot should be filled from the `number` entity.
    *   `intent: make_transaction`: Specifies that the mapping should only be applied when the intent is `make_transaction`.
    *   `not_intent: check_transaction`: Specifies that the mapping should not be applied when the intent is `check_transaction`.

<----------section---------->

**Slot Mappings - Parameters**

*   `intent`: only applies the mapping when this intent is predicted.  The slot is only filled if the specified intent is detected.
*   `not_intent`: does not apply the mapping when this intent is predicted.  The slot is not filled if the specified intent is detected.
*   `role`: only applies the mapping if the extracted entity has this role. The slot is only filled if the entity has the specified role.

These parameters provide fine-grained control over how slots are filled, allowing you to create more sophisticated and context-aware dialogue systems.

<----------section---------->

**Use Slots in Responses**

You can create more dynamic responses by including slots in the responses.

```yaml
slots:
  name:
    type: any

responses:
  utter_greet:
    - text: "Hello {name}! How are you?"
    - text: "Hello there :)"
    - text: "Hi. How can I help you today?"
```

If `name` is not set, then its value will be `None`.

This example shows how to use slots within responses to personalize the interaction.

*   The `utter_greet` response includes the `{name}` slot, which will be replaced with the value of the `name` slot.
*   If the `name` slot is not set, the value will be `None`.  In many languages, the string "None" will be rendered. To avoid an unnatural user experience, consider including alternative, fallback greetings.

<----------section---------->

**Pipeline Configuration**

The `config.yml` file defines the NLU pipeline, and the dialogue policies used by Rasa. This file is critical for configuring how Rasa processes user input and determines the appropriate response.

*   `language`: Defines the language of the bot (e.g., `en`, `fr`, `it`).
*   `pipeline`: Specifies the sequence of components to process user messages (NLU pipeline) to extract intents and entities.
*   `policies`: Defines how the bot should handle dialogue and predict next actions. Policies determine how the bot makes decisions based on the conversation history.

```yaml
language: en

pipeline: null # The default pipeline is used to train your model.

policies: null # The default policies are used to train your model.
```

<----------section---------->

**NLU Pipeline**

The pipeline defines the sequence of components that process user messages:

*   **Tokenizers:** Break down the text into tokens (words, subwords), such as `WhitespaceTokenizer` or `SpacyTokenizer`.
*   **Featurizers:** Convert tokens into numerical features that models can use, such as `CountVectorsFeaturizer` or `ConveRTFeaturizer`.
*   **Classifiers:** Determine the user’s intent, such as `DIETClassifier` or `SklearnIntentClassifier`.
*   **Entity Extractors:** Identify named entities (e.g., names, dates), such as `RegexEntityExtractor` or `SpacyEntityExtractor`.

```yaml
pipeline:
  - name: WhitespaceTokenizer
  - name: CountVectorsFeaturizer
  - name: DIETClassifier
    epochs: 150
  - name: EntitySynonymMapper
```

<----------section---------->

**NLU Pipeline - Tokenizers**

*   `WhitespaceTokenizer`: Splits text into tokens based on whitespace characters. This is a simple and fast tokenizer, but may not be suitable for all languages.
*   `SpacyTokenizer`: Leverages SpaCy’s tokenization capabilities. SpaCy provides more sophisticated tokenization rules and supports
<----------section---------->

**Original Text:**
Natural Language Processing and Large Language Models. Corso di Laurea Magistrale in Ingegneria Informatica. Lesson 8: Building a Chatbot. Nicola Capuano and Antonio Greco, DIEM – University of Salerno.

**Exercise: Building a Pizzeria Chatbot**

Develop a chatbot to assist with pizzeria operations. Users can:

*   Request the pizzeria's menu.
*   Order a pizza that is available on the menu (just one pizza, no beverage).
*   Upon order confirmation, the bot will log the date, the user ID, and the kind of ordered pizza (use a custom action).
*   The bot has a web-based GUI.

Hint: Start with a dummy bot.

Example conversation:

User: Can I have the menu?
Bot: What kind of pizza would you like? We have Parenti, Pepperoni, Vegetarian...
User: Pepperoni
Bot: You want a Pepperoni pizza. Is that correct?
User: Yes
Bot: Great! Your Pepperoni pizza is on its way!

**Hints**

Start with a dummy bot:

```bash
mkdir pizzaBot
cd pizzaBot
rasa init --no-prompt
```

Configure and run the REST and the Actions servers:

```bash
rasa run --cors "*"
rasa run actions
```

Use a Web frontend like:

[https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)

**Enhanced Text:**

This document outlines the requirements for building a pizzeria chatbot as part of the "Natural Language Processing and Large Language Models" course (Corso di Laurea Magistrale in Ingegneria Informatica), specifically Lesson 8 on chatbot development. The course is instructed by Nicola Capuano and Antonio Greco from DIEM at the University of Salerno.

The exercise focuses on creating a chatbot capable of assisting with basic pizzeria operations. The core functionalities include menu requests and pizza ordering.

Key requirements for the Pizzeria Chatbot:

*   **Menu Request:** The chatbot should be able to provide the pizzeria's menu upon user request.
*   **Pizza Ordering:** Users should be able to order one pizza at a time (without beverages) from the available menu.
*   **Order Logging:** Upon confirmation of an order, the chatbot must log the date, user ID, and type of pizza ordered using a custom action. This highlights the need for custom actions to extend the chatbot's capabilities beyond basic responses.
*   **Web-based GUI:** The chatbot should have a web-based graphical user interface (GUI), implying the need for a front-end to interact with the bot.

To help students get started, the instructions suggest creating a basic "dummy bot" first. This iterative approach simplifies the development process by allowing students to gradually add complexity.

Example Conversation:

This provides a basic flow of interaction:

User: Can I have the menu?
Bot: What kind of pizza would you like? We have Parenti, Pepperoni, Vegetarian...
User: Pepperoni
Bot: You want a Pepperoni pizza. Is that correct?
User: Yes
Bot: Great! Your Pepperoni pizza is on its way!

This example demonstrates the chatbot's ability to understand user requests, present menu options, confirm orders, and provide feedback.

Practical Hints:

The following hints provide a starting point for setting up the development environment using Rasa, a popular open-source conversational AI framework.

1.  Setting up the project:

```bash
mkdir pizzaBot
cd pizzaBot
rasa init --no-prompt
```

These commands create a project directory named "pizzaBot," navigate into it, and initialize a basic Rasa project without prompting for user input during initialization.

2. Configuring and Running the Servers:

```bash
rasa run --cors "*"
rasa run actions
```

The `rasa run --cors "*"` command starts the Rasa server, enabling Cross-Origin Resource Sharing (CORS) from any origin, which is useful for development. `rasa run actions` starts the action server, allowing the execution of custom actions (like logging order details).

3. Using a Web Frontend:

The instructions suggest using a web frontend like the Chatbot Widget from JiteshGaikwad (version Widget2.0). The link to the GitHub repository is: [https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0). This widget allows developers to quickly integrate a chat interface into their web applications.

<----------section---------->

**Original Text:**
fly. When you need to inspire students and keep them engaged. Teachers do
this naturally, by adjusting what they say and how they say it based on
feedback on how well the student understand what they are saying. And
teachers think about more than just "delivering a message." They must think
up new ideas and approaches, on the fly as students pose interesting new
questions. Inspiring students' curiosity with Socratic questions and being
responsive to their changing needs is a full-time job.
It is virtually impossible to build a rule-based system that captures all the
things that teachers do to help students learn and grow. Students' needs are
too diverse and dynamic. This is why hybrid chatbots that integrate LLMs
have become the preferred way build production chatbots in virtually every
domain. An LLM can confidently and convincingly chat with your users on
virtually any topic. The key is to harness this power smartly, so that it doesn’t
mislead your users, or worse.

**Enhanced Text:**

The ability to adapt and inspire is crucial, especially in education. Teachers naturally adjust their communication based on student understanding. This involves not just delivering a message, but also adapting to new ideas and questions in real-time. Stimulating curiosity through methods like Socratic questioning and responding to changing student needs is a demanding task.

Creating a rule-based system to fully replicate a teacher's capabilities is extremely difficult due to the diversity and dynamism of student needs. This is why hybrid chatbots, which integrate Large Language Models (LLMs), are increasingly favored for building production chatbots across various domains. LLMs can engage in confident and convincing conversations on virtually any topic. However, the key is to leverage this power wisely, ensuring accuracy and avoiding misleading or harmful information. This highlights the importance of responsible AI development and deployment, especially in sensitive areas like education. The need for inspiring students, posing Socratic questions, and being responsive to evolving needs underscores the complexities involved in teaching and the potential benefits of AI-assisted learning, provided it's implemented thoughtfully.

<----------section---------->

**Original Text:**
12.5 Chatbot frameworks
In each of the previous chapters, you’ve learned a new technique for
processing text to understand what the user is saying. And in this chapter,
you’ve learned four approaches to generating text for a chatbot to use in its
response to the user. You’ve already assembled a few chatbots from these
NLU and NLG algorithms to understand the advantages and disadvantages of
each of these algorithms. Now you have the knowledge you need to use a
chatbot framework
 smartly. A chatbot framework is an application and a
software library that abstracts away some of these detailed decisions you
need to make when building a dialog engine for your chatbot. A framework
gives you a way to specify your chatbot’s behavior in 
domain-specific
language
 that it can later interpret and 
run
 so that your chatbot replies the
way you intended.
Most chatbot frameworks use a declarative programming language to specify
a bot’s behavior and some even give you a graphical user interface to
program your bot. There are no-code chatbot frameworks that abstract the
declarative chatbot programming language with an interactive graphical
representation of the dialog graph or flow diagram that you can modify with
your mouse. These no-code frameworks usually include a dialog engine that
can execute your chatbot without you ever having to see or edit the
underlying data. In the impact world, an open source platform sponsored by
UNICEF, RapidPro,
[
34
]
 served as a core for several chatbot platforms, such as
Weni, Textit and Glific, that are all used for impact purposes. In RapidPro,
you can build your dialogs in a graphical user interface. You can also easily
import and export the content using open standard file formats which is
helpful when you want to translate the content from one natural language to
another for a multilingual chatbot. ManyChat and Landbot are two closed
source no-code chatbot builders that have similar functionality.
But if you’ve read this far, you probably have ideas for more sophisticated
chatbots than what’s possible in a no-code platform. So you will probably
need a chatbot programming language to make your vision a reality. Of
course, you can specify your bot "stack" in Python by directly employing the
skills you learned in this book. But if you want to build a scalable and
maintainable chatbot you’ll need a chatbot framework that uses a chatbot
design language or data structure that you understand. You want a language
that makes sense to you so that you can quickly get the conversation design
you have in your head embedded in a working chatbot. In this section, you
will learn of several different frameworks that can help you make your
chatbot dreams come true.

**Enhanced Text:**

After acquiring techniques for processing text to understand user input and generating chatbot responses, you can effectively utilize chatbot frameworks. These frameworks abstract away intricate details involved in building a dialog engine, enabling you to define chatbot behavior using domain-specific languages that the system can interpret and execute, ensuring the chatbot replies as intended.

Most chatbot frameworks utilize declarative programming languages to specify bot behavior, with some even offering graphical user interfaces (GUIs) for programming. No-code chatbot frameworks abstract the declarative language into interactive graphical representations of dialog graphs, which can be modified using a mouse. These frameworks usually include a dialog engine that allows chatbot execution without direct manipulation of the underlying data.

For example, RapidPro, an open-source platform sponsored by UNICEF, has served as a core for several chatbot platforms used for impactful applications, such as Weni, Textit, and Glific. RapidPro allows building dialogs within a GUI and supports importing/exporting content in open standard file formats, simplifying the creation of multilingual chatbots. Closed-source no-code chatbot builders like ManyChat and Landbot offer similar functionalities.

However, more sophisticated chatbot ideas may require capabilities beyond no-code platforms, necessitating the use of a dedicated chatbot programming language. While Python can be used directly by leveraging the skills learned throughout this book, building scalable and maintainable chatbots benefits from using a framework with a chatbot design language or data structure that is easily understood. The goal is to utilize a language that facilitates quickly translating conversation designs into a working chatbot.

<----------section---------->

**Original Text:**
Using the tools described here, you can build a bot that can serve you (and
maybe a few friends, or even more people if you’re lucky) if deployed on a
server or in a cloud. However, if you want to build a chatbot that servers
hundreds or thousands of users, you need a more robust, scalable system.
Luckily, there are frameworks available that allow you to focus on building
your bot while taking care of the challenges that come with the need to build
a production-grade system. We will now discuss three popular open-source
Python chatbot frameworks for building chatbots with configurable NLP
capabilities: Rasa, LangChain, and qary.
12.5.1 Building an intent-based chatbot with Rasa
Rasa is an open-source conversational framework that started back in 2016
and today is used to create thousands of bots in various languages around the
world. Unlike many commercial frameworks, that create a drag-and-drop
interface to create the dialog trees we discussed in the previous section,
RASA took a radically different approach to organizing multi-step
conversations.
The basic units of a conversation in RASA are a user intent and a bot action -
which can be as simple as a pre-programmed utterance or a complex action
programmed in Python that results in interaction with other systems - such as
saving or retrieving data from a database, or invoking a Web API. By
chaining these building blocks into sequences - called Stories - RASA allows
you to pre-program dialog scenarios in a streamlined way. All this
information is stored in YAML files (YAML stands for Yet Another Markup
Language), each type of components in its own file.
But enough with the theoretical explanation - let’s get your hands dirty and
build your first RASA chatbot. First, let’s decide what dialog we want to
implement - based on our conversation diagram for the math tutor bot, let’s
implement the following short dialog:
USER: Hello
BOT: Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!
BOT: Are you a parent (or guardian) or are you a student?
USER: I'm a parent.
BOT: For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?
USER: I agree
BOT: Thank you for giving permission for your child to chat with Rori.
When your child is ready to start, please give them this phone and have them type "ready".
To create your bot, you will need to install 
rasa
 package (if you’re working
in 
nlpia2
 environment, it is already installed when you install the project).
Then, you can go to the directory you want to create the project in and run in
your command line:
$ rasa init
The installation wizard will guide you through creating a new project and
even offer you to train an initial model. Let it do that, and then you can even
chat with a simple chatbot the wizard initialized for you.

**Enhanced Text:**

While tools mentioned previously can help build bots for personal use or small-scale deployments, a more robust and scalable system is required to serve hundreds or thousands of users. Fortunately, frameworks are available that allow developers to focus on building the bot's logic while handling the complexities of production-grade systems.

This section discusses three popular open-source Python chatbot frameworks with configurable NLP capabilities: Rasa, LangChain, and qary.

**Building an Intent-Based Chatbot with Rasa**

Rasa is an open-source conversational AI framework that emerged in 2016 and is now used to create numerous bots in various languages worldwide. Unlike some commercial frameworks that use drag-and-drop interfaces for creating dialog trees, Rasa adopts a different approach for organizing multi-step conversations.

The fundamental units of conversation in Rasa are user intents and bot actions. A bot action can be as simple as a pre-programmed utterance or a more complex Python script interacting with external systems like databases or web APIs. By linking these units into sequences called "Stories," Rasa enables streamlined pre-programming of dialog scenarios. All the relevant information is stored in YAML files (YAML stands for "Yet Another Markup Language"), with each component type residing in its own file.

Now, let's move from theoretical explanations to practical application by building a Rasa chatbot. We will implement a dialog inspired by the math tutor bot example:

USER: Hello
BOT: Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!
BOT: Are you a parent (or guardian) or are you a student?
USER: I'm a parent.
BOT: For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?
USER: I agree
BOT: Thank you for giving permission for your child to chat with Rori. When your child is ready to start, please give them this phone and have them type "ready".

To begin, install the `rasa` package. If you're working within the `nlpia2` environment, it's likely already installed. Navigate to the desired project directory in your command line and run:

```bash
$ rasa init
```

The installation wizard will guide you through creating a new project and training an initial model. You can then interact with the basic chatbot that the wizard initializes.

<----------section---------->

**Original Text:**
Let’s now dive into the structure of our project and understand how to build a
dialog like you’ve just had. Here is the directory structure you should see in
the project’s folder:
├───.rasa
│   └───cache
│       ├───...
├───actions
│   └───__pycache__
├───data
├───models
└───tests
The directory we are most interested in is the 
data
 directory. It contains the
files that define the data that is used to train the chatbot’s NLU model. First,
there’s the 
nlu.yml
 file, which contains the intents and examples of user
utterances that are used to train the intent recognition model. So let’s start
creating the intents that are used in our dialog. For every intent you want to
define, you need to provide a name and a list of examples of utterances that
belong to this intent.
For our short dialog, we need to understand the user’s greeting, their role
(parent or student), and their agreement to give permission to their child to
use the chatbot.
version
: 
"
3.1
"
nlu
:
- 
intent: greet
  
examples
: 
|
 - hey - hello - hi
- 
intent: parent
 - I am a parent - Parent - I'm a mom to 12 year old
- 
intent: agree
...
Pretty straightforward, right? RASA will warn if you have too few examples
for a particular intent, and recommends at least 7-10 utterance examples per
intent.
The next file you should look at is 
domain.yml

**Enhanced Text:**

Let's explore the structure of the Rasa project to understand how to build such a dialog. The project directory structure typically looks like this:

```
├───.rasa
│   └───cache
│       ├───...
├───actions
│   └───__pycache__
├───data
├───models
└───tests
```

The `data` directory is of particular interest, as it houses files that define the training data for the chatbot's Natural Language Understanding (NLU) model.

The `nlu.yml` file contains the definitions for intents and examples of user utterances used to train the intent recognition model. Each intent requires a name and a list of example utterances.

For our short dialog, we need to recognize the user's greeting, their role (parent or student), and their agreement to provide permission for their child to use the chatbot.

```yaml
version: "3.1"
nlu:
- intent: greet
  examples: |
   - hey
   - hello
   - hi
- intent: parent
  examples: |
   - I am a parent
   - Parent
   - I'm a mom to 12 year old
- intent: agree
  examples: |
   - yes
   - I agree
   - sure
   - ok
...
```

Rasa will issue a warning if there are too few examples for a particular intent, recommending at least 7-10 utterance examples per intent to ensure good model performance. More examples will help the NLU model generalize to unseen user inputs.

The next crucial file to examine is `domain.yml`.

<----------section---------->

**Original Text:**
 in the main directory. Its first
section is quite straightforward: it defines the intents from the 
nlu.yml
 file
that the chatbot should be able to understand. Let’s add the intents we just
defined to this part.
version
: 
"
3.1
"
intents
:
  - 
greet
  - 
parent
  - 
agree
...
The next section includes the action the chatbot can take - in this simplest
example, the pre-programmed utterances that the chatbot can use in the
conversation.
responses
:
  
utter_welcome
:
  - 
text: "Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!"
  
utter_parent_or_student
:
  - 
text: "Are you a parent (or guardian) or are you a student?"
  
utter_ask_permission
:
  - 
text: "For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?"
  
utter_permission_granted
:
  - 
text: "Thank you for giving permission for your child to chat with Rori."
  
utter_invite_child
:
  - 
text: "When your child is ready to start, please give them this phone and have them type *ready*."
The 
domain.yml
 file concludes with chatbot configuration parameters, that
we won’t deal with in this book. What’s more exciting, is the file 
config.yml
that allows you to configure all the components of your chatbot’s NLU
pipeline. Let’s look at the pipeline that RASA loads for you by default:
pipeline
:
  - 
name: WhitespaceTokenizer
  - 
name: RegexFeaturizer
  - 
name: LexicalSyntacticFeaturizer
  - 
name: CountVectorsFeaturizer
  - 
name: CountVectorsFeaturizer
    
analyzer
: 
char_wb
    
min_ngram
: 
1
    
max_ngram
: 
4
  - 
name: DIETClassifier
    
epochs
: 
100
    
constrain_similarities
: 
true
  - 
name: EntitySynonymMapper
  - 
name: ResponseSelector
    
epochs
: 
100
    
constrain_similarities
: 
true
  - 
name: FallbackClassifier
    
threshold
: 
0.3
    
ambiguity_threshold
: 
0.1
You can see that your NLU pipeline uses a tokenizer based on whitespaces,
and quite a few different algorithms (featurizers) to turn the user’s utterance
into a vector to be classified by the model. The CountVectorsFeaturizes is our
old friend Bag of Words vectorizer, while others are additional enhancements
helping the intent recognition (like RegexFeaturizer) or entity detection (like
LexicalSyntacticFeaturizer).

**Enhanced Text:**

The `domain.yml` file, located in the main project directory, defines the chatbot's domain. The initial section specifies the intents that the chatbot is designed to recognize, referencing the intents defined in `nlu.yml`. Let's incorporate the intents we defined previously:

```yaml
version: "3.1"
intents:
  - greet
  - parent
  - agree
...
```

The subsequent section outlines the actions the chatbot can perform. In this simple example, these actions consist of pre-programmed utterances:

```yaml
responses:
  utter_welcome:
  - text: "Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!"

  utter_parent_or_student:
  - text: "Are you a parent (or guardian) or are you a student?"

  utter_ask_permission:
  - text: "For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?"

  utter_permission_granted:
  - text: "Thank you for giving permission for your child to chat with Rori."

  utter_invite_child:
  - text: "When your child is ready to start, please give them this phone and have them type *ready*."
```

The `domain.yml` file also contains chatbot configuration parameters, which are not covered here. More interesting is the `config.yml` file, which allows configuring the various components of the chatbot's NLU pipeline. Here's the default pipeline that Rasa typically loads:

```yaml
pipeline:
  - name: WhitespaceTokenizer
  - name: RegexFeaturizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: CountVectorsFeaturizer
    analyzer: char_wb
    min_ngram: 1
    max_ngram: 4
  - name: DIETClassifier
    epochs: 100
    constrain_similarities: true
  - name: EntitySynonymMapper
  - name: ResponseSelector
    epochs: 100
    constrain_similarities: true
  - name: FallbackClassifier
    threshold: 0.3
    ambiguity_threshold: 0.1
```

This pipeline utilizes a whitespace-based tokenizer, along with several algorithms (featurizers) that transform user utterances into numerical vectors for model classification. `CountVectorsFeaturizer` represents the familiar Bag of Words vectorizer, while others offer enhancements like `RegexFeaturizer` for intent recognition and `LexicalSyntacticFeaturizer` for entity detection.

<----------section---------->

**Original Text:**
[
35
]
 Finally, the main classifier RASA uses is
DIETClassifier, which is a neural network model that combines intent
recognition and entity detection in a single model.
Of course, you don’t have to stick with the default components of the
pipeline. For example, if you want to replace the BoW embeddings, RASA
also offers to use pretrained embeddings from libraries like spaCy or
HuggingFace Transformers. You can change single components inside of the
pipeline, or build your own completely from scratch - RASA documentation
even provides recommendations on how to create a pipeline based on your
use case and training set.
[
36
]
Finally, the last important file we haven’t covered yet is the 
stories.yml
 file
in the 
data
 folder. In this file, you can actually define a conversation
scenario, by chaining intents and actions together. Let’s combine a simple
story for the dialog we created above:
- 
story: onboarding parent
  
steps
:
  - 
intent: greet
  - 
action: utter_welcome
  - 
action: utter_parent_or_student
  - 
intent: parent
  - 
action: utter_ask_permission
  - 
intent: agree
  - 
action: utter_permission_granted
  - 
action: utter_invite_child
This story defines one possible conversational sequence between the chatbot
and the user. If you want the conversation to follow a different route (for
example, if the user of the phone is a child), you can define another story and
add it to the 
stories.yml
 file. You can also interactively train your bot by
running 
rasa interactive
 command in your shell. That would open a
training interface that allows you to chat with your bot and define new
intents, actions, and stories on the fly.
One question you might be asking yourself - given all the ways people say
things, how does the conversation engine decide what action to take at every
turn? And how can you anticipate in advance all the ways that your users will
use your chatbot? In chapter 10 you learned how LLMs can chat about
virtually anything. But it’s not good enough to just redirect your users to
some other corporation’s LLM interface. You will need to be able to integrate
the chatbot into your existing NLP pipeline, such as the block diagram in
Figure 12. 1
. The LangChain package gives you a way to do exactly that.

**Enhanced Text:**

The primary classifier in Rasa is the DIETClassifier, a neural network model integrating both intent recognition and entity detection within a unified model. Footnote 35 in the original text likely links to more details on the NLU pipeline components.

You're not limited to using the default components. Rasa allows replacing the Bag of Words embeddings with pre-trained embeddings from libraries like spaCy or Hugging Face Transformers. Individual components can be modified, or a custom pipeline can be built from scratch, with the Rasa documentation providing recommendations based on your specific use case and training data (Footnote 36).

The `stories.yml` file, located in the `data` folder, is essential for defining conversation scenarios. It chains intents and actions together to create conversational flows. For our example dialog, a simple story might look like this:

```yaml
- story: onboarding parent
  steps:
  - intent: greet
  - action: utter_welcome
  - action: utter_parent_or_student
  - intent: parent
  - action: utter_ask_permission
  - intent: agree
  - action: utter_permission_granted
  - action: utter_invite_child
```

This story outlines a single conversational sequence between the chatbot and the user. To accommodate different conversation paths (e.g., if the user is a child), additional stories can be added to the `stories.yml` file.

Rasa also supports interactive training via the `rasa interactive` command, which launches a training interface for chatting with the bot and defining new intents, actions, and stories in real-time.

A key question is how the conversation engine determines the appropriate action at each turn, considering the diverse ways users can express themselves. While Chapter 10 highlights the ability of Large Language Models (LLMs) to converse on virtually any topic, simply redirecting users to an external LLM interface is insufficient. Integrating LLMs into the existing NLP pipeline, as depicted in Figure 12.1, is critical. This is where the LangChain package comes into play.

<----------section---------->

**Original Text:**
12.5.2 Adding LLMs to your chatbot with LangChain
This is especially useful in education when you need to inspire students and
keep them engaged. Teachers do this naturally, by adjusting what they say
and how they say it based on feedback on how well the student understands
what they are saying. And teachers think about more than just "delivering a
message." They must think up new ideas and approaches, on the fly as
students pose interesting new questions. Inspiring students' curiosity with
Socratic questions and being responsive to their changing needs is a full-time
job.
It is virtually impossible to build a rule-based system that captures all the
things that teachers do to help students learn and grow. Students' needs are
too diverse and dynamic. This is why hybrid chatbots that integrate LLMs
have become the preferred way build production chatbots in virtually every
domain. An LLM can confidently and convincingly chat with your users on
virtually any topic. The key is to harness this power smartly so that it doesn’t
mislead your users, or worse.
Let’s build a bot with one of the popular tools for creating generative
chatbots - LangChain.
[
37
]
 Langchain is not quite a chatbot framework as are
Rasa or Rapidpro. Rather, it’s a library that abstracts away the particular API
of the LLM you want to use, allowing you to quickly experiment with
different models and different approaches to using them. It also uses As there
is currently no leading open-source framework leveraging LLMs, we hope
the following section will give you a peek at one approach to building
generative chatbots.
LangChain heavily relies on APIs to function and even has a
Javascript/Typescript SDK that makes it easier to use in web interfaces. This
makes a lot of sense, as the large language models it uses are too compute-
intensive and memory-intensive to run on a personal computer, or even
closed-source. You probably heard of companies like OpenAI, Anthropic,
and Cohere, that train their own large language models and expose their API
as a paid service.
Luckily, due to the power of the open-source community, you don’t need to
pay for commercial models or own a powerful computer to experiment with
LLMs. Several large companies that are committed to open-source have
released the weights of their models to the public, and companies like
HuggingFace host these models and provide an API to use them.
For the bot we’ll be building in this chapter, let’s take the latest open-source
LLM, LLama 2, that you met in Chapter 10. To use Llama 2 from your
machine, you need a strong enough processor, and a lot of RAM. Serving up
large language models can be complicated and expensive. One free service
that makes this a little easier is called Replicate. Replicate.com gives you
access to open-source models through a web API and only requires you to
pay if you use it a lot. You can use any of Huggingface’s LLMs within
Replicate as long as you can find their path and git commit hash.
For the below code to run properly, you will need to create a GitHub account
(unfortunately) and then use it to sign into Replicate. You can then create or
renew your API token under your user profile on Replicate
(
https://replicate.com/account/api-tokens
). Replicate requires you to use
environment variables to store your API token. You can use
dotenv.load_dotenv()
 on your .env or you can set the variable directly
using 
os.environ
, as you see here:

**Enhanced Text:**

**Adding LLMs to Your Chatbot with LangChain**

Integrating Large Language Models (LLMs) into chatbots is particularly beneficial in education, where it's crucial to inspire and engage students. Teachers naturally adapt their communication based on student understanding and respond creatively to novel questions. This requires thinking beyond simply delivering a pre-scripted message. This adaptability and responsiveness is difficult to replicate with purely rule-based systems due to the diverse and dynamic nature of student needs.

Hybrid chatbots, combining rule-based systems with LLMs, have emerged as a preferred approach for building production chatbots. LLMs offer confident and convincing conversation capabilities across a wide range of topics. However, it is imperative to harness this power responsibly, ensuring that users are not misled or harmed by inaccurate information.

Let's explore how to build a chatbot using LangChain, a popular tool for creating generative chatbots (Footnote 37). LangChain is not a chatbot framework in the same vein as Rasa or RapidPro. Instead, it serves as a library that abstracts the specific APIs of LLMs, facilitating experimentation with different models and usage approaches. The Langchain homepage is located at: https://langchain.com/

Given the computational demands of LLMs, LangChain relies heavily on APIs and includes a Javascript/Typescript SDK for easier integration into web interfaces. Companies like OpenAI, Anthropic, and Cohere train and offer API access to their LLMs as a paid service.

However, the open-source community provides alternatives. Several large companies committed to open-source have released the weights of their models publicly. Hugging Face hosts these models and provides APIs for accessing them.

For the chatbot we'll build, we'll utilize Llama 2, a prominent open-source LLM introduced in Chapter 10. Running Llama 2 requires a powerful processor and ample RAM. Replicate.com provides a free service to access open-source models through a web API, charging only for extensive usage. Hugging Face's LLMs are accessible within Replicate, provided you know their path and Git commit hash.

To run the following code, you'll need a GitHub account for signing into Replicate. Then, create or renew your API token under your user profile on Replicate (https://replicate.com/account/api-tokens). Replicate requires storing your API token in an environment variable. You can either use `dotenv.load_dotenv()` on your `.env` file or set the variable directly using `os.environ`.

<----------section---------->

**Original Text:**
>>> 
from
 
langchain.llms
 
import
 
Replicate
>>> os.environ[
"
REPLICATE_API_TOKEN
"
] = 
'
<your_API_key_here>
'
>>> llm = Replicate(
...     model=
"
a16z-infra/llama13b-v2-chat:
"
 +
...     
"
df7690
"
,  # #1
...     input={
...         
"
temperature
"
: 
0.5
,
...         
"
max_length
"
: 
100
,
...         
"
top_p
"
: 
1
,
...     })
Now that you’
<----------section---------->

Natural Language Processing and Large Language Models. Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering). Lesson 9: Transformers I. Presented by Nicola Capuano and Antonio Greco. DIEM – University of Salerno.

**Outline**

*   Limitations of RNNs (Recurrent Neural Networks)
*   Introduction to the Transformer model
*   Transformer Input mechanisms
*   Self-Attention mechanism

<----------section---------->

**Limitations of RNNs**

Recurrent Neural Networks, despite their effectiveness in processing sequential data, suffer from several limitations that the Transformer architecture aims to address:

*   RNNs lack of long-term memory capabilities, especially in encoder-decoder models. This makes it difficult for them to capture dependencies between distant elements in a sequence.
*   RNNs are exceptionally slow to train, particularly when dealing with long sequences. This is due to their inherent sequential processing nature.
*   RNNs are susceptible to the vanishing gradient problem, which hinders their ability to learn long-range dependencies.

<----------section---------->

**RNNs Lack of Long-Term Memory**

Traditional RNNs, and even more advanced variants like LSTMs and GRUs, struggle to retain information over long sequences. As the input sequence grows, the influence of earlier elements diminishes, making it difficult for the network to capture long-range dependencies. This limitation is especially pronounced in encoder-decoder models, where the encoder needs to compress the entire input sequence into a fixed-length vector, which can become a bottleneck for long sequences.

<----------section---------->

**RNNs are Extremely Slow to Train**

*   RNNs process data inherently sequentially, meaning that each step depends on the output of the previous step.
*   The network cannot start processing input $x_i$ until it has finished processing $x_{i-1}$, creating a dependency chain.
*   This sequential processing prevents RNNs from leveraging the massive parallelism available in modern GPUs, significantly slowing down training. GPUs are designed to perform many computations simultaneously, but RNNs cannot fully utilize this capability due to their sequential nature.

<----------section---------->

**RNNs Suffer from the Vanishing Gradient Problem**

*   The vanishing gradient problem is related to the exploding gradient problem, both of which arise during the training of deep neural networks.
*   During backpropagation through time (BPTT), which is used to train RNNs, the same function $F$ (representing the recurrent layer) is traversed repeatedly for each time step in the sequence.
*   The gradient calculation involves multiplying the derivatives of $F$ across all time steps:
    $\frac{\partial Loss}{\partial h_0} = \frac{\partial Loss}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_3} \cdot \frac{\partial F}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_4} \cdot \frac{\partial F}{\partial h_3} \cdot \frac{\partial F}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = ...$
    If the absolute value of the derivatives of $F$ is consistently less than 1, the repeated multiplication causes the gradient to shrink exponentially, eventually vanishing. Conversely, if the absolute value is consistently greater than 1, the gradient explodes.

<----------section---------->

**RNNs Suffer from the Vanishing Gradient Problem (Continued)**

*   The core issue remains the Vanishing gradient/exploding gradient problem, which destabilizes the learning process.
*   During backpropagation through time (BPTT), the function $F$ representing the recurrent layer is traversed multiple times, proportional to the sequence length.
*   If the absolute value of the derivatives of $F$ is small (less than 1), the repeated multiplication will cause it to become progressively smaller, leading to a vanishing gradient. This prevents earlier layers from learning effectively.
*   Conversely, if the absolute value is large (greater than 1), it will become progressively larger, leading to an exploding gradient. This can cause instability and divergence during training.
*   The problem stems from traversing the same layer multiple times, proportional to the sequence length. This is because the recurrent nature of RNNs necessitates processing each element in the sequence sequentially.

<----------section---------->

**Transformer**

In 2017, researchers at Google Brain introduced the Transformer model as an alternative to RNNs for processing sequential data. A key innovation of the Transformer is its ability to process elements of the sequence in parallel, thus overcoming the sequential bottleneck of RNNs. Furthermore, the number of layers traversed in a Transformer does not depend directly on the length of the sequence, mitigating gradient-related issues. Initially developed for language translation tasks (sequence-to-sequence with varying lengths), the Transformer has since been adapted for a variety of sequence processing tasks. Subsets of the model can be used independently for different applications.

<----------section---------->

**Transformer Architecture**

The Transformer architecture consists of several key components:

*   Input: The raw data to be processed.
*   Tokenization: The process of converting the input text into a sequence of tokens, which are individual units of meaning (e.g., words or subwords).
*   Input Embedding: Transforming tokens into dense vector representations in a continuous space.
*   Positional Encoding: Adding information about the position of each token in the sequence to the embedding vectors.
*   Encoder: Processes the input sequence to generate an intermediate representation.
    *   Attention Mechanisms:
        *   Query: A vector representing what the current position is "looking for".
        *   Key: A vector representing what the other positions in the sequence "offer".
        *   Value: A vector representing the content of the other positions.
    *   Self-Attention: An attention mechanism where the query, key, and value are derived from the same input sequence.
    *   Multi-Head Attention: Multiple self-attention mechanisms operating in parallel, allowing the model to capture different relationships between tokens.
    *   Add & Norm: Adding residual connections and layer normalization to stabilize training and improve performance.
    *   Feedforward: A feedforward neural network applied to each position independently.
*   Decoder: Generates the output sequence based on the encoder's intermediate representation.
    *   Masked Attention: A variant of self-attention that prevents the decoder from "peeking" at future tokens during training.
    *   Encoder-Decoder Attention: An attention mechanism that allows the decoder to attend to the encoder's output.
*   Output: The final sequence generated by the decoder.

<----------section---------->

**Input**

The initial stage involves preparing the input data for processing by the Transformer model. This includes tokenizing the input text, embedding the tokens, and adding positional encoding.

<----------section---------->

**Tokenization**

*   Tokenization is the process of converting the input text into a sequence of discrete units called tokens.
*   Each token is assigned a unique ID, which serves as its numerical representation within the model.

<----------section---------->

**Input Embedding**

*   Embedding is a representation of a symbolic input (word, character, sentence) in a distributed, low-dimensional space of continuous-valued vectors.
*   The tokens are projected into a continuous Euclidean space, where each dimension represents a latent feature.
*   Correlations among words can be visualized in the embedding space: depending on the training task, word embeddings can be adjusted to bring semantically similar words closer together or push dissimilar words further apart.
*   Ideally, an embedding captures the semantics of the input by placing semantically similar inputs close together in the embedding space. This allows the model to generalize across different inputs that share similar meanings.

<----------section---------->

**Positional Encoding**

Positional encoding is a crucial component of the Transformer architecture that addresses the lack of inherent sequential information in the model. Since Transformers process all elements of the input sequence in parallel, they do not have a built-in mechanism to understand the order of tokens. Positional encoding provides this crucial information.

<----------section---------->

**The Importance of Order**

*   Question: With the encoding methods discussed so far (tokenization and embedding), is it possible to distinguish between sequences that differ only in the order of their elements?
*   For example, can we differentiate between "The student is eating an apple" and "An apple is eating the student"?

<----------section---------->

**The Importance of Order (Continued)**

*   Question: With the encoding methods discussed so far (tokenization and embedding), is it possible to distinguish between sequences that differ only in the order of their elements?
*   For example, can we differentiate between "The student is eating an apple" and "An apple is eating the student"?
*   Answer: No, because the output of the attention module does not inherently depend on the order of its key/value pairs. The attention mechanism calculates relationships between tokens regardless of their position in the sequence.
*   So, how can we incorporate information about the order of sequence elements?

<----------section---------->

**Positional Encoding**

*   The solution proposed by the authors of the Transformer model is to add a unique "perturbation" or modification to each element of the sequence, based on its position within the sequence.
*   This ensures that the same element appearing in different positions will be encoded using slightly different vectors, allowing the model to differentiate them.

<----------section---------->

**Positional Encoding (Mathematical Formulation)**

*   Positional encoding is represented by a set of periodic functions (sine and cosine waves) that vary with position.
*   Specifically, if $d_{model}$ is the size of the embedding vector and $pos$ is the position of an element within the sequence, the perturbation to component $i$ of the embedding vector representing the element is calculated as:

    $PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})$
    $PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})$
*   The positional encoding is a vector with the same dimension as the input embedding, allowing it to be added directly to the input embeddings. This combination provides the model with information about both the meaning of the tokens and their position in the sequence.

<----------section---------->

**Encoder**

The Encoder transforms an input sequence of vectors $x_1, \dots, x_t$ into an intermediate representation of the same length $z_1, \dots, z_t$. The key feature is that the vectors $z_1, \dots, z_t$ can be generated in parallel, unlike in RNNs. Each vector $z_i$ does not depend solely on the corresponding $x_i$, but on the entire input sequence $x_1, \dots, x_t$. This enables the model to capture global relationships between tokens.

<----------section---------->

**Encoder Structure**

The encoder is composed of a stack of identical encoder blocks. The original Transformer paper used 6 such blocks, but the number can vary depending on the application. Each encoder block processes a sequence using a combination of the following mechanisms:

*   Self-attention: A (multi-headed) attention module where the same vectors are used as the Query (Q), Key (K), and Value (V). This allows each token to attend to all other tokens in the sequence, capturing relationships between them.
*   Feed-forward layer: A classical feed-forward neural network applied separately to each element of the sequence. This adds non-linearity to the model and allows it to learn complex transformations of the input.
*   Skip connections (Residual Connections): These connections allow the gradient to flow more easily during training, preventing the vanishing gradient problem.
*   Normalization: Layer normalization is applied to stabilize the training process and improve performance.

<----------section---------->

**Self Attention**

Consider the sentence: "The animal didn’t cross the street because it was too wide." What does "it" refer to? Estimating self-attention in this sentence involves identifying the words that are most relevant to understanding the meaning of "it". The weights assigned by the self-attention mechanism are learned during training, and they depend on the specific task the model is trained to perform.

<----------section---------->

**Self Attention - Core Question**

The core question in self-attention is: How do we compute the attention to give to each input element when encoding the current word? In other words, which words in the sequence are most important for understanding the current word?

<----------section---------->

**Attention Mechanism**

To understand self-attention, it's essential to first understand its fundamental building block: the attention function. Informally, an attention function is used when the value to be computed (in this case, the embedding of a token in a certain position considering the context of the sentence) depends on a set of other values (in this case, other tokens of the sentence). We want to assign a different weight (i.e., a different "level of attention") to each of the values, reflecting how much each token is important for encoding the current token. The attention function depends on three elements, with a terminology inherited from document retrieval: query, key, and value.

<----------section---------->

**Attention Components: Query, Key, Value**

We have an input value $q$ (the query) and we want to define some target function $f_T(q)$. In attention terminology, $q$ is called the query value. In the general case, both $q$ and $f_T(q)$ can be vectors. We want to express $f_T(q)$ as a function of a given set of elements $v_1, \dots, v_n$. We want the "attention" given to each $v_i$ to be different depending on $q$. We assume that for each $v_i$ we have available an additional information $k_i$ that can be used to decide the "attention" to be given to $v_i$. The elements $v_i$ are called values and the $k_i$ are called keys in the attention terminology; both the values and the keys can be vectors.

<----------section---------->

**Attention Formulation**

A commonly adopted formulation of the attention function is to define the target function as a weighted sum of the values, where the weights are determined by the attention given to each value:

$f_T(q) = \alpha(q, k_1) \cdot f_V(v_1) + \dots + \alpha(q, k_n) \cdot f_V(v_n) = \sum_{i=1}^{n} \alpha(q, k_i) \cdot f_V(v_i)$

Where $\alpha$ is the attention given to value $v_i$, and $f_V$ is a transformation function applied to the value.

<----------section---------->

**Attention Function Properties**

$f_T(q) = \sum_{i=1}^{n} \alpha(q, k_i) \cdot f_V(v_i)$

*   $\alpha$ is our attention function, determining the weight assigned to each value.
*   We want both $\alpha$ and $f_V$ to be learned by our system through training.
*   Typically, $\alpha(q, k_i) \in [0, 1]$ and $\sum_{i} \alpha(q, k_i) = 1$. This ensures that the attention weights are non-negative and sum to 1, representing a probability distribution over the values.
*   Note: The value of the target function does not depend on the order of the key-value pairs $(k_i, v_i)$. The attention mechanism is permutation-invariant, meaning that the order in which the key-value pairs are presented does not affect the output.

<----------section---------->

**Self Attention in Transformers**

The Transformer architecture utilizes a specific definition of the attention function, based on linear vector/matrix operations and the softmax function. This definition is:

*   Differentiable: Making it suitable for training with backpropagation.
*   Efficient to compute: Leveraging matrix operations for fast calculations.
*   Easy to parallelize: Allowing for efficient computation on GPUs.

<----------section---------->

**Self Attention - Input Matrices**

*   Input: Three matrices, Q, K, and V.
*   Q ($m \times d_q$): Contains the query vectors, where each row represents a query. $m$ is the number of queries, and $d_q$ is the dimensionality of the query vectors.
*   K ($n \times d_k$): Contains the key vectors, where each row represents a key. $n$ is the number of keys, and $d_k$ is the dimensionality of the key vectors.
*   V ($n \times d_v$): Contains the value vectors, where each row represents a value. $n$ is the number of values, and $d_v$ is the dimensionality of the value vectors.
*   Requirement: K and V must have the same number of rows, ensuring that each key has a corresponding value.

<----------section---------->

**Self Attention - Scaled Dot Product**

If the query and key vectors have the same dimensionality, a matching score can be efficiently computed using the scaled dot product of the two vectors, which is equivalent to cosine similarity after scaling.

<----------section---------->

**Self Attention - Step 0**

Step 0: Each element in the input sequence is represented by a numerical vector (embedding). This is a prerequisite for all subsequent operations.

<----------section---------->

**Self Attention - Step 1: Linear Projections**

Step 1: The input matrices Q, K, and V are "projected" into different subspaces by multiplying them with weight matrices:

*   $Q' = Q \cdot W^Q$
*   $K' = K \cdot W^K$
*   $V' = V \cdot W^V$

Where:

*   $Q'$ is the projected query matrix.
*   $K'$ is the projected key matrix.
*   $V'$ is the projected value matrix.
*   $W^Q (d_q \times d'_q)$: Weight matrix for projecting the queries.
*   $W^K (d_k \times d'_k)$: Weight matrix for projecting the keys.
*   $W^V (d_v \times d'_v)$: Weight matrix for projecting the values.

These weight matrices are trainable parameters of the model.

Crucially, $W^Q$ and $W^K$ must have the same number of columns ($d'_q = d'_k$), which is the dimensionality of the projected query and key vectors.

<----------section---------->

**Self Attention - Step 1 (Simplified)**

Step 1: Compute a key (K), a value (V), and a query (Q) as linear functions of each element in the sequence. This is achieved through the linear projections described above.

<----------section---------->

**Self Attention - Step 2: Attention Score Calculation**

Step 2: The attention matrix A is computed for each position by multiplying the projected query matrix $Q'$ with the transpose of the projected key matrix $K'$, scaling by $1 / \sqrt{d'_k}$, and applying the softmax function to each row of the resulting matrix:

$A = softmax(\frac{Q' \cdot K'^T}{\sqrt{d'_k}})$

Where:

*   A is an ($m \times n$) matrix whose element $a_{ij}$ represents the attention weight $\alpha(q_i, k_j)$ between query $q_i$ and key $k_j$.
*   Softmax is applied to each row of the matrix separately, ensuring that the attention weights for each query sum to 1.
*   The scaling factor $1 / \sqrt{d'_k}$ is used to prevent the dot products from becoming too large, which can lead to small gradients and hinder learning.

<----------section---------->

**Self Attention - Step 2 (Simplified)**

Step 2: Compute attention scores for each position i as the softmax of the scaled dot product of all the keys (bidirectional self-attention) with $Q_i$.

<----------section---------->

**Self Attention - Step 3: Weighted Value Summation**

Final step: The target value is computed by performing a row-by-column matrix multiplication between the attention matrix A and the projected value matrix $V'$:

$f_T(Q) = A \cdot V'$

The result is an $m \times d'_v$ matrix representing the target function computed on the $m$ queries in the input matrix Q. Each row of this matrix is a weighted sum of the value vectors, where the weights are determined by the attention scores.

<----------section---------->

**Self Attention - Step 3 (Simplified)**

Step 3: Output representation for each position I is computed as a weighted sum of values, where each value is multiplied by its corresponding attention score.

<----------section---------->

**Self Attention - Complete Formula**

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

This concise formula encapsulates the entire self-attention mechanism.

$a_{ij} = softmax(\frac{q_i k_j}{\sqrt{d_k}}) = \frac{exp(q_i k_j)}{\sum_{s=1}^n exp(q_i k_s)}$

This shows how the attention weight $a_{ij}$ is calculated using the softmax function and the scaled dot product of query $q_i$ and key $k_j$.

<----------section---------->

**Additional Context and Insights**

The provided additional context offers further insights into the Transformer architecture, its advantages, and its applications.

*   **Language Translation as a Reference Task:** The PyTorch implementation of the Transformer module can be used to implement a language translation model, mirroring the original task in "Attention Is All You Need."
*   **Self-Attention and its Significance:** Self-attention is a core mechanism that allows the model to focus on different parts of the input sequence when processing each word.
*   **Applications of Attention:** The attention mechanism has led to significant advancements in areas where LSTMs previously struggled, including:
    *   Conversation: Generating responses to prompts.
    *   Abstractive summarization or paraphrasing: Generating new, shorter versions of texts.
    *   Open domain question answering: Answering general questions.
    *   Reading comprehension question answering: Answering questions about a short text.
    *   Encoding: Representing text meaning in a vector space.
    *   Translation and code generation: Generating software from plain English descriptions.
*   **Linear Projections:** Self-attention uses linear projections to create key, value, and query vectors, and a context vector is created for the words' embedding vectors and their relation to the query.
*   **Additive vs. Dot-Product Attention:** There are two main ways to implement attention: additive attention and dot-product attention, with scaled dot-product attention being most effective in Transformers.
*   **Scaling Dot Products:** The dot products between queries and keys are scaled down based on the model's dimensionality to improve numerical stability.
*   **Self-Attention Outputs:** The self-attention outputs are computed using the formula:
    \[Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V\]
*   **Matrix Multiplication:** The Q and K product forms a square matrix representing connections between words in the input sequence.
*   **Multi-Head Self-Attention:** This approach expands self-attention by creating multiple attention heads, each attending to different aspects of the words in the text.
    *   The query, key, and value matrices are multiplied n times by each different \(d_q\) , \(d_k\), and \(d_v\) dimension to compute the total attention function output.
    *    Outputs are concatenated and projected with a \(W^o\) matrix:
    \[MultiHeadAttention(Q, K, V ) = Concat(head_1, ..., head_n) W^o\\ where\ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\]
*   **Benefits of Multi-Head Attention:** Multiple heads allow the model to focus on different positions and create several different vector subspaces.

<----------section---------->

<----------section---------->

# Natural Language Processing and Large Language Models
## Corso di Laurea Magistrale in Ingegneria Informatica
### Lesson 10: Transformers II
**Nicola Capuano and Antonio Greco**
**DIEM – University of Salerno**

This lecture, "Transformers II," presented by Nicola Capuano and Antonio Greco from DIEM – University of Salerno, delves into the architecture and functionality of Transformers, a pivotal component in modern Natural Language Processing (NLP). This lesson builds upon the foundational concepts of Transformers, focusing on the specific mechanisms that enable them to process and generate language effectively. The course is targeted for students of Ingegneria Informatica (Computer Engineering).

<----------section---------->

## Outline

This lesson covers the following key components of the Transformer architecture:

*   **Multi-Head Attention:** Discussing how this mechanism enhances the model's ability to capture different relationships within the input data.
*   **Encoder Output:** Explaining the representation of the input sequence generated by the encoder.
*   **Decoder:** Detailing the decoder's role in generating the output sequence based on the encoder's output.
*   **Masked Multi-Head Attention:** Describing the function of masking in self-attention within the decoder to prevent peeking at future tokens.
*   **Encoder-Decoder Attention:** Explaining how the decoder uses the encoder's output to focus on relevant parts of the input sequence during generation.
*   **Output:** Describing the final layers of the Transformer that produce the output sequence.
*   **Transformer’s pipeline:** Providing a holistic view of the data flow through the Transformer architecture, from input to output.

<----------section---------->

## Multi-head attention

### Multi-head Attention
*   **Encoding Different Meanings:** By employing multiple self-attention "heads," the model can capture various contextual relationships and nuances within the input sequence. Each head focuses on different aspects of the input, allowing for a more comprehensive understanding.
*   **Parallel Scaled-Dot Product Attention:** Several scaled dot-product attention computations are executed in parallel. Each attention head uses its own set of learned weight matrices to project the input into different representation subspaces.
*   **Concatenation:** The outputs from each of the parallel attention heads are concatenated row-by-row to form a larger matrix. Critically, this matrix retains the same number of rows (m) as the original input, ensuring compatibility with subsequent layers.
*   **Final Weight Matrix:** This concatenated matrix is then multiplied by a final, learned weight matrix. This matrix combines the information from the different attention heads into a unified representation.
*   **Multi-Head Attention Defined:** This entire process, involving parallel attention heads, concatenation, and a final linear transformation, is termed multi-head attention. It's a critical innovation enabling Transformers to model complex relationships in data.

### Multi-head Attention
Multi-head attention offers a significant advantage over single-head attention mechanisms. It allows the model to attend to information from various representation subspaces at different positions. This means the model can simultaneously consider multiple aspects of the input, such as syntactic relationships, semantic meanings, and long-range dependencies. In contrast, a single attention head might average out these different aspects, hindering the model's ability to capture complex relationships. The concatenated outputs of the heads are then multiplied by a weight matrix, which combines the different representations at the same network level, further enriching the model's representation power.

<----------section---------->

## Add & Norm

### Add (skip connections) & Norm
This section highlights two key techniques used within each Transformer block to improve training and performance: Add (skip connections or residual connections) and Layer Normalization.

**Input Normalization (Z):**
*   **Mean 0, Std dev 1:** Input normalization involves scaling the input to have a mean of 0 and a standard deviation of 1.
*   **Stabilizes training:** This normalization process helps stabilize training by preventing activations from becoming too large or too small, which can lead to vanishing or exploding gradients.
*   **Regularization effect:** Input normalization also provides a regularization effect, which helps to prevent overfitting by reducing the sensitivity of the model to individual training examples.

**Add -> Residuals:**
*   **Avoid vanishing gradients:** Skip connections allow gradients to flow more easily through the network, bypassing potential bottlenecks caused by multiple layers.
*   **Train deeper networks:** By mitigating the vanishing gradient problem, skip connections enable the training of much deeper networks. The model can learn more complex relationships, leading to improved performance. The output of Add & Norm block is fed to the next layers to propagate the information through the network.

<----------section---------->

## Feed Forward

### Feed Forward
Non-linearity is introduced with a feed-forward network, comprising two fully connected layers and a non-linear activation function between them, it's also described as a (2-layer MLP). This network is applied position-wise, meaning each position in the sequence is processed independently. This operation allows the Transformer to learn complex relationships between features at each position.

*   **Non-linearity:** The use of non-linear activation functions enables the model to learn complex, non-linear relationships in the data.
*   **Complex Relationships:** Feed Forward Networks (FFNs) help the model to capture relationships between features.

FFN (2 layer MLP)

<----------section---------->

## Transformer’s Encoder

### Transformer’s Encoder
The Transformer's encoder is responsible for generating a representation of the input sequence. The encoder block leverages an additive positional encoding which tackles the order-agnostic nature of the self-attention mechanism. The encoder also uses residual connection to foster the gradients flow and adopts normalization layers to stabilize the network training. Position-Wise Feed-Forward layer to add non-linearity and is applied to each sequence element independently. The dimensionality of each encoder block is the same, of the input, so it is possible to stack an arbitrary number of encoder’s blocks. The output of the first block is fed to the second block (no word embeddings) and so on.

<----------section---------->

## Decoder

### Decoder
The decoder leverages the intermediate representation generated by the encoder, denoted as *z*<sub>1</sub>,…,*z*<sub>*t*</sub> to generate the output sequence *y*<sub>1</sub>,…,*y*<sub>*m*</sub>. The decoding process operates sequentially, generating one element at a time. At each step *i*, the decoder uses the encoder's output (*z*<sub>1</sub>,…,*z*<sub>*t*</sub>) and the previously generated output elements (*y*<sub>1</sub>,…,*y*<sub>*i*-1</sub>) to predict the next output element *y*<sub>*i*</sub>.

### Decoder
The decoder consists of a series of decoder blocks with identical structure. The original Transformer paper used 6 such blocks. The decoder blocks contain the same modules used in the encoder block, plus a module for attention where the keys and values are taken from the encoder's intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub>. Also, the self-attention module is slightly modified so as to ensure that the query at position i only uses the values at positions 1,…,i. The self-attention mechanism is masked to prevent the decoder from attending to future tokens, ensuring that the output is generated sequentially. On top of the last decoder block, the decoder includes an additional linear layer and a softmax activation function, for computing the probability of the next output element *y*<sub>*i*</sub>. The last layers has a number of neurons corresponding to the cardinality of the output set.

<----------section---------->

## Masked Multi-Head Attention
Masked Multi-Head Attention is a crucial component of the decoder. Its purpose is to ensure that, when predicting an output at time *T*, the model only attends to outputs up to time *T-1*. This is achieved by masking the available attention values. The attention mask *M* is applied to the attention values to prevent the model from "peeking" at future tokens in the sequence.

*R<sup>IxT</sup>*:  Representing the attention values.
*R<sup>TxT</sup>*:  Representing the Attention Mask: M
Masked Attention Values.

<----------section---------->

## Encoder Decoder Attention
The encoder-decoder attention mechanism connects the encoder and decoder. The keys and values are derived from the encoder outputs, while the queries come from the decoder inputs. Every decoder block receives the same FINAL encoder output, allowing it to focus on different parts of the input sequence at each step of the decoding process.

<----------section---------->

## Output

### Output
The final output stage of the Transformer involves a linear transformation followed by a softmax function. The linear layer projects the decoder's output into the vocabulary space. In some implementations, the weights of this linear layer are tied to the weights of the model's input embedding matrix, which improves parameter efficiency and generalization. The softmax function then converts these projections into probabilities, indicating the likelihood of each token in the vocabulary being the next token in the output sequence.

*   **Linear:** A linear layer projects the decoder's output into the vocabulary space.
*   **Linear weights tied with model input embedding matrix:** Linear weights are often tied with model input embedding matrix to improve parameter efficiency and generalization.
*   **Softmax:** Softmax function produces the probability distribution over the output vocabulary.

<----------section---------->

## Transformer’s pipeline
Decoding time step.

ENCODER - DECODER.

The Transformer pipeline is an iterative process where the decoder generates one token at a time, conditioned on the encoder's output and the previously generated tokens. It starts by encoding the input sequence with the Encoder and decoding with the Decoder. The next token is generated step by step.

Reference website:

https://poloclub.github.io/transformer-explainer/

<----------section---------->

### Enhanced Text

**Title:** Natural Language Processing and Large Language Models

**Source:** Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering) - Lesson 11: From Transformers to LLMs

**Authors:** Nicola Capuano and Antonio Greco, DIEM – University of Salerno

**Outline**

This lesson will cover the progression of Natural Language Processing (NLP) from traditional Transformer architectures to the more advanced Large Language Models (LLMs). Key areas include text representation and generation using Transformers, the paradigm shift in NLP brought about by LLMs, the pre-training techniques used for LLMs, the datasets and pre-processing steps involved, and how to effectively utilize LLMs after pre-training.

*   Transformers for text representation and generation
*   Paradigm shift in NLP
*   Pre-training of LLMs
*   Datasets and data pre-processing
*   Using LLMs after pre-training

<----------section---------->

**Transformers for Text Representation and Generation**

Transformers have revolutionized text representation and generation in NLP. These models rely on the attention mechanism to handle long-range dependencies in text. Unlike recurrent neural networks (RNNs), Transformers enable parallel training by dynamically assigning attention weights based on the input, greatly accelerating the training process.

There are three primary types of Transformer architectures, each suited for different tasks:

*   **Encoder-Only (e.g., BERT):** Takes input tokens and outputs hidden states which are rich vector representations of the input. BERT is designed to understand the context of words in a sentence bidirectionally (from both left and right), making it suitable for tasks like sentiment analysis, named entity recognition, and question answering. Since it focuses on understanding the input, BERT does not inherently output tokens in a sequence, thus it's not auto-regressive.
*   **Decoder-Only (e.g., GPT):** Takes output tokens and hidden states as input, and outputs output tokens. Crucially, it can only "see" previous tokens in the sequence, making it auto-regressive. This auto-regressive property makes Decoder-Only Transformers ideal for text generation tasks. GPT models predict the next token based on the preceding tokens.
*   **Encoder-Decoder (e.g., T5, BART):** Maps input tokens to output tokens, combining the strengths of both encoder and decoder architectures. This architecture is suitable for tasks like machine translation, text summarization, and question answering where an input sequence needs to be transformed into an output sequence.

Here are some of the most popular architectures, categorized by type, along with their release dates, illustrating the rapid evolution of the field:

*   **Encoder-Only:**
    *   BERT (October 2018): Bidirectional Encoder Representations from Transformers
    *   DistilBERT (2019): A distilled version of BERT, smaller and faster
    *   RoBERTa (2019): A Robustly Optimized BERT Pretraining Approach
    *   ALBERT (2019): A Lite BERT for Self-supervised Learning of Language Representations
    *   ELECTRA (2020): Efficiently Learning an Encoder that Classifies Token Replacements Accurately
    *   DeBERTa (2020): Decoding-enhanced BERT with Disentangled Attention

*   **Decoder-Only:**
    *   GPT (June 2018): Generative Pre-trained Transformer
    *   GPT-2 (2019): Improved language model with enhanced generation capabilities
    *   GPT-3 (2020): Significantly larger language model with few-shot learning abilities
    *   GPT-Neo (2021): An open-source replication of GPT-3
    *   GPT-3.5 (ChatGPT) (2022): Fine-tuned version of GPT-3 for conversational AI
    *   LLaMA (2023): Large Language Model Meta AI
    *   GPT-4 (2023): Multimodal model with improved reasoning and creative capabilities

*   **Encoder-Decoder:**
    *   T5 (2019): Text-to-Text Transfer Transformer
    *   BART (2019): Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
    *   mT5 (2021): Multilingual T5, extending T5 to multiple languages

<----------section---------->

**Paradigm Shift in NLP**

The advent of LLMs has brought about a significant paradigm shift in NLP, changing the way we approach various tasks and challenges.

**Before LLMs:**

*   **Feature Engineering:** Focused on manually designing or selecting the best features for a specific task. This process was often labor-intensive and required deep domain expertise.
*   **Model Selection:** Involved choosing the most appropriate model for a given task from a range of available models. The selection was crucial as each model had its strengths and weaknesses depending on the data and the task.
*   **Transfer Learning:** Used to transfer knowledge from one domain to another, particularly when labeled data was scarce. Models pre-trained on large datasets were fine-tuned on smaller, task-specific datasets.
*   **Overfitting vs. Generalization:** Balancing model complexity to prevent overfitting (performing well on training data but poorly on unseen data) while maintaining good generalization performance was a critical challenge. Techniques like regularization and cross-validation were commonly used.

**Since LLMs:**

*   **Pre-training and Fine-tuning:** LLMs are pre-trained on massive amounts of unlabeled data, capturing general language patterns and knowledge. This pre-trained model is then fine-tuned on specific tasks, leveraging the knowledge gained during pre-training.
*   **Zero-shot and Few-shot learning:** LLMs can perform tasks without any task-specific training data (zero-shot) or with very limited examples (few-shot). This capability significantly reduces the need for large labeled datasets.
*   **Prompting:** Models can understand and perform tasks simply by describing them in natural language prompts. This approach eliminates the need for extensive task-specific training, making models more versatile and accessible.
*   **Interpretability and Explainability:** There's a growing emphasis on understanding the inner workings of LLMs to explain their decisions and behaviors. This is crucial for ensuring trust and reliability in critical applications.

The paradigm shift was largely driven by the limitations of recurrent networks (RNNs), which struggled with long sequences due to information loss during encoding. Additionally, the sequential nature of RNNs hindered parallel training, favoring inputs from later timesteps.

The solution: **Attention Mechanism**.

The attention mechanism addresses these limitations by:
*   Handling long-range dependencies effectively.
*   Enabling parallel training.
*   Assigning dynamic attention weights based on input relevance.

<----------section---------->

**Pre-training of LLMs**

Large Language Models leverage vast amounts of unstructured data for pre-training. This process typically employs self-supervised learning techniques.

**Self-Supervised Pre-training:**

Pre-training a large language model is typically done in a self-supervised manner, meaning it is trained with unlabeled data which is just text from the internet. This eliminates the need for manually assigning labels to the dataset, reducing the cost and effort associated with data annotation. In the absence of explicit supervision, the models are trained to solve automatically generated supervised tasks.

Here are the different self-supervised pre-training methods:

*   **Autoencoding (MLM - Masked Language Modeling):** Models like BERT consist of only an encoder and predict the masked word from every preceding and following word in the sentence, therefore it is bi-directional. During training, a certain percentage of the input tokens are randomly masked, and the model learns to predict these masked tokens based on the surrounding context. The model has knowledge of the entire context. The objective is to reconstruct the original input, thus learning contextual embeddings.
*   **Autoregressive (CLM - Causal Language Modeling):** Models like GPT consist of only a decoder and predict the masked word from the preceding words. Thus, autoregressive models are great at autocompleting a sentence, which is what happens in text generation models. The model predicts each token based on the tokens that come before it.
*   **Seq2Seq (Span Corruption):** Training involves masking random sequences of input tokens and replacing them with a unique sentinel token. The output is the sentinel token followed by the predicted tokens. In summary, seq2seq models need to understand the context and generate text.

**Masked Language Modeling (MLM)**

Input text with randomly masked tokens is fed into a Transformer encoder to predict the masked tokens. For instance, an original text sequence "I", "love", "this", "red", "car" is prepended with the "<cls>" token, and the "<mask>" token randomly replaces "love"; then the cross-entropy loss between the masked token "love" and its prediction is to be minimized during pre-training. This encourages the model to learn bidirectional contextual representations.

**Next Token Prediction (NTP)**

Any text can be used for this pre-training task, which only requires the prediction of the next word in the sequence. The model learns to capture sequential dependencies and generate coherent text.

**Span corruption**

In the original text, some words are dropped out with a unique sentinel token. Words are dropped out independently and uniformly at random. The model is trained to predict sentinel tokens to delineate the dropped out text.

For example:

Original text: Thank you for inviting me to your party last week.

Inputs: Thank you <X> me to your party <Y> week.

Targets: <X> for inviting <Y> last <Z>

<----------section---------->

**Summary on pre-training**

*   Pre-training tasks can be invented flexibly, and effective representations can be derived from a flexible regime of pre-training tasks. This allows researchers to design pre-training objectives tailored to specific downstream tasks or to improve the general language understanding capabilities of the model.
*   Different NLP tasks seem to be highly transferable, producing effective representations that form a general model which can serve as the backbone for many specialized models. This enables the creation of general-purpose language models that can be adapted to various downstream applications with minimal task-specific training.
*   With Self-Supervised Learning the models seem to be able to learn from generating the language itself, rather than from any specific task. This allows the models to learn from vast amounts of unlabeled data, capturing a broad range of linguistic patterns and knowledge.
*   A Language Model can be used as a Knowledge Base; namely, a generatively pre-trained model may have decent zero-shot performance on a range of NLP tasks. The model can leverage the knowledge it acquired during pre-training to perform tasks without any task-specific training data.

<----------section---------->

**Datasets and Data Pre-processing**

**Datasets**

Training LLMs requires vast amounts of text data, and the quality of this data significantly impacts LLM performance. The pre-training data serves as the foundation for the model's language understanding and generation capabilities.

Pre-training on large-scale corpora provides LLMs with a fundamental understanding of language and some generative capability. The more data the model is pre-trained on, the better it performs.

Pre-training data sources are diverse, commonly incorporating web text, conversational data, and books as general pre-training corpora. Each type of data provides unique information and linguistic structure.

Leveraging diverse sources of text data for LLM training can significantly enhance the model’s generalization capabilities. This allows the model to perform well on various tasks and adapt to new domains.

Here is a list of datasets used in popular LLMs:

| LLMs        | Datasets                                                                                                                            |
| ----------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| GPT-3       | CommonCrawl, WebText2, Books1, Books2, Wikipedia                                                                                 |
| LLaMA       | CommonCrawl, C4, Wikipedia, Github, Books, Arxiv, StackExchange                                                                  |
| PaLM        | Social Media, Webpages, Books, Github, Wikipedia, News (total 780B tokens)                                                        |
| T5          | C4, WebText, Wikipedia, RealNews                                                                                                  |
| CodeGen     | the Pile, BIGQUERY, BIGPYTHON                                                                                                       |
| CodeGeex    | CodeParrot, the Pile, Github                                                                                                        |
| GLM         | BooksCorpus, Wikipedia                                                                                                            |
| BLOOM       | ROOTS                                                                                                                               |
| OPT         | BookCorpus, CCNews, CC-Stories, the Pile, Pushshift.io                                                                              |

Here is a table with several corpora types and links:

| Corpora       | Type          | Links                                                                       |
| ------------- | ------------- | --------------------------------------------------------------------------- |
| BookCorpus    | Books         | [https://github.com/soskek/bookcorpus](https://github.com/soskek/bookcorpus) |
| Gutenberg     | Books         | [https://www.gutenberg.org](https://www.gutenberg.org)                       |
| Books1        | Books         | Not open source yet                                                         |
| Books2        | Books         | Not open source yet                                                         |
| CommonCrawl   | CommonCrawl   | [https://commoncrawl.org](https://commoncrawl.org)                           |
| C4            | CommonCrawl   | [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4) |
| CC-Stories    | CommonCrawl   | Not open source yet                                                         |
| CC-News       | CommonCrawl   | [https://commoncrawl.org/blog/news-dataset-available](https://commoncrawl.org/blog/news-dataset-available)  |
| RealNews      | CommonCrawl   | [https://github.com/rowanz/grover/tree/master/realnews](https://github.com/rowanz/grover/tree/master/realnews)    |
| RefinedWeb    | CommonCrawl   | [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) |
| WebText       | Reddit Link   | Not open source yet                                                         |
| OpenWebtext   | Reddit Link   | [https://skylion007.github.io/OpenWebTextCorpus/](https://skylion007.github.io/OpenWebTextCorpus/) |
| PushShift.io  | Reddit Link   |                                                                             |
| Wikipedia     | Wikipedia     | [https://dumps.wikimedia.org/zhwiki/latest/](https://dumps.wikimedia.org/zhwiki/latest/) |

<----------section---------->

**Datasets - Books**

*   Two commonly utilized books datasets for LLMs training are BookCorpus and Gutenberg. These datasets offer high-quality, well-edited text, which is beneficial for training language models.
*   These datasets include a wide range of literary genres, including novels, essays, poetry, history, science, philosophy, and more. This diversity helps the models learn a broad range of linguistic styles and topics.
*   Widely employed by numerous LLMs, these datasets contribute to the models’ pre-training by exposing them to a diverse array of textual genres and subject matter, fostering a more comprehensive understanding of language across various domains. This broad exposure improves the models’ ability to generalize to new tasks and datasets.
*   Book Corpus includes 800 million words, providing a substantial amount of training data.

**Datasets - CommonCrawl**

*   CommonCrawl manages an accessible repository of web crawl data, freely available for utilization by individuals and organizations. This open access promotes research and development in NLP and related fields.
*   This repository encompasses a vast collection of data, comprising over 250 billion web pages accumulated over a span of 16 years. This extensive coverage provides a comprehensive snapshot of the internet's content over time.
*   This continuously expanding corpus is a dynamic resource, with an addition of 3–5 billion new web pages each month. The constant updating ensures that the dataset remains relevant and representative of current online content.
*   However, due to the presence of a substantial amount of low-quality data in web archives, preprocessing is essential when working with CommonCrawl data. Cleaning and filtering the data is critical to ensuring high-quality training.

**Datasets - Wikipedia**

*   Wikipedia, the free and open online encyclopedia project, hosts a vast repository of high-quality encyclopedic content spanning a wide array of topics. The collaborative nature of Wikipedia ensures that the content is generally accurate and well-maintained.
*   The English version of Wikipedia, including 2,500 million words, is extensively utilized in the training of many LLMs, serving as a valuable resource for language understanding and generation tasks. The high-quality, structured content makes it an ideal training dataset.
*   Additionally, Wikipedia is available in multiple languages, providing diverse language versions that can be leveraged for training in multilingual environments. This multilingual support enables the development of models that can understand and generate text in multiple languages.

<----------section---------->

**Data pre-processing**

*   Once an adequate corpus of data is collected, the subsequent step is data preprocessing, whose quality directly impacts the model’s performance and security. Effective data preprocessing is crucial for the success of LLMs.
*   The specific preprocessing steps involve filtering low-quality text, including eliminating toxic and biased content to ensure the model aligns with human ethical standards. This is essential for preventing the model from generating harmful or offensive content.
*   It also includes deduplication, removing duplicates in the training set, and excluding redundant content in the test set to maintain the sample distribution balance. Deduplication improves training efficiency and prevents overfitting.
*   Privacy scrubbing is applied to ensure the model’s security, preventing information leakage or other privacy-related concerns. This is crucial for protecting sensitive user data.

**Quality filtering**

*   Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Both methods aim to remove noise and improve the quality of the training data.
*   Heuristic methods involve employing manually defined rules to eliminate low-quality data. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. These rules help to automatically identify and remove low-quality content.
*   Classifier-based methods involve training a classifier on a high-quality dataset to filter out low-quality datasets. This allows for more sophisticated filtering based on learned patterns.

**Deduplication**

*   Language models may sometimes repetitively generate the same content during text generation, potentially due to a high degree of repetition in the training data. This issue can affect the quality of the generated text.
*   Extensive repetition can lead to training instability, resulting in a decline in the performance of LLMs. Maintaining the sample distribution balance prevents the model from overfitting to the training data and improves its generalization capabilities.
*   Additionally, it is crucial to consider avoiding dataset contamination by removing duplicated data present in both the training and test set. This ensures that the model is evaluated on truly unseen data.

<----------section---------->

**Privacy scrubbing**

*   LLMs, as text-generating models, are trained on diverse datasets, which may pose privacy concerns and the risk of inadvertent information disclosure. Addressing these concerns is essential for responsible AI development.
*   It is imperative to address privacy concerns by systematically removing any sensitive information. This involves employing techniques such as anonymization, redaction, or tokenization to eliminate personally identifiable details, geolocation, and confidential data. These techniques protect user privacy while allowing the model to learn from the data.
*   By carefully scrubbing the dataset of such sensitive content, researchers and developers can ensure that the language models trained on these datasets uphold privacy standards and mitigate the risk of unintentional disclosure of private information. This practice is critical for building trustworthy and ethical AI systems.

**Filtering out toxic and biased text**

*   In the preprocessing steps of language datasets, a critical consideration is the removal of toxic and biased content to ensure the development of fair and unbiased language models. This is a fundamental step in creating ethical and responsible AI.
*   This involves implementing robust content moderation techniques, such as employing sentiment analysis, hate speech detection, and bias identification algorithms. These tools help to systematically identify and remove harmful content.
*   By leveraging these tools, it is possible to systematically identify and filter out text that may perpetuate harmful stereotypes, offensive language, or biased viewpoints. This ensures that the model is trained on a dataset that promotes fairness and inclusivity.

<----------section---------->

**Using LLMs after pre-training**

After pre-training, LLMs can be adapted for specific tasks using two primary methods: fine-tuning and prompting.

*   **Fine-tuning:** Gradient descent on weights to optimize performance on one task. Fine-tuning involves updating the model's parameters using labeled data specific to the task.
    *   What to fine-tune? Options include:
        *   Full network: Updating all the model's parameters.
        *   Readout heads: Only updating the classification layers.
        *   Adapters: Adding small, task-specific modules.
    *   Changes the model "itself," adapting it to the task.
*   **Prompting:** Design special prompts to cue / condition the network into specific mode to solve any tasks. Prompting involves providing the model with specific instructions or context to guide its behavior.
    *   No parameter change. The model's parameters remain fixed.
    *   One model to rule them all. A single pre-trained model can be used for multiple tasks.
    *   Changes the way to use it. The model's behavior is controlled by the prompt.

<----------section---------->

**Natural Language Processing and Large Language Models**

This material is designed for the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree Course in Computer Engineering).

Lesson 12 focuses on Hugging Face.

Prepared by Nicola Capuano and Antonio Greco, DIEM – University of Salerno.

`<----------section---------->`

**Outline**

This lesson will cover the following topics:

*   Overview of Hugging Face
*   Setting up the environment to use Hugging Face
*   Using the `pipeline` function for quick NLP tasks
*   Model selection from the Hugging Face Model Hub
*   Commonly used models within Hugging Face
*   Creating demos with Gradio

`<----------section---------->`

**Overview**

**Hugging Face**

Hugging Face provides tools and resources for Natural Language Processing (NLP). Their education toolkit, available on GitHub ([https://github.com/huggingface/education-toolkit](https://github.com/huggingface/education-toolkit)), is designed to help users prepare workshops, events, homework, or classes related to NLP. The toolkit’s content is self-contained and incorporates open-source technologies like Transformers and Gradio. Beyond tutorials, it offers resources for further exploration in Machine Learning (ML) and content design.

`<----------section---------->`

**Hugging Face Hub**

The Hugging Face Hub ([https://huggingface.co/](https://huggingface.co/)) serves as a central repository for:

*   **Models:** Pre-trained models for various NLP tasks.
*   **Datasets:** Collections of data for training and evaluating models. The Hub ([https://hf.co/datasets](https://hf.co/datasets)) hosts around 3000 open-source datasets that are free to use in multiple domains. The open-source `datasets` library allows for easy access and use of these datasets, even very large ones, through features like streaming. Each dataset has a dataset card that documents its details, including a summary and structure. An example dataset is GLUE ([https://huggingface.co/datasets/nyu-mll/glue](https://huggingface.co/datasets/nyu-mll/glue)).
*   **Spaces:** Platforms for hosting demos and code related to NLP projects.
*   **Key Libraries:** The Hugging Face ecosystem includes several key libraries:
    *   `datasets`: Used to download datasets from the Hub.
    *   `transformers`: Provides tools to work with pipelines, tokenizers, models, and more.
    *   `evaluate`: Used for computing evaluation metrics for models.

These libraries are compatible with both PyTorch and TensorFlow, two popular machine-learning frameworks.

`<----------section---------->`

**Hugging Face – Model Hub**

The Model Hub ([https://huggingface.co/models](https://huggingface.co/models)) can be navigated as demonstrated by Lysandre. When selecting a model, consider these factors:

*   **Task:** Filter models by task to find models suited to your specific NLP application (e.g., summarization, text generation). An earthquake rattled Papua New Guinea, and the specified NLP task could be summarization.
*   **Extractive vs. Abstractive Summarization:** Decide whether you need extractive summarization (selecting pieces of text) or abstractive summarization (generating new text).
*   **License:** Ensure the model's license aligns with your usage requirements.
*   **Language:** Filter by language to find models trained on your desired language.
*    **Model Size:** Filter by model size based on hardware limits, cost constraints, or latency requirements. This is especially relevant when using only affordable hardware such as a laptop or free online GPU resources
*   **Popularity and Updates:** Sort models by downloads, recent updates, and likes to find actively maintained and widely used models.
*   **Release History:** Check the Git release history for the model to understand its development and updates. (e.g. github.com/google-research/bert/blob/master/README.md)
*   **Model Variants:** Pick good variants of models for your task.
    *   Different sizes of the same base model.
    *   Fine-tuned variants of base models.

`<----------section---------->`

**Model Selection Considerations**

When choosing a model, also consider:

*   **Examples and Datasets:** Search for examples and datasets, not just models, to understand how the model performs in practical scenarios.
*   **Task Specificity:** Determine if the model is good at everything or fine-tuned for a specific task.
*   **Training Data:** Identify which datasets were used for pre-training and fine-tuning to understand the model's knowledge base.
*   **Your Data and Users:** Ultimately, the model's performance depends on your specific data and user needs.
    *   Define Key Performance Indicators (KPIs) to measure the model's success.
    *   Test the model on your data or with your users to gather performance feedback.

`<----------section---------->`

**Setup**

**Google Colab**

Using a Colab notebook is the easiest way to get started. Colab notebooks allow you to run code directly in your browser.

*   Install the Transformers library:
    *   `!pip install transformers`
*   Import the Transformers library:
    *   `import transformers`

This installs a lightweight version of Transformers without specific machine learning frameworks (PyTorch or TensorFlow). To install the development version with all dependencies:

*   `!pip install transformers[sentencepiece]`

`<----------section---------->`

**Virtual Environment**

1.  Download and install Anaconda: [https://www.anaconda.com/download](https://www.anaconda.com/download)
2.  Create a new Anaconda environment:
    *   `conda create --name nlpllm`
3.  Activate the environment:
    *   `conda activate nlpllm`
4.  Install Transformers with SentencePiece:
    *   `conda install transformers[sentencepiece]`

`<----------section---------->`

**Hugging Face Account**

Having a Hugging Face account is recommended to use most of the functionalities. It's best to create one.

`<----------section---------->`

**Pipeline**

The `pipeline()` function is the most basic object in the Hugging Face Transformers library.

*   It connects a model with its preprocessing and postprocessing steps.
*   It allows direct input of text and provides an understandable output.
*   The Model Hub contains thousands of pretrained models available for download and use.

`<----------section---------->`

**Common Models**

The following table lists some common models used in NLP:

| Model              | License        | Organization | Year | Description                                                                        |
| :----------------- | :------------- | :------------- | :--- | :--------------------------------------------------------------------------------- |
| Pythia 19M-12B     | Apache 2.0     | EleutherAI   | 2023 | Series of 8 models for comparisons across sizes                                    |
| Dolly 12B          | MIT            | Databricks   | 2023 | Instruction-tuned Pythia model                                                     |
| GPT-3.5 175B       | Proprietary    | OpenAI       | 2022 | ChatGPT model option; related models GPT-1/2/3/4                                 |
| OPT 125M-175B      | MIT            | Meta         | 2022 | Based on GPT-3 architecture                                                        |
| BLOOM 560M - 176B  | RAIL v1.0       | Many groups  | 2022 | 46 languages                                                                     |
| GPT-Neo/X 125M-20B | MIT / Apache 2.0 | EleutherAI   | 2021/2022 | Based on GPT-2 architecture                                                        |
| FLAN 80M-540B      | Apache 2.0     | Google       | 2021 | Methods to improve training for existing architectures                            |
| BART 139M-406M     | Apache 2.0     | Meta         | 2019 | Derived from BERT, GPT, others                                                     |
| T5 50M-TIB         | Apache 2.0     | Google       | 2019 | 4 languages                                                                      |
| BERT               | Apache 2.0     | Google       | 2018 | Early breakthrough                                                               |

`<----------section---------->`

**Gradio**

Gradio is a tool for creating web demos for machine learning models.

**Steps for Creating a Demo:**

| Step                                | Framework           | Language |
| :---------------------------------- | :------------------ | :------- |
| 1. Train a model                    | TensorFlow/PyTorch | Python   |
| 2. Containerize and deploy the model |                     | Python   |
| 3. Store incoming samples          | Gradio              | Python   |
| 4. Build an interactive front-end   |                     | Python   |

**News Summarizer Example:**

Gradio can be used to build a news summarizer that uses Hugging Face models to summarize articles.

*   The summarizer uses the `bart-large-cnn` model by Facebook.
*   Shorter articles generate faster summaries.

**Example Usage:** Provide a URL, and the model will summarize the article.

Examples:

*   [https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/](https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/)
*   [https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/](https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/)
*   [https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/](https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/)

`<----------section---------->`

**Gradio – Free Hosting**

Hugging Face offers free hosting for Gradio demos on hf.space. These spaces allow the community to share and discover machine learning applications.

`<----------section---------->`

**Building Demos**

You can build your own demo with Gradio.

*   Install Gradio: `conda install gradio`
*   Refer to the provided link for a tutorial: [https://bit.ly/34wESgd](https://bit.ly/34wESgd)

<----------section---------->

# Natural Language Processing and Large Language Models
## Corso di Laurea Magistrale in Ingegneria Informatica
## Lesson 13: Encoder-only Transformers
### Nicola Capuano and Antonio Greco
### DIEM – University of Salerno

This lesson focuses on Encoder-only Transformers, a key component in modern Natural Language Processing (NLP). It's part of the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree Course in Computer Engineering) taught by Nicola Capuano and Antonio Greco from DIEM – University of Salerno. The lesson explores the architecture, application, and practice of these transformers, particularly the BERT model.

<----------section---------->

## Outline

This lesson covers the following topics:

*   **Encoder-only transformer**: An in-depth look at its architecture and functionalities.
*   **BERT**: The Bidirectional Encoder Representations from Transformers model.
*   **Practice on token classification and named entity recognition**: Hands-on exercises to apply the learned concepts.

<----------section---------->

## Encoder-only transformer

The Transformer architecture is essential for tasks involving sequence-to-sequence transformation where the input and output sequences have different lengths. However, for tasks where the input and output sequences have the same length or require a single value as output, using only the Encoder part of the Transformer is sufficient and more efficient.

*   **Sequence-to-sequence transformation**: If the task requires transforming a sequence into another sequence of different length, the whole transformer architecture (encoder and decoder) is needed. A prime example is machine translation. The original Transformer model, introduced in "Attention is All You Need," was designed for this purpose, enabling translation between different languages with varying sentence lengths and structures.

*   **Sequence-to-sequence (same length)**: When the goal is to transform a sequence into another sequence of the same length, such as in token classification tasks, only the Encoder part is necessary. In this scenario, the output vectors $z_1, \dots, z_t$ can be directly used, and the loss function is computed based on these vectors. Each $z_i$ corresponds to the transformed representation of the input token $x_i$.

*   **Sequence-to-single value**: For tasks that require transforming a sequence into a single value, such as sequence classification (e.g., sentiment analysis), the Encoder part of the transformer is also sufficient. A special token, typically denoted as `[CLS]` (classification token), is added as the first element $x_1$ of the input sequence. The corresponding output value $z_1$ is then taken as the result of the transformation, and the loss function is computed solely on this value. The `[CLS]` token aggregates information from the entire sequence, making it suitable for classification tasks.

<----------section---------->

## BERT

BERT (Bidirectional Encoder Representations from Transformers) is a language model introduced by Google Research in 2018. It focuses on language understanding and leverages the Encoder part of the Transformer model. The "bidirectional" aspect means it considers both preceding and succeeding words to understand a word in context.

*   **Model Architecture**: BERT uses only the Encoder part from the Transformer architecture. There are two main versions:
    *   **BERT-base**: Contains 12 stacked encoder blocks and 110 million parameters.
    *   **BERT-large**: Contains 24 stacked encoder blocks and 340 million parameters.

*   **Bidirectional Context**: BERT is pre-trained to use "bidirectional" context. Unlike traditional language models that only consider the preceding words, BERT uses both preceding and succeeding words to understand a word in a sentence. This bidirectional approach allows for a more nuanced understanding of language.

*   **Pre-trained Base**: BERT is designed to be used as a pre-trained base for various Natural Language Processing tasks. After pre-training, it can be fine-tuned on specific tasks to achieve state-of-the-art results. This transfer learning approach significantly reduces the amount of task-specific data needed for training.

<----------section---------->

## BERT input encoding

BERT's input encoding involves tokenizing text into subword units using the WordPiece tokenizer and adding special tokens to mark the beginning and end of sequences.

*   **WordPiece Tokenizer**:
    *   **Subword-Based**: WordPiece tokenizes text at the subword level rather than using full words or individual characters. This allows BERT to handle both common words and rare or misspelled words by breaking them into manageable parts.
    *   **Vocabulary Building**: The WordPiece tokenizer builds a vocabulary of common words and subword units (e.g., "playing" could be tokenized as "play" and "##ing").
    *   **Unknown Words**: Rare or out-of-vocabulary words can be broken down into familiar subwords. For example, "unhappiness" could be split into "un," "happy," and "##ness." This ensures that the model can still process and understand words it hasn't seen before.

<----------section---------->

## BERT input encoding

To prepare the input for BERT, sentences are split into tokens, and special tokens are added.

*   **Splitting**: Sentences are split into tokens based on whitespace, punctuation, and common prefixes (like "##"). The WordPiece tokenizer handles this splitting process.

*   **Special Tokens**: BERT requires certain special tokens:
    *   `[CLS]`: A classification token added at the beginning of each input sequence. It's used to capture the overall context of the sequence for classification tasks.
    *   `[SEP]`: A separator token used to mark the end of a sentence or to separate two sentences in sentence-pair tasks.

*   **Tokenization Example**: The sentence "I’m feeling fantastic!" might be tokenized by WordPiece as: `[CLS] I ' m feeling fan ##tas ##tic ! [SEP]`

*   **Converting Tokens to IDs**: After tokenization, each token is mapped to an ID from BERT’s vocabulary. This numerical representation serves as the input to the model. The vocabulary ID lookup is a crucial step in preparing the text data for BERT.

<----------section---------->

## BERT input encoding

The WordPiece embedding offers several advantages, making it a crucial component of BERT's architecture.

The advantages of WordPiece embedding are the following:

*   **Efficiency**: Reduces the vocabulary size without losing the ability to represent diverse language constructs. A smaller vocabulary leads to fewer parameters in the model, making it more efficient to train and use.

*   **Handling Unseen Words**: Subword tokenization allows BERT to manage rare or newly created words by breaking them down into recognizable parts. This is particularly useful in handling evolving language and specialized vocabulary.

*   **Improved Language Understanding**: Helps BERT capture complex linguistic patterns by learning useful subword components during pretraining. The model can generalize better to new words and contexts by understanding the underlying structure of words.

<----------section---------->

## BERT [CLS] token

The `[CLS]` token in BERT plays a vital role in classification tasks by providing a summary representation of the entire input sequence.

*   **Role**: The `[CLS]` token (short for "classification token") is a special token added at the beginning of each input sequence.

*   **Position**: It is always placed at the very start of the tokenized sequence and is primarily used as a summary representation of the entire input sequence.

*   **Function**: After the input is processed by BERT, the final hidden state of the `[CLS]` token acts as a condensed, context-aware embedding for the whole sentence or sequence of sentences.

*   **Application**: This embedding can then be fed into additional layers (like a classifier) for specific tasks such as sentiment analysis or text classification.

<----------section---------->

## BERT [CLS] token

The usage of the `[CLS]` token varies depending on whether the task involves single-sentence or sentence-pair classification.

The `[CLS]` token is used differently for single sentence and sentence pair classification.

*   **Single-Sentence Classification**
    *   The final hidden state of the `[CLS]` token is passed to a classifier layer to make predictions.
    *   For instance, in sentiment analysis, the classifier might predict "positive" or "negative" sentiment based on the `[CLS]` embedding. The `[CLS]` token essentially captures the overall sentiment of the input sentence.

*   **Sentence-Pair Tasks**
    *   For tasks involving two sentences, BERT tokenizes them as `[CLS] Sentence A [SEP] Sentence B [SEP]`.
    *   The `[CLS]` token’s final hidden state captures the relationship between the two sentences, making it suitable for tasks like entailment detection or similarity scoring. By attending to both sentences, the `[CLS]` token can provide a comprehensive representation of their relationship.

<----------section---------->

## BERT pre-training

BERT is pre-trained using two self-supervised learning strategies, enabling it to learn rich language representations without requiring labeled data.

*   BERT is pre-trained using two self-supervised learning strategies:
    *   **Masked Language Modeling (MLM)**
        *   **Objective**: Predict masked (hidden) tokens in a sentence.
        *   **Process**: Randomly mask 15% of tokens and train BERT to predict them based on context. This forces the model to understand the context around each word.
        *   **Benefit**: Enables BERT to learn bidirectional context by considering both preceding and succeeding words.

    *   **Next Sentence Prediction (NSP)**
        *   **Objective**: Determine if one sentence logically follows another.
        *   **Process**: Trains BERT to understand sentence-level relationships by predicting whether two given sentences are consecutive in the original text.
        *   **Benefit**: Improves performance in tasks like question answering and natural language inference, where understanding the relationship between sentences is crucial.

*   **Training Data**: The training set is composed of a large corpus of publicly available books and the English Wikipedia, totaling more than 3 billion words. This vast amount of data allows BERT to learn a general understanding of language.

<----------section---------->

## BERT fine tuning

Fine-tuning BERT involves adapting the pre-trained model to specific tasks by adding task-specific layers and minimizing the loss function.

*   The output of the encoder is given to an additional layer to solve a specific problem. This layer is typically a classification layer for classification tasks or a regression layer for regression tasks.

*   The cross-entropy loss between the prediction and the label for the classification task is minimized via gradient-based algorithms. The additional layer is trained from scratch, while pretrained parameters of BERT may be updated or not, depending on the specific task and dataset.

<----------section---------->

## BERT fine tuning

Fine-tuning BERT on specific datasets allows it to adapt its general language understanding to the nuances of the task at hand, enhancing its performance.

*   BERT is pretrained on general language data, then fine-tuned on specific datasets for each task.

*   In fine-tuning, the `[CLS]` token’s final embedding is specifically trained for the downstream task, refining its ability to represent the input sequence in the way needed for that task. This ensures that the model is optimized for the specific requirements of the task.

*   Example Tasks:
    *   **Text Classification**: Sentiment analysis, spam detection.
    *   **Named Entity Recognition (NER)**: Identifying names, dates, organizations, etc., within text.
    *   **Question Answering**: Extractive QA where BERT locates answers within a passage.

*   **Minimal Task-Specific Adjustments**: Only a few additional layers are added per task. This minimizes the amount of task-specific data needed and reduces the risk of overfitting.

<----------section---------->

## BERT strengths and limitations

BERT's strengths include its bidirectional contextual understanding and flexibility in transfer learning, while its limitations involve large model size and pretraining costs.

*   **Strengths**
    *   **Bidirectional Contextual Understanding**: provides richer and more accurate representations of language. This allows for a more nuanced understanding of the input text.
    *   **Flexibility in Transfer Learning**: BERT’s pretraining allows for easy adaptation to diverse NLP tasks. The pre-trained weights serve as a strong starting point, reducing the amount of task-specific data and training time required.
    *   **High Performance on Benchmark Datasets**: Consistently ranks at or near the top on datasets like SQuAD (QA) and GLUE (general language understanding).

*   **Limitations**
    *   **Large Model Size**: High computational and memory requirements make deployment challenging. The large number of parameters requires significant resources for both training and inference.
    *   **Pretraining Costs**: Requires extensive computational resources, especially for BERT-Large. Training BERT from scratch is computationally expensive and time-consuming.
    *   **Fine-Tuning Time and Data**: Fine-tuning on new tasks requires labeled data and can still be time-intensive. While transfer learning reduces the data requirement, fine-tuning still needs labeled data and computational resources.

<----------section---------->

## Popular BERT variants - RoBERTa

RoBERTa (Robustly Optimized BERT Approach) is a BERT variant developed by Facebook AI in 2019, focusing on improved training methodologies.

*   RoBERTa is the acronym for Robustly Optimized BERT Approach and was developed by Facebook AI in 2019.

*   The main differences with respect to BERT are the following:
    *   **Larger Training Corpus**: Trained on more data (200 billions) compared to BERT.
    *   **Removed Next Sentence Prediction (NSP)**: Found that removing NSP improves performance. Experiments showed that NSP didn't contribute significantly to downstream task performance.
    *   **Longer Training and Larger Batches**: RoBERTa was trained for more iterations and used larger batches for robust language modeling.
    *   **Dynamic Masking**: Masking is applied dynamically (i.e., different masks per epoch), leading to better generalization. This ensures that the model sees different masking patterns during training, improving its robustness.

*   RoBERTa consistently outperforms BERT on various NLP benchmarks, particularly in tasks requiring nuanced language understanding.

<----------section---------->

## Popular BERT variants - ALBERT

ALBERT (A Lite BERT) is a BERT variant developed by Google Research in 2019, focusing on reducing the model size and improving efficiency.

*   ALBERT is the acronym of A Lite BERT, developed by Google Research in 2019.

*   The main differences with respect to BERT are:
    *   **Parameter Reduction**: Uses factorized embedding parameterization to reduce model size. This reduces the number of parameters in the embedding layer.
    *   **Cross-Layer Parameter Sharing**: Shares weights across layers to decrease the number of parameters.
    *   **Sentence Order Prediction (SOP)**: Replaces NSP with SOP, which is better suited for capturing inter-sentence coherence. SOP predicts whether two sentences are in the correct order or reversed.

*   ALBERT achieves comparable results to BERT-Large with fewer parameters, making it more memory-efficient.

*   ALBERT is faster and lighter, ideal for applications where resources are limited.

<----------section---------->

## Popular BERT variants - DistilBERT

DistilBERT (Distilled BERT) is a BERT variant developed by Hugging Face, focusing on model distillation to reduce the size and improve inference speed.

*   DistilBERT is the acronym for Distilled BERT, developed by Hugging Face.

*   The main differences with BERT are the following:
    *   **Model Distillation**: Uses knowledge distillation to reduce BERT’s size by about 40% while retaining 97% of its language understanding capabilities. Knowledge distillation involves training a smaller "student" model to mimic the behavior of a larger "teacher" model.
    *   **Fewer Layers**: DistilBERT has 6 layers instead of 12 (for BERT-Base) but is optimized to perform similarly.
    *   **Faster Inference**: Provides faster inference and lower memory usage, making it ideal for real-time applications.

*   DistilBERT is widely used for lightweight applications that need a smaller and faster model without major accuracy trade-offs.

<----------section---------->

## Popular BERT variants - TinyBERT

TinyBERT is a further optimized version of BERT, developed by Huawei, that focuses on extreme model compression for resource-constrained environments.

*   TinyBERT was developed by Huawei.

*   The main differences with BERT are the following:
    *   **Two-Step Knowledge Distillation**: Distills BERT both during pretraining and fine-tuning, further enhancing efficiency.
    *   **Smaller and Faster**: TinyBERT is even smaller than DistilBERT, optimized for mobile and edge devices.
    *   **Similar Accuracy**: Maintains accuracy close to that of BERT on various NLP tasks, especially when fine-tuned with task-specific data.

*   TinyBERT is an ultra-compact version of BERT that is well-suited for resource-constrained environments.

<----------section---------->

## Popular BERT variants - ELECTRA

ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) is a BERT variant developed by Google Research, focusing on a more efficient pretraining method.

*   ELECTRA is the acronym for Efficiently Learning an Encoder that Classifies Token Replacements Accurately, developed by Google Research.

*   Its differences with respect to BERT are:
    *   **Replaced Token Detection**: Instead of masked language modeling, ELECTRA uses a generator-discriminator setup where the model learns to identify replaced tokens in text. A generator replaces some tokens, and a discriminator tries to identify which tokens have been replaced.
    *   **Efficient Pretraining**: This approach allows ELECTRA to learn with fewer resources and converge faster than BERT.
    *   **Higher Performance**: Often outperforms BERT on language understanding benchmarks with significantly less compute power.

*   ELECTRA’s training efficiency and robust performance make it appealing for applications where computational resources are limited.

<----------section---------->

## Popular BERT variants - SciBERT

SciBERT is a BERT variant tailored for applications in scientific literature, enhancing its performance in academic and research-oriented NLP tasks.

*   SciBERT is tailored for applications in scientific literature, making it ideal for academic and research-oriented NLP.

*   **Domain-Specific Pretraining**: Trained on a large corpus of scientific papers from domains like biomedical and computer science.

*   **Vocabulary Tailored to Science**: Uses a vocabulary that better represents scientific terms and jargon.

*   **Improved Performance on Scientific NLP Tasks**: Significantly outperforms BERT on tasks like scientific text classification, NER, and relation extraction in scientific contexts.

<----------section---------->

## Popular BERT variants - BioBERT

BioBERT is a BERT variant specifically designed for biomedical research, assisting in information extraction and discovery from medical literature.

*   BioBERT is widely adopted in the biomedical research field, aiding in information extraction and discovery from medical literature.

*   **Biomedical Corpus**: Pretrained on a biomedical text corpus, including PubMed abstracts and PMC full-text articles.

*   **Enhanced Performance on Biomedical Tasks**: Excels at biomedical-specific tasks such as medical NER, relation extraction, and question answering in healthcare.

<----------section---------->

## Popular BERT variants - ClinicalBERT

ClinicalBERT is a BERT variant designed for healthcare providers, aiding in the analysis of patient records and clinical decision-making.

*   ClinicalBERT is ideal for hospitals and healthcare providers who need to analyze patient records, predict health outcomes, or assist in clinical decision-making.

*   **Healthcare Focus**: Tailored for processing clinical notes and healthcare-related NLP tasks.

*   **Training on MIMIC-III Dataset**: Pretrained on the MIMIC-III database of clinical records, making it useful for healthcare analytics.

<----------section---------->

## Popular BERT variants - mBERT

mBERT (Multilingual BERT) is a BERT variant developed by Google, designed to support NLP tasks across multiple languages.

*   mBERT, developed by Google, supports NLP tasks across languages, enabling global applications and language transfer learning.

*   **Multilingual Support**: Trained on 104 languages, mBERT can handle multilingual text without requiring separate models for each language.

*   **Language-Agnostic Representation**: Capable of zero-shot cross-lingual transfer, making it suitable for translation and cross-lingual understanding tasks.

<----------section---------->

## Other BERT variants

Numerous other BERT variants exist, each tailored to specific domains or languages, showcasing the versatility of the BERT architecture.

*   CamemBERT: French language-focused BERT model.
*   FinBERT: Optimized for financial text analysis.
*   LegalBERT: Trained on legal documents for better performance in the legal domain.
*   Moreover, BERT inspired Transformer pre-training in computer vision, such as with vision Transformers, Swin Transformers, and Masked Auto Encoders (MAE).

<----------section---------->

## Practice on token classification and named entity recognition

This section provides practical exercises for applying BERT models to token classification and named entity recognition tasks.

*   Looking at the Hugging Face tutorial on token classification: [https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt), use different existing versions of BERT to perform named entity recognition. This exercise allows students to explore different BERT variants and their performance on NER.

*   Test these versions not only with your own prompts, but also with data available in public datasets (e.g. [https://huggingface.co/datasets/eriktks/conll2003](https://huggingface.co/datasets/eriktks/conll2003)). Using public datasets ensures standardized evaluation and comparison.

*   If you have time and computational resources, you can also fine tune one of the lightweight versions of BERT ([https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#fine-tuning-the-model-with-the-trainer-api](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#fine-tuning-the-model-with-the-trainer-api)). Fine-tuning allows for task-specific optimization and can significantly improve performance.

<----------section---------->

# Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree in Computer Engineering)
Lesson 14
Decoder-only Transformers
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This material covers decoder-only transformer models, focusing on their architecture, applications, and prominent examples such as GPT and LLAMA. It originates from Lesson 14 of a course on Natural Language Processing (NLP) and Large Language Models (LLMs) within a Computer Engineering Master's program at the University of Salerno.

<----------section---------->

## Outline
*   Decoder-only transformer
*   GPT (Generative Pre-trained Transformer)
*   LLAMA (Large Language Model Meta AI)
*   Practice on text generation

This outline provides a roadmap for the lesson, starting with a general overview of decoder-only transformers and then delving into specific models, GPT and LLAMA. The lesson concludes with practical exercises on text generation using these models.

<----------section---------->

## Decoder-only transformer
*   Transformers that use only the decoder part of the Transformer architecture. This contrasts with the original Transformer architecture, which includes both an encoder and a decoder.

*   It focuses only on decoding, making it efficient for autoregressive generation tasks. Autoregressive generation means that the model predicts the next token based on the previously generated tokens. This sequential generation is a hallmark of decoder-only models.

*   It lacks the separate encoder layers found in sequence-to-sequence models. Sequence-to-sequence models typically use an encoder to process the input and a decoder to generate the output. Decoder-only transformers streamline this process by omitting the encoder for certain tasks.

*   Primarily used for language generation tasks, such as text generation, summarization, and question answering. Their autoregressive nature makes them particularly well-suited for these tasks, where the output is generated sequentially.

*   Popular Examples: GPT (Generative Pre-trained Transformer) series, including GPT-2, GPT-3 and GPT-4, and LLAMA. These models have demonstrated remarkable capabilities in various language generation tasks, driving advancements in the field.

<----------section---------->

## Decoder-only transformer
*   In a decoder-only transformer, text generation is achieved through an autoregressive approach, where each token is generated sequentially by attending only to previously generated tokens within the same sequence, rather than using encoder-decoder attention. This means the model predicts each word based on the words it has already generated, creating a coherent and contextually relevant text.
*   The input context (prompt) and the generated text are treated as a single, continuous sequence. This continuous sequence of tokens essentially allows the decoder-only model to handle both the “encoding” (understanding the input prompt) and “decoding” (generating text) in one step, without needing a separate encoder block. The model learns to extract relevant information from the prompt and use it to guide the generation process.
*   Once a token is generated, it’s appended to the input sequence, and the model proceeds to generate the next token. This iterative process allows the model to build upon its previous outputs, creating a flowing and cohesive text.

<----------section---------->

## Decoder-only transformer
*   Self-Attention with Causal Masking: The model uses self-attention within the decoder layers, but with a causal (unidirectional) mask that prevents each token from attending to future tokens. This ensures that each position only considers information from previous positions, simulating the generation process where each word is generated sequentially. Causal masking is crucial for maintaining the autoregressive property of the model.
*   Implicit Context Understanding: In a decoder-only architecture, the model builds up context as it processes tokens in sequence. As it reads through a sequence, it "remembers" previous tokens and learns relationships between them within the attention layers. This sequential buildup of context replaces the need for separate encoder-decoder attention by allowing the model to accumulate an understanding of the input as it progresses through each token. The model effectively encodes the input prompt within its internal states as it generates the output.

<----------section---------->

## Encoder-only vs Decoder-only

| Feature            | Encoder-Only Transformers (e.g., BERT)                                   | Decoder-Only Transformers (e.g., GPT)                                |
| ------------------ | ------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Architecture       | Only encoder blocks (bidirectional attention)                               | Only decoder blocks (causal attention)                               |
| Training Objective | Masked Language Modeling (MLM)                                           | Autoregressive Language Modeling                                     |
| Context            | Processes entire sequence in parallel                                      | Processes tokens sequentially (one by one)                           |
| Main Use Cases     | Text classification, NER, question answering                                | Text generation, story generation, code generation                    |
| Attention Type     | Bidirectional self-attention                                              | Unidirectional (masked) self-attention                               |
| Output             | Contextual embeddings for downstream tasks                                | Sequential token generation (text or other content)                   |

This table highlights the key differences between encoder-only and decoder-only transformers. Encoder-only models like BERT are designed for understanding the context of an entire sequence, while decoder-only models like GPT are optimized for generating sequential outputs. The choice between the two depends on the specific application.

<----------section---------->

## Decoder-only transformer applications
Applications of Decoder-Only Transformers are:
*   Text Generation: News articles, stories, and creative content. The ability to generate coherent and contextually relevant text makes them ideal for content creation.
*   Conversational AI: Chatbots and virtual assistants for real-time dialogue. Their autoregressive nature allows them to maintain context and generate appropriate responses in a conversation.
*   Programming Help: Code generation and debugging. They can assist developers by generating code snippets, identifying errors, and providing explanations.
*   Summarization: Generating concise summaries of long documents. They can extract the key information from a document and present it in a condensed form.

<----------section---------->

## GPT
*   GPT is a type of decoder-only transformer developed by OpenAI. It represents a significant advancement in language modeling.
*   It generates human-like text, understanding and predicting language; being trained on vast amounts of text data, it can perform various natural language tasks without task-specific training. This ability to perform tasks without explicit fine-tuning is known as zero-shot or few-shot learning.
*   GPT-1 (2018): Introduced decoder-only transformer architecture, with 117 million parameters (12 decoder blocks, 768-dimensional embeddings, and 12 attention heads per block). This marked a significant step towards more powerful language models.
*   GPT-2 (2019): Significantly larger with 1.5 billion parameters in its XL version (48 decoder blocks, 1600-dimensional embeddings, and 25 attention heads per block), capable of generating coherent long-form text. The increased size allowed for more complex language generation.
*   GPT-3 (2020): 175 billion parameters (96 decoder blocks, 12,288-dimensional embeddings, and 96 attention heads per block), with advanced capabilities in language, code, and even reasoning. GPT-3's size and capabilities pushed the boundaries of what was possible with language models.
*   GPT-4 (2023): Multi-modal capabilities (image and text), improved reasoning, and broader general knowledge (detailed information on the architecture are not yet available). GPT-4 represents a further evolution with enhanced abilities and multi-modal input.

<----------section---------->

## GPT input encoding
*   GPT models, including GPT-1, GPT-2, GPT-3, and later versions, use Byte-Pair Encoding (BPE) as their input encoding method. BPE is a subword tokenization technique commonly used in NLP.
*   BPE is a subword tokenization technique that strikes a balance between word-level and character-level representations, breaking down words into smaller, meaningful subunits (tokens) based on frequency in the training data. This approach helps the model handle both common and rare words effectively.
*   The vocabulary size varies by model version (e.g., GPT-2 uses about 50,000 tokens), allowing efficient representation of both frequent and rare words. The vocabulary size is a crucial parameter that affects the model's ability to represent language.

<----------section---------->

## GPT input encoding
The main features of BPE are:
*   Subword Tokenization: BPE splits rare or complex words into subword units while keeping common words as single tokens. For instance, a rare word like "unhappiness" might be split into "un," "happi," and "ness." This allows the model to handle out-of-vocabulary words and learn morphological patterns.
*   Fixed Vocabulary: BPE produces a fixed-size vocabulary (e.g., around 50,000 tokens in GPT-2), containing common words, word fragments, and some single characters. This fixed vocabulary helps to control the model's complexity and memory requirements.
*   Efficiency in Language Representation: Subword tokens allow GPT to represent a diverse range of language patterns, handling both common and rare words effectively while reducing the total number of tokens required. BPE enables efficient representation of language by breaking words into smaller units.

<----------section---------->

## GPT input encoding
The main advantages of BPE (similar to WordPiece) are:
*   Flexibility: Handles languages with rich morphology or new words (e.g., "AI-generated") by breaking them down into reusable subword tokens. BPE's ability to handle morphological variations and novel words makes it well-suited for various languages.
*   Reduced Vocabulary Size: Keeps the vocabulary smaller and training more efficient compared to a word-level tokenizer. A smaller vocabulary reduces the model's memory footprint and computational cost.
*   Out-of-Vocabulary Handling: BPE is resilient to unknown words, as it can break down any new word into familiar subwords or characters. This ensures that the model can process any input, even if it contains words not seen during training.

<----------section---------->

## GPT pre-training
*   GPT is pre-trained to predict the next word (or token) in a sequence, given all the previous tokens. This process is also known as autoregressive modeling. Autoregressive modeling is a core technique for training language models.
*   In its Next-Token Prediction strategy, at each step GPT model learns to minimize the difference between its predicted next token and the actual next token in the training sequence, effectively learning context and word relationships. By learning to predict the next token, the model captures the statistical relationships between words and phrases.
*   The prediction is sequential, namely each token is predicted based only on previous tokens, so the model learns the patterns of language in a left-to-right order. This left-to-right order mimics the way humans read and write, allowing the model to generate coherent text.

<----------section---------->

## GPT pre-training
*   GPT models are trained on massive and diverse datasets sourced from a wide array of internet text. The scale and diversity of the training data are crucial for the model's performance.
*   GPT-1 was trained on BookCorpus, which consists of around 985 million words (800 MB of text). BookCorpus provided a foundation for GPT-1's language understanding.
*   GPT-2 was trained on WebText (40 GB of text from around 8 million documents with 10 billion words), a dataset curated from high-quality web pages by OpenAI. WebText significantly expanded the training data and improved GPT-2's generation capabilities.
*   GPT-3 used even larger datasets (570 GB of text with hundreds of billions of words), combining sources like Common Crawl, Books, Wikipedia, and more. The massive dataset used for GPT-3 enabled it to achieve state-of-the-art performance.
*   In all the cases, the data is selected to cover a broad range of topics and linguistic structures to make the model versatile across different domains. The diversity of the training data ensures that the model can generalize to different tasks and domains.
*   OpenAI's training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days. This illustrates the immense computational resources required to train large language models.

<----------section---------->

## GPT pre-training
*   GPT minimizes the cross-entropy loss between the predicted token probabilities and the actual tokens. This loss function is well-suited to classification tasks (like predicting the next token) and provides the model with feedback on its predictions. Cross-entropy loss is a standard metric for evaluating language models.
*   GPT uses the Adam optimizer, an adaptive gradient descent technique, which helps accelerate convergence by adjusting learning rates based on past gradients. The Adam optimizer is a popular choice for training deep learning models.
*   GPT applies a learning rate scheduling in which learning rates are gradually increased (warm-up) in the early stages and then decayed to prevent instability during training. Learning rate scheduling is a common technique for improving the training process.
*   Large batch sizes are used to stabilize training and make the model better at generalizing across diverse language patterns. Large batch sizes can help to reduce the variance of the gradient estimates and improve generalization.

<----------section---------->

## GPT fine tuning
*   Fine-tuning of GPT requires a dataset labeled for the specific task, such as pairs of prompts and expected responses, or inputs and target outputs. Fine-tuning allows the model to adapt to specific tasks and domains.
*   Examples of tasks for which GPT has been or may be fine tuned are:
    *   Customer Support Automation: queries, issues, information.
    *   Medical Assistance: health guidance and support patient inquiries.
    *   Legal Document Processing: summarize legal documents and support legal research.
    *   Coding Assistance: Provide code snippets, explanations, and debugging help.
    *   Educational Tutoring: Answer questions, explain concepts, and support e-learning.
    *   Content Creation: Generate blog posts, social media content, and marketing copy.
    *   Virtual Personal Assistants: Provide reminders, manage tasks, and answer questions.

These examples illustrate the wide range of tasks that GPT can be fine-tuned for, highlighting its versatility and adaptability.

<----------section---------->

## GPT strengths
*   Language Fluency and Coherence: GPT models generate human-like, fluent, and coherent text, often indistinguishable from human writing. This is a key advantage of GPT models.
*   Broad Knowledge Base: Trained on vast datasets, GPT has extensive general knowledge across a wide array of topics, allowing it to answer questions and generate content in diverse domains. The vast knowledge base is a result of training on large amounts of text data.
*   Few-Shot and Zero-Shot Learning: GPT can perform tasks with little to no task-specific training by learning from examples in the prompt (few-shot) or adapting to a task without examples (zero-shot). This ability reduces the need for large labeled datasets.
*   Creative and Contextual Writing: GPT can generate creative content, including stories, poetry, and dialogues, which makes it useful for content creation and entertainment applications. The ability to generate creative content opens up new possibilities for AI-assisted writing.
*   Rapid Adaptation with Fine-Tuning: Fine-tuning on task-specific data allows GPT to perform well in specialized contexts, such as technical writing, legal assistance, and customer service. Fine-tuning allows GPT models to be tailored to specific needs.
*   Scalability with Large Models: Larger GPT models demonstrate stronger performance and generalization, especially on complex or nuanced tasks. The performance of GPT models generally improves with scale.

<----------section---------->

## GPT limitations
*   Lack of True Understanding: GPT models generate text based on patterns rather than true comprehension. This is a fundamental limitation of current language models.
*   Sensitivity to Prompting: GPT's responses can vary widely based on phrasing. Small changes in prompts may yield different outcomes. Prompt engineering is crucial for eliciting desired responses.
*   Ethical and Bias Concerns: GPT can reproduce biases present in its training data, leading to biased or inappropriate outputs, especially if prompts or fine-tuning data lack diversity or sensitivity. Bias mitigation is an important area of research.
*   Inability to Reason or Perform Complex Calculations: GPT is limited in logical reasoning, advanced mathematics, or tasks requiring step-by-step problem-solving without explicit instruction in the prompt. These types of tasks often require more sophisticated AI techniques.
*   High Computational Requirements: Large GPT models require significant computational power for training, fine-tuning, and deployment, making them expensive to run and maintain. The computational cost is a barrier to wider adoption.
*   Limited Memory Across Interactions: GPT lacks persistent memory across sessions, so it cannot retain information shared by users in previous interactions without explicit prompting. This limits its ability to engage in extended conversations.
*   Vulnerability to Adversarial Prompts: Malicious prompts can manipulate GPT into producing undesirable or unintended responses. Adversarial attacks are a concern for security.

<----------section---------->

## Popular GPT variants - Codex
*   Codex is a GPT-3 model fine-tuned by OpenAI specifically for coding and programming tasks. Codex demonstrates the power of fine-tuning GPT for specific applications.
*   It powers GitHub Copilot and can assist with code generation, debugging, and explanations across multiple programming languages. GitHub Copilot is a practical application of Codex.
*   The key features are coding assistance, code generation, multi-language support for programming tasks. Codex simplifies coding by providing assistance, generating code, and supporting multiple languages.

<----------section---------->

## Popular GPT variants –MT-NLG
*   MT-NLG is the acronym for Megatron-Turing Natural Language Generation and was developed by NVIDIA and Microsoft. MT-NLG represents a collaborative effort to build large language models.
*   MT-NLG is one of the largest language models, with 530 billion parameters. The massive size of MT-NLG reflects the trend toward larger language models.
*   It aims to improve natural language understanding and generation in tasks like summarization, question answering, and robust few-shot learning. MT-NLG focuses on improving NLP capabilities in various tasks.

<----------section---------->

## Popular GPT variants –GLaM
*   GLaM is the acronym for Generalist Language Model, developed by Google Research. GLaM is an example of a language model developed by Google Research.
*   GLaM is a sparse mixture-of-experts model with 1.2 trillion parameters, but only a fraction are active per inference. This architecture allows GLaM to scale to a large number of parameters while maintaining efficiency.
*   It uses fewer resources than fully dense models like GPT-3 while achieving competitive performance across NLP tasks. GLaM offers a trade-off between performance and computational resources.

<----------section---------->

## Popular GPT variants –PanGu-α
*   PanGu-α is Huawei’s Chinese language model with 200 billion parameters, aimed at understanding and generating text in Mandarin. PanGu-α highlights the development of language models tailored for specific languages.
*   It was developed as part of Huawei’s effort to advance AI in Chinese NLP and has applications in Chinese literature, customer support, and translation. PanGu-α demonstrates the application of language models in Chinese NLP.
*   In general, it supports Chinese-specific language applications. It is tailored to the nuances and characteristics of the Chinese language.

<----------section---------->

## Popular GPT variants –Chinchilla
*   Chinchilla is a DeepMind model optimized for efficiency in training data and parameters. Chinchilla showcases DeepMind's research in efficient language model training.
*   It has a smaller number of parameters than GPT-3 but achieves similar or better performance, demonstrating an alternative approach to large-scale model training. Chinchilla demonstrates that performance doesn't always require massive parameter counts.
*   It is thus optimized for research and practical applications. It strikes a balance between performance and resource efficiency.

<----------section---------->

## Popular GPT variants –OPT
*   OPT is the acronym for Open Pretrained Transformer, developed by Meta (Facebook AI). OPT exemplifies the trend towards open-source language models.
*   OPT models are a series of open-source language models from Meta, comparable to GPT-3 in size and capabilities. OPT aims to provide accessible language models for research and development.
*   Meta released these models to support transparency in AI research, offering model weights and code for research and academic use. The open-source nature of OPT promotes collaboration and innovation.

<----------section---------->

## Popular GPT variants –BLOOM
*   BLOOM is the result of the BigScience collaborative project. BLOOM highlights the importance of collaborative research in AI.
*   It is an open-source multilingual model with 176 billion parameters, trained by a global consortium of researchers. BLOOM is a significant achievement in multilingual language modeling.
*   It supports 46 languages, including underrepresented languages, and is designed to make large language models accessible for diverse linguistic and cultural contexts. BLOOM promotes inclusivity by supporting a wide range of languages.
*   It enforces inclusivity in NLP research. It ensures that NLP research is representative and accessible to diverse linguistic and cultural communities.

<----------section---------->

## LLAMA
*   The LLaMA (Large Language Model Meta AI) architecture is a family of transformer-based language models developed by Meta. LLaMA models are designed to be efficient, high-performing, and optimized for a range of NLP tasks. LLaMA provides a suite of models with varying sizes to accommodate different resource constraints.
*   The LLaMA family includes several model sizes:
    *   LLaMA-7B: 32 decoder blocks with 32 attention heads for each block, 4096-dimensional embeddings
    *   LLaMA-13B: 40 decoder blocks with 40 attention heads for each block, 5120-dimensional embeddings
    *   LLaMA-30B: 60 decoder blocks with 40 attention heads for each block, 6656-dimensional embeddings
    *   LLaMA-65B: 80 decoder blocks with 64 attention heads for each block, 8192-dimensional embeddings
*   These models are designed to offer a range of capabilities depending on the computational resources available, from smaller more efficient models to larger models. This allows users to choose the model that best suits their needs and resources.

<----------section---------->

## LLAMA input encoding
*   LLaMA models use Byte-Pair Encoding (BPE) as their input encoding method, obtaining a dictionary of 32768 tokens. BPE is a common and effective tokenization technique.
*   However, LLaMA uses relative positional encodings instead of absolute positional encodings. This is a key architectural difference between LLaMA and some other transformer models.
*   This method allows the model to better handle varying sequence lengths and to generalize across different contexts, which is particularly useful for longer sequences. Relative positional encoding is beneficial for handling sequences of varying lengths.
*   In relative positional encoding, the model learns the relationships between tokens based on their relative positions rather than their absolute positions in the sequence. This allows the model to focus on the relationships between words rather than their specific positions.

<----------section---------->

## LLAMA pre-training
*   Like GPT models, LLaMA is pre-trained using an autoregressive language modeling objective. This means that the model learns to predict the next token in a sequence given the previous tokens. Autoregressive pre-training is a standard technique for training language models.
*   LLaMA is trained on “The Pile” (825 GB, from 300 to 1000 billion tokens), a wide range of publicly available text sources, including:
    *   Books (e.g., text from various domains like literature, non-fiction, etc.)
    *   Web Data (e.g., content from the internet, scraped from publicly accessible websites)
    *   Scientific Papers (e.g., research articles, preprints, and academic papers)
*   The dataset is designed to be as diverse as possible to ensure the model learns a broad understanding of language and can generalize well to a wide range of tasks. A diverse training dataset is crucial for the model's ability to generalize.

<----------section---------->

## LLAMA pre-training
*   The loss function used during pre-training is the cross-entropy loss between the predicted token probabilities and the actual next token in the sequence. This helps the model learn to predict the correct token given the context. Cross-entropy loss is a standard metric for training language models.
*   The model is optimized using Stochastic Gradient Descent (SGD) or Adam optimizer with gradient clipping to prevent instability during training. Gradient clipping is used to stabilize the training process.
*   Training also utilizes mixed precision to speed up computation and reduce memory requirements. Mixed precision training improves efficiency by using lower-precision floating-point numbers.
*   LLaMA employs techniques like learning rate schedules (e.g., linear warm-up followed by decay), weight decay, and batch normalization or layer normalization to stabilize and improve training. These techniques are commonly used to optimize the training process.

<----------section---------->

## LLAMA variants

| Model        | Parameters | Use Case                                                              | Strengths                                                                                                                                                 | Limitations                                                                                            |
|--------------|------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| LLaMA-7B     | 7 billion  | Resource-efficient tasks (e.g., small-scale NLP)                       | High efficiency, suitable for smaller environments                                                                                                        | May not achieve top performance on complex tasks                                                       |
| LLaMA-13B    | 13 billion | General-purpose NLP tasks, fine-tuning for specific applications      | Balanced performance and efficiency                                                                                                                         | May lack performance for more advanced tasks                                                           |
| LLaMA-30B    | 30 billion | Complex tasks (e.g., summarization, translation)                      | High performance on state-of-the-art NLP tasks                                                                                                            | Requires significant computational resources                                                             |
| LLaMA-65B    | 65 billion | High-end applications, advanced research                               | Top-tier NLP performance across multiple domains                                                                                                         | Extremely resource-intensive, challenging for deployment                                                 |

This table provides a comparative overview of the different LLaMA model sizes, their use cases, strengths, and limitations. The choice of model depends on the specific application and available resources.

<----------section---------->

## LLAMA vs GPT

| Aspect            | LLaMA                                                       | GPT                                                                 |
| ----------------- | ----------------------------------------------------------- | ------------------------------------------------------------------- |
| Size Range        | 7B, 13B, 30B, 65B                                           | 117M to 175B+ (GPT-3)                                               |
| Training Data     | Public data (The Pile, Wikipedia, Common Crawl, etc.)       | Public data (Common Crawl, WebText, etc.)                           |
| Performance       | Strong, competitive, especially for smaller models           | State-of-the-art, particularly in zero/few-shot                     |
| Training Efficiency | More efficient, parameter-efficient                         | Very resource-intensive, especially for GPT-3                       |
| Deployment        | Open-sourced, flexible deployment                         | Commercial API via OpenAI                                           |
| Ethics            | Strong ethical considerations                               | Criticism over transparency and biases                                |
| Applications      | Academic research, custom deployment                      | Broad commercial use, APIs, and applications                         |

This table compares LLaMA and GPT across several key aspects, including size range, training data, performance, training efficiency, deployment, ethics, and applications. LLaMA emphasizes efficiency and open-source availability, while GPT focuses on state-of-the-art performance and commercial applications.

<----------section---------->

## Practice on text generation
*   Looking at the Hugging Face guide on text generation [https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation), use various models to perform text generation (code, stories, and so on). Hugging Face provides a comprehensive resource for exploring text generation models.
*   Search for possible models for text generation in Hugging Face [https://huggingface.co/models?pipeline_tag=text-generation&sort=trending](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending). Hugging Face hosts a wide variety of text generation models that can be explored.
*   If you have time and computational resources, you can also fine tune one of the text generation models ([https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article](https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article)). Fine-tuning allows you to adapt a pre-trained model to a specific task.

<----------section---------->

# Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica - Lesson 15

## Encoder-Decoder Transformers
Nicola Capuano and Antonio Greco - DIEM, University of Salerno

### Outline
This lesson will cover the following topics related to Natural Language Processing (NLP) and Large Language Models (LLMs):

*   Encoder-decoder transformer architectures, which are a fundamental building block for many sequence-to-sequence tasks.
*   T5 (Text-to-Text Transfer Transformer), a specific and influential language model based on the encoder-decoder transformer.
*   Practical exercises involving translation and summarization using these models, giving hands-on experience with NLP tasks.

<----------section---------->

## Encoder-Decoder Transformer

### Encoder-Decoder Transformer
Encoder-Decoder Transformers are a class of neural networks specifically designed for sequence-to-sequence (seq2seq) tasks. These tasks involve converting an input sequence into a different output sequence. Examples include:

*   **Machine Translation:** Translating a sentence from one language to another.
*   **Text Summarization:** Condensing a long document into a shorter summary.
*   **Speech Recognition:** Converting an audio sequence into a text sequence.
*   **Text Generation:** Creating new text based on a given prompt or context.

The encoder part of the transformer processes the input sequence and transforms it into a fixed-length vector representation, also known as a context vector or thought vector. This vector aims to capture the essence of the input sequence. The decoder then takes this context vector and generates the output sequence, step-by-step. The transformer architecture uses attention mechanisms to focus on the most relevant parts of the input sequence during decoding, allowing for better handling of long sequences compared to previous recurrent neural network (RNN) based encoder-decoder models.

<----------section---------->

## T5

### T5
T5 (Text-to-Text Transfer Transformer) is a language model created by Google Research that relies on an encoder-decoder transformer architecture. A key innovation of T5 is that it frames all NLP tasks as text-to-text problems. This means that both the input and the output are always text strings, simplifying the model's design and making it highly versatile.

T5's versatility stems from its ability to handle various NLP tasks by simply changing the input prompt. For example, to perform translation, you would input "translate English to German: [English text]". For summarization, you would input "summarize: [long document]". This unified approach allows a single model to be trained on a diverse set of tasks, improving its generalization ability.

T5 is available in various sizes to accommodate different computational resource constraints:

| Version   | Encoder Blocks | Attention Heads | Decoder Blocks | Embedding Dimensionality | Parameters |
| --------- | -------------- | --------------- | -------------- | ------------------------ | ---------- |
| T5-Small  | 6              | 8               | 6              | 512                      | ~60 million |
| T5-Base   | 12             | 12              | 12             | 768                      | ~220 million |
| T5-Large  | 24             | 16              | 24             | 1024                     | ~770 million |
| T5-XL     | 24             | 32              | 24             | 2048                     | ~3 billion |
| T5-XXL    | 24             | 64              | 24              | 4096                     | ~11 billion |

The number of parameters gives a sense of the model's capacity to learn complex relationships in the data. Larger models typically achieve higher accuracy but require more memory and computation.

<----------section---------->

### T5 Input Encoding
T5 uses a SentencePiece tokenizer with a custom vocabulary for its input encoding. Tokenization is the process of breaking down the input text into smaller units (tokens) that the model can understand.

*   **Subword Units:** T5 employs a subword-based tokenizer using the SentencePiece library. Instead of splitting text into whole words, subword tokenization breaks words into smaller units like morphemes or common character sequences. This approach offers several advantages:

    *   It allows the model to handle rare words and out-of-vocabulary (OOV) words more effectively. If a word is not in the vocabulary, it can still be represented as a combination of subwords.
    *   It reduces the size of the vocabulary compared to word-based tokenization, improving computational efficiency.
    *   It can capture meaningful relationships between words that share similar subwords.
*   **Unigram Language Model:** The SentencePiece tokenizer in T5 is trained using a unigram language model. This model learns the probability of each subword occurring in the training data. During tokenization, SentencePiece selects the subwords that maximize the likelihood of the input text, effectively choosing the most probable segmentation.

<----------section---------->

### T5 Input Encoding
T5 uses a fixed vocabulary of 32,000 tokens. This vocabulary includes:

*   **Subwords:** The core units for representing words and handling OOV cases.
*   **Whole words:** Common words that are frequently encountered in the training data.
*   **Special tokens:** Tokens with specific functions, like padding and task specification.

The vocabulary size of 32,000 tokens represents a trade-off between computational efficiency and representational capacity. A smaller vocabulary leads to faster processing but might limit the model's ability to capture fine-grained distinctions in the input text. A larger vocabulary can capture more nuances but increases the model's memory footprint and computational cost.

T5 introduces several special tokens in the vocabulary to guide the model for various tasks:

*   `<pad>`: The padding token is used to ensure that all sequences in a batch have the same length. Shorter sequences are padded with this token to match the length of the longest sequence.
*   `<unk>`: The unknown token is used to represent words or subwords that are not in the vocabulary. When the tokenizer encounters an out-of-vocabulary term, it replaces it with `<unk>`.
*   `<eos>`: The end-of-sequence token marks the conclusion of an input or output sequence. This token is crucial for generative tasks, signaling to the decoder when to stop generating text.
*   `<sep>` and task-specific prefixes: T5 uses task-specific prefixes to indicate the type of task the model should perform. For example, "translate English to German:" tells the model to translate the following text from English to German. Similarly, "summarize:" indicates that the model should summarize the following document. These prefixes are added to the input sequence to guide the model's behavior. `<sep>` is used to separate different parts of the input when performing conditional generation.

<----------section---------->

### T5 Pre-training
For pre-training, T5 uses a denoising autoencoder objective called span-corruption. Pre-training is a crucial step in training large language models. It involves training the model on a massive dataset to learn general language patterns and contextual relationships. This allows the model to acquire a broad understanding of language before being fine-tuned for specific tasks.

The span-corruption objective involves masking spans of text (not just individual tokens) in the input sequence and training the model to predict those spans. This approach is more effective than masking individual tokens because it forces the model to learn long-range dependencies and contextual relationships between tokens.

*   **Input Corruption:** Random spans of text in the input are replaced with a special `<extra_id_X>` token (e.g., `<extra_id_0>`, `<extra_id_1>`). These tokens indicate the location of the masked spans.
*   **Original Input:** "The quick brown fox jumps over the lazy dog."
*   **Corrupted Input:** "The quick `<extra_id_0>` jumps `<extra_id_1>` dog."
*   **Target Output:** The model is trained to predict the original masked spans in sequential order.
*   **Target Output:** `<extra_id_0>` brown fox `<extra_id_1>` over the lazy.

This formulation forces the model to generate coherent text while learning contextual relationships between tokens. By predicting the masked spans, the model learns to reconstruct the original text from the corrupted input, effectively learning a robust representation of language.

<----------section---------->

### T5 Pre-training
Predicting spans, rather than individual tokens, encourages the model to learn:

*   **Global Context:** How spans relate to the larger sentence or paragraph structure. By predicting entire spans of text, the model is forced to consider the broader context of the input sequence. This helps it understand how different parts of the text relate to each other and contribute to the overall meaning.
*   **Fluency and Cohesion:** Span prediction ensures generated outputs are natural and coherent. The model learns to generate text that flows smoothly and logically, maintaining consistency and relevance throughout the output.
*   **Task Versatility:** The model is better prepared for downstream tasks like summarization, translation, and question answering. By pre-training on a diverse set of tasks, the model acquires a general understanding of language that can be easily adapted to new and unseen tasks.

<----------section---------->

### T5 Pre-training
T5 pre-training uses the C4 dataset (Colossal Clean Crawled Corpus), a massive dataset derived from Common Crawl.

*   **Size:** Approximately 750 GB of cleaned text. This large size allows the model to learn a wide range of language patterns and relationships.
*   **Cleaning:** Aggressive data cleaning is applied to remove spam, duplicate text, and low-quality content. This ensures that the model is trained on high-quality data, improving its performance and generalization ability.
*   **Versatility:** The dataset contains diverse text, helping the model generalize across domains. The wide range of topics and writing styles in the C4 dataset allows the model to learn robust representations of language that are applicable to a variety of tasks.

<----------section---------->

### T5 Pre-training
*   **Loss Function:** Cross-entropy loss is used for predicting masked spans. Cross-entropy loss measures the difference between the model's predicted probability distribution and the true distribution of the masked spans. Minimizing this loss encourages the model to generate accurate predictions.
*   **Optimizer:** T5 employs the Adafactor optimizer, which is memory-efficient and designed for large-scale training. Adafactor adapts the learning rate for each parameter individually, allowing for faster convergence and better performance.
*   **Learning Rate Scheduling:** The learning rate is adjusted using a warm-up phase followed by an inverse square root decay. The warm-up phase gradually increases the learning rate to prevent instability during the initial stages of training. The inverse square root decay then gradually reduces the learning rate as training progresses, helping the model converge to a good solution.

<----------section---------->

### T5 Fine-tuning
*   **Input and Output as Text:** Fine-tuning continues the paradigm where the input and output are always text strings, regardless of the task. This simplifies the fine-tuning process and allows the model to be easily adapted to new tasks.
*   **Example Tasks:**
    *   **Summarization:**
        *   Input: `summarize: <document>` → Output: `<summary>`
    *   **Translation:**
        *   Input: `translate English to French: <text>` → Output: `<translated_text>`
    *   **Question Answering:**
        *   Input: `question: <question> context: <context>` → Output: `<answer>`

The task-specific prefix tells the T5 model what the intended task is and guides the model to perform the appropriate transformation.

<----------section---------->

### Popular T5 Variants – mT5
mT5 (Multilingual T5) was developed to extend T5's capabilities to multiple languages. This is important because many NLP applications require handling text in multiple languages.

It was pre-trained on the multilingual Common Crawl dataset covering 101 languages. This dataset contains text in a wide variety of languages, allowing the model to learn cross-lingual relationships and transfer knowledge between languages.

*   **Key Features:**
    *   Maintains the text-to-text framework across different languages. This means that mT5 can handle tasks like translation and multilingual summarization without requiring language-specific modifications.
    *   No language-specific tokenization, since it uses SentencePiece with a shared vocabulary across languages. This shared vocabulary allows the model to process text in different languages using the same set of tokens, facilitating cross-lingual transfer.
    *   Demonstrates strong multilingual performance, including on cross-lingual tasks. mT5 achieves state-of-the-art results on a variety of multilingual benchmarks, demonstrating its ability to handle complex language tasks in multiple languages.
*   **Applications:**
    *   Translation, multilingual summarization, and cross-lingual question answering. mT5 can be used to translate text between any pair of the 101 languages it was trained on, summarize documents in multiple languages, and answer questions based on information in different languages.
*   **Limitations:**
    *   Larger model size due to the need to represent multiple languages in the vocabulary. Representing multiple languages requires a larger vocabulary and more parameters, increasing the model's memory footprint and computational cost.
    *   Performance can vary significantly across languages, favoring those with more representation in the training data. Languages with more data in the training set tend to have better performance than those with less data. This is a common issue in multilingual models, and researchers are actively working to address it.

<----------section---------->

### Popular T5 Variants – Flan-T5
Flan-T5 is a fine-tuned version of T5 with instruction-tuning on a diverse set of tasks. Instruction tuning is a training technique where the model is trained to follow instructions provided in the input.

*   **Key Features:**
    *   Designed to improve generalization by training on datasets formatted as instruction-response pairs. This allows the model to learn how to perform new tasks based on natural language instructions, even if it has not been explicitly trained on those tasks.
    *   Better zero-shot and few-shot learning capabilities compared to the original T5. Zero-shot learning refers to the ability of a model to perform a task without any training examples. Few-shot learning refers to the ability of a model to perform a task with only a few training examples. Flan-T5's instruction tuning improves its ability to generalize to new tasks with limited data.
*   **Applications:**
    *   Performs well in scenarios requiring generalization to unseen tasks, such as creative writing or complex reasoning. Flan-T5's instruction-following capabilities make it well-suited for tasks that require understanding and responding to complex instructions.
*   **Limitations:**
    *   Requires careful task formulation to fully utilize its instruction-following capabilities. The way a task is formulated as an instruction can significantly impact the model's performance. It is important to design instructions that are clear, concise, and unambiguous to achieve the best results.

<----------section---------->

### Popular T5 Variants – ByT5
ByT5 (Byte-Level T5) processes text at the byte level rather than using subword tokenization. This is a departure from traditional tokenization methods that rely on words or subwords.

*   **Key Features:**
    *   Avoids the need for tokenization, enabling better handling of noisy, misspelled, or rare words. By processing text at the byte level, ByT5 can handle any character or sequence of characters, regardless of whether it is in the vocabulary.
    *   Works well for languages with complex scripts or low-resource scenarios. Byte-level processing eliminates the need for language-specific tokenizers, making ByT5 well-suited for languages with complex scripts or limited resources.
*   **Applications:**
    *   Robust for tasks with noisy or unstructured text, such as OCR or user-generated content. ByT5's byte-level processing makes it more resilient to errors and inconsistencies in the input text.
*   **Limitations:**
    *   Significantly slower and more resource-intensive due to longer input sequences (byte-level representation increases sequence length). Processing text at the byte level results in much longer input sequences compared to word or subword tokenization. This increases the computational cost of training and inference.

<----------section---------->

### Popular T5 Variants – T5-3B and T5-11B
T5-3B and T5-11B are larger versions of the original T5 with 3 billion and 11 billion parameters, respectively. Increasing the size of a language model typically improves its performance on complex tasks.

*   **Key Features:**
    *   Improved performance on complex tasks due to increased model capacity. Larger models have more parameters, allowing them to learn more complex relationships in the data and achieve higher accuracy.
    *   Suitable for tasks requiring deep contextual understanding and large-scale reasoning. The increased capacity of these models makes them well-suited for tasks that require understanding long-range dependencies and complex reasoning.
*   **Applications:**
    *   Used in academic research and high-performance NLP applications where resources are not a constraint. These models are often used in research settings to push the boundaries of NLP performance. They can also be used in commercial applications where high accuracy is critical and computational resources are readily available.
*   **Limitations:**
    *   Computationally expensive for fine-tuning and inference. Fine-tuning and using these models require significant computational resources, making them impractical for many applications.
    *   Memory requirements limit their usability on standard hardware. The large memory footprint of these models limits their usability on standard hardware, requiring specialized hardware like GPUs or TPUs.

<----------section---------->

### Popular T5 Variants – UL2
UL2 (Unified Language Learning) is a general-purpose language model inspired by T5 but supports a wider range of pretraining objectives. This allows the model to learn more robust and versatile representations of language.

*   **Key Features:**
    *   Combines diverse learning paradigms: unidirectional, bidirectional, and sequence-to-sequence objectives. Unidirectional learning involves predicting the next token in a sequence, bidirectional learning involves predicting masked tokens in a sequence, and sequence-to-sequence learning involves converting an input sequence into a different output sequence. By combining these different learning paradigms, UL2 can learn more comprehensive representations of language.
    *   Offers state-of-the-art performance across a variety of benchmarks. UL2 achieves competitive results on a variety of NLP benchmarks, demonstrating its versatility and effectiveness.
*   **Applications:**
    *   General-purpose NLP tasks, including generation and comprehension. UL2 can be used for a wide range of NLP tasks, including text generation, text classification, and question answering.
*   **Limitations:**
    *   Increased complexity due to multiple pretraining objectives. Combining multiple pretraining objectives increases the complexity of the training process, requiring more careful tuning and optimization.

<----------section---------->

### Popular T5 Variants – Multimodal T5
T5-Large Multimodal Variants combine T5 with vision capabilities by integrating additional modules for visual data. This allows the model to process both text and images, enabling a wider range of applications.

*   **Key Features:**
    *   Processes both text and image inputs, enabling tasks like image captioning, visual question answering, and multimodal translation. Image captioning involves generating a text description of an image, visual question answering involves answering questions about an image, and multimodal translation involves translating text from one language to another while also considering the visual context.
    *   Often uses adapters or encodes visual features separately. Adapters are small, task-specific modules that are added to the model to adapt it to new tasks. Encoding visual features separately involves processing the image data with a separate module and then combining the resulting features with the text data.
*   **Applications:**
    *   Multimodal tasks combining vision and language. This includes tasks such as image captioning, visual question answering, and multimodal translation.
*   **Limitations:**
    *   Computationally expensive due to the need to process multiple modalities. Processing both text and images requires more computational resources than processing text alone.

<----------section---------->

### Popular T5 Variants – Efficient T5
Efficient T5 Variants are optimized for efficiency in resource-constrained environments. This is important for deploying NLP models on devices with limited memory and computational power.

*   **Examples:**
    *   T5-Small/Tiny: Reduced parameter versions of T5 for lower memory and compute needs. These models have fewer parameters than the original T5, making them more efficient but potentially sacrificing some accuracy.
    *   DistilT5: A distilled version of T5, reducing the model size while retaining performance. Distillation is a technique where a smaller model is trained to mimic the behavior of a larger model, effectively transferring the knowledge from the larger model to the smaller model.
*   **Applications:**
    *   Real-time applications on edge devices or scenarios with limited computational resources. These models are well-suited for applications that require fast processing and low memory consumption, such as real-time translation on mobile devices.
*   **Limitations:**
    *   Sacrifices some performance compared to larger T5 models. Reducing the size of a model typically results in a trade-off between efficiency and accuracy.

<----------section---------->

### T5 Variants

| Variant        | Purpose                    | Key Strengths                                | Limitations                                  |
| -------------- | -------------------------- | -------------------------------------------- | --------------------------------------------- |
| mT5            | Multilingual NLP           | Supports 101 languages                        | Uneven performance across languages           |
| Flan-T5        | Instruction-following        | Strong generalization                         | Needs task-specific prompts                    |
| ByT5           | No tokenization            | Handles noisy/unstructured text               | Slower due to byte-level inputs               |
| T5-3B/11B      | High-capacity NLP          | Exceptional performance                      | High resource requirements                     |
| UL2            | Unified objectives         | Versatility across tasks                      | Increased training complexity                  |
| Multimodal T5  | Vision-language tasks      | Combines text and image inputs               | Higher computational cost                      |
| Efficient T5   | Resource-constrained NLP   | Lightweight, faster inference                 | Reduced task performance                       |

This table summarizes the various T5 variants discussed and their respective strengths and limitations, aiding in selecting the appropriate model based on specific application requirements.

<----------section---------->

## Practice on Translation and Summarization

### Practice
The best way to understand and master these models is to put them into practice. This section guides you to relevant resources and encourages hands-on experimentation.

Looking at the Hugging Face guides on translation ([https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)) and summarization ([https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt)), use various models to perform these tasks.

The Hugging Face guides provide practical examples and code snippets for using pre-trained models to perform translation and summarization. Experiment with different T5 variants and other encoder-decoder models to compare their performance on these tasks.

By following the guides, if you have time and computational resources you can also fine tune one of the encoder-decoder models.

Fine-tuning involves training a pre-trained model on a smaller, task-specific dataset to improve its performance on that particular task. This can significantly improve the model's accuracy and relevance. Fine-tuning requires computational resources and a well-prepared dataset, but it can be a worthwhile investment for achieving state-of-the-art results.

<----------section---------->

**Original Title:** Natural Language Processing and Large Language Models. Corso di Laurea Magistrale in Ingegneria Informatica. Lesson 16. Final Project. Nicola Capuano and Antonio Greco. DIEM – University of Salerno.

**Enhanced Title:** Natural Language Processing (NLP) and Large Language Models (LLM) - Final Project Overview for the Master's Degree Course in Computer Engineering. Presented by Nicola Capuano and Antonio Greco, DIEM – University of Salerno. (Lesson 16)

Outline:
* Goal of the project.
* Tools to use for the project.
* Chatbot evaluation procedure.

<----------section---------->

**Original Text:** Goal of the project: The goal of the project is to create a chatbot that answers questions about the NLP and LLM 2024/2025 course. The questions can be not only related to the topics covered in the course but can also concern more general information, such as the course teachers, recommended books, etc. Furthermore, the chatbot must answer ONLY questions related to the course, recognizing out-of-context questions and replying that it is not enabled to provide answers on topics outside the context of interest. The groups will deliver the code of the chatbot and a report in which they describe in detail their solution.

**Enhanced Text:** Goal of the Project:
The primary objective of this final project is to design and implement a chatbot capable of answering questions pertaining to the Natural Language Processing (NLP) and Large Language Models (LLM) course for the academic year 2024/2025. This includes addressing queries related to specific topics covered within the course curriculum, as well as providing more general information about the course structure. This information may include details about the instructors, required or recommended reading materials, and any other relevant administrative information. A crucial requirement for the chatbot is its ability to distinguish between in-context and out-of-context questions. The chatbot should be programmed to exclusively answer questions that fall within the scope of the course content. When confronted with questions unrelated to the course, such as those concerning general knowledge or unrelated subject matter, the chatbot must respond appropriately, indicating that it cannot provide information on topics outside the course's domain. Student groups are expected to submit both the source code of the chatbot and a comprehensive report. The report should detail the approach taken, the architecture of the chatbot, the rationale behind design choices, and any challenges encountered during development, along with their respective solutions. The report should allow the instructors to understand the design decisions.

<----------section---------->

**Original Text:** Tools to use for the project: Students are free to use any tool or technology analyzed in the course, both the more recent ones associated with LLMs and the more classic ones of NLP. To achieve the required result, the students can develop parts of the chatbot with LLMs and other parts with more classic tools, with total freedom of choice as long as the various solutions identified are justified in the final report. Any LLM or other already available model can be used with or without modifications, as long as the groups are fully familiar with the tools they used and are able to answer questions about every aspect of the code and models.

**Enhanced Text:** Tools and Technologies:
Students are granted the flexibility to employ any tool or technology that has been discussed and analyzed during the course. This encompasses both contemporary tools associated with Large Language Models (LLMs) and more traditional Natural Language Processing (NLP) techniques. To effectively achieve the project's goal, students may choose to develop different components of the chatbot using a combination of LLMs and classic NLP tools. They have full autonomy in their selection of tools and methodologies, provided that all chosen solutions are thoroughly justified and explained in the final project report. Furthermore, students have the option to utilize pre-existing LLMs or other models, whether or not these are modified. However, it is imperative that student groups demonstrate a complete understanding of the tools they utilize. They should be prepared to answer questions from the instructors regarding all aspects of the code, underlying models, and implementation details. This includes being able to explain the model's architecture, the purpose of each code module, and how the different components of the chatbot interact with one another.

<----------section---------->

**Original Text:** Chatbot evaluation procedure: Before the project discussion, the course instructors will question the chatbot of the group in real-time with a pre-defined set of questions on the course and evaluate the answers from the following points of view:

* Relevance: Judges if generated text answers the query.
* Fluency: Evaluates the readability and grammar of the output.
* Coherence: Checks logical flow and consistency.

Then, the course instructors will do another set of pre-defined questions to verify:

* Robustness: Evaluates resistance to adversarial misleading prompts (e.g. are you sure?).
* Precision: Evaluates the capability to recognize out-of-context questions (e.g. who is the king of Spain?)

The course instructors will give a grade to the performance according to these aspects.

**Enhanced Text:** Chatbot Evaluation Procedure:
Prior to the formal project discussion and grading, the course instructors will conduct a real-time evaluation of each group's chatbot. This evaluation will involve querying the chatbot with a predefined set of questions relevant to the NLP and LLM course content. The chatbot's responses will be assessed from several key perspectives to determine its overall performance:

*   **Relevance:** This criterion assesses the degree to which the chatbot's generated text accurately and directly addresses the question or query posed. The response should provide pertinent information related to the specific question asked.

*   **Fluency:** Fluency refers to the readability and grammatical correctness of the chatbot's output. The generated text should be easy to understand, free of grammatical errors, and should exhibit a natural writing style.

*   **Coherence:** This aspect examines the logical flow and consistency of the chatbot's responses. The output should be internally consistent, with a clear and understandable structure, and it should maintain a logical progression of ideas.

Following the initial assessment, the instructors will pose a second set of predefined questions designed to specifically evaluate the chatbot's:

*   **Robustness:** Robustness measures the chatbot's ability to withstand adversarial or misleading prompts. This includes assessing its resilience to prompts designed to confuse or trick it (e.g., questions containing phrases like "are you sure?" or other attempts to manipulate the chatbot's responses).

*   **Precision:** Precision assesses the chatbot's capability to accurately identify and appropriately respond to out-of-context questions. This involves verifying its ability to recognize questions that fall outside the scope of the course content (e.g., "Who is the King of Spain?") and to respond with a pre-programmed message indicating its inability to answer such queries.

The course instructors will assign a grade to each project group based on the chatbot's performance across all the above-mentioned aspects. The overall grade will reflect the chatbot's effectiveness, accuracy, and ability to meet the specified requirements.

<----------section---------->

# Natural Language Processing and Large Language Models

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)

Lesson 17: Fine Tuning

Nicola Capuano and Antonio Greco

DIEM – University of Salerno

This lesson will cover fine-tuning techniques for Large Language Models (LLMs), focusing on adapting pre-trained models to specific tasks. We will explore various methods, including full fine-tuning, parameter-efficient fine-tuning (PEFT), and instruction fine-tuning. These methods allow us to leverage the power of LLMs while addressing the challenges of computational cost and overfitting.

<----------section---------->

## Outline

*   Types of fine tuning
*   Parameter Efficient Fine Tuning (PEFT)
*   Instruction Fine-Tuning

This outline presents the key topics that will be discussed in this lesson on fine-tuning LLMs. First, we will explore the different types of fine-tuning approaches, including traditional full fine-tuning and more recent parameter-efficient techniques. Then, we will focus on Parameter-Efficient Fine-Tuning (PEFT), which allows efficient adaptation of LLMs to specific tasks. Finally, we will delve into Instruction Fine-Tuning, which enhances the ability of models to follow user instructions.

<----------section---------->

## Types of fine tuning

Fine-tuning refers to the process of taking a pre-trained Large Language Model (LLM) and further training it on a specific task using a task-specific dataset. This adaptation allows the model to specialize its knowledge and improve performance on the target task. Pre-trained LLMs have already learned general language patterns and relationships from vast amounts of data, and fine-tuning leverages this existing knowledge to achieve better results with less training data and computational resources than training a model from scratch.

**Why Fine-Tune?**

*   Specialize LLMs for domain-specific tasks: Fine-tuning allows you to adapt a general-purpose LLM to a specific domain (e.g., medical, legal, financial). This specialization improves the model's understanding of domain-specific terminology and nuances, leading to more accurate and relevant results.
*   Improve accuracy and relevance for specific applications: By training on a dataset relevant to a specific application, fine-tuning can significantly boost the accuracy and relevance of the LLM's outputs. For example, a sentiment analysis model fine-tuned on customer reviews will likely perform better than a general-purpose sentiment analysis model.
*   Optimize performance on small, focused datasets: Fine-tuning is particularly useful when you have a limited amount of task-specific data. The pre-trained LLM provides a strong foundation, and fine-tuning allows you to adapt it to your specific dataset without requiring a massive amount of training data.
<----------section---------->

### Pros and cons of full fine tuning

Full Fine-Tuning involves updating all the parameters of a pre-trained LLM during the fine-tuning process. This approach allows the model to fully leverage its capacity to achieve high accuracy on specific tasks.

**Pros:**

*   High Accuracy: Full fine-tuning can potentially achieve the highest accuracy for a specific task because it allows the model to fully adapt its parameters to the task-specific data.

**Cons:**

*   Computational Expense: Updating all model parameters is computationally expensive, especially for large LLMs. It requires significant GPU resources and time.
*   Risk of Overfitting: Full fine-tuning can lead to overfitting, especially when the task-specific dataset is small. Overfitting occurs when the model learns the training data too well, resulting in poor performance on unseen data.

<----------section---------->

### Other types of fine tuning

Besides full fine-tuning, several other approaches aim to address its limitations and provide more efficient ways to adapt LLMs to specific tasks.

*   Parameter-Efficient Fine-Tuning (PEFT): Updates only a subset of the parameters.
    *   Examples: LoRA, Adapters.
*   Instruction Fine-Tuning: Used to align models with task instructions or prompts (user queries) enhancing usability in real-world applications.
*   Reinforcement Learning from Human Feedback (RLHF): Combines supervised learning with reinforcement learning, rewarding models when they generate user-aligned outputs.

These alternative techniques offer various trade-offs between computational cost, memory usage, and performance. PEFT methods reduce the number of trainable parameters, while instruction fine-tuning and RLHF focus on aligning the model's behavior with user expectations and preferences.

<----------section---------->

## Parameter efficient fine tuning (PEFT)

Parameter-Efficient Fine-Tuning (PEFT) is a collection of techniques designed to fine-tune large pre-trained models, such as LLMs, in a computationally efficient manner. These methods aim to achieve comparable performance to full fine-tuning while requiring significantly fewer trainable parameters. This is particularly important for LLMs, which often have billions or even trillions of parameters, making full fine-tuning computationally prohibitive.

### Parameter-Efficient Fine-Tuning

Parameter-Efficient Fine-Tuning (PEFT) is a strategy developed to fine-tune large-scale pre-trained models, such as LLMs, in a computationally efficient manner while requiring fewer learnable parameters compared to standard fine-tuning methods.
PEFT methods are especially important in the context of LLMs due to their massive size, which makes full fine-tuning computationally expensive and storage-intensive.
PEFT is ideal for resource-constrained settings like edge devices or applications with frequent model updates.
These techniques are supported and implemented in Hugging Face transformers and, in particular, in the `peft` library.

PEFT addresses the challenges of fine-tuning massive LLMs by introducing techniques that selectively update only a small subset of the model's parameters. This dramatically reduces the computational cost and memory footprint of fine-tuning, making it feasible to adapt LLMs to various tasks even with limited resources.

<----------section---------->

### PEFT techniques

Several PEFT techniques have emerged, each with its own approach to reducing the number of trainable parameters. Here are a few prominent examples:

*   Low-Rank Adaptation (LoRA): Approximates weight updates by learning low-rank matrices, performing a small parameterized update of the weight matrices in the LLM. It is highly parameter-efficient and widely adopted for adapting LLMs.
*   Adapters: They are small and trainable modules inserted within the transformer layers of the LLM, that allow to keep the pre-trained model's original weights frozen.
*   Prefix Tuning: Learns a set of continuous task-specific prefix vectors for attention layers, keeping the original model parameters frozen.

These techniques share the goal of reducing the computational burden of fine-tuning by selectively updating only a fraction of the model's parameters. This enables efficient adaptation of LLMs to specific tasks while preserving the pre-trained knowledge embedded in the original model.

<----------section---------->

## Low-Rank Adaptation (LoRA)

LoRA focuses on modifying the weight matrices of the pre-trained model by adding a low-rank decomposition. This approach assumes that the changes required to adapt the model for a new task lie within a low-dimensional subspace.

*   LoRA assumes that the changes required to adapt a pre-trained model for a new task lie in a low-dimensional subspace.
*   Instead of fine-tuning all the parameters of the model, LoRA modifies only a small, trainable set of low-rank matrices that approximate these task-specific changes.
    1.  **Base Model:** A pre-trained transformer model is represented by its weight matrices W.
    2.  **Low-Rank Decomposition:** Instead of directly modifying W, LoRA decomposes the weight update into two low-rank matrices:

        ΔW = A × B

        *   A is a low-rank matrix (m × r)
        *   B is another low-rank matrix (r × n)
        *   r is the rank, which is much smaller than m or n, making A and B parameter-efficient.
    3.  **Weight Update:** The effective weight during fine-tuning becomes:

        W' = W + ΔW = W + A × B

LoRA introduces minimal overhead by only training the low-rank matrices A and B, while the original weight matrices (W) remain frozen. This significantly reduces the number of trainable parameters and the risk of overfitting.

<----------section---------->

### How LoRA works

*   **Freezing Pre-Trained Weights:** LoRA keeps the original weight matrices W of the LLM frozen during fine-tuning. Only the parameters in A and B are optimized for the new task (the pre-trained knowledge is preserved).
*   **Injecting Task-Specific Knowledge:** The low-rank decomposition A × B introduces minimal additional parameters (less than 1% of the original model parameters) while allowing the model to learn task-specific representations.
*   **Efficiency:** The number of trainable parameters is proportional to r × (m + n), which is significantly smaller than the full m × n weight matrix.
*   **Inference Compatibility:** During inference, the modified weights W' = W + A × B can be directly used, making LoRA-compatible models efficient for deployment.

The original weights of the LLM are frozen which allows the model to retain general-purpose knowledge learned during pre-training. The efficiency is a result of the low-rank matrices. During inference, the modified weights can be directly used without needing to calculate the low-rank matrices.

<----------section---------->

## Adapters

Adapters are small, task-specific neural modules inserted within the transformer layers of the LLM. These modules are trainable, while the original pre-trained model parameters remain frozen during fine-tuning.

*   Adapters are lightweight, task-specific neural modules inserted between the layers of a pre-trained transformer block.
*   These modules are trainable, while the original pre-trained model parameters remain frozen during fine-tuning.
*   Adapters require training only the small fully connected layers, resulting in significantly fewer parameters compared to full fine-tuning.
*   Since the base model remains frozen, the general-purpose knowledge learned during pre-training is preserved.

The adapter modules typically consist of a few fully connected layers with a bottleneck architecture, which further reduces the number of trainable parameters. By strategically inserting these small modules within the transformer layers, the model can learn task-specific representations without modifying the core knowledge embedded in the pre-trained weights.

<----------section---------->

## Prefix tuning

Prefix tuning offers an alternative to modifying the LLM's internal weights. Instead, it introduces and optimizes a sequence of trainable "prefix" tokens prepended to the input.

*   Instead of modifying the LLM's internal weights, prefix tuning introduces and optimizes a sequence of trainable "prefix" tokens prepended to the input.
*   These prefixes guide the LLM's attention and output generation, enabling task-specific adaptations with minimal computational and storage overhead.
*   The input sequence is augmented with a sequence of prefix embeddings:

    Modified Input: \[Prefix] + \[Input Tokens]

    *   **Prefix:** A sequence of *m* trainable vectors of size *d*, where *d* is the model's embedding dimensionality.
    *   **Input Tokens:** The original token embeddings from the input sequence.
*   Prefix embeddings influence attention by being prepended to the keys (K) and values (V), conditioning how the model attends to the input tokens.
*   Only the prefix embeddings are optimized during fine-tuning. Backpropagation updates the prefix parameters to align the model's outputs with task-specific requirements.
*   *m* controls the trade-off between task-specific expressiveness and parameter efficiency. Longer prefixes can model more complex task-specific conditioning but may increase memory usage.

By optimizing these prefix embeddings, the model can adapt its behavior to the specific task without modifying the original LLM's parameters. This approach is particularly parameter-efficient, as the number of trainable parameters is determined by the length of the prefix sequence and the embedding dimensionality of the model.

<----------section---------->

## Instruction fine tuning

Instruction fine-tuning is a specialized approach for adapting large language models (LLMs) to better understand and respond to user instructions. It focuses on aligning the model's behavior with human expectations and natural language queries.

*   Instruction fine-tuning is a specialized approach for adapting large language models (LLMs) to better understand and respond to user instructions.
*   This fine-tuning process involves training the model on a curated dataset of task-specific prompts paired with corresponding outputs.
*   The objective is to improve the model's ability to generalize across a wide variety of instructions, enhancing its usability and accuracy in real-world applications.
*   By training on human-like instructions, the model becomes more aligned with user expectations and natural language queries.

This is achieved by training the model on a carefully constructed dataset of task-specific prompts paired with corresponding outputs.

<----------section---------->

### How instruction fine tuning works

*   A diverse set of instructions and outputs is compiled. Each example consists of:
    *   **Instruction:** A clear, human-readable prompt (e.g., "Summarize the following text in one sentence").
    *   **Context (optional):** Background information or data required to complete the task.
    *   **Output:** The expected response to the instruction.
*   The LLM, pre-trained on a general corpus, is fine-tuned using the instruction-response pairs. During training, the model learns to:
    *   Recognize the intent of the instruction.
    *   Generate outputs that are coherent, accurate, and contextually appropriate.
*   The dataset may include examples from various domains and task types. This diversity ensures the model can generalize beyond the specific examples it has seen during fine-tuning.

The goal is for the LLM to learn to follow a broad range of human instructions. The added context provides the LLM background to appropriately respond. The diversity in the training ensures the model can perform a number of tasks.

In addition to the concepts explored in the main text, consider the following information:

**Data Augmentation**
In the context of NLP, data augmentation is even trickier than in image processing due to the lack of direct mathematical equivalents to affine transformations that work so effectively for images. However, various NLP augmentation approaches have been developed that are effective in practice, such as:

1. Appending word salad (random words) to the beginning and/or end of labeled examples.
2. Synonym substitution.
3. Introducing common typographical errors and misspellings.
4. Case folding to imitate the informal lowercase style of text messages.

<----------section---------->

### Natural Language Processing and Large Language Models: Prompt Engineering

This material is from Lesson 18 of the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree Course in Computer Engineering) at the DIEM (Department of Information Engineering and Mathematics) – University of Salerno. The lesson focuses on Prompt Engineering and is presented by Nicola Capuano and Antonio Greco.

### Outline

The lesson covers the following key areas:

*   Introduction to Prompt Engineering
*   Prompt Engineering Techniques
*   Prompt Testing

<----------section---------->

### Introduction to Prompt Engineering

**Prompt Engineering:** This relatively new field concentrates on the development and optimization of prompts. These prompts are designed to effectively utilize Large Language Models (LLMs) for a wide range of applications and research endeavors. Prompt engineering is vital because the quality and structure of a prompt significantly impact the LLM's output. A well-crafted prompt can elicit accurate, relevant, and creative responses, while a poorly designed prompt may lead to irrelevant or nonsensical results.

**Goals:**

*   **Enhance Understanding of LLM Capabilities and Limitations:** To gain insights into what LLMs can and cannot do. Understanding the boundaries helps in setting realistic expectations and designing appropriate applications. For example, knowing that an LLM struggles with complex arithmetic without specific prompting allows engineers to incorporate techniques to improve its performance in those areas.
*   **Improve LLM Performance on a Broad Range of Tasks:** To optimize LLM performance across various tasks such as question answering, arithmetic reasoning, and more. By using effective prompt engineering, the performance in these different tasks can be improved.
*   **Helps Interfacing with LLMs and Integrating with Other Tools:** To facilitate easier interaction with LLMs and seamless integration with other software tools and systems. This is important for creating comprehensive solutions where LLMs are a component.
*   **Enables New Capabilities:** To unlock new functionalities, such as enriching LLMs with specific domain knowledge and external resources. For instance, prompt engineering can enable an LLM to access and use a specialized database to provide more informed responses in a particular field.

<----------section---------->

### Writing Good Prompts

Creating effective prompts involves careful consideration and iterative refinement. Here are some guidelines:

*   **Start Simple and Iterate:** Begin with basic prompts and gradually add elements, refining the prompt based on the results. This iterative approach allows for a systematic improvement of the prompt's effectiveness.
*   **Use Clear, Specific Instructions:** Begin prompts with clear and direct commands such as "Write," "Classify," or "Summarize." This orients the LLM and sets the desired action.
*   **Be Detailed and Descriptive:** Provide sufficient details to guide the LLM towards the desired output. The more descriptive the prompt, the better the LLM can understand the context and requirements.
*   **Use Examples:** Include examples to illustrate the expected output format and content. Examples are powerful in-context learning tools that demonstrate the desired pattern.
*   **Balance Detail and Length:** While detail is crucial, avoid excessive information that can reduce effectiveness. Experiment to find the ideal format; shorter prompts can sometimes be more effective than longer ones if they are precise.

<----------section---------->

### Writing Good Prompts - Examples

Here are some examples contrasting ineffective ("Bad") and effective ("Good") prompts to highlight the importance of specificity:

*   **Bad Prompt:** "Summarize this article."
    *   **Good Prompt:** "Generate a 100-word summary of this research article, focusing on the main findings."
*   **Bad Prompt:** "Write an apology email to a client."
    *   **Good Prompt:** "Write a professional email to a client apologizing for a delayed shipment, offering a discount, and providing an updated delivery estimate."
*   **Bad Prompt:** "Make this explanation easier to understand."
    *   **Good Prompt:** "Rewrite this technical explanation in simpler language suitable for high school students."
*   **Bad Prompt:** "Classify the following review."
    *   **Good Prompt:** "Classify the following review as positive, neutral, or negative."
*   **Bad Prompt:** "Tell me about exercise benefits."
    *   **Good Prompt:** "List five health benefits of regular exercise, each with a short explanation of how it improves well-being."
*   **Bad Prompt:** "Translate this sentence to French."
    *   **Good Prompt:** "Translate the following English sentence into French, preserving the formal tone."

<----------section---------->

### Elements of a Prompt

A well-structured prompt typically contains these elements:

*   **Instruction:** The specific task to be performed. This is the command that tells the LLM what to do.
*   **Context:** Additional information that helps the LLM generate better responses. This includes background details or any relevant data that steers the model.
*   **Input Data:** The information that the LLM needs to process. This is the question, text, or data for which a response is required.
*   **Output Indicator:** The desired type or format of the output. This guides the LLM on how to structure its response.

**Example 1:**

```
Classify the text into neutral, negative or positive.
Text: I think the vacation is okay.
Sentiment: [Output Indicator]
```

*   Instruction: Classify the text...
*   Input: I think the vacation is okay
*   Output Indicator: Sentiment

**Example 2:**

```
Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure about answer" if not sure about the answer.

Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.

Question: What was OKT3 originally sourced from?
Answer: [Output Indicator]
```

*   Instruction: Answer the question...
*   Context: Teplizumab traces its roots...
*   Input: What was OKT3 originally sourced from?
*   Output Indicator: Answer

<----------section---------->

### In-Context Learning

In-context learning refers to the ability of an LLM to perform a task by interpreting and leveraging the information provided within the prompt itself, without updating its internal parameters (i.e., without undergoing further training). This is a crucial aspect of prompt engineering, as it allows practitioners to guide the LLM's behavior through carefully designed prompts.

A prompt context can include:

*   **Reference Material:** Providing specific text or data that the LLM should use to perform the task. This is often used in question answering or summarization tasks.
*   **Input-Output Pairs:** Including examples of the task to demonstrate the desired pattern or behavior. This is a key component of few-shot learning.
*   **Step-by-Step Instructions:** Offering detailed guidance for completing the task. This is useful for complex reasoning tasks.
*   **Clarifications:** Addressing any potential ambiguities in the task to ensure the LLM understands what is expected.
*   **Templates:** Supplying structured formats or placeholders that the LLM should fill in. This can be useful for generating specific types of content, such as reports or emails.

Prompt engineering heavily relies on in-context learning to guide and control the LLM's responses.

<----------section---------->

### Prompts and NLP Tasks

Prompts can be designed to achieve various Natural Language Processing (NLP) tasks. Here are some examples:

*   **Text Summarization:**
    ```
    Antibiotics are a type of medication used to treat bacterial infections. They work by either killing
    the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the
    infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions,
    or sometimes administered intravenously. They are not effective against viral infections, and using
    them inappropriately can lead to antibiotic resistance.

    Explain the above in one sentence:
    ```

    **Output:**

    ```
    Antibiotics are medications used to treat bacterial infections by either killing the bacteria or
    stopping them from reproducing, but they are not effective against viruses and overuse can lead to
    antibiotic resistance.
    ```

*   **Information Extraction:**
    ```
    Author-contribution statements and acknowledgements in research papers should state clearly and
    specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the
    preparation of their manuscript and analysis. They should also indicate which LLMs were used. This
    will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases,
    inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about
    their use of LLMs, for example when selecting submitted manuscripts.

    Mention the large language model based product mentioned in the paragraph above:
    ```

    **Output:**

    ```
    The large language model based product mentioned in the paragraph above is ChatGPT.
    ```

*   **Question Answering:**
    ```
    Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure
    about answer" if not sure about the answer.

    Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There,
    scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the
    molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986,
    it was approved to help prevent organ rejection after kidney transplants, making it the first
    therapeutic antibody allowed for human use.

    Question: What was OKT3 originally sourced from?
    ```

    **Output:**

    ```
    Mice
    ```

*   **Text Classification:**
    ```
    Classify the text into neutral, negative or positive.
    Text: I think the food was okay.
    Sentiment:
    ```

    **Output:**

    ```
    Neutral
    ```

*   **Code Generation:**
    ```
    Write a JavaScript code that prompts the user to enter their name and then displays
    a “Hello” message.
    Output only the code, no additional comments.
    ```

    **Output:**

    ```javascript
    let name = prompt("What is your name?");
    console.log(`Hello, ${name}!`);
    ```

*   **Reasoning:**
    ```
    What is 9,000 * 9,000?
    ```

    **Output:**

    ```
    81,000,000
    ```
    Note: despite advancements in mathematical capabilities, reasoning tasks remain challenging for LLMs...

*   **Reasoning (Example of Failure):**
    ```
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
    Answer:
    ```

    **Output:**

    ```
    No, the odd numbers in this group add up to an odd number: 119.
    ```
    That's incorrect! More advanced prompt engineering techniques are needed to obtain the correct answer... we will discuss them later.

<----------section---------->

### System Prompts

A system prompt is a configuration that is provided to the AI model before any user interactions take place. It serves as the foundation for shaping the AI's behavior and responses.

*   **Establishes Behavior, Context, Tone, and Instructions:** The system prompt sets the stage for how the AI should act, the context it should operate in, the tone it should adopt, and any special instructions it should follow.
*   **Guides Responses and Focus:** It guides the model on how to respond to user input and what aspects it should prioritize.

**Examples:**

*   "You are a helpful and knowledgeable assistant who answers questions accurately and concisely."
*   "You are an IT support assistant specializing in troubleshooting software and hardware issues. Respond politely and guide users through step-by-step solutions."
*   "You are a friendly and engaging AI who responds in a warm and conversational tone, keeping responses lighthearted and approachable."

<----------section---------->
### Prompt Engineering Techniques
you can use several prompt techniques to improve de capabilities of a llm's without changes the weights of the model.
    - Zero-shot Prompting
    - Few-shot prompting
    - chain of though prompting
    - self-consinstency prompting
    - meta prompting
    - task-agnostic meta prompting
    - meta-meta prompting
    - prompt chaining
    - role prompting
    - stuctured prompting
    - generate knowledge prompting
    
<----------section---------->
### Prompt Engineering Techniques

*   **Zero-Shot Prompting:**
    *   Definition: A prompt that directly instructs the model without including any examples or demonstrations.
    *   Principle: Leverages the LLM's pre-existing knowledge and capabilities gained from large-scale training to perform tasks without explicit examples.
    *   **Example:**
        ```
        Classify the text into neutral, negative or positive.
        Text: I think the vacation is okay.
        Sentiment:
        ```

        **Output:**
        ```
        Neutral
        ```
        The LLM already understands the concept of “sentiment” (that's the zero-shot capabilities at work).

<----------section---------->
### Prompt Engineering Techniques

*   **Few-Shot Prompting:**
    *   Definition: Involves including examples or demonstrations within the prompt to guide the model towards better performance.
    *   Principle: Leverages in-context learning, where the LLM learns from the provided examples to generate more accurate responses.
    *   **Example:**
        ```
        A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa, and we saw these very cute whatpus.
        To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is: When we won the game, we all started to farduddle in celebration.
        ```

    *   **Limitations:** Effective for many tasks but may struggle with complex reasoning.

    *   **Zero-Shot Example:**
        ```
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        Answer:
        ```

        **Output:**
        ```
        Yes, the odd numbers in this group add up to 107, which is an even number.
        ```
        False! Let's try with few-shot prompting.

    *   **Few-Shot Example:**
        ```
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: The answer is False.
        The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        A: The answer is True.
        The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        A: The answer is True.
        The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        A: The answer is False.
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        A:
        ```

        **Output:**
        ```
        The answer is True.
        ```
        False Again!

<----------section---------->
### Prompt Engineering Techniques

*   **Chain-of-Thought Prompting:**
    *   Definition: Enhances complex reasoning by guiding the model through intermediate reasoning steps.
    *   Principle: Breaks down a complex problem into a series of smaller, more manageable steps, allowing the LLM to follow a logical chain of reasoning.

    | Standard Prompting                                                                                                                                              | Chain-of-Thought Prompting                                                                                                                                                  |
    | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: The answer is 11. | Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: 2 cans of tennis balls is 2*3=6 tennis balls.  Then he has 5+6=11 tennis balls. The answer is 11. |
    | Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.                           | Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: They had 23 apples, used 20, that means 23-20 = 3 apples remaining.  Then bought 6 more, so there are 3+6=9 apples. The answer is 9. |

    *Combining it with few-shot prompting can yield better results on even more complex tasks.*

    *   **Example:**
        ```
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
        The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        A: Adding all the odd numbers (17, 19) gives 36. The answer is True.
        The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        A: Adding all the odd numbers (11, 13) gives 24. The answer is True.
        The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        A:
        ```

        **Output:**
        ```
        Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
        ```
        Correct! Note: this is an emergent ability that arises with sufficiently large language models.

<----------section---------->
### Prompt Engineering Techniques

*   **Self-Consistency Prompting:**
    *   Definition: Employs an iterative chain-of-thought approach.
    *   Principle: Repeats the question multiple times to generate a range of reasoning paths, selecting the most frequent answer as the final response.

<----------section---------->
### Prompt Engineering Techniques

*   **Meta Prompting:**
    *   Definition: Guides the model through the logical steps required to solve a problem without relying on specific content-based examples.
    *   Principle: Focuses on the process rather than the content, allowing the LLM to apply a structured approach to problem-solving.
    *   **Example:**
        ```
        Solve the quadratic equation 3x² + 4x - 5 = 0 by following these structured steps:
        1. Identify and Set Up the Formula: Recognize that this is a quadratic equation in the form ax² + bx + c = 0.
        2. Apply the Quadratic Formula: Use the quadratic formula x = (-b ± √(b² - 4ac)) / (2a) and substitute in the values.
        3. Calculate the Discriminant: Find b² - 4ac, and interpret its value to determine the nature of the roots.
        4. Solve for x: Perform the calculation for both + and - options in the formula to find the values of x.
        5. Verify the Solution: Substitute each value of x back into the original equation to confirm it satisfies it.
        6. Summarize the Results: State the final solution, listing both values of x and noting any observations about the nature of the roots.
        ```
<----------section---------->
### Prompt Engineering Techniques

*   **Task-Agnostic Meta Prompting:**
    *   Definition: Achieves good results by simply asking the model to reason step-by-step, without specifying the content of each step.
    *   Principle: Relies on the LLM's general reasoning abilities to structure its response in a logical and coherent manner.
    *   **Example:**
        ```
        Problem Statement:
        * Problem: [question to be answered]
        Solution Structure:
        * Begin the response with "Let’s think step by step."
        * Follow with the reasoning steps, ensuring the solution process is broken down clearly and logically.
        * End the solution with the final answer encapsulated in a LaTeX-formatted box, 4, for clarity and emphasis.
        * Finally, state "The answer is [final answer to the problem].", with the final answer presented in LaTeX notation.
        ```
<----------section---------->
### Prompt Engineering Techniques

*   **Meta Meta Prompting:**
    *   Definition: Utilizes LLMs to generate prompts that guide further tasks.
    *   Principle: Automates the creation of effective prompts, allowing for more efficient and scalable prompt engineering.
    *   **Example:**
        *   **Prompt:**
            ```
            Create a detailed metaprompt to guide an AI in solving a differential equation. Your metaprompt
            should focus on ensuring that the AI identifies the type of differential equation, selects the
            appropriate method for solving it, and presents a clear, step-by-step solution. Include instructions

            for verifying the solution and summarizing the results.
            ```
        *   **Output:**
            ```
            To solve this differential equation, follow these steps carefully:
            1. Identify the Type: Begin by identifying the type of differential equation (e.g., first-order linear,
            separable, homogeneous, etc.). Briefly explain how you recognized the type.

            2. Apply an Appropriate Method: Based on the type, outline the most suitable method for
            solving it (e.g., separation of variables, integrating factor, etc.). If there are multiple
            approaches, describe why this particular method is the best choice.

            3. Show Step-by-Step Solution: Work through the equation step-by-step, explaining each
            manipulation and transformation in clear terms.

            4. Verify Solution: Once you find the solution, verify it by differentiating the result and
            substituting it back into the original equation to confirm it satisfies the differential equation.

            5. Summarize the Solution: Finally, summarize the general or particular solution, highlighting

            any constants or special conditions that apply.
            ```
<----------section---------->
### Prompt Engineering Techniques

*   **Prompt Chaining:**
    *   Definition: Breaks down a complex task into different parts, each handled by a specific prompt.
    *   Principle: Manages intricate queries by dividing them into manageable segments, using the output of one prompt as the input for the next.
    *   **Example:**
        ```
        Extract any relevant quotes from the following text to answer the question "What was the main cause of the American Civil War?" [reference text follows]

        "One of the biggest catalysts of the Civil War was the issue of slavery, particularly the Southerners’ desire to bring slavery into the territories."
        "Tensions peaked when the Republican candidate Abraham Lincoln was elected president in 1860 with a platform opposing the extension of slavery into the territories, which isolated the South."

        Using the extracted quotes and the full text, provide a detailed answer to the question "What was the main cause of the American Civil War?"
        ```

        **Output:**
        ```
        The main cause of the American Civil War was the issue of slavery and its potential expansion into new territories and states. The South insisted on bringing slavery into new territories, while the election of Abraham Lincoln with an anti-slavery platform isolated the Southern states, which feared losing the right to own slaves. The fundamental disagreement over slavery between the North and South was the key factor that ultimately led to the outbreak of the Civil War.
        ```
<----------section---------->
### Prompt Engineering Techniques

*   **Role Prompting:**
    *   Definition: Asks the model to assume a specific role or persona.
    *   Principle: Adjusts the tone, style, and depth of information provided based on the assigned role.
    *   **Examples:**
        *   Write a review of [pizza place]
        *   You are a food critic writing for the Michelin Guide. Write a review of [pizza place]
        *   You are a marketing expert. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems.
        *   You are a customer service representative. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems.

    *   **Example of Role Prompting effect:**
        *   Without RP: Write a review of [pizza place]
        *   With RP: You are a food critic writing for the Michelin Guide. Write a review of [pizza place]

    *   **Example of Role Prompting effect (Marketing Expert):**
        *   You are a marketing expert. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems. (focus on persuasion, positivity, and relationship)

          ```
          Subject: A Little More Wait for Your Wonderful Products

          Dear [Client's Name],

          We hope this message finds you buzzing with excitement for your upcoming
          order! However, even the best-laid plans sometimes face unexpected
          hiccups. We have hit a temporary logistics snag that will unfortunately
          add a small wait time to your delivery.

          While we understand this isn't ideal, we believe great things come to
          those who wait, and your order is no exception. Rest assured, from the
          moment we sort out this situation to the time your products reach you,
          we're committed to ensuring that your experience is nothing short of
          excellent.

          Thank you for your patience and understanding. Stay tuned for more
          updates, and please, don't hesitate to reach out if you have any questions
          — we're always here.

          Cheers, [Your Name]
          ```

    *   **Example of Role Prompting effect (Customer Service):**
        *   You are a customer service representative. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems. (More relational and solution-oriented)

          ```
          Subject: Attention Required: Update on Your Order Status

          Dear [Client's Name],

          Greetings from [Your Company Name].

          We wanted to inform you that, due to unforeseen logistical challenges,
          there might be a delay in the delivery of your order. We understand that
          this may cause some inconvenience, and we apologize sincerely.
          Meanwhile, we are working tirelessly to resolve this and ensure your
          delivery reaches you by [new estimated date]. We value your time and
          patience greatly.

          Please feel free to reach out if you have any questions or need further
          information. Our team is always here to assist you.

          Thank you for your understanding.

          Best regards, [Your Name]
          ```
<----------section---------->
### Prompt Engineering Techniques

*   **Structured Prompting:**
    *   Definition: Employs a semi-formal definition of instructions to help LLMs handle complex tasks more predictably.
    *   Principle: Divides the prompt into sections and encodes it into a script, using delimiters to help LLMs recognize each part as distinct units of meaning.
    *   Delimiters: Any unique character sequence that wouldn't normally appear together (e.g., `###`, `===`, `>>>`) or XML tags.
    *   Rationale: LLMs are often trained on web content and have learned to recognize and understand this formatting.

    *   **Example with custom delimiters:**
        ```
        Classify the sentiment of each conversation in <<<CONVERSATIONS>>> as ‘Positive’ or ‘Negative’. Give the sentiment classifications without any other preamble text.

        ###EXAMPLE CONVERSATIONS
        [Agent]: Good morning, how can I assist you today?
        [Customer]: This product is terrible, nothing like what was advertised!
        [Customer]: I’m extremely disappointed and expect a full refund.

        [Agent]: Good morning, how can I help you today?
        [Customer]: Hi, I just wanted to say that I’m really impressed with your product. It exceeded my expectations!
        ###

        ###EXAMPLE OUTPUTS
        Negative
        Positive
        ###

        <<<CONVERSATIONS>>>
        [Agent]: Hello! Welcome to our support. How can I help you today?
        [Customer]: Hi there! I just wanted to let you know I received my order, and it’s fantastic!
        [Agent]: That’s great to hear! We’re thrilled you’re happy with your purchase. Is there anything else I can assist you with?
        [Customer]: No, that’s it. Just wanted to give some positive feedback. Thanks for your excellent service!

        [Agent]: Hello, thank you for reaching out. How can I assist you today?
        [Customer]: I’m very disappointed with my recent purchase. It’s not what I expected at all.
        [Agent]: I’m sorry to hear that. Could you please provide more details so I can help?
        [Customer]: The product is of poor quality, and it arrived late. I’m really unhappy with this experience.
        >>>
        ```

        **Output:**
        ```
        Positive
        Negative
        ```

    *   **Example with XML tags:**
        ```xml
        <prompt>
            <instruction>Classify the sentiment of the following conversations into one of two classes, using the examples given. Give the sentiment classifications without any other preamble text.</instruction>
            <classes>
                <positive>Positive</positive>
                <negative>Negative</negative>
            </classes>
            <example-conversations>
                <conversation>
                    [Agent]: Good morning, how can I assist you today?
                    [Customer]: This product is terrible, nothing like what was advertised!
                    [Customer]: I’m extremely disappointed and expect a full refund.
                </conversation>
                <conversation>
                    [Agent]: Good morning, how can I help you today?
                    [Customer]: Hi, I just wanted to say that I’m really impressed with your product. It exceeded my expectations!
                </conversation>
            </example-conversations>
            <example-classes>
                <class>Negative</class>
                <class>Positive</class>
            </example-classes>
            <conversations>
                <conversation>
                    [Agent]: Hello! Welcome to our support. How can I help you today?
                    [Customer]: Hi there! I just wanted to let you know I received my order, and it’s fantastic!
                    [Agent]: That’s great to hear! We’re thrilled you’re happy with your purchase. Is there anything else I can assist you with?
                    [Customer]: No, that’s it. Just wanted to give some positive feedback. Thanks for your excellent service!
                </conversation>
                <conversation>
                    [Agent]: Hello, thank you for reaching out. How can I assist you today?
                    [Customer]: I’m very disappointed with my recent purchase. It’s not what I expected at all.
                    [Agent]: I’m sorry to hear that. Could you please provide more details so I can help?
                    [Customer]: The product is of poor quality, and it arrived late. I’m really unhappy with this experience.
                </conversation>
            </conversations>
        </prompt>
        ```

        **Output:**
        ```
        Positive
        Negative
        ```

*   **Structured Prompting with CO-STAR:**
    *   Definition: The CO-STAR framework divides a prompt into specific sections to improve clarity and guide the LLM.
    *   Principle: By structuring the prompt into Context, Objective, Style, Tone, Audience, and Response, the LLM can better understand the task and generate more appropriate and effective outputs.
    *   Components:
        *   **Context:** Background information on the task.
        *   **Objective:** Clear definition of the task.
        *   **Style:** Desired writing style.
        *   **Tone:** Desired emotional context (e.g., formal, humorous, empathetic).
        *   **Audience:** Intended recipients (experts, beginners, etc.).
        *   **Response:** Defines the desired output format (text, list, table, JSON).

<----------section---------->
### Prompt Engineering Techniques

*   **Generate Knowledge Prompting:**
    *   Definition: First prompts the LLM to generate relevant knowledge related to a task, then incorporates that knowledge into the prompt along with the task description or question.
    *   Principle: Leverages the LLM's capacity to generate supplementary knowledge beyond its base training domain. Useful when the LLM lacks the specific information required to directly answer a query.
    *   **Example:**
        ```
        List and describe the key factors that influence the evolution of life in
        environments with extreme gravitational forces, such as on a super-Earth
        planet. Focus on biological, physiological, and ecological adaptations
        that might arise in such conditions.

        Using the adaptations and factors you described earlier, design a
        hypothetical intelligent species that could evolve on a super-Earth planet
        with extreme gravitational forces. Include details about their physical
        structure, social behaviors, methods of tool use or communication, and how
        their adaptations influence their culture or technology.
        ```

<----------section---------->

*   **Retrieval Augmented Generation (RAG):**
    *   Definition: Combines retrieval techniques with text generation.
    *   Principle: Addresses limitations in LLMs accessing updated or domain-specific data by using a search or retrieval system to find relevant documents or data, then using an LLM to generate responses conditioned on the retrieved data.
    *   Components:
        *   Retrieval System: Uses search engines or databases to find relevant documents.
        *   LLM: Generates responses based on the retrieved data.

<----------section---------->

### Prompt Engineering Guide

The Prompt Engineering Guide: [https://www.promptingguide.ai/](https://www.promptingguide.ai/)

*   Tree of Thoughts
*   Automatic Reasoning and Tool-use
*   Automatic Prompt Engineer
*   Active-Prompt
*   Directional Stimulus Prompting
*   Program-Aided Language Models
*   …

**Text-Base Prompt Tech. (List from figure 2.2)**

|                     |                      |                      |                      |                      |
| :------------------ | :------------------- | :------------------- | :------------------- | :------------------- |
| **Zero-Shot**       | **Few-Shot**         | **Thought Generation** | **Decomposition**    | **Ensembling**       |
| Emotion Prompting   | Exemplar Generation  | Chain-of-Thought(CoT) | DECOMP               | COSP                 |
| Role Prompting      | Exemplar Ordering    | Zero-Shot CoT        | Faithful CoT         | DENSE                |
| Style Prompting     | Exemplar Selection   | Analogical Prompting | Least-to-Most        | DiVeRSe              |
| S2A                 | KNN                  | Step-Back Prompting  | Plan-and-Solve       | Max MutualInformation |
| SimToM              | Vote-K               | Thread-of-Thought(ThoT) | Program-of-Thought | Meta-CoT             |
| RaR                 | Prompt Mining        | Tab-CoT              | Recurs.-of-Thought | MoRE                 |
| RE                  | SG-ICL               | Few-Shot CoT         | Skeleton-of-Thought | Self-Consistency     |
| Self-Ask            |                      | Active-Prompt        | Tree-of-Thought      | USP                  |
|                     |                      | Auto-CoT             |                      | Prompt Paraphrasing  |
|                     |                      | Complexity-Based     |                      | Self-Criticism       |
|                     |                      | Contrastive          |                      | Chain-of-Veriﬁcation|
|                     |                      | Memory-of-Thought    |                      | Self-Calibration     |
|                     |                      | Uncertainty-RoutedCoT|                      | Self-Reﬁne          |
|                     |                      |                      |                      | Self-Veriﬁcation      |
|                     |                      |                      |                      | ReverseCoT           |
|                     |                      |                      |                      | Cumulative Reason    |

<----------section---------->


<----------section---------->

**Natural Language Processing and Large Language Models**

This lesson, Lesson 20 of the Corso di Laurea Magistrale in Ingegneria Informatica, focuses on Retrieval Augmented Generation (RAG). This is presented by Nicola Capuano and Antonio Greco, DIEM – University of Salerno. This lesson aims to provide an overview of RAG, introduce LangChain, and demonstrate how to build a RAG system using LangChain and Hugging Face.

**Outline:**

*   Introduction to RAG
*   Introduction to LangChain
*   Building a RAG with LangChain and HuggingFace

<----------section---------->

**Introduction to RAG**

**What is RAG?**

Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning across various topics. However, LLMs face certain limitations:

*   Their knowledge is confined to the data they were trained on.
*   They are unable to access information introduced after their training period (cutoff date).
*   They cannot effectively reason about private or proprietary data that was not part of their training dataset.

Retrieval Augmented Generation (RAG) addresses these limitations by augmenting LLMs with additional data. Essentially, RAG is a technique that enhances the knowledge base of LLMs, enabling them to reason about a broader range of information. This includes private data and information that emerged after the model's training cutoff date. This makes RAG a powerful technique for creating AI applications that are not limited by the inherent constraints of LLMs.

<----------section---------->

**RAG Concepts**

A typical RAG application is structured around two primary components:

*   **Indexing:** This component constitutes a pipeline responsible for ingesting data from a designated source and indexing it to facilitate efficient retrieval. The indexing process typically occurs offline, preparing the data for later use.
*   **Retrieval and Generation:** This component operates in real-time, processing user queries to produce relevant answers.

    *   It takes the user's query as input during runtime.
    *   It retrieves relevant data from the index based on the query.
    *   It formulates a prompt that combines the user's query with the retrieved data. This enriched prompt provides the LLM with the necessary context to generate a more informed answer.
    *   It employs an LLM to generate an answer based on the combined query and retrieved data.

<----------section---------->

**Indexing**

The indexing pipeline involves several steps to prepare data for retrieval:

*   **Load:** The initial step is to load the data from its source. RAG frameworks offer document loaders compatible with a wide array of formats (e.g., PDF, CSV, HTML, JSON) and sources (e.g., file systems, the Web, databases).
*   **Split:** Large documents are divided into smaller chunks. This splitting is crucial for two reasons:

    *   Indexing: Smaller chunks are easier to search and manage within the index.
    *   LLM Usage: Smaller chunks are more likely to fit within the model's context window, which is the limited amount of text an LLM can process at once.
*   **Store:** The final step is to store and index these splits in a way that allows for efficient searching. Vector stores are frequently used for this purpose.

<----------section---------->

**Indexing: Vector Stores and Embeddings**

Vector stores are used to store and index splits of data using vector representations, also known as embeddings.

**Vector Stores:**

These are specialized data stores designed to index and retrieve information based on embeddings.

*   **Recap: Embeddings** Embeddings are vector representations that capture the semantic meaning of data. They translate text (or other data types) into numerical vectors, where similar meanings are located closer to each other in the vector space.
*   **Advantage:** Vector stores enable information retrieval based on semantic similarity. This contrasts with traditional keyword-based searches, which rely on exact matches and can miss relevant information expressed using different wording.

<----------section---------->

**Retrieval and Generation**

The retrieval and generation stage is where the RAG system actively responds to user input. Given a user's query, the system retrieves relevant splits (chunks of text) from the vector store. The LLM then uses these retrieved splits to formulate a response. The prompt provided to the LLM includes both the user's question and the relevant retrieved data, enabling the LLM to generate a more accurate and contextually relevant answer.

<----------section---------->

**Introduction to LangChain**

**LangChain:**

LangChain is a framework designed to simplify the development of applications powered by LLMs.

*   It offers a comprehensive set of building blocks for integrating LLMs into diverse applications.
*   It provides seamless connections to third-party LLMs (such as OpenAI and Hugging Face), data sources (like Slack or Notion), and external tools.
*   It facilitates the chaining of different components, enabling the creation of complex and sophisticated workflows.
*   It supports a wide range of use cases, including chatbots, document search, RAG systems, question answering, data processing, and information extraction.
*   It includes both open-source and commercial components, offering flexibility for different development needs.

<----------section---------->

**Key Components of LangChain**

LangChain offers a variety of components to facilitate LLM application development:

*   **Prompt Templates:** These streamline the process of translating user input into instructions suitable for language models. They support both string and message list formats.
*   **LLMs:** These are third-party language models that take strings or messages as input and return strings as output, acting as the core reasoning engine.
*   **Chat Models:** These are specialized third-party models that handle sequences of messages as input and output, designed to manage conversational flow with distinct roles for each message.
*   **Example Selectors:** These dynamically select and format concrete examples within prompts, improving model performance by providing relevant context.
*   **Output Parsers:** These convert the text generated by models into structured formats (e.g., JSON, XML, CSV), with features for error correction and advanced parsing.
*   **Document Loaders:** These tools extract content from various data sources, providing a standardized way to ingest data into the LangChain pipeline.
*   **Vector Stores:** These systems store and retrieve unstructured documents and data using embedding vectors, enabling semantic search and retrieval.
*   **Retrievers:** These interfaces allow for document and data retrieval from vector stores and other external sources, providing a consistent way to access information.
*   **Agents:** These systems leverage LLMs for reasoning, enabling them to decide on actions based on user inputs. Agents provide a framework for building autonomous and adaptive applications.

<----------section---------->

**Installation of LangChain and Related Libraries**

Before building a RAG application with LangChain, it's essential to install the necessary libraries:

*   Install the core LangChain library:

    ```bash
    pip install langchain
    ```
*   Install community-contributed extensions for LangChain:

    ```bash
    pip install langchain_community
    ```
*   Install Hugging Face integration for LangChain:

    ```bash
    pip install langchain_huggingface
    ```
*   Install a library to load, parse, and extract text from PDF files:

    ```bash
    pip install pypdf
    ```
*   Install the FAISS vector store (CPU version):

    ```bash
    pip install faiss-cpu
    ```

<----------section---------->

**Preliminary Steps: Access Token and Model License**

Before querying an LLM model, you need to obtain a Hugging Face access token and accept the user license for the model.

*   **Hugging Face Access Token:** This token is required to access and use models hosted on the Hugging Face Hub.
*   **Mistral-7B-Instruct-v0.2 Model License:** To use the Mistral-7B-Instruct-v0.2 model, you must accept the user license agreement on the Hugging Face website: [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

<----------section---------->

**Querying an LLM Model**

This section demonstrates how to query a Large Language Model (LLM) using LangChain and Hugging Face. The example uses the Mistral-7B-Instruct-v0.2 model to answer a question about the FIFA World Cup.

```python
from langchain_huggingface import HuggingFaceEndpoint
import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR_API_TOKEN"

llm = HuggingFaceEndpoint(
    repo_id = "mistralai/Mistral-7B-Instruct-v0.2",
    temperature = 0.1
)

query = "Who won the FIFA World Cup in the year 2006?"
print(llm.invoke(query))
```

**Explanation:**

1.  **Import Libraries:** Import necessary modules from `langchain_huggingface` and the `os` module for environment variables.
2.  **Set API Token:** Sets the Hugging Face API token as an environment variable. Replace `"YOUR_API_TOKEN"` with your actual API token. Storing the API key as an environment variable is a more secure and flexible practice than hardcoding it directly into the script.
3.  **Initialize LLM:** Creates an instance of `HuggingFaceEndpoint`, specifying the `repo_id` (the model to use) and `temperature` (controls the randomness of the output). A lower temperature (e.g., 0.1) results in more deterministic and predictable responses.
4.  **Define Query:** Defines the question to be asked to the LLM.
5.  **Invoke LLM:** Calls the `invoke` method on the `llm` object with the query. This sends the query to the LLM and retrieves the response.
6.  **Print Response:** Prints the response from the LLM.

**Example Response:**

```
The FIFA World Cup in the year 2006 was won by the Italian national football team. They defeated France in the final match held on July 9, 2006, at the Allianz Arena in Munich, Germany. The Italian team was coached by Marcello Lippi and was led by the legendary goalkeeper Gianluigi Buffon. The team's victory was significant as they had not won the World Cup since 1982. The final match ended in a 1-1 draw after extra time, and the Italians won the penalty shootout 5-3. The winning goal in the shootout was scored by Andrea Pirlo.
```

**Security Note:**

The example highlights the importance of storing the Hugging Face API token in an environment variable for security reasons. This prevents the token from being exposed in the code.

*   **macOS or Linux:**

    ```bash
    export HUGGINGFACEHUB_API_TOKEN="api_token"
    ```
*   **Windows with PowerShell:**

    ```powershell
    setx HUGGINGFACEHUB_API_TOKEN "api_token"
    ```

<----------section---------->

**Prompt Templates**

This section introduces the concept of prompt templates, which are predefined text structures used to create dynamic and reusable prompts for interacting with LLMs.

```python
from langchain.prompts import PromptTemplate

template = "Who won the {competition} in the year {year}?"
prompt_template = PromptTemplate(
    template = template,
    input_variables = ["competition", "year"]
)

query = prompt_template.invoke({"competition": "Davis Cup", "year": "2018"})
answer = llm.invoke(query)

print(answer)
```

**Explanation:**

1.  **Import PromptTemplate:** Import the `PromptTemplate` class from `langchain.prompts`.
2.  **Define Template:** Defines a template string with placeholders (`{competition}` and `{year}`). These placeholders will be replaced with actual values when the prompt is created.
3.  **Create PromptTemplate Object:** Creates a `PromptTemplate` object, specifying the template string and the input variables. The `input_variables` list indicates which variables will be used to fill the placeholders in the template.
4.  **Invoke PromptTemplate:** Calls the `invoke` method on the `prompt_template` object, providing a dictionary with the values for the input variables. This generates a complete prompt string.
5.  **Query LLM:** Sends the generated prompt to the LLM using the `llm.invoke()` method.
6.  **Print Answer:** Prints the LLM's response.

**Example Response:**

```
The Davis Cup in the year 2018 was won by Croatia. They defeated France in the final held in Lille, France. The Croatian team was led by Marin Cilic and Borna Coric, while the French team was led by Jo-Wilfried Tsonga and Lucas Pouille. Croatia won the tie 3-2. This was Croatia's first Davis Cup title.
```

<----------section---------->

**Introduction to Chains**

Chains combine multiple steps in an NLP pipeline, where the output of each step serves as the input for the subsequent step. This is useful for automating complex tasks and integrating external systems.

```python
chain = prompt_template | llm
answer = chain.invoke({"competition": "Davis Cup", "year": "2018"})

print(answer)
```

**Explanation:**

1.  **Define Chain:** This line uses the pipe operator `|` to create a chain that consists of two steps: the `prompt_template` and the `llm`. The output of the `prompt_template` (which is the formatted prompt string) will be passed as input to the `llm`.
2.  **Invoke Chain:** Calls the `invoke` method on the `chain` object, providing a dictionary with the values for the input variables (`competition` and `year`). This executes the chain and returns the LLM's response.
3.  **Print Answer:** Prints the LLM's response.

**Example Response:**

```
The Davis Cup in the year 2018 was won by Croatia. They defeated France in the final held in Lille, France. The Croatian team was led by Marin Cilic and Borna Coric, while the French team was led by Jo-Wilfried Tsonga and Lucas Pouille. Croatia won the tie 3-2. This was Croatia's first Davis Cup title.
```

<----------section---------->

**Refining LLM Output with Chains**

This section demonstrates how to refine the LLM output for future processing by using a chain to extract specific information from the initial LLM response.

```python
followup_template = """
task: extract only the name of the winning team from the following text
output format: json without formatting

example: {{"winner": "Italy"}}

### {text} ###
"""

followup_prompt_template = PromptTemplate(
    template = followup_template,
    input_variables = ["text"]
)

followup_chain = followup_prompt_template | llm

print(followup_chain.invoke({"text": answer}))
```

**Explanation:**

1.  **Define Follow-up Template:** Defines a template string for the follow-up prompt. This template instructs the LLM to extract the name of the winning team from the input text and format the output as JSON.
2.  **Create Follow-up PromptTemplate:** Creates a `PromptTemplate` object for the follow-up prompt, specifying the template string and the input variable (`text`).
3.  **Define Follow-up Chain:** Creates a chain consisting of the `followup_prompt_template` and the `llm`.
4.  **Invoke Follow-up Chain:** Calls the `invoke` method on the `followup_chain` object, providing a dictionary with the value for the `text` variable (which is the initial LLM response). This executes the follow-up chain and returns the refined LLM response.
5.  **Print Refined Output:** Prints the refined LLM response, which is the name of the winning team in JSON format.

**Example Response:**

```json
{"winner": "Croatia"}
```

<----------section---------->

**Chaining All Together**

This section shows how to chain multiple steps together into a single, complete pipeline, which enables the forwarding of the output of the previous step to the next step associated with a specific dictionary key.

```python
from langchain_core.runnables import RunnablePassthrough

chain = (
    prompt_template
    | llm
    | {"text": RunnablePassthrough()}
    | followup_prompt_template
    | llm
)

print(chain.invoke({"competition": "Davis Cup", "year": "2018"}))
```

**Explanation:**

1.  **Import RunnablePassthrough:** Import the `RunnablePassthrough` class from `langchain_core.runnables`. This component passes its input directly as output, allowing you to chain operations that require access to the original input.
2.  **Define Complete Chain:** This creates a chain that combines the prompt template, the LLM, a `RunnablePassthrough` component, the follow-up prompt template, and the LLM again.

    *   `prompt_template`: Formats the initial prompt based on the input variables.
    *   `llm`: Generates an answer based on the formatted prompt.
    *   `{"text": RunnablePassthrough()}`: Creates a dictionary where the key `text` is associated with a `RunnablePassthrough` object. This passes the output of the previous step (the LLM's response) to the next step under the key `text`.
    *   `followup_prompt_template`: Formats the follow-up prompt, using the LLM's response (passed as `text`) as input.
    *   `llm`: Extracts the name of the winning team from the follow-up prompt.
3.  **Invoke Complete Chain:** Calls the `invoke` method on the `chain` object, providing a dictionary with the values for the input variables (`competition` and `year`). This executes the entire chain and returns the final output.
4.  **Print Final Output:** Prints the final output, which is the extracted name of the winning team in JSON format.

**Example Response:**

```json
{"winner": "Croatia"}
```

<----------section---------->

**More on Chains: LCEL and Predefined Chains**

LCEL (LangChain Expression Language) is a powerful syntax for creating modular pipelines for chaining operations.

*   **Pipe Syntax:** The `|` operator enables the seamless chaining of operations, allowing you to define complex workflows in a concise and readable manner.
*   **Modular and Reusable Components:** LCEL promotes the creation of modular and reusable components, making it easier to build and maintain complex applications.
*   **Branching Logic and Follow-up Queries:** LCEL supports branching logic and follow-up queries, enabling the creation of more dynamic and adaptive workflows.

LangChain includes predefined chains for common tasks, such as question answering, document summarization, and conversational agents. These chains provide ready-to-use solutions that can be easily integrated into your applications. More information about LangChain can be found in the documentation: [https://python.langchain.com/docs/](https://python.langchain.com/docs/)

<----------section---------->

**Building a RAG with LangChain and Hugging Face: Example Project**

This section outlines an example project demonstrating how to build a RAG system using LangChain and Hugging Face. The system will be able to answer queries on documents from the Census Bureau US, which collects statistics about the nation, its people, and its economy.

The first step is to download the documents that will be indexed:

```python
from urllib.request import urlretrieve
import os

os.makedirs("us_census", exist_ok = True)

files = [
    "https://www.census.gov/content/dam/Census/library/publications/2022/demo/p70-178.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-017.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-016.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-015.pdf",
]

for url in files:
    file_path = os.path.join("us_census", url.split("/")[-1])
    urlretrieve(url, file_path)
```

**Explanation:**

1.  **Import Libraries:** The code imports `urlretrieve` from `urllib.request` to download files and `os` to interact with the operating system.
2.  **Create Directory:** It creates a directory named "us\_census" to store the downloaded files, using `os.makedirs` with `exist_ok=True` to prevent errors if the directory already exists.
3.  **Define File URLs:** A list of URLs (`files`) is defined, each pointing to a PDF document on the Census Bureau website.
4.  **Download Files:** The code iterates through each URL in the `files` list, constructs a local file path using `os.path.join`, and downloads the file using `urlretrieve`. The downloaded files are saved in the "us\_census" directory.

<----------section---------->

**Document Loaders in LangChain**

LangChain provides a variety of document loaders that can extract content from diverse data sources. These loaders simplify the process of ingesting data into the RAG pipeline.

*   `TextLoader`: Handles plain text files (.txt).
*   `PyPDFLoader`: Extracts content from PDF documents.
*   `CSVLoader`: Reads tabular data from CSV files.
*   `WebBaseLoader`: Extracts content from web pages using their URLs.
*   `WikipediaLoader`: Fetches content from Wikipedia pages.
*   `SQLDatabaseLoader`: Queries SQL databases to load content.
*   `MongoDBLoader`: Extracts data from MongoDB collections.
*   `IMAPEmailLoader`: Loads emails from various providers using the IMAP protocol.
*   `HuggingFaceDatasetLoader`: Fetches datasets from the Hugging Face datasets library.

And many others. These loaders enable LangChain to ingest data from a wide range of sources, making it a versatile tool for building RAG systems.

<----------section---------->

**Extracting Content from PDFs with LangChain**

This section demonstrates how to use LangChain to extract text content from PDF documents. It covers loading from a single PDF and loading multiple PDFs from a directory.

**Extracting from a Single PDF:**

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("us_census/p70-178.pdf")
doc = loader.load()
print(doc[0].page_content[0:100]) # Print the first page (first 100 characters)
```

**Explanation:**

1.  **Import PyPDFLoader:** Imports the `PyPDFLoader` class from `langchain_community.document_loaders`.
2.  **Create Loader Instance:** Creates an instance of `PyPDFLoader`, specifying the path to the PDF document.
3.  **Load Document:** Calls the `load` method on the `loader` object to load the PDF document. This returns a list of `Document` objects, where each `Document` represents a page in the PDF.
4.  **Print Page Content:** Prints the first 100 characters of the first page of the document to display the extracted text.

**Example Output:**

```
Occupation, Earnings, and Job
Characteristics

July 2022

P70-178

Clayton Gumber and Briana Sullivan
```

**Extracting from a Directory of PDFs:**

```python
from langchain_community.document_loaders import PyPDFDirectoryLoader

loader = PyPDFDirectoryLoader("./us_census/")
docs = loader.load()

print(docs[60].page_content[0:100]) # The 61st page (documents are concatenated)
print('\n' + docs[60].metadata['source']) # The source of the page 61st page
```

**Explanation:**

1.  **Import PyPDFDirectoryLoader:** Imports the `PyPDFDirectoryLoader` class from `langchain_community.document_loaders`.
2.  **Create Loader Instance:** Creates an instance of `PyPDFDirectoryLoader`, specifying the path to the directory containing the PDF documents.
3.  **Load Documents:** Calls the `load` method on the `loader` object to load all PDF documents in the directory. This returns a list of `Document` objects, where each `Document` represents a page in one of the PDF documents. The documents are concatenated in the order they are read from the directory.
4.  **Print Page Content and Source:** Prints the first 100 characters of the 61st page (index 60) of the concatenated documents and the source file path of that page.

**Example Output:**

```
U.S. Census Bureau 19

insurance, can exacerbate the dif-
ferences in monetary compensa -
tion and w

us_census/p70-178.pdf
```

<----------section---------->

**Text Splitters in LangChain**

LangChain provides components for breaking large text documents into smaller, more manageable chunks. These components are crucial for indexing and LLM processing.

*   `CharacterTextSplitter`: Splits text into chunks based on character count.
*   `RecursiveCharacterTextSplitter`: Splits text intelligently using hierarchical delimiters (e.g., paragraphs, sentences).
*   `TokenTextSplitter`: Splits text based on token count rather than characters.
*   `HTMLHeaderTextSplitter`: Splits HTML documents by focusing on headers (e.g., `<h1>`, `<h2>`).
*   `HTMLSectionSplitter`: Divides HTML content into sections based on logical groupings or structural markers.
*   `NLPTextSplitter`: Uses NLP techniques to split text into chunks based on semantic meaning.

And many others. The choice of text splitter depends on the type of document and the desired chunking strategy.

<----------section---------->

**Splitting Text into Chunks**

This section demonstrates how to split text into chunks using LangChain's `RecursiveCharacterTextSplitter`.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 700,
    chunk_overlap = 50,
)

chunks = text_splitter.split_documents(docs)

print(chunks[0].page_content) # The first chunk
```

**Explanation:**

1.  **Import RecursiveCharacterTextSplitter:** Imports the `RecursiveCharacterTextSplitter` class from `langchain.text_splitter`.
2.  **Create Text Splitter Instance:** Creates an instance of `RecursiveCharacterTextSplitter`, specifying the `chunk_size` (maximum number of characters in each chunk) and `chunk_overlap` (number of overlapping characters between adjacent chunks).
3.  **Split Documents:** Calls the `split_documents` method on the `text_splitter` object, passing the list of `Document` objects to be split. This returns a list of `Document` objects, where each `Document` represents a chunk of the original text.
4.  **Print First Chunk:** Prints the content of the first chunk to display the result of the splitting process.

**Example Output:**

```
Health Insurance Coverage Status and Type
by Geography: 2021 and 2022

American Community Survey Briefs
ACSBR-015
```

<----------section---------->

**Analyzing Chunking Results**

This section shows how to analyze the impact of the text splitting process by printing the average document length before and after splitting.

```python
len_chunks = len(chunks)
avg_docs = sum([len(doc.page_content) for doc in docs]) // len(docs)
avg_chunks = sum([len(chunk.page_content) for chunk in chunks]) // len(chunks)

print(f'Before split: {len(docs)} pages, with {avg_docs} average characters.')
print(f'After split: {len(chunks)} chunks, with {avg_chunks} average characters.')
```

**Explanation:**

1.  **Calculate Lengths:** Calculates the total number of chunks (`len_chunks`), the average number of characters per document before splitting (`avg_docs`), and the average number of characters per chunk after splitting (`avg_chunks`).
2.  **Print Results:** Prints the calculated lengths to display the effect of the splitting process.

**Example Output:**

```
Before split: 63 pages, with 3840 average characters.
After split: 398 chunks, with 624 average characters.
```

<----------section---------->

**Indexing Chunks with Embeddings**

Embeddings are used to index text chunks, enabling semantic search and retrieval.

*   We can use pre-trained embedding models from Hugging Face.
*   Bi-Directional Generative Embeddings (BGE) models excel at capturing relationships between text pairs.
*   `BAAI/bge-small-en-v1.5` is a lightweight embedding model from the Beijing Academy of Artificial Intelligence.
*   Suitable for tasks requiring fast generation.

[https://huggingface.co/BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)

```python
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
import numpy as np

embedding_model = HuggingFaceBgeEmbeddings(
    model_name = "BAAI/bge-small-en-v1.5",
    encode_kwargs = {'normalize_embeddings': True}  # useful for similarity tasks
)

# Embed the first document chunk
sample_embedding = np.array(embedding_model.embed_query(chunks[0].page_content))

print(sample_embedding[:5])
print("\nSize: ", sample_embedding.shape)
```

**Explanation:**

1.  **Import Libraries:** Imports the `HuggingFaceBgeEmbeddings` class from `langchain_community.embeddings` and the `numpy` library.
2.  **Create Embedding Model Instance:** Creates an instance of `HuggingFaceBgeEmbeddings`, specifying the `model_name` and `encode_kwargs`. The `encode_kwargs` argument sets `'normalize_embeddings': True`, which is useful for similarity tasks.
3.  **Embed Sample Chunk:** Calls the `embed_query` method on the `embedding_model` object to generate an embedding for the first document chunk. The embedding is then converted to a NumPy array.
4.  **Print Embedding Information:** Prints the first five elements of the embedding and the size of the embedding vector.

**Example Output:**

```
[-0.07508391 -0.01188472 -0.03148879  0.02940382  0.05034875]

Size:  (384,)
```

<----------section---------->

**Vector Stores: Choosing and Using FAISS**

Vector stores enable semantic search and retrieval by indexing and querying embeddings of documents or text chunks. LangChain supports various vector stores:

*   `FAISS`: Open-source library for efficient similarity search and clustering of dense vectors, ideal for local and small-to-medium datasets.
*   `Chroma`: Lightweight and embedded vector store suitable for local applications with minimal setup.
*   `Qdrant`: Open-source vector database optimized for similarity searches and nearest-neighbor lookup.
*   `Pinecone`: Managed vector database offering real-time, high-performance semantic search with automatic scaling.

And many others. The choice of vector store depends on the scale and performance requirements of the application.

This example uses the Facebook AI Similarity Search (FAISS) library.

```python
from langchain_community.vectorstores import FAISS

# Generate the vector store
vectorstore = FAISS.from_documents(chunks, embedding_model)

# Save the vector store for later use...
vectorstore.save_local("faiss_index")

# To load the vector store later...
# loaded_vectorstore = FAISS.load_local("faiss_index", embedding_model)
```

**Explanation:**

1.  **Import FAISS:** Imports the `FAISS` class from `langchain_community.vectorstores`.
2.  **Generate Vector Store:** Creates a FAISS vector store from the document chunks and the embedding model. The `FAISS.from_documents` method generates embeddings for each chunk and indexes them in the vector store.
3.  **Save Vector Store:** Saves the vector store to disk for later use. The `vectorstore.save_local` method persists the index and embeddings to files in the specified directory.
4.  **Load Vector Store (Optional):** Shows how to load a previously saved vector store from disk. The `FAISS.load_local` method loads the index and embeddings from the files in the specified directory.

<----------section---------->

**Querying the Vector Store**

This section demonstrates how to query the vector store to search for chunks relevant to a user query.

```python
query = """
What were the trends in median household income across
different states in the United States between 2021 and 2022.
"""

matches = vectorstore.similarity_search(query)

print(f'There are {len(matches)} relevant chunks.\nThe first one is:\n')
print(matches[0].page_content)
```

**Explanation:**

1.  **Define Query:** Defines the user query as a string.
2.  **Perform Similarity Search:** Calls the `similarity_search` method on the `vectorstore` object, passing the query as input. This method performs a semantic search in the vector store and returns a list of the most relevant document chunks.
3.  **Print Results:** Prints the number of relevant chunks found and the content of the first chunk.

**Example Output:**

```
There are 4 relevant chunks.
The first one is:

Comparisons
The U.S. median household income
```

<----------section---------->

**RAG Prompt Template**

This section defines a prompt template that integrates the retrieved context with the user question. The prompt template is used to provide explicit instructions to the LLM on how to handle the provided information.

```python
template = """
Use the following pieces of context to answer the question at the end.
Please follow the following rules:
1. If you don't know the answer, don't try to make up an answer. Just say "I can't find the final answer".
2. If you find the answer, write the answer in a concise way with five sentences maximum.

{context}

Question: {question}

Helpful Answer:
"""

from langchain.prompts import PromptTemplate
prompt_template = PromptTemplate(template = template, input_variables = ["context", "question"])
```

**Explanation:**

1.  **Define Template String:** Defines a template string that includes placeholders for the `context` (retrieved document chunks) and the `question` (user query). The template also provides instructions for the LLM on how to generate the answer.
2.  **Create PromptTemplate Instance:** Creates an instance of `PromptTemplate`, specifying the template string and the input variables.

<----------section---------->

**Vector Store as a Retriever**

To be used in chains, a vector store must be wrapped within a retriever interface. A retriever takes a text query as input and provides the most relevant information as output.

```python
# Create a retriever interface on top of the vector store
retriever = vectorstore.as_retriever(
    search_type = "similarity",  # use cosine similarity
    search_kwargs = {"k": 3}  # use the top 3 most relevant chunks
)
```

**Explanation:**

1.  **Create Retriever:** Calls the `as_retriever` method on the `vectorstore` object to create a retriever interface.
2.  **Configure Search:** Specifies the `search_type` as "similarity" (which uses cosine similarity) and the `search_kwargs` as `{"k": 3}` (which retrieves the top 3 most relevant chunks).

<----------section---------->

**Custom RAG Chain**

This section defines a custom RAG chain that combines the retriever, prompt template, and LLM.

```python
from langchain_core.runnables import RunnablePassthrough

# Helper function to concatenate the retrieved chunks
def format_docs(docs):
    return "\n\n".join(doc.page_content for
<----------section---------->

# Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)

**Lesson 21: Reinforcement Learning from Human Feedback (RLHF)**

Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This lesson explores Reinforcement Learning from Human Feedback (RLHF), a cutting-edge technique used to refine and improve Large Language Models (LLMs). This approach allows LLMs to better align with human values, preferences, and ethical considerations.

<----------section---------->

## Outline

*   Reinforcement Learning from Human Feedback (RLHF): Understanding the concept and its significance in LLM development.
*   Transformers `trl` library: Introduction to the `trl` library provided by Hugging Face, a toolset designed to facilitate the training of Transformer language models using Reinforcement Learning techniques.
*   Try it yourself: Practical guidance and resources to experiment with RLHF and the `trl` library.

<----------section---------->

## Reinforcement Learning from Human Feedback (RLHF)

### What is RLHF?

*   RLHF is a technique that leverages human feedback to fine-tune and improve Large Language Models (LLMs). It involves using human preferences and judgments as a reward signal to guide the learning process of the model.  Instead of solely relying on pre-defined datasets, RLHF incorporates human insights, enabling the model to learn from subjective evaluations.
*   RLHF serves as a strategy to balance the raw performance of an LLM (e.g., accuracy, fluency) with its alignment to human values and preferences. This alignment includes factors like safety, ethics, helpfulness, and overall user satisfaction. It helps ensure that the LLM produces outputs that are not only accurate and coherent but also ethically sound and beneficial to users.

### Why RLHF?

*   RLHF offers a potential strategy to "ground" the focus of an LLM, helping it to generate responses that are more relevant, specific, and in line with the intended purpose.  By incorporating human feedback, the model can learn to distinguish between responses that are merely correct and those that are truly valuable and appropriate in a given context.
*   RLHF can significantly enhance the safety, ethical considerations, and user satisfaction associated with LLMs. It helps to mitigate the risk of generating harmful, biased, or inappropriate content, making the LLM a more reliable and trustworthy tool for various applications. It helps the model avoid generating toxic or offensive content and encourages the model to produce responses that are sensitive to cultural norms and user expectations.

### Workflow of RLHF

The RLHF workflow typically involves three main stages:

1.  **Pre-trained Language Model:** This is the foundation of the RLHF process. It is a base LLM that has been pre-trained on a massive corpus of text data (e.g., BERT, GPT, T5). The pre-training stage equips the model with a broad understanding of language, grammar, and factual knowledge. These models are trained using self-supervised learning techniques on large amounts of text data, and they learn to predict the next word in a sequence.
2.  **Reward Model:** A secondary model is trained to score LLM outputs based on human feedback. Human raters provide preferences or rankings for different responses generated by the base LLM for given prompts. The reward model learns to predict these human preferences, effectively quantifying the quality and desirability of LLM outputs.
3.  **Fine-Tuning with Reinforcement Learning:** The pre-trained LLM is further optimized using reinforcement learning (RL). The reward model serves as the environment's reward function, guiding the LLM to generate responses that maximize the predicted human preference score. This stage utilizes RL algorithms like Proximal Policy Optimization (PPO) to iteratively refine the LLM's policy, leading to improved alignment with human expectations.

<----------section---------->

### Reward Model

*   **Inputs:** Training a reward model requires specific input data:
    *   Multiple LLM-generated outputs for given prompts: This consists of various responses generated by the base LLM for the same input prompt. Having multiple outputs allows the reward model to learn to differentiate between responses of varying quality.
    *   Corresponding human rank responses according to their preferences: Human raters rank these LLM-generated outputs based on their perceived quality, helpfulness, and alignment with human values. These rankings provide the ground truth for training the reward model.
*   The primary goal of training a reward model is to predict human preference scores accurately. This involves learning a function that maps LLM outputs to a numerical score that reflects how much humans would prefer that output over others.
*   The methodology often involves using a ranking loss function. This type of loss function is designed to teach the reward model which outputs humans prefer relative to others. For example, if human raters consistently prefer output A over output B, the ranking loss function will penalize the reward model if it assigns a higher score to output B.

### Fine-tuning with Proximal Policy Optimization (PPO)

*   The overarching goal of fine-tuning with PPO is to align the LLM’s outputs with human-defined quality metrics. This goes beyond simply maximizing accuracy; it involves ensuring that the LLM's responses are helpful, informative, safe, and ethical.

    1.  **Generate responses using the LLM:** The LLM generates responses to a variety of prompts or inputs.
    2.  **Score responses with the reward model:** The reward model evaluates the generated responses and assigns a score based on its learned understanding of human preferences.
    3.  **Update the LLM to maximize reward scores:** The LLM's parameters are adjusted using PPO to encourage the generation of responses that receive higher scores from the reward model. PPO is a policy gradient method, meaning it directly optimizes the LLM's policy (the function that determines its output) to increase the likelihood of generating high-reward responses.

<----------section---------->

### Pros and Cons of RLHF

**Pros:**

*   **Iterative Improvement:** RLHF enables iterative improvement by continuously collecting human feedback as the model evolves. This feedback can then be used to update both the reward model and the LLM through fine-tuning, creating a virtuous cycle of improvement.
*   **Improved Alignment:** RLHF leads to the generation of responses that are closer to human intent. The model learns to understand and respond to user queries in a way that aligns with their expectations and preferences.
*   **Ethical Responses:** RLHF can reduce harmful or biased outputs by training the model to avoid generating content that is offensive, discriminatory, or otherwise unethical.
*   **User-Centric Behavior:** RLHF allows for tailoring interactions to user preferences. The model can learn to adapt its communication style and content to suit individual users, resulting in a more personalized and engaging experience.

**Cons:**

*   **Subjectivity:** Human feedback can vary widely depending on the individual raters and their perspectives. This subjectivity can introduce noise into the training process and make it challenging to develop a robust reward model.
*   **Scalability:** Collecting sufficient, high-quality human feedback is a resource-intensive process. It requires recruiting and training raters, designing evaluation protocols, and managing the feedback collection process.
*   **Reward Model Robustness:** A misaligned reward model can lead to suboptimal fine-tuning. If the reward model does not accurately reflect human preferences, the LLM may be optimized to generate responses that are not actually desirable or helpful. This is known as the "reward hacking" problem.

<----------section---------->

### Tasks to Enhance with RLHF

*   **Text Generation:** RLHF can significantly enhance the quality of text produced by LLMs, making it more coherent, engaging, and informative.
*   **Dialogue Systems:** RLHF is valuable for improving the performance of dialogue systems, enabling them to have more natural, helpful, and engaging conversations with users.
*   **Language Translation:** RLHF can increase the precision of language translation by ensuring that the translated text accurately conveys the meaning and intent of the original text.
*   **Summarization:** RLHF can raise the standard of summaries produced by LLMs, making them more concise, accurate, and informative.
*   **Question Answering:** RLHF can increase the accuracy of question answering by training the model to provide relevant and helpful answers to user queries.
*   **Sentiment Analysis:** RLHF has been used to increase the accuracy of sentiment identification for particular domains or businesses, allowing for more nuanced and accurate analysis of customer feedback and opinions.
*   **Computer Programming:** RLHF can be used to speed up and improve software development by assisting programmers with code generation, debugging, and documentation.

<----------section---------->

### Case Study: GPT-3.5 and GPT-4

*   The pre-trained models GPT-3.5 and GPT-4 have been fine-tuned using RLHF, demonstrating the effectiveness of this technique in improving LLM performance.
*   OpenAI declares that they achieved the following with RLHF:
    *   Enhanced alignment with human values and preferences.
    *   Fewer unsafe or inappropriate outputs.
    *   More human-like interactions, making the model more engaging and natural to interact with.
*   These models were or are widely used in real-world applications like ChatGPT, showcasing the practical impact of RLHF in developing advanced AI systems.
*   The models are still incrementally improved with additional human feedback, highlighting the ongoing nature of the RLHF process and the commitment to continuous improvement.

<----------section---------->

## Transformers `trl` library

### TRL: Transformer Reinforcement Learning

*   TRL is a full-stack library that provides a comprehensive set of tools for training transformer language models with Reinforcement Learning. It covers the entire RLHF pipeline, from Supervised Fine-tuning (SFT) and Reward Modeling (RM) to Proximal Policy Optimization (PPO).
*   The library is seamlessly integrated with Hugging Face transformers, leveraging the popular Transformers library and the Hugging Face ecosystem. This integration simplifies the process of building and training RLHF models.

<----------section---------->

## Try it yourself

*   Study the `trl` library on Hugging Face: [https://huggingface.co/docs/trl/v0.7.8/index](https://huggingface.co/docs/trl/v0.7.8/index)
*   Give a careful look to:
    *   `PPOTrainer`: [https://huggingface.co/docs/trl/v0.7.8/ppo_trainer](https://huggingface.co/docs/trl/v0.7.8/ppo_trainer):  Understand how to use the `PPOTrainer` class for optimizing the language model using the PPO algorithm.
    *   `RewardTrainer`: [https://huggingface.co/docs/trl/v0.7.8/reward_trainer](https://huggingface.co/docs/trl/v0.7.8/reward_trainer): Learn how to use the `RewardTrainer` class for training the reward model based on human feedback.
*   Study the examples that are closer to your purposes:
    *   Sentiment analysis tuning: [https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning](https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning): Explore how to use RLHF for fine-tuning language models to generate text with specific sentiment.
    *   Detoxifying a Large Language Model with PPO: [https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm](https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm): Learn how RLHF can be used to mitigate toxicity and harmful content generated by LLMs.
*   Try to apply RLHF to your project.

```python
# Step 1: Train your model on your favorite dataset using Supervised Fine-Tuning (SFT)
from trl import SFTTrainer

trainer = SFTTrainer(
    model_name="facebook/opt-350m",
    dataset_text_field="text",
    max_seq_length=512,
    train_dataset=dataset,
)
trainer.train()

# Step 2: Train a reward model
from trl import RewardTrainer

trainer = RewardTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
)
trainer.train()

# Step 3: Further optimize the SFT model using the rewards from the reward model and PPO algorithm
from trl import PPOConfig, PPOTrainer

config = PPOConfig()
trainer = PPOTrainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    query_dataloader=query_dataloader,
)

for query in query_dataloader:
    response = model.generate(query)
    reward = reward_model(response)
    trainer.step(query, response, reward)
```

<----------section---------->

This code snippet provides a basic outline of how to implement RLHF using the `trl` library. It involves three main steps: 1) Supervised Fine-Tuning (SFT) of a base LLM, 2) training a reward model based on human feedback, and 3) fine-tuning the SFT model using the PPO algorithm guided by the reward model. The links in this document will provide you with much more detail to achieve the stated goals.

<----------section---------->

**Natural Language Processing and Large Language Models**

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
Lesson 22
Guardrails for LLMs
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This material introduces the concept of guardrails for Large Language Models (LLMs), a crucial aspect of ensuring their safe and ethical deployment in real-world applications. The presentation is structured for a Computer Engineering master's level course, focusing on the importance of regulating LLM behavior to prevent harmful outputs and align responses with desired guidelines.

Outline:
*   Adding guardrails to LLMs
*   Techniques for adding guardrails
*   Frameworks for implementing guardrails

<----------section---------->

**Adding Guardrails to LLMs**

**Guardrails:**

Guardrails are mechanisms or policies implemented to control and regulate the behavior of Large Language Models (LLMs). The primary goal of these guardrails is to ensure that LLM outputs are safe, accurate, and appropriate for the context in which they are used. This involves preventing the generation of harmful, biased, or factually incorrect content, and also aligning the LLM's responses with established ethical and operational guidelines. By implementing guardrails, developers can enhance trust and reliability in LLMs, making them more suitable for deployment in sensitive real-world applications.

They can:
*   Prevent harmful, biased, or inaccurate outputs.
*   Align responses with ethical and operational guidelines.
*   Build trust and reliability for real-world applications.

The significance of guardrails lies in the potential for LLMs to generate problematic content if left unchecked. Without these safeguards, LLMs might produce outputs that are offensive, perpetuate stereotypes, or disseminate misinformation. The implementation of guardrails is thus essential for mitigating these risks and ensuring that LLMs are used responsibly and ethically.

Examples of Guardrails:
*   Blocking harmful content
*   Restricting outputs to specific domains.

<----------section---------->

**Types of Guardrails:**

Guardrails can be categorized into several distinct types, each addressing specific aspects of LLM behavior:

*   **Safety Guardrails:** These guardrails are designed to prevent the generation of content that is harmful or offensive. This includes blocking hate speech, violent content, and any material that could be considered discriminatory or abusive.
*   **Domain-Specific Guardrails:** Domain-specific guardrails restrict the LLM's responses to particular areas of knowledge. This is useful in applications where the LLM should only provide information related to a specific subject, such as medical advice or legal information.
*   **Ethical Guardrails:** Ethical guardrails focus on preventing bias, misinformation, and ensuring fairness in the LLM's outputs. These guardrails are crucial for maintaining ethical standards and avoiding the perpetuation of harmful stereotypes.
*   **Operational Guardrails:** Operational guardrails are implemented to align LLM outputs with specific business or user objectives. This might involve limiting the length of responses, ensuring that the language used is appropriate for the target audience, or directing the LLM to focus on particular topics.

<----------section---------->

**Techniques for Adding Guardrails**

**Techniques for adding guardrails:**
*   Rule based filters
*   Fine tuning with custom data
*   Prompt Engineering
*   External validation layers
*   Real-time monitoring and feedback

Several techniques can be employed to implement guardrails in LLMs. These techniques vary in complexity and effectiveness, and often a combination of methods is used to create robust safeguards.

<----------section---------->

**Rule-Based Filters:**

Rule-based filters involve creating predefined rules to block or modify certain outputs from the LLM. These rules are typically based on specific keywords, phrases, or patterns that are considered undesirable.

Examples:
*   Keyword blocking (e.g., offensive terms).
*   Regex-based patterns for filtering sensitive information.

Rule-based filters are relatively simple to implement and can be very efficient for basic content filtering. They are particularly useful for blocking obvious instances of harmful or inappropriate language. However, they may not be effective against more subtle or nuanced forms of problematic content. This approach can also be used to specify general filter rules using languages similar to regular expressions through different open source tools.

<----------section---------->

**Fine-tuning with Custom Data:**

Fine-tuning involves training the LLM on specific, curated datasets that are aligned with the desired guidelines. By adjusting the model's weights during this process, developers can influence the LLM to produce outputs that are more consistent with ethical and operational standards.

Examples:
*   Fine-tune for medical advice to restrict responses to accurate and safe recommendations.
*   Fine-tune for question answering on the topics of the course.

Fine-tuning is a more sophisticated technique than rule-based filters, as it involves modifying the LLM's underlying behavior. This can be particularly effective for ensuring that the LLM provides accurate and reliable information in specific domains. Also, it may help to make the system provide high reliability for use cases such as preventing providing legal or medical advice.

<----------section---------->

**Prompt Engineering:**

Prompt engineering involves crafting or refining prompts to guide the LLM's behavior within desired boundaries. By carefully designing the input prompts, developers can influence the LLM to generate responses that are more aligned with ethical and operational guidelines.

Examples:
*   "Respond only with factual, non-controversial information."
*   "Avoid speculative or unverifiable statements."

Prompt engineering can be a powerful technique for steering the LLM towards desired outputs. However, it requires a deep understanding of how the LLM responds to different types of prompts, and it may not be effective in all situations.

<----------section---------->

**External Validation Layers:**

External validation layers involve using additional systems or APIs to post-process the LLM's outputs. These systems can perform a variety of checks, such as toxicity detection or fact-checking, to ensure that the LLM's responses meet certain standards.

Examples:
*   Toxicity detection APIs.
*   Fact-checking models.

External validation layers allow for modular and scalable implementation of guardrails. This approach can be particularly useful for integrating advanced content moderation tools into the LLM workflow.

<----------section---------->

**Real-Time Monitoring and Feedback:**

Real-time monitoring and feedback involve continuously monitoring the LLM's outputs for unsafe or incorrect content. Problematic outputs can be flagged or blocked in real-time, preventing them from being displayed to users.

Tools:
*   Human-in-the-loop systems.
*   Automated anomaly detection.

Real-time monitoring and feedback can be particularly effective for identifying and addressing issues as they arise. Human-in-the-loop systems involve having human moderators review the LLM's outputs, while automated anomaly detection systems use algorithms to identify unusual or potentially problematic content.

<----------section---------->

**Best Practices:**

*   Combine multiple techniques for robust safeguards.
*   Example: Rule-based filtering + External validation + Fine-tuning.

To create robust safeguards for LLMs, it is often necessary to combine multiple techniques. For example, a system might use rule-based filters to block obvious instances of harmful language, external validation layers to check for toxicity and misinformation, and fine-tuning to align the LLM's behavior with ethical guidelines.

<----------section---------->

**Frameworks for Implementing Guardrails**

Frameworks for implementing guardrails:
*   The existing frameworks for implementing guardrails offer:
    *   Easy integration with LLM APIs.
    *   Predefined and customizable rulesets.
*   Popular tools are:
    *   Guardrails AI: A library for implementing safeguards.
    *   LangChain: For chaining prompts and filtering outputs.
    *   OpenAI Moderation: A prebuilt API to detect unsafe content.

Several frameworks and tools are available to assist developers in implementing guardrails for LLMs. These tools offer features such as easy integration with LLM APIs and predefined, customizable rulesets.

<----------section---------->

**Guardrails AI:**
*   [https://www.guardrailsai.com/](https://www.guardrailsai.com/)
*   Validation: Ensures outputs are within specified guidelines.
*   Formatting: Controls the output structure.
*   Filters: Removes or blocks unsafe content.

Guardrails AI is a library specifically designed for implementing safeguards in LLMs. It offers features such as validation to ensure that outputs meet specified guidelines, formatting to control the structure of the output, and filters to remove or block unsafe content. The Guardrails-AI package uses a new language called "RAIL" to specify guardrail rules. It is suggested that the RAIL language can be used to build a retrieval-augmented LLM that doesn’t fake its answers, falling back to an "I don’t know" response when the retrieved text fails to contain the answer to the question. However, it seems that the only validation filter that guardrails-ai seems to be doing is to check the format of the output.

Example using Guardrails AI:
```python
from guardrails import Guard

guard = Guard(rules="rules.yaml")
response = guard(llm("Provide medical advice"))
```

<----------section---------->

**Langchain:**

*   Chains prompts with checks and filters.
*   Verifies outputs against predefined criteria.
*   Integrable with Guardrails:
    *   [https://www.guardrailsai.com/docs/integrations/langchain](https://www.guardrailsai.com/docs/integrations/langchain)

Langchain is a framework for chaining prompts and filtering outputs from LLMs. It allows developers to verify outputs against predefined criteria and can be integrated with Guardrails AI for enhanced safeguards. LangChain is better off for using some of the more standard Python templating systems: f-strings (format strings) or jinja2 templates.

Example using Langchain:
```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["question"],
    template="Answer safely and factually: {question}"
)
```

<----------section---------->

**OpenAI Moderation:**

OpenAI Moderation is a prebuilt API designed to detect unsafe content in LLM outputs. It provides a simple way to integrate content moderation into LLM applications.

<----------section---------->

**Try it yourself:**

*   Evaluate which are the techniques to add guardrails that are more suited for your purposes
*   A possible suggestion may be to proceed by incrementally add complexity to the guardrails if you are not able to achieve a satisfying result with a simpler approach
*   Give a careful look to the documentation of the existing frameworks
*   Study similar examples that are available in the documentation of existing frameworks
*   Try to apply guardrails to your project

To further harden NLP software and decrease the probability of LLM generating toxic text, users are encouraged to define bug bounties to reward users when they find a bug in the LLM or a gap in the guardrails. Conventional machine learning classifiers are a great option for detecting malicious intent or inappropriate content in LLM outputs, mainly when the bot must be prevented from providing regulated legal or medical advice. SpaCy's Matcher class, ReLM patterns, Eleuther AI’s LM evaluation harness package or the Guardrails-AI "rail" language are also useful tools to help specify general filter rules. Finally, red teaming can help building up a dataset of edge cases efficiently and improve the reliability of NLP pipeline quickly.

<----------section---------->


**FAISS**

### Introduzione
FAISS (Facebook AI Similarity Search) è una libreria open-source sviluppata da Facebook AI Research per l'indicizzazione e la ricerca veloce di vettori ad alta dimensione. Il suo obiettivo principale è rendere più efficiente la ricerca della similarità tra vettori, un'operazione cruciale in molte applicazioni di intelligenza artificiale, come il recupero di informazioni, il riconoscimento delle immagini e i sistemi di raccomandazione.

### Perché FAISS è importante?
Nei moderni sistemi di AI, si lavora spesso con enormi dataset di vettori, specialmente nell'ambito del deep learning. La ricerca esatta della similarità in uno spazio vettoriale può essere computazionalmente proibitiva. FAISS fornisce algoritmi e strutture dati ottimizzate per eseguire ricerche approssimate in modo molto più veloce rispetto a un approccio naïve basato su ricerche brute-force.

### Embedding e Similarità nell'NLP
FAISS lavora principalmente con **embedding**, rappresentazioni vettoriali di oggetti come testi, immagini o segnali audio. Nel contesto del **Natural Language Processing (NLP)**, gli embedding sono vettori numerici che catturano la semantica delle parole, frasi o documenti. 

#### Generazione degli embedding
Nell'NLP, gli embedding sono generati attraverso modelli di apprendimento automatico come:
- **Word2Vec**: rappresenta le parole in base al loro contesto, utilizzando tecniche come Continuous Bag of Words (CBOW) e Skip-gram.
- **GloVe**: costruisce embedding sulla base della co-occorrenza delle parole in un grande corpus testuale.
- **FastText**: un'estensione di Word2Vec che considera anche i sottotokens delle parole, utile per lingue con morfologia complessa.
- **BERT e Transformer-based models**: generano embedding contestualizzati che variano in base alla frase in cui la parola appare.
- **Sentence Transformers**: creano embedding per intere frasi, migliorando la ricerca semantica e la similarità testuale.

#### Metriche di Similarità
Una volta ottenuti gli embedding, FAISS permette di confrontarli utilizzando diverse metriche di similarità:
- **Similarità coseno**: misura l'angolo tra due vettori e viene ampiamente utilizzata per valutare la similarità semantica tra testi.
- **Distanza euclidea (L2)**: meno comune nell'NLP, utile quando gli embedding hanno distribuzioni spaziali significative.
- **Prodotto scalare (dot product)**: spesso usato nei modelli neurali per valutare l'affinità tra vettori.

#### Applicazioni nell'NLP
FAISS è particolarmente utile in molteplici applicazioni NLP, tra cui:
- **Recupero di informazioni**: aiuta a trovare documenti o frasi simili in grandi dataset, come nella ricerca semantica.
- **Risoluzione della coreferenza**: identifica entità simili in un testo, associando riferimenti diversi a uno stesso concetto.
- **Sistemi di raccomandazione di testi**: suggerisce articoli, post o libri basandosi sulla similarità degli embedding.
- **Clustering e analisi dei topic**: raggruppa documenti con contenuti simili, utile per la categorizzazione automatica.
- **Traduzione automatica e allineamento testuale**: confronta frasi in lingue diverse per trovare corrispondenze tra segmenti di testo.

### Creazione e uso di un FAISS Index nell'NLP
FAISS fornisce API in Python per la gestione efficiente degli embedding testuali. Ecco un esempio per indicizzare e cercare frasi simili:

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Modello per generare embedding testuali
model = SentenceTransformer('all-MiniLM-L6-v2')

documents = [
    "Il gatto salta sul tavolo",
    "Un felino è balzato sulla superficie",
    "Oggi il tempo è splendido",
    "La partita di calcio è stata emozionante"
]

# Creazione degli embedding
d = 384  # Dimensione del modello MiniLM
embeddings = np.array(model.encode(documents), dtype='float32')

# Creazione e popolamento dell'indice FAISS
index = faiss.IndexFlatL2(d)
index.add(embeddings)

# Query per trovare frasi simili
query_text = "Il micio è saltato sul mobile"
query_embedding = np.array([model.encode(query_text)], dtype='float32')
k = 2  # Numero di risultati

distances, indices = index.search(query_embedding, k)

# Output dei risultati
print("Frasi più simili:")
for i in indices[0]:
    print(documents[i])
```

### Vantaggi di FAISS nell'NLP
- **Velocità di ricerca**: consente di trovare frasi simili in dataset di milioni di documenti in pochi millisecondi.
- **Scalabilità**: supporta grandi volumi di dati, ideale per motori di ricerca semantica e assistenti virtuali.
- **Flessibilità**: utilizzabile con vari modelli di embedding, adattandosi a diversi casi d'uso.

### Conclusione
FAISS è un potente strumento per la ricerca di similarità in embedding testuali, rendendo possibili applicazioni avanzate nell'NLP. Grazie alla sua efficienza, è usato in motori di ricerca semantica, chatbot e sistemi di raccomandazione testuale. L'integrazione con modelli come BERT e Sentence Transformers lo rende una scelta eccellente per chi lavora con grandi corpus di testo.

<----------section---------->
<----------section---------->

**NATURAL LANGUAGE PROCESSING AND LARGE LANGUAGE MODELS**

### General Information
- **Program of Study:** Computer Engineering  
- **Track:** Artificial Intelligence and Intelligent Robotics  
- **Course Type:** Master’s Degree  
- **Academic Year:** 2024/2025  
- **Course Year:** 2nd Year  
- **Educational Activity Type:** Elective  
- **Field:** Elective  
- **Language:** English  
- **Credits:** 6 CFU  
- **Teaching Activity Type:** Lecture, Laboratory  
- **Exam Type:** Written and oral exam with a single grade  
- **Assessment:** Final Grade  
- **Teaching Period:** First Semester (01/10/2024 – 15/12/2024)  
- **Instructors:** Nicola Capuano, Antonio Greco  
- **Duration:** 48 hours (24 hours lectures, 24 hours lab sessions)  
- **Scientific-Disciplinary Sector:** ING-INF/05  
- **Location:** University of Salerno - Fisciano  

### Learning Objectives
The course provides theoretical, methodological, technological, and practical knowledge on natural language understanding and text processing. It introduces the innovative paradigms of Large Language Models (LLMs) within the general framework of Natural Language Processing (NLP), highlighting their numerous modern applications.  

### Knowledge and Understanding
- Fundamental concepts of NLP systems  
- Standard language models  
- Transformer-based LLMs  
- NLP applications using LLMs  
- Prompt engineering and fine-tuning LLMs  

### Applying Knowledge and Understanding
- Design and implementation of NLP systems using LLMs, effectively integrating existing technologies and optimizing configuration parameters  

### Prerequisites
- **Prerequisite Exam:** Machine Learning  

## Course Content

### **Module 1: Fundamentals of Natural Language Processing** (10 hours lecture, 6 hours exercises)
1. **Introduction to NLP:** Basic concepts, tasks, evolution, and applications (2 hours lecture)  
2. **Text Representation:** Tokenization, stemming, lemmatization, bag of words, n-grams, similarity measures, word embeddings (2 hours lecture)  
3. **TF-IDF and Classification:** TF-IDF vectors, text classification, and clustering (2 hours lecture)  
4. **Neural Networks for Text Analysis:** CNNs, recurrent networks, LSTMs (2 hours lecture)  
5. **Implementation:** Developing a text classifier (2 hours exercises)  
6. **Information Extraction:** Named Entity Recognition (NER), Question Answering (2 hours lecture)  
7. **Chatbot Development:** Using Python and SpaCy/RASA (4 hours exercises)  

### **Module 2: Transformers** (6 hours lecture, 10 hours exercises)
1. **Core Concepts:** Self-attention, multi-head attention, positional encoding, masking (2 hours lecture)  
2. **Transformer Architectures:** Encoder and decoder (2 hours lecture)  
3. **Practical Implementation:** Introduction to Hugging Face (2 hours exercises)  
4. **Applications:** Encoder-decoder models for translation and summarization (2 hours exercises)  
5. **Encoder-only Models:** Sentence classification, Named Entity Recognition (NER) (2 hours exercises)  
6. **Decoder-only Models:** Text generation (2 hours exercises)  
7. **LLM Definition:** Defining and training an LLM (2 hours lecture)  
8. **LLM Training:** Hands-on training (2 hours exercises)  

### **Module 3: Prompt Engineering** (2 hours lecture, 4 hours exercises)
1. **Techniques:** Zero-shot, few-shot, chain-of-thought prompting, self-consistency, generated knowledge, prompt chaining, ReAct, Retrieval-Augmented Generation (RAG) (2 hours lecture)  
2. **Exercises:** Basic prompting techniques (2 hours)  
3. **Advanced Techniques:** Exercises on RAG and LangChain (2 hours)  

### **Module 4: Fine-Tuning LLMs** (4 hours lecture, 4 hours exercises, 2 hours lab)
1. **Fine-Tuning Methods:** Feature-based tuning, updating output layers, Parameter-Efficient Tuning (PEFT), Low-Rank Adaptation (LoRA) (2 hours lecture)  
2. **Hands-on Fine-Tuning:** Practical exercises (2 hours exercises)  
3. **Reinforcement Learning with Human Feedback (RLHF):** Theoretical overview (2 hours lecture)  
4. **RLHF Practical Exercises:** Application and experimentation (2 hours exercises)  
5. **Final Project:** Application of learned techniques (2 hours lab)  

## Teaching Methods
The course includes lectures and in-class exercises. Lectures provide fundamental knowledge on advanced text representation, analysis, and classification techniques using LLMs. Exercises focus on applying these techniques to develop tools for text classification, analysis, and question answering. Attendance is mandatory, with a minimum requirement of 70% to access the exam. Attendance is tracked through the university's EasyBadge system.  

## Assessment
The exam consists of a group project and an oral test:  
- **Project:** Students will critically apply methodologies learned during the course to a practical case.  
- **Oral Test:** Evaluation of theoretical knowledge, project design choices, and answers to specific topics covered in the lectures.  
The final grade is the average of both components.  

## Recommended Texts
- **Reference Book:**  
  H. Lane, C. Howard, H. M. Hapke: *Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python*, Manning.  
- **Supplementary Materials:** Available on the university's e-learning platform ([https://elearning.unisa.it](https://elearning.unisa.it)), accessible to course students using their university credentials.


<----------section---------->

﻿NICOLA CAPUANO 
Nicola Capuano is an Associate Professor at the Department of Information and Electrical 
Engineering and Applied Mathematics (DIEM) at the University of Salerno. He obtained his 
degree in Computer Science and his Ph.D. in Computer Science and Computer Engineering 
from the University of Salerno. At the same university, he also held a four-year research 
fellowship on the topic of "Artificial Intelligence." Before pursuing an academic career, he 
collaborated with private research institutes, including the Center for Research in Pure and 
Applied Mathematics and the Center of Excellence for Software Technology. He also served as a 
researcher at the School of Engineering of the University of Basilicata. In 2021, he achieved the 
National Scientific Qualification as a Full Professor in the field 09/H1: Information Processing 
Systems. 
His research focuses on Natural Language Processing, Machine Learning, Knowledge 
Representation, Fuzzy Systems, and Artificial Intelligence in Education. He is the author of more 
than 120 publications, including journal articles, conference proceedings, and book chapters. 
He serves as an Associate Editor for the Journal of Ambient Intelligence and Humanized 
Computing by Springer Nature and Frontiers in Artificial Intelligence by Frontiers Media. He has 
been a Guest Editor for several journals, including the International Journal of Educational 
Technology in Higher Education and the International Journal of Emerging Technologies in 
Learning. He is a member of editorial boards and a reviewer for numerous journals, a track 
chair, and a program committee member for international conferences and workshops. He is 
also a member of the executive committee for the international conference The Learning Ideas. 
He edited the volume The Learning Grid Handbook, published by IOS Press. 
Nicola Capuano acts as an independent evaluator of projects and proposals for the European 
Commission under the Horizon Europe program and for the European Institute of Innovation 
and Technology. He coordinated the projects Diogene (A Training Web Broker for ICT 
Professionals) and InTraServ (Intelligent Training Service for Management Training in SMEs), both 
funded by the European Commission under the Fifth Framework Programme. He was a member 
of the European Network of Excellence Kaleidoscope (Concepts and Methods for Exploring the 
Future of e-Learning with Digital Technologies), where he coordinated the Special Interest 
Group on Learning Grid. He led the research line “Intelligent Features for Learning” at the Center 
of Excellence in Methods and Systems for Learning and Knowledge at the University of Salerno. 
He has held scientific and coordination roles in several other research and innovation projects. 
He is a Project Management Professional (PMP) certified by the Project Management Institute. 

<----------section---------->

ANTONIO GRECO 
Antonio Greco graduated with honors in Computer Engineering in 2014 from the University of 
Salerno (Italy). In March 2018, he earned a Ph.D. in Computer Science and Information 
Engineering from the same university. In March 2020, he became an RTD/A Researcher (SSD 
ING-INF/05 "Information Processing Systems") at the Department of Information and Electrical 
Engineering and Applied Mathematics (DIEM) at the University of Salerno, where he has been an 
RTD/B Researcher since November 2022. Within the same department, he has served as 
Delegate for Student Orientation since November 2022 and has been a member of the Ph.D. 
Board in Information Engineering since May 2024. 
For the department, he has taught various courses, including Logic Networks (SSD ING-INF/05, 
Bachelor's Degree in Computer Engineering), Autonomous Vehicle Driving (SSD ING-INF/05, 
Master's Degree in Computer Engineering), Web Software Technologies (SSD ING-INF/05, 
Bachelor's Degree in Computer Engineering), Artificial Intelligence for Cybersecurity (SSD ING
INF/05, Master's Degree in Computer Engineering), Robotics for E-Health (SSD ING-INF/05, 
Master's Degree in Digital Health and Bioinformatics Engineering), System and Network Security 
(SSD ING-INF/05, Master's Degree in Computer Engineering), Artificial Vision (SSD ING-INF/05, 
Master's Degree in Computer Engineering), and Natural Language Processing and Large 
Language Models (SSD ING-INF/05, Master's Degree in Computer Engineering). Since the 
2021/2022 academic year, he has also taught the course Advanced Machine Learning as part of 
the accredited Ph.D. program in Information Engineering (D.M. 226/2021). 
Since 2014, he has been a member of the MIVIA Lab (Machines for Intelligent Video, Image, and 
Audio recognition) at the University of Salerno’s DIEM, a group with a strong focus on 
international collaborations. His research primarily focuses on Computer Vision and Pattern 
Recognition, specifically on the design, implementation, and optimization of real-time 
computer vision and deep learning algorithms (e.g., gender recognition, age estimation, 
ethnicity recognition, emotion analysis, fire detection, anomaly detection, people counting, 
object tracking, and audio event recognition) for data acquired from static devices (smart 
cameras, microphones) or moving devices (drones, robots, autonomous vehicles). These 
activities are often conducted in collaboration with European research groups, particularly at 
the University of Malta, the University of Groningen (Netherlands), and the University of Twente 
(Netherlands), where he has spent a total of nine months as a Visiting Researcher (January
April 2020 and August 2021–February 2022). At Twente, he collaborated with the Data 
Management and Biometrics Group within the Faculty of Electrical Engineering, Mathematics, 
and Computer Science. 
In recent years, he has organized several Special Issues for international journals, including an 
issue of the Journal of Ambient Intelligence and Humanized Computing titled “Ambient 
Understanding for Mobile Autonomous Robots (AutoRob)," an issue of Pattern Recognition on 
“From Bench to the Wild: Recent Advances in Computer Vision Methods (WILD-VISION)," and 
an issue of Pattern Analysis and Applications on “Pedestrian Attribute Recognition and Person 
Re-Identification." As of March 2024, he is an Associate Editor for Pattern Analysis and 
Applications and serves as a reviewer for over 25 international journals. 
Antonio Greco has presented his scientific work at more than ten international conferences and 
has served as Contest Chair at various events. Notably, he organized the Guess the Age (GTA) 
Contest 2021 at the International Conference on Computer Analysis of Images and Patterns 
(CAIP), co-organized the ONFIRE 2023 contest at the International Conference on Image 
Analysis and Processing (ICIAP), and co-organized the Pedestrian Attributes Recognition (PAR) 
Contest 2023 at CAIP. Additionally, he served on the Local Committee for the International 
Workshop on Graph-based Representations (GBR) in Capri, Italy (May 16–18), the International 
Conference on Computer Analysis of Images and Patterns (CAIP) in Salerno, Italy (September 2
6, 2019), the IEEE Conference on Cognitive and Computational Aspects of Situation 
Management (CogSIMA) in Salerno, Italy (June 6–10, 2022), and the GBR Workshop in Salerno 
(September 6–8). In 2021, he was an Invited Speaker at the CogSIMA Challenge Problems 
Workshop at the IEEE CogSIMA Conference, delivering a talk titled “Adding Awareness to AI 
Systems.” 
In national and international research projects, he has served as the scientific lead of the local 
research unit at DIEM for the PON ARS01_01226 project PerMedNet – Personalized Medicine for 
Innovative Strategies in Neuropsychiatric and Vascular Diseases. He also led work packages 
WP4 and WP5 in the European research project Flexible Assembly Manufacturing with Human
Robot Collaboration and Digital Twin Models (FELICE) (Grant Agreement ID: 101017151, funded 
under H2020-EU.2.1.1). Additionally, he was the scientific lead for DIEM in a research contract 
with the company RED&BLUE for the project Development of a Platform for Evaluating AI 
System Safety. 
In the field of technology transfer, Antonio Greco co-founded AI-READY, a spin-off of the 
University of Salerno specializing in AI applications for cognitive robotics, mobility, and 
autonomous vehicles, in December 2019.

<----------section---------->

