## LESSON 1 ##

## What is Natural Language Processing?

### NLP in the Press

NLP is rapidly gaining importance, evidenced by its frequent mentions in the media. Several news articles underscore the rising influence and potential impact of NLP technologies:

*   **"New powerful AI bot creates angst among users: Are robots ready to take our jobs?"** This highlights the anxiety surrounding the automation potential of AI and NLP.
*   **"A Smarter Robot: A new chatbot shows rapid advances in artificial intelligence."** - *The New York Times*. This points to the advancements in AI, particularly in creating more sophisticated chatbots.
*   **"What is ChatGPT, the viral social media AI?"** - *The Washington Post*. This introduces ChatGPT, a widely discussed AI model that has garnered significant attention on social media.
*   **"This AI chatbot is dominating social media with its frighteningly good essays."** - *CNN*. This highlights the impressive text generation capabilities of AI chatbots, particularly in essay writing.
*   **"ChatGPT may be coming for our jobs. Here are the 10 roles that AI is most likely to replace."** - *Business Insider*. This article discusses the potential job displacement due to AI advancements, specifically naming ten roles most at risk.
*   **"Microsoft co-founder Bill Gates: ChatGPT ‘will change our world’"** - *Reuters*. This emphasizes the transformative potential of ChatGPT, as recognized by influential figures like Bill Gates.

<----------section---------->

### Importance of NLP

The significance of NLP is further emphasized by quotes from prominent figures:

*   **"Natural language is the most important part of Artificial Intelligence"** - *John Searle, Philosopher*. This statement emphasizes the central role of natural language understanding in achieving true artificial intelligence.
*   **"Natural language processing is a cornerstone of artificial intelligence, allowing computers to read and understand human language, as well as to produce and recognize speech"** - *Ginni Rometty, IBM CEO*. This quote highlights NLP as a fundamental technology enabling computers to interact with human language in various forms.
*   **"Natural language processing is one of the most important fields in artificial intelligence and also one of the most difficult"** - *Dan Jurafsky, Professor of Linguistics and Computer Science at Stanford University*. This acknowledges both the importance and the challenges associated with NLP research and development.

<----------section---------->

### Definitions

Various definitions of NLP highlight its interdisciplinary nature and goals:

*   **"Natural language processing is the set of methods for making human language accessible to computers"** - *Jacob Eisenstein*. This definition focuses on enabling computers to process and understand human language.
*   **"Natural language processing is the field at the intersection of computer science and linguistics"** - *Christopher Manning*. This definition underscores the confluence of computer science and linguistics in the study and development of NLP.
*   **"Make computers to understand natural language to do certain tasks humans can do such as translation, summarization, questions answering"** - *Behrooz Mansouri*. This outlines the objective of NLP: to replicate human language-based tasks using computers, including translation, summarization, and question answering.
*   **"Natural language processing is an area of research in computer science and artificial intelligence concerned with processing natural languages such as English or Mandarin. This processing generally involves translating natural language into data that a computer can use to learn about the world. And this understanding of the world is sometimes used to generate natural language text that reflects that understanding."** - *(Natural Language Processing in Action)*. This comprehensive definition explains the process of converting natural language into a machine-understandable format, enabling computers to learn and generate human-like text.

<----------section---------->

### Natural Language Understanding (NLU)

NLU is a subfield of NLP dedicated to enabling machines to understand human language.

*   It focuses on extracting meaning, context, and intent from text, going beyond mere keyword recognition.
*   Text is transformed into a numerical representation called an embedding, allowing mathematical operations and comparisons. Embeddings capture semantic relationships between words and phrases.

**Who uses Embeddings:**

*   **Search Engines:** Use embeddings to interpret the underlying meaning of search queries, allowing for more accurate and relevant search results. For example, understanding that "best phone under $500" is semantically similar to "top-rated smartphone less than 500 dollars."
*   **Email Clients:** Utilize embeddings to detect spam and classify emails by importance, filtering out unwanted messages and prioritizing important correspondence.
*   **Social Media:** Employ embeddings to moderate posts by identifying hate speech, offensive content, and misinformation. They also help understand user sentiment towards specific topics or brands.
*   **CRM Tools:** Analyze customer inquiries and route them to the appropriate departments by understanding the context and intent of the message. For instance, a question about billing is routed to the finance department.
*   **Recommender Systems:** Suggest articles, products, or content to users based on their past behavior and preferences. Embeddings help identify items with similar semantic content.

<----------section---------->

### Natural Language Generation (NLG)

NLG is the complementary subfield of NLP focused on creating human-like text.

*   It involves generating coherent, contextually appropriate text that is often indistinguishable from text written by a human.
*   NLG relies on numerical representations of meaning and sentiment to produce text that effectively conveys the intended message.

**Applications:**

*   **Machine Translation:** Converts text from one language to another, enabling cross-lingual communication. Examples include Google Translate and DeepL.
*   **Text Summarization:** Creates concise summaries of long documents, preserving key information. This is useful for quickly understanding the main points of articles, reports, and books.
*   **Dialogue Processing:** Powers chatbots and virtual assistants by providing relevant and context-aware responses in conversations. This allows for more natural and engaging interactions.
*   **Content Creation:** Generates articles, reports, stories, poetry, and other forms of creative writing. This can assist writers in brainstorming ideas, creating drafts, and automating content production.

<----------section---------->

### Example: Conversational Agents

Conversational agents (chatbots, virtual assistants) integrate several NLP components to simulate human conversation:

*   **Speech recognition:** Converts spoken language into text.
*   **Language analysis:** Analyzes the structure and meaning of the text.
*   **Dialogue processing:** Manages the flow of the conversation and determines appropriate responses.
*   **Information retrieval:** Accesses and retrieves relevant information to answer user queries.
*   **Text to speech:** Converts text responses into spoken language.

**Example conversation:**

*   User: "Open the pod bay doors, Hal."
*   Hal: "I’m sorry, Dave, I’m afraid I can’t do that."
*   User: "What are you talking about, Hal?"
*   Hal: "I know that you and Frank were planning to disconnect me, and I'm afraid that's something I cannot allow to happen."

This exchange from *2001: A Space Odyssey* illustrates the capabilities and potential limitations of conversational agents, highlighting issues of intent recognition and autonomous decision-making.

<----------section---------->

### Conversational Agents in Movies

Conversational agents are frequently depicted in movies, reflecting the ongoing fascination with creating machines that can communicate and interact like humans. These cinematic representations often explore both the benefits and the potential risks associated with advanced AI.

<----------section---------->

### NLP is Hard

The complexities of NLP arise from the inherent ambiguities in human language:

"I made her duck… what does it mean?"

*   **Duck:** Does "duck" refer to the waterfowl (noun) or the action of lowering one's head (verb)?
*   **Make:** Does "make" mean to cook (prepare food) or to cause someone to do something?
*   **Her:** Does "her" indicate something belonging to her or something done for her?

**Possible meanings:**

*   I cooked waterfowl for her.
*   I cooked waterfowl belonging to her.
*   I created the (plaster?) duck she owns.
*   I caused her to quickly lower her head or body.
*   I waved my magic wand and turned her into undifferentiated waterfowl. (Humorous interpretation)

This example demonstrates the multiple layers of interpretation that can be applied to even a short sentence.

<----------section---------->

### Ambiguity

Natural language is inherently ambiguous, leading to challenges in NLP.

*   **One input can mean many different things:** A single sentence or phrase can have multiple valid interpretations depending on context and understanding.
*   **Many inputs can mean the same thing:** Different sentences or phrases can convey the same meaning, making it difficult for machines to recognize semantic equivalence.

**Levels of ambiguity:**

*   **Lexical ambiguity:** Arises from the multiple meanings of words. For example, the word "bank" can refer to a financial institution or the side of a river.
*   **Syntactic ambiguity:** Occurs when a sentence can be parsed in multiple ways, leading to different interpretations. For example, "I saw the man on the hill with a telescope."
*   **Interpreting partial information:** Involves understanding the reference of pronouns and other incomplete information. For example, in the sentence "John told Bill that he was wrong," it is unclear whether "he" refers to John or Bill.
*   **Contextual information:** Context significantly impacts the meaning of a sentence. The same sentence can have different interpretations based on the surrounding text and the overall situation.

<----------section---------->

### Ambiguity Examples

*   "I saw bats… ?" This could mean seeing the animal or seeing baseball bats.
*   "Call me a cab… ?" This could be a request to phone a taxi, or mean something else entirely.

<----------section---------->

### NLP and Linguistics

NLP techniques draw upon various aspects of linguistics to better understand and process human language:

*   **Phonetics:** Understanding the physical sounds of speech and how they are produced and perceived. This is important for speech recognition and synthesis.
*   **Morphology:** Knowledge of the structure and formation of words, including their meaningful components (morphemes). This helps in tasks such as stemming and lemmatization.
*   **Syntax:** Understanding the rules and structures governing the arrangement of words in sentences. This is essential for parsing and grammatical analysis.
*   **Semantics:** Insight into the meaning of words, phrases, and sentences. This allows for a deeper understanding of the content and intent of the text.
*   **Pragmatics:** Understanding how context influences the interpretation of meaning. This enables machines to go beyond literal meanings and understand implied or intended messages.

<----------section---------->

### NLP vs Linguistics

**Linguistics:**

*   Primarily focused on the study of language itself.
*   Explores the structure, meaning, and use of language from a theoretical perspective.
*   May employ computational methods and tools as part of computational linguistics, but the primary goal is linguistic analysis and understanding.

**NLP:**

*   Focused on creating computational capabilities that utilize human language.
*   Designs and implements algorithms to understand and generate human language for practical applications.
*   Applies results from linguistics to develop working systems for tasks such as machine translation, speech recognition, and text summarization.

<----------section---------->

## Applications of Natural Language Processing

### NLP Killer Applications

Key applications of NLP that have demonstrated significant impact and potential include:

*   **Language translation:** Automatically translating text or speech from one language to another.
*   **Email smart filtering:** Classifying emails to filter spam or prioritize important messages.
*   **Smart assistant:** Providing virtual assistants capable of understanding and responding to user requests.
*   **Sentiment analysis:** Determining the emotional tone or subjective attitude expressed in a text.
*   **Document analysis:** Extracting key information from documents, such as dates, names, and entities.
*   **Chatbots:** Engaging in conversations with users for customer service, information retrieval, or entertainment.
*   **Semantic searches:** Providing search results based on the meaning and context of the query, rather than just keyword matching.
*   **Automatic summarization:** Creating concise summaries of long documents.

<----------section---------->

### Applications by Business Sector

NLP is utilized across various business sectors to improve efficiency, enhance customer experience, and gain valuable insights:

*   **Healthcare:**
    *   Process and interpret patient data, including medical records, to assist in diagnosis, treatment plans, and patient care. NLP helps extract relevant information from unstructured clinical notes.
    *   Extract information from unstructured data such as doctor's notes, research papers, and patient feedback to identify trends and patterns.
*   **Finance:**
    *   Analyze market sentiment to better manage risk and detect fraudulent activities. NLP algorithms can analyze news articles, social media posts, and financial reports to gauge market sentiment.
    *   Generate insights from financial reports and news, identifying key trends and anomalies.
*   **E-commerce and Retail:**
    *   Provide personalized recommendations based on customer behavior and preferences. NLP helps analyze customer reviews and product descriptions to match products to individual needs.
    *   Improve search functionalities by understanding the context and intent of user queries.
    *   Implement customer service chatbots to answer frequently asked questions and resolve issues.
    *   Employ sentiment analysis to gauge customer satisfaction and identify market trends.
*   **Legal:**
    *   Automate document analysis, aiding in legal research. NLP can quickly identify relevant case law and statutes.
    *   Streamline the review process for contracts and legal documentation by extracting key clauses and identifying potential risks.
*   **Customer Service:**
    *   Automate responses to customer inquiries using chatbots, improving efficiency and reducing wait times.
    *   Guide users through troubleshooting steps and provide helpful information.
    *   Analyze customer feedback to identify areas for improvement and address common concerns.
*   **Education:**
    *   Automate grading of essays and other written assignments, providing personalized feedback to students.
    *   Provide learning tools such as language translation and vocabulary assistance.
    *   Summarize and generate educational materials, creating study guides and other resources.
*   **Automotive:**
    *   Develop intelligent navigation systems that understand voice commands and provide real-time traffic updates.
    *   Implement voice-activated controls for music, climate, and other vehicle functions.
*   **Technology:**
    *   Assist in software development by generating code snippets and completing code. AI code assistants powered by NLP suggest code based on natural language descriptions.
    *   Enhance code quality through automated reviews and suggestions, identifying potential bugs and improving readability.
*   **Media and Entertainment:**
    *   Assist in generating scripts, articles, and creative writing, helping writers overcome writer's block and produce high-quality content.
    *   Enhance user engagement with interactive storytelling and personalized media experiences.

<----------section---------->

### Many Other Applications…

NLP is pervasive and its applications are surprisingly diverse. A search engine provides more relevant results by indexing web pages and documents in a way that takes meaning into account. Autocomplete uses NLP to predict and complete user input in search engines and mobile keyboards. Spelling correctors, grammar checkers, and style coaches in word processors and browser plugins use NLP to improve writing quality. Chatbots use NLP-driven natural language search to find appropriate responses.

NLP pipelines that generate text are used to compose short replies in chatbots and virtual assistants, and to assemble longer passages of text. The Associated Press uses NLP “robot journalists” to write financial news articles and sporting event reports. Bots compose weather forecasts resembling human-generated forecasts.

NLP spam filters in email programs helped email surpass telephone and fax in the '90s. Spam filters retain their edge in the email cat-and-mouse game, but may be losing in social networks. An estimated 20% of tweets about the 2016 US presidential election were composed by chatbots.

The following table categorizes NLP applications:

| Category          | Application                                                          |
| :---------------- | :------------------------------------------------------------------- |
| Search            | Web Documents, Autocomplete                                          |
| Editing           | Spelling, Grammar, Style                                             |
| Dialog            | Chatbot, Assistant, Scheduling                                       |
| Writing           | Index, Concordance, Table of contents                                |
| Email             | Spam filter, Classification, Prioritization                            |
| Text mining       | Summarization, Knowledge extraction, Medical diagnoses                 |
| Law               | Legal inference, Precedent search, Subpoena classification             |
| News              | Event detection, Fact checking, Headline composition                    |
| Attribution       | Plagiarism detection, Literary forensics, Style coaching               |
| Sentiment analysis | Community morale monitoring, Product review triage, Customer care        |
| Behavior prediction | Finance, Election forecasting, Marketing                              |
| Creative writing  | Movie scripts, Poetry, Song lyrics                                    |

<----------section---------->

### Hype Cycle

The Gartner Hype Cycle for Emerging Technologies (2023) positions NLP and related technologies within the innovation lifecycle, including:

*   API-Centric SaaS
*   Generative AI
*   AI TRiSM
*   WebAssembly (Wasm)
*   Federated Machine Learning
*   Industry Cloud Platforms
*   Internal Developer Portal
*   Cloud Sustainability
*   Homomorphic Encryption
*   Value Stream Management Platforms
*   Reinforcement Learning
*   Software Engineering
*   Cloud Development Environments
*   Graph Data Science
*   AI Simulation
*   Causal AI
*   Postquantum Cryptography
*   Neuro-Symbolic AI
*   Augmented FinOps
*   Generative Cybersecurity AI
*   Cybersecurity
*   Mesh Architecture

The cycle depicts stages from Innovation Trigger to Peak of Inflated Expectations, Trough of Disillusionment, Slope of Enlightenment, and finally, Plateau of Productivity. The estimated time to reach the plateau varies for each technology. Understanding this cycle helps manage expectations and plan for the adoption of these technologies.

<----------section---------->

### NLP Market

NLP presents a promising career path due to:

*   Growing demand for NLP applications across various industries.
*   Projected employment growth of 22% between 2020 and 2030, indicating a strong job market.

The global NLP market is projected to increase from USD 18.9 billion in 2023 to USD 61.8 billion in 2028.

Key geographic regions include:

*   North America
*   Europe
*   Asia Pacific
*   Middle East & Africa
*   Latin America

This growth is driven by increasing adoption of NLP technologies in diverse applications, underscoring the field's economic potential.

<----------section---------->

## History of NLP

### First Steps of NLP

NLP has experienced periods of rapid advancement and relative stagnation, reflecting the challenges and evolving approaches in the field.

*   Its history is influenced by the growth of computational resources and shifts in research methodologies.

**1950's and 1960's**

*   Machine translation was the initial application that sparked interest in NLP, driven by the Cold War and the desire to automatically translate documents.
*   The first machine translation systems used dictionary lookup and basic word order rules to produce translations. These systems quickly faced limitations due to the complexity and ambiguity of language.
*   The 1950s saw considerable excitement and optimism, with researchers predicting that machine translation could be solved within a few years.

<----------section---------->

### Machine Translation in 50s

**Example:**

Given:

```
Dictionary: Red -> Rosso
            House -> Casa
```

Translate:

```
The red house -> Il rosso casa   (incorrect)
```

But it should be:

```
La casa rossa (correct)
```

Dictionary lookup alone is insufficient to deal with syntactical differences between languages, such as adjective-noun order. This example highlights the need for more sophisticated methods beyond simple word-for-word translation.

<----------section---------->

### How to deal with language ambiguity?

This question became central to NLP research, prompting investigations into linguistic structures and computational methods.

<----------section---------->

### Generative Grammars

**1957: Chomsky’s Generative Grammar**

*   Noam Chomsky's theory of generative grammar proposed a system of rules for generating all possible grammatically correct sentences in a language.
*   This enabled prediction of grammatical correctness, providing a formal framework for understanding language structure.
*   Chomsky's work heavily influenced research in machine translation and other NLP tasks.

**1966: The Reality Check**

*   Early translation systems failed to meet expectations due to their inability to handle the ambiguity and complexity of natural language.
*   This led to a reassessment of the feasibility of machine translation and a shift in research focus.

<----------section---------->

### ALPAC Report

**Automatic Language Processing Advisory Committee**

*   Established to assess advancements in computational linguistics and machine translation.
*   The 1966 ALPAC report recommended halting research into machine translation due to the lack of significant progress and cost-effectiveness.
*   The report suggested shifting the focus from developing end-to-end machine translation systems to enhancing tools that assist human translators.
*   The ALPAC report had a significant impact on NLP and AI research, contributing to the first AI winter, a period of reduced funding and interest in the field.

[https://www.mt-archive.net/50/ALPAC-1966.pdf](https://www.mt-archive.net/50/ALPAC-1966.pdf)

<----------section---------->

### ELIZA

A pioneering conversational agent.

*   Created by Joseph Weizenbaum in the 1960s at MIT.
*   Designed to simulate a conversation between a psychotherapist and a patient, using simple pattern matching and response generation techniques.

**Features and Limitations:**

*   Demonstrated the potential of computer-based conversation, showing that even simple programs could create the illusion of understanding.
*   Utilized pattern matching and substitution to generate responses based on keywords and phrases in the user's input.
*   Limited in its ability to handle complex conversations or maintain context beyond a few exchanges. It lacked genuine understanding of the topics being discussed.
*   Often produced irrelevant or repetitive responses, highlighting the limitations of simple pattern-based approaches.

[https://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm](https://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm)

<----------section---------->

### The Turing Test

"I propose to consider the question: can machines think? ... We can only see a short distance ahead, but we can see plenty there that needs to be done" - *Alan Turing, Computing Machinery and Intelligence, 1950*

The Turing Test, proposed by Alan Turing in his 1950 paper, is a benchmark for evaluating a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.

**Turing Test aka The Imitation game:**

*   A human, a computer, and an interrogator are placed in separate rooms and communicate via written messages.
*   The interrogator must distinguish between the human and the machine based on their responses.

<----------section---------->

### The Turing Test

**Capabilities for passing the Turing Test**

*   **Natural Language Understanding:** Ability to interpret user input and extract meaning.
*   **Knowledge Representation:** Ability to store and access relevant information.
*   **Automated Reasoning:** Ability to generate appropriate and logical responses based on stored knowledge.
*   **Natural Language Generation:** Ability to produce human-like textual responses.
*   **Context Management:** Ability to maintain and utilize context across multiple exchanges in a conversation.
*   **Adaptability and Learning:** Ability to adapt responses based on user behavior and feedback.

<----------section---------->

### The Turing Test

**Successes with Turing test**

*   A (controversial) success in 2014: a chatbot mimicking the answer of a 13 years old boy managed to fool judges.
*   Since then, other (controversial) successes have been claimed.

**Limitations of Turing Test**

*   Not reproducible: The results can be subjective and vary depending on the judges and the specific interactions.
*   Is emulating humans necessary for achieving intelligence? Some argue that intelligence should be measured by the ability to solve problems, not by mimicking human behavior.
*   Many AI researchers have shifted focus to other benchmarks that are more measurable and objective.
*   Less commonly used today as a primary measure of AI progress.

<----------section---------->

### Raise of Symbolic Approaches

**1970's and 1980's:**

*   Programmers started creating structured representations of real-world information for computer understanding, leading to the development of ontologies.
*   Complex rule-based systems were developed for various NLP tasks, including parsing, morphology, semantics, and reference resolution.

**Main applications were:**

*   **Expert Systems:** Mimicked human expertise in specific domains, using rule-based reasoning to solve problems and provide advice.
*   **Information Retrieval:** Enhanced search and data extraction by using structured representations and rule-based systems to understand and process queries.

**Main limitations were:**

*   **Flexibility:** Challenges in adapting to new or ambiguous contexts, as the systems were limited by their predefined rules.
*   **Scalability:** Difficulty handling large-scale or diverse data, as the rule-based systems were often complex and time-consuming to develop and maintain.

<----------section---------->

### Statistical Revolution

**1990's:**

*   The computing power increased substantially, allowing for the development and training of more complex statistical models.
*   Statistical models with simple representations started to outperform complex hand-coded linguistic rules in many NLP tasks.
*   These models learn patterns from data, enabling them to handle variations and complexities in natural language more effectively.
*   Large corpora became essential for training statistical models, providing the data needed to learn accurate patterns and relationships.
*   Long Short-Term Memory (LSTM) networks was invented by Hochreiter and Schmidhuber in 1997, providing a powerful tool for modeling sequential data and capturing long-range dependencies.

*"Whenever I fire a linguist, our machine translation performance improves" - Fred Jelinek, IBM*

This quote reflects the shift from rule-based linguistic approaches to data-driven statistical methods in NLP.

<----------section---------->

### Advances in NLP

**2000's**

*   Increased Use of Neural Networks: Neural networks began to gain traction in NLP, offering improved performance in various tasks.
*   Introduction of Word Embeddings: Words are represented as dense vectors of numbers, capturing semantic relationships and contextual information.
    *   Words with similar meanings are associated with similar vectors, allowing for mathematical operations and comparisons.
    *   Early algorithms struggled to efficiently learn these representations, limiting the scalability and effectiveness of word embeddings.

**2006: launch of Google Translate**

*   The first commercially successful NLP system that demonstrates the potential of statistical machine translation.
*   Utilized statistical models to automatically translate documents, marking a significant milestone in the field.

<----------section---------->

### Deep Learning Era

**2010's:**

*   LSTM and CNN became widely adopted for NLP, providing powerful tools for modeling sequential data and extracting features from text.
*   The availability of large text corpora enabled the training of increasingly complex models, leading to significant improvements in NLP performance.

**Word2Vec (2013):**

*   Efficient Estimation of Word Representations in Vector Space
*   The first algorithm to efficiently learn word embeddings, revolutionizing the field of NLP and enabling a wide range of applications.
*   Enables semantic operations with word vectors, allowing for tasks such as word similarity and analogy detection.
*   Paved the way for more advanced models such as GloVe, fastText, ELMo, BERT, COLBERT, GPT, ...

<----------section---------->

### Deep Learning Era

**Sequence-to-Sequence Models (2014):**

*   Introduction of the encoder-decoder architecture, providing a framework for modeling sequence-to-sequence transformations.
    *   **Encoder:** Encodes the input sequence into a context vector, capturing the essence of the input.
    *   **Decoder:** Decodes the output sequence from the context vector, generating the desired output.
*   Useful for automatic translation, question answering, text summarization, text generation, ...

**Example:**

```
The red house -> Context vector [0.3, 0.6, -0.2, ..., 0.1] -> La casa rossa
```

The encoder transforms the English sentence into a numerical representation (context vector), and the decoder generates the equivalent Italian sentence from this vector.

<----------section---------->

### Virtual Assistants

A Virtual Assistant performs a range of tasks or services based on user input in natural language.

Many VA were launched in 2010's:

*   **2011:** Siri launched by Apple on iOS devices, popularizing voice-based interaction with mobile devices.
*   **2014:** Cortana introduced by Microsoft for Windows Phone, expanding the reach of virtual assistants to desktop and laptop computers.
*   **2014:** Alexa launched by Amazon with the Echo, pioneering voice-controlled smart home devices and establishing a new market for virtual assistants.
*   **2015:** Google Assistant introduced, integrating voice interaction with Android and Google Home, further expanding the accessibility of virtual assistants.

<----------section---------->

### Deep Learning Era

**Transformer (2017):**

*   **Attention Is All You Need**
*   Integration of attention mechanisms that allows models to focus on relevant parts of the input sequence, improving performance in various NLP tasks.
*   Allows a greater passage of information between the decoder and the encoder, facilitating better context understanding and more accurate generation.
*   Defined and adopted by Google for the translator, establishing a new standard for machine translation and other NLP tasks.
*   It remains the dominant architecture in NLP today, forming the foundation for many state-of-the-art models.

*[Diagram of Transformer Architecture]*

The Transformer architecture, with its attention mechanism, revolutionized the field of NLP by enabling parallel processing and capturing long-range dependencies in text.

<----------section---------->

### Large Language Models (LLM)

After transformers, the next step was scaling...

*   LLM leverage extensive data and computational power to understand and generate human-like text, pushing the boundaries of NLP capabilities.

*   List of LLMs: GPT-4, ChatGPT, InstructGPT, Codex, Flan-PaLM, LLaMA, BLOOM, OPT, UL2, PaLM, Gopher, Chinchilla, Titan, Jurassic-1, Ernie 3.0, PanGu, etc.*

These models, with their massive scale and sophisticated architectures, can perform a wide range of NLP tasks with impressive accuracy and fluency.

<----------section---------->

### LLM Applications

*   **Text Generation:** Producing articles, stories, and creative writing, enabling automated content creation and assistance for writers.
*   **Machine Translation:** Translating between languages with high accuracy, facilitating cross-lingual communication.
*   **Chatbots:** Engaging in human-like conversations for customer support and interaction, providing personalized assistance and resolving issues.
*   **Code Generation:** Generating and suggesting code snippets, completing code, and assisting with programming tasks, enabling faster and more efficient software development.
*   **Question Answering:** Providing answers based on a given context or database, enabling information retrieval and knowledge discovery.
*   **Text Summarization:** Condensing long documents into concise summaries, saving time and effort in information consumption.
*   **Writing Assistance:** Generating and completing text, improving grammar, and enhancing style, helping writers improve the quality of their writing.

<----------section---------->

### Multimodal LLM

Integrate and process multiple types of data

*   **Image-to-Text:** generating descriptive text from images (CLIP), enabling automated image captioning and understanding.
*   **Text-to-Image:** creating images based on textual descriptions (DALL-E), enabling creative image generation and artistic expression.
*   **Audio-to-Text:** converting spoken language into written text (Whisper), enabling speech recognition and transcription.
*   **Text-to-Audio:** composing or generating audio, such as music, from textual descriptions (Jukebox), enabling automated music composition and audio creation.

<----------section---------->

### Multimodal LLM

Integrate and process multiple types of data

*   **Video-to-Text:** Generating textual descriptions or summaries from video content (VideoBERT), enabling video analysis and understanding.
*   **Text-to-Video:** Video content from textual descriptions (Sora), enabling the creation of photorealistic videos from textual prompts.

Example Prompt: Photorealistic closeup video of two pirate ships battling each other as they sail inside a cup of coffee

This example illustrates the power of multimodal LLMs to generate complex and imaginative video content from textual descriptions.

<----------section---------->

## Lesson 2 ##

**Outline:**

The lesson covers the following key areas:

*   Tokenization: The process of breaking down text into smaller units (tokens).
*   Bag of Words Representation: A method for representing text based on the frequency of words.
*   Token Normalization: Techniques to standardize tokens for better analysis.
*   Stemming and Lemmatization: Approaches to reduce words to their root form.
*   Part of Speech Tagging: Identifying the grammatical role of each word.
*   Introducing spaCy: An overview of the spaCy library, a powerful tool for NLP.

<----------section---------->

**Prepare the Environment**

To follow the practical exercises in this lesson, a suitable development environment is required. Jupyter notebooks are recommended for their interactive nature.

*   Install the Jupyter Extension for Visual Studio Code: This allows you to create and run Jupyter notebooks directly within Visual Studio Code.
*   Install Jupyter: Use the command `pip install jupyter` to install the Jupyter package.
*   Create and activate a virtual environment: This helps to isolate project dependencies.
    *   `python -m venv .env`: Creates a virtual environment in the `.env` directory.
    *   `source .env/bin/activate`: Activates the virtual environment (on Linux/macOS). On Windows, use `.env\Scripts\activate`.
*   Alternative: Google Colab notebooks: <https://colab.research.google.com/>, which provides a cloud-based Jupyter notebook environment.

The following Python packages are also needed for this section:

*   `pip install numpy pandas`: Installs the `numpy` library for numerical computations and `pandas` for data manipulation and analysis.

<----------section---------->

**Text Segmentation**

Text segmentation is the process of dividing text into meaningful units. This is a crucial step in NLP, as it prepares the text for further analysis. The segmentation process can be performed at different levels:

*   Paragraph Segmentation: Dividing a document into individual paragraphs, often based on whitespace or specific formatting.
*   Sentence Segmentation: Breaking a paragraph into individual sentences, typically using punctuation marks as delimiters.
*   Word Segmentation: Dividing a sentence into individual words, typically using whitespace as a delimiter. This process faces challenges with languages that do not use spaces between words.

**Tokenization**

Tokenization is a specialized form of text segmentation. It involves breaking text into small units called tokens. Tokens are the fundamental building blocks for many NLP tasks. Tokenization involves identifying the elementary units within a text, so that the text is easier to process.

<----------section---------->

**What is a Token?**

A token is a unit of text that is treated as a single, meaningful element. Tokens can be:

*   Words: The most common type of token, representing individual words in the text.
*   Punctuation Marks: Symbols that punctuate sentences, such as periods, commas, question marks, etc. These are often treated as separate tokens.
*   Emojis: Visual symbols representing emotions or concepts. These have become increasingly important in modern text analysis.
*   Numbers: Digits and numerical expressions. These can be important for certain types of analysis.
*   Sub-words: Smaller units within words, such as prefixes (e.g., "re-", "pre-") or suffixes (e.g., "-ing", "-ed") that carry intrinsic meaning. Sub-word tokenization is particularly useful for handling rare words or words not seen during training.
*   Phrases: Multi-word expressions treated as single units (e.g., "ice cream", "natural language processing"). These are often identified using techniques like n-gram analysis.

<----------section---------->

**Tokenizer**

A basic approach to tokenization is to use whitespaces as the delimiter between words. This can be easily implemented using string splitting functions in programming languages. However, this approach has limitations.

*   Languages with Continuous Orthographic Systems: This method is not suitable for languages like Chinese, Japanese, and Thai, which do not use spaces to separate words. These languages require more sophisticated segmentation techniques.

```python
sentence = "Leonardo da Vinci began painting the Mona Lisa at the age of 51."
token_seq = sentence.split()
print(token_seq)
# Output: ['Leonardo', 'da', 'Vinci', 'began', 'painting', 'the', 'Mona', 'Lisa', 'at', 'the', 'age', 'of', '51.']
```

In the example above, the `split()` function separates the sentence into words based on whitespace. However, it fails to separate "51" and "." which ideally should be two tokens. Addressing punctuation and other challenges requires more advanced tokenization methods that are discussed later in the lesson. The simple approach provides a starting point to illustrate how more advanced algorithms can then address these situations.

<----------section---------->

**Bag of Words Representation**

The Bag of Words (BoW) model is a way of representing text data when modeling text with machine learning algorithms. The BoW model is simple to understand and implement, and it offers a great deal of flexibility for customization.

**Turning Words into Numbers**

Machine learning models require numerical input data. Therefore, text must be converted into a numerical representation. The lesson will explore a few of these approaches.

**One-hot Vectors**

One-hot encoding is a basic way to turn words into numbers:

*   Vocabulary: First, a vocabulary is created that lists all unique tokens that will be tracked.
*   Representation: Each word is represented by a vector with all 0s except for a 1 corresponding to the index of the word in the vocabulary.

```python
import numpy as np
import pandas as pd

token_seq = ['Leonardo', 'da', 'Vinci', 'began', 'painting', 'the', 'Mona', 'Lisa', 'at', 'the', 'age', 'of', '51.']
vocab = sorted(set(token_seq))
onehot_vectors = np.zeros((len(token_seq), len(vocab)), int)

for i, word in enumerate(token_seq):
    onehot_vectors[i, vocab.index(word)] = 1

df = pd.DataFrame(onehot_vectors, columns=vocab, index=token_seq)
print(df)
```

This code creates a one-hot vector for each word in the `token_seq` based on the sorted unique tokens, `vocab`. Pandas DataFrame is used to provide a tabular view of the vectors.

<----------section---------->

**One-hot Vectors: Features**

One-hot vectors have positive and negative features.

**Positive features:**

*   No information is lost: This allows you to reconstruct the original document from a table of one-hot vectors. With some additional book-keeping in a separate vector, you could even reconstruct the order of the words.

**Negative Features:**

*   One-hot vectors are super-sparse: This results in a large table even for a short sentence. Many of the entries in the matrix are zero which creates a sparse matrix.
*   Large vocabulary size: A language vocabulary typically contains at least 20,000 common words. This grows to millions when you consider word variations (conjugations, plurals, etc.) and proper nouns (names of people, places, organizations, etc.).

<----------section---------->

**One-hot Vectors: Practical Limitations**

The sparseness of one-hot vectors can lead to impractical storage requirements:

Let's assume the following:

*   A million tokens in your vocabulary.
*   A small library of 3,000 short books with 3,500 sentences each and 15 words per sentence.

Calculations:

1.  Total number of tokens: 15 words/sentence * 3,500 sentences/book * 3,000 books = 157,500,000 tokens
2.  Bits per token: Each token is represented by a one-hot vector of 1 million bits.
3.  Total bits: 10<sup>6</sup> bits/token * 157,500,000 tokens = 157.5 x 10<sup>12</sup> bits
4.  Storage in Terabytes: 157.5 x 10<sup>12</sup> bits / (8 bits/byte * 1024<sup>4</sup> bytes/TB) ≈ 17.9 TB

As shown, one-hot encoding can lead to large memory requirements even for a relatively small dataset, thus not practical. This highlights the need for more efficient text representations.

<----------section---------->

**Bag-of-Words (BoW)**

BoW representation addresses the storage challenges of one-hot encoding by creating a vector obtained by summing all the one-hot vectors of the words in the text.

*   Representation: One bag (vector) for each sentence or short document.
*   Compression: Compresses a document down to a single vector representing its essence. This compression occurs through vector addition.
*   Lossy Transformation: You can't reconstruct the initial text from the BoW vector. The word order is lost.
*   Binary BoW: Each word presence is marked as 1 or 0, regardless of its frequency. This is a simplification of the basic BoW where the absence or presence of a token is the only information used, and the count of tokens is not retained.

<----------section---------->

**Binary BoW: Example**

The following code demonstrates how to generate a vocabulary and create Binary BoW vectors for a text corpus:

Generating a vocabulary for a text corpus…

```python
sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))
print(vocab)
```

This produces a sorted list of unique words present in the combined sentences. Punctuation marks attached to the end of the words are included in the tokens.

<----------section---------->

**Binary BoW: Example**

Generating a BoW vector for each text…

```python
import numpy as np
import pandas as pd

sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))

bags = np.zeros((len(sentences), len(vocab)), int)
for i, sentence in enumerate(sentences):
    for j, word in enumerate(sentence.split()):
        bags[i, vocab.index(word)] = 1

df = pd.DataFrame(bags, columns=vocab)
print(df.transpose())
```

This code snippet initializes a matrix `bags` with dimensions corresponding to the number of sentences and the vocabulary size. It iterates through the sentences, marking the presence of each word in the corresponding BoW vector with a 1. Finally, it displays the BoW vectors in a transposed DataFrame.

For display purposes only: The visualization is not typically how these vectors are employed but gives insight to the method.

<----------section---------->

**Binary BoW: Interpretation**

The Binary BoW representation is useful for:

*   Analyzing Word Usage: The example demonstrates how words are used in different sentences, which helps understanding context.
*   Document Comparison: Overlap in word usage across sentences provides a way to compare and search for similar documents.

<----------section---------->

**Bag-of-Words Overlap**

Measuring the bag of words overlap for two texts…

*   Word Overlap: Overlap in words can provide an estimate of how similar two texts are.
*   Meaning Similarity: Assuming word usage correlates with meaning, the overlap can indicate semantic similarity.

A common method for measuring overlap is using the dot product of the BoW vectors.

```python
import numpy as np
sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

all_words = " ".join(sentences).split()
vocab = sorted(set(all_words))

bags = np.zeros((len(sentences), len(vocab)), int)
for i, sentence in enumerate(sentences):
    for j, word in enumerate(sentence.split()):
        bags[i, vocab.index(word)] = 1

print(np.dot(bags[0], bags[2])) #comparing sentence 0 and 2
print(np.dot(bags[3], bags[5])) #comparing sentence 3 and 5
```

This example calculates the dot product between the BoW vectors of sentences to quantify their similarity. A higher dot product indicates greater overlap in word usage. For sentence 0 and 2, the dot product is higher since both talk about Leonardo da Vinci. For sentence 3 and 5, the dot product is lower, since these sentences have less similar topics. This dot product calculation can be useful, but also contains many false positive connections that can negatively impact the results if not treated with caution.

<----------section---------->

**Token Normalization**

Token normalization is the process of transforming tokens into a standard form. This process can reduce the vocabulary size and improve the accuracy of NLP tasks.

**Tokenizer Improvement**

A simple whitespace-based tokenizer can be improved by handling:

*   Whitespace Variations: Considering other whitespace characters like `\t` (tab), `\n` (newline), and `\r` (return).
*   Punctuation: Removing punctuation marks (commas, periods, quotes, semicolons, dashes, etc.).

This can be achieved using regular expressions:

```python
import re

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
print(token_seq)
# Output: ['Leonardo', 'was', 'born', 'in', 'Vinci', 'Italy', 'in', '1452']
```

This example uses `re.split()` with a regular expression to split the sentence, handling multiple whitespace characters and common punctuation. Empty strings that come from having nothing between characters in the regular expression match are then filtered out.

<----------section---------->

**Tokenizer Improvement: Edge Cases**

The below cases are common in text and highlight the complexity of tokenization. Regular expressions can be used, but quickly create large complex tokenizers.

*   The company’s revenue for 2023 was $1,234,567.89.
*   The CEO of the U.N. (United Nations) gave a speech.
*   It’s important to know the basics of A.I. (Artificial Intelligence).
*   He didn’t realize the cost was $12,345.67.
*   Her new book, ‘Intro to NLP (Natural Language Processing)’, is popular.
*   The temperature in Washington, D.C. can reach 100°F in the summer.

<----------section---------->

**Case Folding**

Case folding is a token normalization technique that reduces all characters to lowercase.

*   Examples:
    *   Tennis → tennis
    *   A → a
    *   Leonardo → leonardo
*   Also known as Case normalization

**Advantages:**

*   Improves Text Matching: Increases recall in search engines by treating different capitalization variants as the same. For instance, a search for "tennis" will also find documents containing "Tennis".

**Disadvantages:**

*   Loss of Distinction: Removes distinction between proper nouns and common nouns. This may be problematic if named entity recognition is needed in the system.
*   Meaning Alteration: May change the original meaning (e.g., US → us). "US" refers to the United States, while "us" is a pronoun.
<----------section---------->

**Case Folding: Example**

```python
import re

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
token_seq = [x.lower() for x in token_seq] # case folding

print(token_seq)
# Output: ['leonardo', 'was', 'born', 'in', 'vinci', 'italy', 'in', '1452']
```

This code converts all tokens to lowercase, normalizing the casing. However, it eliminates capitalization that may be meaningful.
Approaches to address this are:

*   Normalize First Word: Normalize only the first word in the sentence. This will still cause problems if the first word is a proper noun.
*   Detect Proper Nouns: Detect proper nouns before normalizing remaining words, but can require training a new model if named entity recognition is not already part of your process.

<----------section---------->

**Stop Words**

Stop words are common words that occur with high frequency but carry little information about the meaning of a sentence. They often include articles, prepositions, conjunctions, and forms of the verb "to be".

*   Filtering: These words can be filtered out to reduce noise and focus on more informative words.
*   Example stop words: "a", "about", "after", "all", "also", "an", "and", "any", "are", etc.

```python
stop_words = [
    "a", "about", "after", "all", "also", "an", "and", "any", "are", "as", "at", "be", "because",
    "been", "but", "by", "can", "co", "corp", "could", "for", "from", "had", "has", "have", "he",
    "her", "his", "if", "in", "inc", "into", "is", "it", "its", "last", "more", "most", "mr",
    "mrs", "ms", "mz", "no", "not", "of", "on", "one", "only", "or", "other", "out", "over", "s",
    "says", "she", "so", "some", "such", "than", "that", "the", "their", "there", "they", "this",
    "to", "up", "was", "we", "were", "when", "which", "who", "will", "with", "would"
]

sentence = "Leonardo was born in Vinci, Italy, in 1452."
token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
token_seq = [token for token in token_seq if token] # remove void tokens
token_seq = [x.lower() for x in token_seq] # case folding
token_seq = [x for x in token_seq if x not in stop_words] # remove stop words

print(token_seq)
# Output: ['leonardo', 'born', 'vinci', 'italy', '1452']
```

The code removes punctuation, performs case folding, and then filters out stop words from the token sequence.

<----------section---------->

**Stop Words: Considerations**

Removing stop words isn't always beneficial.

**Disadvantages:**

*   Loss of Relational Information: Stop words can provide important relational information, even if they individually carry little meaning.

*   Mark reported to the CEO → Mark reported CEO
*   Suzanne reported as the CEO to the board → Suzanne reported CEO board

The stop words "to", and "as" indicate the relationship between the CEO and the other people in the sentences.

Here is a list of Italian stop words:

a, affinché, agl’, agli, ai, al, all’, alla, alle, allo, anziché, avere, bensì, che, chi, cioè, come, comunque, con, contro, cosa, da, dacché, dagl’, dagli, dai, dal, dall’, dalla, dalle, dallo, degl’, degli, dei, del, dell’, delle, dello, di, dopo, dove, dunque, durante, e, egli, eppure, essere, essi, finché, fino, fra, giacché, gl’, gli, grazie, i, il, in, inoltre, io, l’, la, le, lo, loro, ma, mentre, mio, ne, neanche, negl’, negli, nei, nel, nell’, nella, nelle, nello, nemmeno, neppure, noi, nonché, nondimeno, nostro, o, onde, oppure, ossia, ovvero, per, perché, perciò, però, poiché, prima, purché, quand’anche, quando, quantunque, quasi, quindi, se, sebbene, sennonché, senza, seppure, si, siccome, sopra, sotto, su, subito, sugl’, sugli, sui, sul, sull’, sulla, sulle, sullo, suo, talché, tu, tuo, tuttavia, tutti, un, una, uno, voi, vostro

<----------section---------->

**Putting All Together**

This example shows how to combine tokenization, case folding, and stop word removal into a complete preprocessing pipeline:

```python
import re
import numpy as np
import pandas as pd

stop_words = [
    "a", "about", "after", "all", "also", "an", "and", "any", "are", "as", "at", "be", "because",
    "been", "but", "by", "can", "co", "corp", "could", "for", "from", "had", "has", "have", "he",
    "her", "his", "if", "in", "inc", "into", "is", "it", "its", "last", "more", "most", "mr",
    "mrs", "ms", "mz", "no", "not", "of", "on", "one", "only", "or", "other", "out", "over", "s",
    "says", "she", "so", "some", "such", "than", "that", "the", "their", "there", "they", "this",
    "to", "up", "was", "we", "were", "when", "which", "who", "will", "with", "would"
]

sentences = [
    "Leonardo da Vinci began painting the Mona Lisa at the age of 51.",
    "Leonardo was born in Vinci, Italy, in 1452.",
    "In addition to being a painter, Leonardo da Vinci was also a skilled engineer.",
    "Tennis is played on a rectangular court with a net in the middle.",
    "The four Grand Slam tournaments are the most prestigious events in tennis.",
    "A tennis match is typically played as a best of three or best of five sets."
]

def tokenize(sentence):
    token_seq = re.split(r'[-\s.,;!?]+', sentence) # remove punctuation
    token_seq = [token for token in token_seq if token] # remove void tokens
    token_seq = [x.lower() for x in token_seq] # case folding
    token_seq = [x for x in token_seq if x not in stop_words] # remove stop words
    return token_seq

tok_sentences = [tokenize(sentence) for sentence in sentences]
all_tokens = [x for tokens in tok_sentences for x in tokens]
vocab = sorted(set(all_tokens))

bags = np.zeros((len(tok_sentences), len(vocab)), int)
for i, sentence in enumerate(tok_sentences):
    for j, word in enumerate(sentence):
        bags[i, vocab.index(word)] = 1

df = pd.DataFrame(bags, columns=vocab)
print(df.transpose())
```

<----------section---------->

**Putting All Together: Output**

The DataFrame shows the binary BoW representation for each sentence after preprocessing. The columns are the unique tokens in the vocabulary, and the rows are the sentences. A '1' indicates the presence of a token in the sentence, and a '0' indicates its absence.

```
          0    1    2    3    4    5
1452     0    1    0    0    0    0
51       1    0    0    0    0    0
addition 0    0    1    0    0    0
age      1    0    0    0    0    0
began    1    0    0    0    0    0
being    0    0    1    0    0    0
best     0    0    0    0    0    1
born     0    1    0    0    0    0
court    0    0    0    1    0    0
da       1    0    1    0    0    0
engineer 0    0    1    0    0    0
events   0    0    0    0    1    0
five     0    0    0    0    0    1
four     0    0    0    0    1    0
grand    0    0    0    0    1    0
italy    0    1    0    0    0    0
leonardo 1    1    1    0    0    0
lisa     1    0    0    0    0    0
match    0    0    0    0    0    1
middle   0    0    0    1    0    0
mona     1    0    0    0    0    0
net      0    0    0    1    0    0
painter  0    0    1    0    0    0
painting 1    0    0    0    0    0
played   0    0    0    1    0    1
prestigious 0    0    0    0    1    0
rectangular 0    0    0    1    0    0
sets     0    0    0    0    0    1
skilled  0    0    1    0    0    0
slam     0    0    0    0    1    0
tennis   0    0    0    0    1    1
three    0    0    0    0    0    1
tournaments 0    0    0    0    1    0
typically 0    0    0    0    0    1
vinci    1    1    0    0    0    0
```

<----------section---------->

**Using NLTK**

The Natural Language Toolkit (NLTK) is a popular Python library for NLP.

*   Installation: `pip install nltk`
*   Features: It includes refined tokenizers and stop-word lists for many languages.

```python
import nltk

nltk.download('punkt') # download the Punkt tokenizer models
text = "Good muffins cost $3.88\nin New York. Please buy me two of them.\n\nThanks."

print(nltk.tokenize.word_tokenize(text)) # word tokenization
print(nltk.tokenize.sent_tokenize(text)) # sentence tokenization
```

The `nltk.tokenize.word_tokenize()` function splits the text into words, and `nltk.tokenize.sent_tokenize()` splits the text into sentences. The `punkt` model must be downloaded to run the example.

```
['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
['Good muffins cost $3.88\nin New York.', 'Please buy me two of them.', 'Thanks.']
```

<----------section---------->

**Using NLTK: Stop Words**

NLTK provides stop-word lists for various languages.

```python
import nltk

nltk.download('stopwords') # download the stop words corpus
text = "This is an example sentence demonstrating how to remove stop words using NLTK."

tokens = nltk.tokenize.word_tokenize(text)
stop_words = set(nltk.corpus.stopwords.words('english'))
filtered_tokens = [x for x in tokens if x not in stop_words]

print("Original Tokens:", tokens)
print("Filtered Tokens:", filtered_tokens)
```

This code downloads the English stop words corpus, tokenizes the text, and removes the stop words from the token list.
The nltk.download('stopwords') line downloads the necessary data for stop word removal.

```
Original Tokens: ['This', 'is', 'an', 'example', 'sentence', 'demonstrating', 'how', 'to', 'remove', 'stop', 'words', 'using', 'NLTK', '.']
Filtered Tokens: ['This', 'example', 'sentence', 'demonstrating', 'remove', 'stop', 'words', 'using', 'NLTK', '.']
```

<----------section---------->

**Stemming and Lemmatization**

Stemming and lemmatization are techniques to reduce words to their root form, which can improve generalization and reduce vocabulary size.

**Stemming**

Stemming identifies a common stem among various forms of a word.

*   Example: "Housing" and "houses" share the same stem: "house".
*   Function: Removes suffixes to group words with similar meanings under the same token (stem).
*   Stem Imperfection: A stem isn’t required to be a properly spelled word. "Relational", "Relate", "Relating" all stem to "Relat".

**Benefits:**

*   Vocabulary Generalization: Helps generalize the vocabulary by treating different forms of the same word as the same.
*   Information Retrieval: Improves recall by matching different forms of a query term.

<----------section---------->

**A Naive Stemmer**

```python
def simple_stemmer(word):
    suffixes = ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

words = ["running", "happily", "stopped", "curious", "cries", "effective", "runs", "management"]
print({simple_stemmer(word) for word in words})
# Output: {'cur', 'runn', 'stopp', 'run', 'effect', 'manage', 'happi', 'cr'}
```

This is a basic example of a stemmer.

*   Limitation: Doesn't handle exceptions, multiple suffixes, or words that require complex modifications.
*   Accuracy: NPL libraries include more accurate stemmers. This example shows the basic method for shortening the token set with a custom set of rules that can be designed for a specific use case.

<----------section---------->

**Porter Stemmer**

The Porter Stemmer is a widely used algorithm for stemming English words. It consists of a series of rules applied in stages to remove suffixes and reduce words to their stem. Here are a few of the steps:

*   Step 1a: Remove 's' and 'es' endings
    *   cats → cat, buses → bus
*   Step 1b: Remove 'ed', 'ing', and 'at' endings
    *   hoped → hope, running → run
*   Step 1c: Change 'y' to 'i' if preceded by a consonant
    *   happy → happi, cry → cri
*   Step 2: Remove "nounifying" endings such as ational, tional, ence, and able
    *   relational → relate, dependence → depend

<----------section---------->

**Porter Stemmer (Continued)**

Here are the remaining steps in the Porter Stemmer:

*   Step 3: Remove adjective endings such as icate, ful, and alize
    *   duplicate → duplic, hopeful → hope
*   Step 4: Remove adjective and noun endings such as ive, ible, ent, and ism
    *   effective → effect, responsible → respons
*   Step 5a: Remove stubborn e endings
    *   probate → probat, rate → rat
*   Step 5b: Reduce trailing double consonants ending in l to a single l
    *   controlling → controll → control, rolling → roll → rol

```python
import nltk

texts = [
    "I love machine learning.",
    "Deep learning is a subset of machine learning.",
    "Natural language processing is fun.",
    "Machine learning can be used for various tasks."
]

stemmed_texts = []
stemmer = nltk.stem.PorterStemmer() # Initialize the Porter Stemmer

for text in texts:
    tokens = nltk.tokenize.word_tokenize(text.lower())
    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]
    stemmed_texts.append(' '.join(stemmed_tokens))

for text in stemmed_texts:
    print(text)
```

```
i love machin learn
deep learn is a subset of machin learn
natur languag process is fun
machin learn can be use for variou task
```

The code demonstrates the use of the Porter Stemmer to reduce words to their stems. Tokens are lowercased and are required to be alpha characters, and then stemmed, ensuring consistent processing.

<----------section---------->

**Snowball Project**

The Snowball project provides stemming algorithms for several languages.

*   URL: <https://snowballstem.org/>

Italian stemming examples:

| Input           | Stem     |
|-----------------|----------|
| abbandonata     | abbandon |
| abbandonate     | abbandon |
| abbandonati     | abbandon |
| abbandonato     | abbandon |
| abbandonava     | abbandon |
| abbandonera     | abbandon |
| abbandoneranno  | abbandon |
| abbandonerà      | abbandon |
| abbandono        | abbandon |
| abbandono        | abbandon |
| abbaruffato      | abbaruff |
| abbassamento     | abbass   |
| abbassando       | abbass   |
| abbassandola     | abbass   |
| abbassandole     | abbass   |
| abbassare        | abbass   |
| abbassarono      | abbass   |

The Italian stemming algorithm is supported in NLTK. The stemming done in Italian and many languages is not simple truncation, but contextual translation that leverages understanding of the structure of the language.

```python
import nltk
nltk.download('punkt')
stemmer = nltk.stem.PorterStemmer()
```

<----------section---------->

Other example words:

| Input          | Stem      |
|----------------|-----------|
| pronto         | pront     |
| pronuncerà     | pronunc   |
| pronuncia      | pronunc   |
| pronunciamento | pronunc   |
| pronunciare    | pronunc   |
| pronunciarsi   | pronunc   |
| pronunciata    | pronunc   |
| pronunciate    | pronunc   |
|
<----------section---------->

## Lesson 3 ##

## Outline

This lesson covers the following topics:

*   **Term Frequency:**  The basic measure of how often a term appears in a document.

*   **Vector Space Model:** A model that represents documents as vectors in a multi-dimensional space, where each dimension corresponds to a term.

*   **TF-IDF:**  Term Frequency-Inverse Document Frequency, a weighting scheme that refines term frequency by considering how common or rare a term is across the entire corpus (collection of documents).

*   **Building a Search Engine:** An application of the concepts learned in the lesson, demonstrating how to use term frequency, vector space models, and TF-IDF to create a basic search engine.

<----------section---------->

## Term Frequency

### Bag of Words (BoW)

The Bag of Words model is a simplified representation of text used in NLP.  It treats a document as an unordered collection of words, disregarding grammar and word order while focusing on word counts.

*   **Vector Space Model of Text:** In this model, text is represented numerically. Each document becomes a vector.

*   **One-Hot Encoding:** Each word in the vocabulary is assigned a unique index.  A one-hot vector for a word has a '1' at its index and '0' everywhere else.

*   **Binary BoW:** Create one-hot vectors for each word in a document. Combine these vectors using a logical OR operation. The resulting vector indicates the presence or absence of each word in the document (1 if present, 0 if absent).

*   **Standard BoW:** Create one-hot vectors for each word.  Sum these vectors element-wise. The resulting vector contains the counts of each word in the document.

*   **Term Frequency (TF):** Represents the number of times each word occurs within a specific document. It reflects the importance of a word in that document.

**Assumption:** The core assumption is that the more frequently a word appears in a document, the more relevant or important it is to the document's content. This is a simplistic assumption, as common words may not carry much meaning.

<----------section---------->

### Calculating TF

Here's a Python example demonstrating how to calculate term frequency using the spaCy library.

```python
# Sample sentence
sentence = "The faster Harry got to the store, the faster Harry, the faster, would get home."

import spacy
nlp = spacy.load("en_core_web_sm") # Load the English language model
doc = nlp(sentence)

# Extract tokens, convert to lowercase, remove stop words and punctuation
tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct]

tokens
```

**Explanation:**

1.  **Sample Sentence:** The code begins with a sample sentence to demonstrate the term frequency calculation.

2.  **spaCy Initialization:** It loads the spaCy English language model (`en_core_web_sm`), which is necessary for tokenizing the text.

3.  **Token Extraction:** It iterates through the processed document (`doc`), extracting each token.  The `.lower_` attribute converts each token to lowercase. The code also filters out stop words (`tok.is_stop`) and punctuation (`tok.is_punct`) to focus on meaningful terms.

```
{'faster', 'harry', 'got', 'store', 'faster', 'harry', 'faster', 'home'}
```

This output shows the tokens extracted after removing stop words and punctuation.  Notice that there are duplicates, which is correct, as we are counting term frequency. This representation is technically a Python `set`, where elements are unique and unordered. The next block will correctly count the term frequencies.

```python
# Build BoW with word count
import collections

bag_of_words = collections.Counter(tokens) # Counts the elements of a list
bag_of_words
```

**Explanation:**

1.  **`collections.Counter`:** The `collections.Counter` class efficiently counts the occurrences of each token in the `tokens` list.

```
Counter({'faster': 3, 'harry': 2, 'got': 1, 'store': 1, 'home': 1})
```

This output shows the word counts. For example, "faster" appears 3 times, "harry" appears 2 times, and the other words appear once.

```python
# Most common words
bag_of_words.most_common(2) # Most common 2 words
```

**Explanation:**

1.  **`most_common(2)`:** This method returns the two most frequent words and their counts as a list of tuples.

```
(('faster', 3), ('harry', 2))
```

This output shows that "faster" is the most frequent word (3 occurrences), followed by "harry" (2 occurrences).

<----------section---------->

### Limits of TF

Term Frequency alone has limitations in determining the importance of a word.

**Example:**

*   In document A, the word "dog" appears 3 times.
*   In document B, the word "dog" appears 100 times.

Based solely on term frequency, one might assume "dog" is more important to document B. However, this doesn't consider the length of the document.

**Additional information:**

*   Document A is a 30-word email to a veterinarian.
*   Document B is the novel War & Peace (approx. 580,000 words).

Now, with context, it becomes clearer that "dog" is likely more significant to the *short* email (Document A).

<----------section---------->

### Normalized TF

To address the limitations of raw term frequency, we normalize it by the document length.  This is sometimes called *weighted TF*.

Normalized (weighted) TF is calculated as:

*   TF (term, document) = (Number of occurrences of the term in the document) / (Total number of words in the document)

*   TF (dog, document A) = 3/30 = 0.1
*   TF (dog, document B) = 100/580000 = 0.00017

This normalized TF gives a more accurate representation of the word's importance within the document, relative to its length.

```python
import pandas as pd

counts = pd.Series(bag_of_words) # from dict to Pandas Series
counts / counts.sum() # Calculate TF
```

**Explanation:**

1.  **Pandas Series:**  The code converts the `bag_of_words` (a `Counter` object) to a Pandas Series, which is a one-dimensional labeled array. This facilitates easier calculations.

2.  **TF Calculation:** It divides each word count in the `counts` Series by the total sum of all word counts. This normalizes the term frequencies by the total number of words.

```
faster    0.375
harry     0.250
got       0.125
store     0.125
home      0.125
dtype: float64
```

This output shows the normalized term frequencies. "faster" now accounts for 37.5% of the words in the processed sentence, "harry" accounts for 25%, and the remaining words each account for 12.5%.

<----------section---------->

## Vector Space Model

The Vector Space Model (VSM) represents documents as vectors in a high-dimensional space, where each dimension corresponds to a unique term (word) in the corpus. The value in each dimension represents the weight of that term in the document.

### NLTK Corpora

The Natural Language Toolkit (NLTK) is a Python library that provides various resources for NLP tasks, including access to several text corpora.

*   NLTK corpora are collections of text documents used for training and testing NLP algorithms.
*   NLTK provides convenient tools for accessing corpus data.
*   [https://www.nltk.org/howto/corpus.html](https://www.nltk.org/howto/corpus.html)

### Reuters 21578 corpus

The Reuters-21578 corpus is a widely used dataset for NLP, particularly for text classification.

*   It contains thousands of news articles published by Reuters in 1987.
*   The news articles are categorized into 90 different topics.
*   It's a standard benchmark dataset for evaluating text classification algorithms.

<----------section---------->

### Using Reuters 21578

```python
import nltk

nltk.download('reuters') # Download the reuters corpus
ids = nltk.corpus.reuters.fileids() # Ids of the documents
sample = nltk.corpus.reuters.raw(ids[0]) # First document

print(len(ids), "samples.\n") # Number of documents
print(sample)
```

**Explanation:**

1.  **Import NLTK:** Imports the NLTK library.

2.  **Download Corpus:** Downloads the Reuters corpus.  This might take a few seconds if you haven't downloaded it before.

3.  **File IDs:** Retrieves a list of file identifiers (IDs) for each document in the corpus.

4.  **Raw Text:**  Retrieves the raw text content of the first document (identified by `ids[0]`).

5.  **Output:** Prints the total number of documents in the corpus and the content of the first document.

```
Taiwan had a trade trade surplus of 15.6 billion dlrs last
year, 95 pct of it with the U.S.

The surplus helped swell Taiwan's foreign exchange reserves
to 53 billion dlrs, among the world's largest.

“We must quickly open our markets, remove trade barriers and
cut import tariffs to allow imports of U.S. Products, if we
want to defuse problems from possible U.S. Retaliation," said
Paul Sheen, chairman of textile exporters <Taiwan Safe Group>.

A senior official of South Korea's trade promotion
association said the trade dispute between the U.S. And Japan
might also lead to pressure on South Korea, whose chief exports
are similar to those of Japan.

Last year South Korea had a trade surplus of 7.1 billion
dirs with the U.S., Up from 4.9 billion dlrs in 1985.

In Malaysia, trade officers and businessmen said tough
curbs against Japan might allow hard-hit producers of
semiconductors in third countries to expand their sales to the
U.S.
```

This output is the raw text of the first news article in the Reuters corpus, discussing trade surpluses.

<----------section---------->

### Using Reuters 21578 (TF Calculation)

```python
doc = nlp(sample)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]

bag_of_words = collections.Counter(tokens)
counts = pd.Series(bag_of_words).sort_values(ascending=False) # Sorted series
counts = counts / counts.sum() # Calculate TF

print(counts.head(10))
```

**Explanation:**

1.  **Process Text:** This code processes the `sample` text (the first Reuters article) using spaCy, similar to the previous example.

2.  **Tokenization and Filtering:** It extracts tokens, converts them to lowercase, and removes stop words, punctuation, and whitespace.

3.  **Term Frequency Calculation:** It calculates the term frequencies (TF) of the tokens in the document, as described before.

4.  **Output:** It prints the top 10 most frequent terms (and their normalized frequencies) in the first news article.

```
u.s.          0.039627
said         0.037296
trade        0.034965
japan        0.027972
dlrs         0.013986
exports      0.013986
tariffs      0.011655
imports      0.011655
billion      0.011655
electronics    0.009324
dtype: float64
```

The output lists the top 10 most relevant words in the first news article, based on their term frequencies after normalization.  The term "u.s." is the most frequent, followed by "said", "trade", and "japan", which gives an idea about the article’s content.

### Corpus Processing

```python
ids_subset = ids[:100] # To speed-up we consider only the first 100 elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    doc = nlp(sample)
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # Print the state

df = pd.DataFrame(counts_list) # List of series to dataframe
df = df.fillna(0) # Change NaNs to 0
```

**Explanation:**

1.  **Subset of IDs:** To reduce processing time, the code initially considers only the first 100 documents (`ids[:100]`) of the Reuters corpus.

2.  **Iterate and Process:** It iterates through the subset of document IDs, retrieving the raw text for each document, processing it with spaCy (tokenizing, lowercasing, removing stop words/punctuation/spaces), and calculating the normalized term frequencies.

3.  **Store Term Frequencies:** The term frequencies for each document are stored in a list (`counts_list`).

4.  **Create DataFrame:**  Finally, the list of term frequency series is converted into a Pandas DataFrame (`df`). Missing values (NaNs) are filled with 0, indicating that a particular term doesn't appear in that document.

5.  **Progress Output:** The `print` statement displays the progress of processing, updating the output in place using `\r` (carriage return) and `end=""` to overwrite the previous line.

```
Sample 100 of 100 processed.
```

This indicates that the code processed all 100 selected samples.

```python
df
```

```
      u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[100 rows x 3089 columns]
```

The output `df` is a DataFrame where:

*   Rows represent individual documents (news articles).
*   Columns represent unique terms (words) from the corpus.
*   Values represent the normalized term frequency (TF) of each word in each document.

<----------section---------->

### Corpus Processing (Inefficiency)

The previous code, while functional, is inefficient due to the full spaCy pipeline being run on each document, even though only tokenization is needed.

```python
ids_subset = ids[:100] # To speed-up we consider only the first 100 elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    doc = nlp(sample)
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # Print the state

df = pd.DataFrame(counts_list) # List of series to dataframe
df = df.fillna(0) # Change NaNs to 0
df
```

spaCy, by default, performs various NLP tasks when you call `nlp(text)`:

*   Part-of-speech tagging (POS)
*   Lemmatization
*   Dependency parsing
*   Named entity recognition (NER)

But, in this case, only tokenization is needed, making the other steps redundant and time-consuming.

### Corpus Processing (Optimization)

To optimize the process, the unnecessary steps in the spaCy pipeline can be disabled.

When you call `nlp` on a text, spaCy:

*   First tokenizes the text to produce a `Doc` object.
*   The `Doc` is then processed in several steps (pipeline).

```python
# Show the current pipeline components
print(nlp.pipe_names)
```

**Explanation:**

This code prints the names of the components currently enabled in the spaCy pipeline. These components are executed sequentially when you process text with the `nlp` object.

```
['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
```

This output indicates that the pipeline includes components for `tok2vec` (converting tokens to vectors), `tagger` (part-of-speech tagging), `parser` (dependency parsing), `attribute_ruler` (applying rules based on token attributes), `lemmatizer` and `ner` (named entity recognition).
<----------section---------->

### Corpus Processing (Optimization)

```python
ids_subset = ids # We now consider all elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    # disable=["tok2vec", "tagger", “parser", “attribute_ruler", “lemmatizer", "ner"]
    doc = nlp(sample, disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # Print the state

df = pd.DataFrame(counts_list) # List of series to dataframe
df = df.fillna(0) # Change NaNs to 0
```

**Explanation:**

1.  **Disable Pipeline Components:** The key change is the `disable` argument when calling `nlp(sample, disable=[...])`. This tells spaCy to skip the specified pipeline components, performing only tokenization.  The commented line clarifies the purpose.

2.  **Process All Documents**: The code is modified to process all elements by setting `ids_subset = ids`.

3.  The rest of the code remains the same, processing the text and creating the DataFrame.

```
Sample 10788 of 10788 processed.
```

This output indicates that all 10,788 documents in the corpus have been processed.

```python
df
```

```
         u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[10788 rows x 49827 columns]
```

The resulting DataFrame `df` is the same as before, but it was generated much faster, representing the term frequencies for all documents in the corpus. It has 10788 rows (documents) and 49827 columns (unique terms).

<----------section---------->

### Corpus Processing: Term-Document Matrix

The resulting DataFrame is a **Term-Document Matrix**.

*   Rows represent documents.
*   Columns represent terms (words) from the vocabulary.
*   Elements in the matrix represent the weight of each term in each document. The weights can be raw TF, normalized TF, or TF-IDF (explained later).

```python
df
```

```
         u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[10788 rows x 49827 columns]
```

<----------section---------->

### Vector Space Model

The Vector Space Model (VSM) is a mathematical representation of documents as vectors in a multi-dimensional space.

*   Each dimension of the space corresponds to a term (word) in the vocabulary.
*   The value along each dimension represents the weight or importance of that term in the document.
*   It is built from a term-document matrix, like the one previously created.

#### 2D Example: with normalized TF

Consider a simplified vocabulary of just two words: w1 and w2. Documents can then be represented as points in a 2D space.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![2d_example_vector_space.png](2d_example_vector_space.png)

This image illustrates a 2D vector space where documents are points. The coordinates of each point represent the normalized term frequencies of words w1 and w2 in that document.

<----------section---------->

### Document Similarity

Once documents are represented as vectors, their similarity can be calculated using various distance or similarity measures.

#### Euclidean Distance:

*   Measures the straight-line distance between two vectors.
*   Sensitive to the magnitude of the vectors (longer vectors will have larger distances, even if they point in the same direction).
*   In NLP, differences in document length can skew Euclidean distance, making it less reliable for comparing documents of different sizes.
*   Less commonly used in NLP compared to cosine similarity. The *direction* of the vectors is often more important than the magnitude when considering text.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![euclidean_distance.png](euclidean_distance.png)

This image visually represents Euclidean distance as the length of the line connecting the document points. Documents that are farther apart are considered less similar.

<----------section---------->

#### Document Similarity: Cosine Similarity

Cosine similarity is a more appropriate measure for text documents.

*   Measures the cosine of the angle between two vectors.
*   Focuses on the direction of the vectors, ignoring their magnitude (length).
*   Effective for normalized text representations because it focuses on the relative proportions of terms.
*   Widely used in NLP for document similarity, information retrieval, and text classification.

```
sim(A, B) = cos(θ) = (A · B) / (|A| * |B|)
```

Where:

*   `A · B` is the dot product of vectors A and B.
*   `|A|` and `|B|` are the magnitudes (Euclidean norms) of vectors A and B, respectively.
*   `θ` is the angle between the vectors.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![cosine_similarity.png](cosine_similarity.png)

This image visually represents cosine similarity by showing the angle between document vectors. Smaller angles indicate higher similarity.

<----------section---------->

#### Properties of Cosine Similarity

Cosine similarity produces a value between -1 and 1:

*   **1:** The vectors point in the same direction across all dimensions.
    *   Documents use the same words in similar proportions, suggesting they likely discuss the same topic.
*   **0:** The vectors are orthogonal (perpendicular) in all dimensions.
    *   Documents share no common words, indicating they likely discuss completely different topics.
*   **-1:** The vectors point in opposite directions across all dimensions.
    *   Impossible with raw TF (word counts cannot be negative).
    *   Can occur with other word representations (e.g., word embeddings, discussed in advanced NLP).

#### Calculate Cosine Similarity

```python
import numpy as np

def sim(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

# Example:
print(sim(df.iloc[0], df.iloc[1]))
print(sim(df.iloc[0], df.iloc[2]))
print(sim(df.iloc[0], df.iloc[0]))
```

**Explanation:**

1.  **`sim(vec1, vec2)` function:**
    *   Calculates the dot product of the two vectors.
    *   Calculates the Euclidean norm (magnitude) of each vector using `np.linalg.norm()`.
    *   Divides the dot product by the product of the norms to get the cosine similarity.
2.  **Example Usage:**
    *   `df.iloc[0]` and `df.iloc[1]` represent the first and second rows (documents) in the DataFrame.
    *   `df.iloc[0]` and `df.iloc[2]` represent the first and third rows (documents).
    *   `df.iloc[0]` and `df.iloc[0]` calculates the cosine similarity of the first document with itself (should be 1).

```
0.14261893769917347
0.2365347461078053
1.0
```

The output shows the cosine similarity between the specified documents.

```python
# Compare TF matrix subset (documents 0 and 1)
df.loc[[0, 1], (df.loc[0] > 0) & (df.loc[1] > 0)]
```

**Explanation:**

This code selects a subset of the DataFrame `df`:

*   `df.loc[[0, 1], ... ]` selects rows 0 and 1 (the first two documents).
*   `(df.loc[0] > 0) & (df.loc[1] > 0)` creates a boolean mask to select only the columns (terms) where both document 0 and document 1 have a term frequency greater than 0.  In other words, it selects terms that appear in *both* documents.
*   The result is a DataFrame showing the term frequencies for those common terms in those two documents.

```
       said      min      year       pct  government
0  0.037296  0.002331  0.009324  0.004662    0.002331
1  0.042254  0.028169  0.014085  0.056338    0.014085
```

This output shows the term frequencies for words that appear in both document 0 and document 1. Looking at these values helps to understand *why* the cosine similarity between those documents has the value it does. For example, they both use the term `said` frequently.

```python
# Try also with other documents
```

This line is a suggestion to experiment with different document pairs to observe the resulting cosine similarity and the shared terms.

<----------section---------->

## TF-IDF

### Inverse Document Frequency

Term Frequency (TF) alone does not account for the uniqueness of a word across the corpus. Some words appear frequently in *many* documents.

*   Common words (e.g., "the," "is," "and") appear in most documents and don't help distinguish between them.
*   Even domain-specific words can be common across a specialized corpus. For example, in a corpus about astronomy, terms like "planet" or "star" are frequent but not discriminatory.

Inverse Document Frequency (IDF) addresses this by measuring the importance of a word *relative to the entire corpus*.  It reduces the weight of common words and increases the weight of rare words.

### Inverse Document Frequency Formula

IDF increases the weight of rare words and decreases the weight of common words across documents.

```
idf(t, D) = log(N / |{d ∈ D: t ∈ d}|)
```

Where:

*   `t` represents a term (word).
*   `D` represents the entire corpus (set of documents).
*   `N` is the total number of documents in the corpus: `N = |D|`.
*   `|{d ∈ D: t ∈ d}|` is the number of documents in the corpus where the term `t` appears (i.e., the number of documents `d` where `tf(
<----------section---------->

## Lesson 4 ##

Text Classification

This document provides an outline and detailed explanation of text classification within the context of Natural Language Processing (NLP) and Large Language Models. It is part of a course on Ingegneria Informatica (Computer Engineering) at the University of Salerno. The material is presented by Nicola Capuano and Antonio Greco from the DIEM (Department of Industrial Engineering and Mathematics).

**Outline**

*   Text Classification: An introduction to the process of categorizing text documents.
*   Topic Labelling Example: A practical demonstration of text classification using the Reuters news dataset.
*   Sentiment Analysis Exercise: A hands-on exercise involving sentiment classification using the IMDB dataset.

<----------section---------->

**Text Classification**

Text Classification is defined as the task of assigning predefined categories or labels to text documents. This process is based solely on the text content of the documents. Metadata or other attributes associated with the document are not considered, distinguishing it from document classification. Furthermore, the classes are predefined, differentiating it from document clustering where classes are discovered automatically.

Common purposes for text classification include:

*   Topic labeling: Identifying the main subject or theme of the text.
*   Intent detection: Determining the user's intention behind a given text input, such as a query or command.
*   Sentiment analysis: Ascertaining the emotional tone or attitude expressed in the text (e.g., positive, negative, neutral).

<----------section---------->

**Definition**

To formally define text classification, consider the following:

*   A set of documents: D = {d<sub>1</sub>, ..., d<sub>n</sub>}, where each d<sub>i</sub> represents a text document.
*   A set of predefined classes: C = {c<sub>1</sub>, ..., c<sub>m</sub>}, where each c<sub>j</sub> represents a category or label.

The goal of text classification is to find a classifier function:

f: D x C -> {True, False}

This function assigns a Boolean value (True or False) to each pair (d<sub>i</sub>, c<sub>j</sub>) ∈ D x C, indicating whether document d<sub>i</sub> belongs to class c<sub>j</sub> or not. The classifier function effectively makes a binary decision for each document-class combination.

<----------section---------->

**Types of Classification**

Text classification can be further categorized into different types based on the number of classes a document can be assigned to:

*   **Single-label:** Each document in the dataset D is assigned to only one class from the set of classes C. An example would be assigning a news article to one specific category such as 'sports' or 'politics'.

*   **Binary:** This is a special case of single-label classification where the set of classes C contains only two classes. This simplifies the problem to a decision between a class and its complement (the other class). For example, classifying emails as either 'spam' or 'not spam'.

*   **Multi-label:** In multi-label classification, each document can be assigned to multiple classes from the set C. An example is categorizing a news article into multiple categories such as 'business' and 'finance'. Multi-label classification can be approached by transforming it into a series of binary classification problems, one for each class.

<----------section---------->

**ML-Based Classification**

Machine learning (ML) techniques are frequently employed in text classification. The general process involves:

1.  **Training:** A machine learning model is trained on a set of annotated text documents. This training set contains documents that are already labelled with the correct categories.
2.  **Annotation:** Each document in the training set is associated with one or more class labels, which serve as the ground truth for training the model.
3.  **Prediction:** After the model is trained, it can predict the category (or categories) for new, unseen documents.
4.  **Confidence Measure:** The classifier may provide a confidence measure, indicating the certainty of its prediction.
5.  **Vector Representation:** A crucial step in preparing the text for a machine learning model is converting the text into a numerical vector representation. Techniques like TF-IDF (Term Frequency-Inverse Document Frequency) are often used for this purpose.

<----------section---------->

**Topic Labelling Example**

**Classifying Reuters News**

The Reuters-21578 dataset is a widely used benchmark dataset for text classification, particularly for topic labeling. Its characteristics are:

*   Multi-class and multi-label: Documents can belong to multiple topics simultaneously.
*   90 distinct classes: Covering a broad range of news topics.
*   7,769 training documents, 3,019 test documents: Providing a sufficient amount of data for training and evaluating models.
*   The number of words per document ranges from 93 to 1,263: Indicating variable document lengths.
*   Skewness: The distribution of documents across classes is uneven:
    *   Some classes have over 1,000 documents.
    *   Other classes have fewer than 5 documents.
    *   Most documents are assigned either one or two labels, while some documents are labeled with up to 15 categories, adding complexity to the classification task.

Further statistics about the dataset are available at https://martin-thoma.com/nlp-reuters/.

<----------section---------->

**Corpus Management**

This section demonstrates how to load and explore the Reuters dataset using the NLTK (Natural Language Toolkit) library in Python.

```python
import nltk
from nltk.corpus import reuters

nltk.download('reuters')

ids = reuters.fileids()

training_ids = [id for id in ids if id.startswith("training")]
test_ids = [id for id in ids if id.startswith("test")]
categories = reuters.categories()

print("{} training items:".format(len(training_ids)), training_ids)
print("{} test items:".format(len(test_ids)), test_ids)
print("{} categories:".format(len(categories)), categories)

print("\nCategories of '{}':".format(training_ids[0]), reuters.categories(training_ids[0]))
print("Categories of '{}':".format(test_ids[2]), reuters.categories(test_ids[2]))
print("Items within the category 'trade'", reuters.fileids('trade'))
```

The code performs the following actions:

1.  Imports the necessary libraries: `nltk` for natural language processing and `reuters` corpus from `nltk.corpus`.
2.  Downloads the Reuters corpus if it is not already present.
3.  Retrieves the file identifiers (`fileids`) of all documents in the corpus.
4.  Separates the file identifiers into training and test sets based on their prefixes ("training" and "test").
5.  Retrieves the list of all categories in the corpus.
6.  Prints the number of training items, test items, and categories, along with the lists themselves (truncated for brevity).
7.  Shows the categories associated with specific documents from the training and test sets.
8.  Lists the file identifiers that belong to the 'trade' category.

<----------section---------->

## Lesson 5 ##

Word embeddings

This lesson focuses on word embeddings, a crucial technique in Natural Language Processing (NLP), especially relevant in the context of Large Language Models (LLMs). We will explore the limitations of traditional methods like TF-IDF, delve into the concept of word embeddings, discuss how they are learned, examine Word2Vec alternatives, and finally, understand how to work with these embeddings effectively.

## Outline
This lecture will cover the following key topics:
*   Limitations of TF-IDF: Understanding why TF-IDF falls short in capturing semantic relationships between words.
*   Word Embeddings: Introducing the concept of representing words as dense vectors in a continuous vector space.
*   Learning Word Embeddings: Exploring methods like Word2Vec for learning these vector representations from text data.
*   Word2Vec Alternatives: Discussing other approaches like GloVe and FastText that offer improvements or different perspectives.
*   Working with Word Embeddings: Practical aspects of using pre-trained word embeddings and training your own.

<----------section---------->

## Limitations of TF-IDF
TF-IDF (Term Frequency-Inverse Document Frequency) is a traditional method for representing text data. It counts the frequency of terms within a document, weighted by the inverse document frequency across a corpus. While effective for certain tasks, it has notable limitations.

TF-IDF counts terms according to their exact spelling, meaning it treats different words as completely distinct, even if they have similar meanings.  This is a significant drawback because it fails to capture the underlying semantic relationships between words.

Texts with the same meaning will have completely different TF-IDF vector representations if they use different words.

**Examples:**
*   The movie was amazing and exciting.
*   The film was incredible and thrilling.
These two sentences convey similar sentiments, yet their TF-IDF vectors will be dissimilar because they use different words.

*   The team conducted a detailed analysis of the data and found significant correlations between variables.
*   The group performed an in-depth examination of the information and discovered notable relationships among factors.
These sentences express nearly identical ideas but will have distinct TF-IDF representations.

<----------section---------->

### Term Normalization
Term normalization techniques aim to address some of these limitations by grouping words with similar spellings together.

Techniques like stemming and lemmatization help normalize terms. Stemming reduces words to their root form (e.g., "running" becomes "run"), while lemmatization reduces words to their dictionary form (lemma) considering the context (e.g., "better" becomes "good"). Words with similar spellings are collected under a single token.

**Disadvantages:**
*   They fail to group most synonyms. Stemming and lemmatization primarily focus on morphological similarities, not semantic ones.

*   They may group together words with similar/same spelling but different meanings (polysemy and homonymy). This can lead to confusion and inaccurate representations.

*   She is leading the project vs. The plumber leads the pipe.
    The word "leads" has different meanings (guiding vs. pipe connection), but stemming/lemmatization might group them under the same token.

*   The bat flew out of the cave vs. He hit the ball with a bat.
    "Bat" refers to an animal vs. a piece of sports equipment, but these distinct meanings might be conflated.

<----------section---------->

### TF-IDF Applications
TF-IDF is sufficient for many NLP applications:
*   Information Retrieval (Search Engines): Ranking documents based on their relevance to a search query.
*   Information Filtering (Document Recommendation): Suggesting documents to users based on their past interactions.
*   Text Classification: Categorizing documents into predefined classes.

Other applications require a deeper understanding of text semantics:
*   Text generation (Chatbot): Creating coherent and contextually relevant responses.
*   Automatic Translation: Converting text from one language to another while preserving meaning.
*   Question Answering: Providing accurate answers to questions based on a given text.
*   Paraphrasing and Text Rewriting: Generating alternative versions of a text with the same meaning.

TF-IDF's inability to capture semantic similarity makes it inadequate for these advanced tasks, where understanding the underlying meaning of the text is crucial.

<----------section---------->

## Bag-of-Words (recap)
Before diving into word embeddings, let's briefly recap the Bag-of-Words (BoW) model, another traditional text representation technique.

Each word is assigned an index that represents its position in the vocabulary:
*   the 1st word (e.g., apple) has index 0
*   the 2nd word (e.g., banana) has index 1
*   the 3rd word (e.g., king) has index 2
*   ...

Each word is then represented by a one-hot vector:
*   apple = (1,0,0,0,…,0)
*   banana = (0,1,0,0,…,0)
*   king = (0,0,1,0,…,0)

This means that for a vocabulary of size *V*, each word is represented by a vector of length *V* with a single '1' at the index corresponding to the word and '0' everywhere else.

<----------section---------->

## Bag-of-Words (recap)
With this encoding, the distance between any pair of vectors is always the same. It does not capture the semantics of words. Furthermore, it is not efficient since it uses sparse vectors.

**Note:** The figure shows only three dimensions of a space where dimensions equals the cardinality of the vocabulary.  This highlights that the one-hot vectors are orthogonal (perpendicular) to each other, meaning they are equidistant and have no inherent relationship in the vector space.

The key issues with BoW representation are:
1.  **Lack of Semantic Meaning**: The one-hot encoding provides no information about the relationships between words. Words with similar meanings are treated as completely unrelated.
2.  **Sparsity**: One-hot vectors are highly sparse, meaning most of their elements are zero. This is inefficient in terms of storage and computation, especially for large vocabularies.
3. **Fixed Distance**: The Euclidean distance between any two one-hot vectors is the same, regardless of the actual similarity between the words.

<----------section---------->

## Word Embeddings
Word Embeddings: A technique for representing words with vectors (A.K.A. Word Vectors) that are:
*   Dense: Unlike one-hot vectors, word embeddings have mostly non-zero elements.
*   With dimensions much smaller than the vocabulary size: This makes them more efficient to store and process.  Typical dimensions range from 50 to 300.
*   In a continuous vector space: This allows for mathematical operations to be performed on the vectors, capturing semantic relationships.

**Key feature:** Vectors are generated so that words with similar meanings are close to each other. The position in the space represents the semantics of the word.

Word Embeddings:
*   king and queen are close to each other: because they both represent royalty.
*   apple and banana are close to each other: because they are both fruits.
The words of the first group are far from those of to the second group: highlighting the semantic distinction between royalty and fruit.

**Example:**
*   Apple = (0.25,0.16)
*   Banana = (0.33,0.10)
*   King = (0.29,0.68)
*   Queen = (0.51,0.71)
This is a simplified illustration with 2D vectors; real word embeddings have much higher dimensionality.

<----------section---------->

### Word Embedding: Properties
Word embeddings enable semantic text reasoning based on vector arithmetic.

**Examples:**
*   Subtracting royal from king we arrive close to man: king – royal ≈ man
*   Subtracting royal from queen we arrive close to woman: queen – royal ≈ woman
*   Subtracting man from king and adding woman we arrive close to queen: king – man + woman ≈ queen

These examples highlight the ability of word embeddings to capture relationships between words through vector operations.

<----------section---------->

### Semantic Queries
Word embeddings allow for searching words or names by interpreting the semantic meaning of a query.

**Examples:**
*   Query: "Famous European woman physicist"
    ```
    wv['famous'] + wv['European'] + wv['woman'] + wv['physicist'] ≈ wv['Marie_Curie'] ≈ wv['Lise_Meitner'] ≈ …
    ```
    This demonstrates how combining the vectors of multiple words can lead to a result close to words representing related concepts.
*   Query: “Popular American rock band”
    ```
    wv['popular'] + wv['American'] + wv['rock'] + wv['band'] ≈ wv['Nirvana'] ≈ wv['Pearl Jam'] ≈ …
    ```
    Similarly, this query combines characteristics to find relevant musical groups.

<----------section---------->

### Analogies
Word embeddings enable answering analogy questions by leveraging their semantic relationships.

**Examples:**
*   Who is to physics what Louis Pasteur is to germs?
    ```
    wv['Louis_Pasteur'] – wv['germs'] + wv['physics'] ≈ wv['Marie_Curie']
    ```
*   Marie Curie is to science as who is to music?
    ```
    wv['Marie_Curie'] – wv['science'] + wv['music'] ≈ wv['Ludwig_van_Beethoven']
    ```
*   Legs is to walk as mouth is to what?
    ```
    wv['legs'] – wv['walk'] + wv['mouth'] ≈ wv['speak'] or wv['eat']
    ```
These examples showcase the capability to solve analogy problems, further demonstrating the power of capturing semantic relationships.

<----------section---------->

## Visualizing Word Embeddings
Google News Word2vec 300-D vectors projected onto a 2D map using PCA. Semantic proximity approximates geographical proximity.

This section discusses how high-dimensional word embeddings can be visualized in lower dimensions (e.g., 2D) using techniques like Principal Component Analysis (PCA).

news corpus, cities that are similar in size and culture are clustered close together despite being far apart geographically, such as San Diego and San Jose, or vacation destinations such as Honolulu and Reno.

This phenomenon occurs because word embeddings capture semantic relationships, which can reflect cultural and functional similarities even when geographical proximity is absent.

Fortunately you can use conventional algebra to add the vectors for cities to the vectors for states and state abbreviations. As you discovered in chapter 4, you can use tools such as principal components analysis to reduce the vector dimensions from your 300 dimensions to a human-understandable 2D representation. PCA enables you to see the projection or “shadow” of these 300-D vectors in a 2D plot. Best of all, the PCA algorithm ensures that this projection is the best possible view of your data, keeping the vectors as far apart as possible. PCA is like a good photographer that looks at something from every possible angle before composing the optimal photograph. You don’t even have to normalize the length of the vectors after summing the city + state + abbrev vectors, because PCA takes care of that for you.

We saved these augmented city word vectors in the nlpia package so you can load them to use in your application. In the following code, you use PCA to project them onto a 2D plot.

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
us_300D = get_data('cities_us_wordvectors')
us_2D = pca.fit_transform(us_300D.iloc[:, :300])
```

Figure 6.8 shows the 2D projection of all these 300-D word vectors for US cities:
Figure 6.8 Google News Word2vec 300-D vectors projected onto a 2D map using PCA

Listing 6.11 Bubble chart of US cities
The 2D vectors producted by PCA are for visualization. Retain the original 300-D Word2vec vectors for any vector reasoning you might want to do.
The last column of this DataFrame contains the city name, which is also stored in the DataFrame index.

Memphis, Nashville,
Charlotte, Raleigh, and Atlanta
Houston and Dallas
nearly coincide.
Ft. Worth
El Paso
San Diego
LA, SF, and San Jose
America/Los_…(0.9647851, –0.7217035)
Portland, OR
Honolulu, Reno,
Mesa, Tempe, and Phoenix

Size: population
Position: semantics
Color: time zone
America/Phoenix
America/New_York
America/Anchorage
America/Indiana/Indianapolis
America/Los_Angeles
America/Boise
America/Denver
America/Kentucky/Louisville
America/Chicago
Pacific/Honolulu

The PCA projection effectively reduces the 300 dimensions to 2 while attempting to preserve the distances between the points as much as possible. Note that population, timezone, and the semantic meaning of a city influence its position on this 2D map.

<----------section---------->

## Learning Word Embeddings

### Word2Vec
Word embeddings was introduced by Google in 2013 in the following paper:
*   T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space in 1st International Conference on Learning Representations, ICLR 2013

The paper defines Word2Vec:
*   A methodology for the generation of word embeddings
*   Based on neural networks
*   Using unsupervised learning on a large unlabeled textual corpus

Word2Vec is a seminal method in the field of word embeddings. It utilizes neural networks trained on large amounts of text data without explicit labels (unsupervised learning) to generate vector representations of words.  These embeddings capture semantic relationships by placing semantically similar words closer to each other in the vector space.

<----------section---------->

### Word2Vec
Idea: words with similar meanings are often found in similar contexts.
*   Context: a sequence of words in a sentence

**Example:**
*   Consider the sentence Apple juice is delicious.
*   Remove one word.
*   The remaining sentence is ____ juice is delicious.
*   Ask someone to guess the missing word.
*   Terms such as banana, pear or apple would probably be suggested.
*   These terms have similar meanings and used in similar contexts.

This illustrates the core idea behind Word2Vec: words that appear in similar contexts tend to have similar meanings. The surrounding words provide valuable information about the semantics of the missing word.

<----------section---------->

### Continuous Bag-of-Word
A neural network is trained to predict the central token of a context of m tokens.
*   **Input:** the bag of words composed of the sum of all one-hot vectors of the surrounding tokens.
*   **Output:** a probability distribution over the vocabulary with its maximum in the most probable missing word.
*Example:* Claude Monet painted the Grand Canal in Venice in 1806.

The Continuous Bag-of-Words (CBOW) model is one of the two main architectures in Word2Vec. It takes a set of context words as input and tries to predict the target word. The surrounding words are represented as one-hot encoded vectors, which are then summed (or averaged) to create a single input vector. The network outputs a probability distribution over the entire vocabulary, aiming to have the highest probability for the actual target word.

<----------section---------->

### Continuous Bag-of-Word
|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

The CBOW architecture consists of an input layer, a hidden layer, and an output layer. The input and output layers have a size equal to the vocabulary size (|V|), while the hidden layer has a size equal to the desired word embedding dimension (n). This hidden layer is the key to generating the word embeddings.

<----------section---------->

### SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH
Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

This provides a practical guideline for choosing between the two Word2Vec architectures. Skip-gram is better for smaller datasets and capturing the meaning of less frequent words, while CBOW is faster and performs well with larger datasets and frequent words.

<----------section---------->

### Continuous bag of words vs. bag of words
In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surrounding words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

**Example:** for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

This clarifies the distinction between the general Bag-of-Words approach, where word order is ignored, and the Continuous Bag-of-Words model, which considers the context of surrounding words within a sliding window to capture relationships.

<----------section---------->

### Continuous Bag-of-Word
Ten 5-gram examples from the sentence about Monet

CONTINUOUS BAG-OF-WORDS APPROACH
In the continuous bag-of-words approach, you’re trying to predict the center word based on the surrounding words. Instead of creating pairs of input and output tokens, you’ll create a multi-hot vector of all surrounding terms as an input vector. The multi-hot input vector is the sum of all one-hot vectors of the surrounding tokens to the center, target token.

Based on the training sets, you can create your multi-hot vectors as inputs and map them to the target word as output. The multi-hot vector is the sum of the one-hot vectors of the surrounding words’ training pairs wt-2 + wt-1 + wt+1 + wt+2 . You then build the training pairs with the multi-hot vector as the input and the target word wt as the output. During the training, the output is derived from the softmax of the output node with the highest probability.

| Input word wt-2 | Input word wt-1 | Input word wt+1 | Input word wt+2 | Expected output wt |
|-----------------|-----------------|-----------------|-----------------|--------------------|
| Monet           | painted         | Claude          | Monet             | Claude             |
| Claude          | painted         | the             | Monet             | painted            |
| Claude          | Monet           | the             | Grand             | painted            |
| Monet           | painted         | Grand           | Canal           | the                |
| painted         | the             | Canal           | of              | Grand              |
| the             | Grand           | of              | Venice            | Canal              |
| Grand           | Canal           | Venice          | in              | of               |
| Canal           | of              | in              | 1908            | Venice             |
| of              | Venice          | 1908            | in              | in               |
| Venice          | in              | 1908            |                  |                  |

target word w t = word to be predicted
surrounding words w t-2, w t-1 = input words
surrounding words w t+1, w t+2

painted the Grand Canal of Venice in 1908.

This table illustrates the training data generation process for the CBOW model. For each target word, a context of surrounding words is created, and the model learns to predict the target word from this context.  The input is the multi-hot vector representing the context, and the output is the target word.

<----------section---------->

### Continuous Bag-of-Word
After the training is complete the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words.
Similar words are found in similar contexts …
… their weights to the hidden layer adjust in similar ways
… this result in similar vector representations

SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH
Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

Continuous bag of words vs. bag of words
In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surround-ing words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

Example for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

the highest probability will be converted to 1, and all remaining terms will be set to 0.
This simplifies the loss calculation.
 After training of the neural network is completed, you’ll notice that the weights have been trained to represent the semantic meaning. Thanks to the one-hot vector conversion of your tokens, each row in the weight matrix represents each word from the vocabulary for your corpus. After the training, semantically similar words will have similar vectors, because they were trained to predict similar surrounding words. This is purely magical!
 After the training is complete and you decide not to train your word model any further, the output layer of the network can be ignored. Only the weights of the inputs to the hidden layer are used as the embeddings. Or in other words: the weight matrix is your word embedding. The dot product between the one-hot vector representing the input term and the weights then represents the word vector embedding.

Retrieving word vectors with linear algebra
The weights of a hidden layer in a neural network are often represented as a matrix: one column per input neuron, one row per output neuron. This allows the weight matrix to be multiplied by the column vector of inputs coming from the previous layer to generate a column vector of outputs going to the next layer . So if you multiply (dot product) a one-hot row vector by the trained weight matrix, you’ll get a vector that is one weight from each neuron (from each matrix column). This also works if you take the weight matrix and multiply it (dot product) by a one-hot column vector for the word you are interested in.

Of course, the one-hot vector dot product just selects that row from your weight matrix that contains the weights for that word, which is your word vector. So you could easily retrieve that row by just selecting it, using the word’s row number or index num-ber from your vocabulary.

WE of Monet

The weights connecting the input layer to the hidden layer are the word embeddings. These weights are adjusted during training to minimize the prediction error.  Words appearing in similar contexts will have their weights adjusted in similar ways, resulting in similar vector representations. The output layer is discarded after training.

<----------section---------->

### Skip-Gram
Alternative training method for Word2Vec
*   A neural network is trained to predict a context of m tokens based on the central token
*   **Input:** the one-hot vector of the central token
*   **Output:** the one-hot vector of a surrounding word (one training iteration for each surrounding word)

output example skip-grams are shown in figure 6.3. The predicted words for these skip-grams are the neighboring words “Claude,” “Monet,” “the,” and “Grand.”

WHAT IS A SKIP-GRAM? Skip-grams are n -grams that contain gaps because you skip over intervening tokens. In this example, you’re predicting “Claude” from the input token “painted,” and you skip over the token “Monet.”
The structure of the neural network used to predict the surrounding words is similar to the networks you learned about in chapter 5. As you can see in figure 6.4, the net-work consists of two layers of weights, where the hidden layer consists of n neurons; n is the number of vector dimensions used to represent a word. Both the input and out-put layers contain M neurons, where M is the number of words in the model’s vocabu-lary. The output layer activation function is a softmax, which is commonly used for classification problems.

WHAT IS SOFTMAX ?
The softmax function is often used as the activation function in the output layer of neural networks when the network’s goal is to learn classification problems. The softmax will squash the output results between 0 and 1, and the sum of all outputs will always add up to 1. That way, the results of an output layer with a softmax function can be considered as probabilities.
 For each of the K output nodes, the softmax output value can be calculated using the normalized exponential function:

```
σ(z)j = exp(z_j) / Σ_{k=1}^{K} exp(z_k)
```

**Example 3D vector:**
v = [0.5, 0.9, 0.2]

word w t = input word

painted the Grand Canal of Venice in 1908.
surrounding words w t-2 , w t-1 = words to be predicted
surrounding words w t+1 , w t+2

The Skip-gram model is the second main architecture in Word2Vec. Unlike CBOW, it takes a single word as input and tries to predict the surrounding words (context). For each word in the corpus, the skip-gram model generates multiple training examples by selecting words within a certain window size. This helps to capture the context in which the central word is used and create stronger word associations.

<----------section---------->

### Skip-Gram
|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

The “squashed” vector after the softmax activation would look like this:

**Example 3D vector after softmax:**
σ(v) = [0.309, 0.461, 0.229]

Notice that the sum of these values (rounded to three significant digits) is approximately 1.0, like a probability distribution.

Figure 6.4 shows the numerical network input and output for the first two surrounding words. In this case, the input word is “Monet,” and the expected output of the network is either “Claude” or “painted,” depending on the training pair.

This reinforces the architecture of the Skip-gram model, which is similar to CBOW, with input and output layers of size |V| (vocabulary size) and a hidden layer of size *n* (embedding dimension). The softmax function is used to normalize the output probabilities, ensuring they sum to 1.

<----------section---------->

### Skip-Gram
Ten 5-gram examples from the sentence about Monet

NOTE When you look at the structure of the neural network for word embedding, you’ll notice that the implementation looks similar to what you discovered in chapter 5.

How does the network learn the vector representations?
To train a Word2vec model, you’re using techniques from chapter 2. For example, in table 6.1, wt represents the one-hot vector for the token at position t. So if you want to train a Word2vec model using a skip-gram window size (radius) of two words, you’re considering the two words before and after each target word. You would then use your 5-gram tokenizer from chapter 2 to turn a sentence like this:

```python
sentence = "Claude Monet painted the Grand Canal of Venice in 1806."
```

into 10 5-grams with the input word at the center, one for each of the 10 words in the original sentence.

The training set consisting of the input word and the surrounding (output) words are now the basis for the training of the neural network. In the case of four surrounding words, you would use four training iterations, where each output word is being pre-dicted based on the input word.

Each of the words are represented as one-hot vectors before they are presented to the network (see chapter 2). The output vector for a neural network doing embedding is similar to a one-hot vector as well. The softmax activation of the output layer nodes (one for each token in the vocabulary) calculates the probability of an output word being found as a surrounding word of the input word. The output vector of word probabilities can then be converted into a one-hot vector where the word with the highest probability will be converted to 1, and all remaining terms will be set to 0.

| Input word wt | Expected output wt-2 | Expected output wt-1 | Expected output wt+1 | Expected output wt+2 |
|---------------|----------------------|----------------------|----------------------|----------------------|
| Claude        |                      |                      | Monet                |                      |
| Monet         |                      | Claude               | painted              |                      |
| painted       | Claude               | Monet                | the                  | Grand                |
| the           | Monet                | painted              | Grand                | Canal                |
| Grand         | painted              | the                  | Canal                | of                   |
| Canal         | the                  | Grand                | of                   | Venice               |
| of            | Grand                | Canal                | Venice               | in                   |
| Venice        | Canal                | of                   | in                   | 1908                 |
| in            | of                   | Venice               | 1908                 |                      |
| 1908          | Venice               | in                   |                      |                      |

This section elaborates on the training process of the Skip-gram model. It highlights how 5-grams are generated from a sentence and used to train the neural network. The one-hot encoded input word is used to predict the surrounding words, and the softmax activation function helps to calculate the probabilities of each word being a surrounding word.

<----------section---------->

### Skip-Gram
After the training is complete the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words.
Similar words are found in similar contexts …
… their weights to the hidden layer adjust in similar ways
… this result in similar vector representations

This reiterates the importance of the weights connecting the input layer to the hidden layer as the final word embeddings.  These weights are adjusted during the training process, and words found in similar contexts will have similar weight adjustments, leading to similar vector representations.

<----------section---------->

### CBOW vs Skip-Gram
**CBOW**
*   Higher accuracies for frequent words, much faster to train, suitable for large datasets

**Skip-Gram**
*   Works well with small corpora and rare terms

**Dimension of Embeddings (n)**
*   Large enough to capture the semantic meaning of tokens for the specific task
*   Not so large that it results in excessive computational expense

This provides a concise summary of the trade-offs between CBOW and Skip-gram models and guidelines for choosing the appropriate embedding dimension (*n*). The embedding dimension should be large enough to capture semantic information but not so large as to increase the computational complexity of training.

<----------section---------->

### Improvements to Word2Vec
**Frequent Bigrams**
*   Some words often occur in combination
*   Elvis is often followed by Presley forming a bigram
*   Predicting Presley after Elvis doesn't add much value
*   To let the network focus on useful predictions frequent bigrams and trigrams are included as terms in the Word2vec vocabulary
*   Inclusion criteria: co-occurrence frequency greater than a threshold

```
score(wi, wj) = (count(wi, wj) - δ) / (count(wi) * count(wj))
```

*   Examples: Elvis\_Presley, New\_York, Chicago\_Bulls, Los\_Angeles\_Lakers, etc.

This section covers improvements to the basic Word2Vec model. Incorporating frequent bigrams (two-word sequences) and trigrams (three-word sequences) can improve the model's ability to capture the meaning of phrases. The scoring function helps determine which word combinations should be treated as single tokens. The *δ* parameter is a discount factor.

<----------section---------->

### Improvements to Word2Vec
**Subsampling Frequent Tokens**
*   Common words (like stop-words) often don’t carry significant information
*   Being frequent, they have a big influence on the training process

**To reduce their effect...**
*   During training (skip-gram method), words are sampled in inverse proportion to their frequency
*   **Probability of sampling:**

```
P(wi) = 1 - sqrt(t / f(wi))
```
Where ```f(wi)``` is the frequency of a word across the corpus, and ```t``` represents a frequency threshold above which you want to apply the subsampling probability.

*   The effect is like the IDF effect on TF-IDF vectors

Subsampling frequent tokens helps to reduce the impact of common words (like "the," "a," "is") on the training process. These words often don't contribute much to the semantic meaning of a sentence, and their high frequency can skew the word embeddings. The provided formula determines the probability of sampling a word, giving less weight to more frequent words. This is analogous to the inverse document frequency (IDF) weighting in TF-IDF.

<----------section---------->

### Improvements to Word2Vec
**Negative Sampling**
*   Each training example causes the network to update all weights
*   With thousands or millions of words in the vocabulary, this makes the process computationally expensive

**Instead of updating all weights...**
*   Select 5 to 20 negative words (words not in the context)
*   Update weights only for the negative words and the target word
*   Negative words are selected based on their frequency
*   Common words are chosen more often than rare words
*   The quality of embeddings in maintained

Negative sampling is a technique to reduce the computational cost of training Word2Vec. Instead of updating the weights for all words in the vocabulary for each training example, only a small number of "negative" words (words that are not in the context of the target word) are updated. These negative words are chosen based on their frequency, with more frequent words being selected more often.

<----------section---------->

## Word2Vec Alternatives

### GloVe
Global Vectors for Word Representation
*   Introduced by researchers from Stanford University in 2014
*   Uses classical optimization methods like Singular Value Decomposition instead of neural networks

**Advantages:**
*   Comparable precision to Word2Vec
*   Significantly faster training times
*   Effective on small corpora

[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)

GloVe (Global Vectors for Word Representation) is an alternative word embedding technique that leverages global word co-occurrence statistics. Instead of using neural networks, it uses Singular Value Decomposition (SVD) on the word co-occurrence matrix. GloVe often trains faster than Word2Vec and can be effective even with smaller datasets.

<----------section---------->

### FastText
Introduced by Facebook researchers in 2017
*   Based on sub-words, predicts the surrounding n-character grams rather than the surrounding words
*   Example: the word whisper would generate the following 2- and 3-character grams: wh, whi, hi, his, is, isp, sp, spe, pe, per, er

**Advantages:**
*   Particularly effective for rare or compound words
*   Can handle misspelled words
*   Available in 157 languages

[https://fasttext.cc/](https://fasttext.cc/)

FastText is another word embedding technique that differs from Word2Vec and GloVe by considering sub-word information. It represents words as bags of character n-grams. This allows FastText to handle rare words and misspelled words more effectively and is particularly useful for morphologically rich languages. FastText is available in many languages, making it a versatile option for multilingual NLP tasks.

<----------section---------->

### Static Embeddings
Word2Vec, GloVe, FastText are Static Embeddings
*   Each word is represented by a single static vector that captures the average meaning of the word based on the training corpus
*   Once trained, vectors do not change based on context
*   This does not account for polysemy and
<----------section---------->

## Lesson 6 ##

Neural Networks for NLP

This lesson delves into the application of neural networks within the field of Natural Language Processing (NLP), particularly focusing on Recurrent Neural Networks (RNNs) and their variants. The material is presented by Nicola Capuano and Antonio Greco from the Department of Enterprise Engineering (DIEM) at the University of Salerno, as part of the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree in Computer Engineering).

## Outline

The lecture covers the following topics:

*   **Recurrent Neural Networks (RNNs):** Introduction to the basic architecture and principles of RNNs.
*   **RNN Variants:** Exploration of different types of RNNs like Bidirectional RNNs, Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs).
*   **Building a Spam Detector:** Practical application of RNNs for classifying SMS messages as spam or not spam.
*   **Introduction to Text Generation:** Overview of generative models and the use of RNNs for generating text.
*   **Building a Poetry Generator:** Developing a poetry generator using RNNs, providing a hands-on experience with text generation.

<----------section---------->

## Recurrent Neural Networks

### Neural Networks and NLP
Neural networks are extensively employed in text processing tasks. However, standard feedforward neural networks have a significant limitation: they lack memory. Each input is processed independently, without any regard to previous inputs or the maintenance of a state. This means that to process a sequence of words (i.e., a text), the entire sequence must be presented to the network at once. The entire text is treated as a single, large data point. Examples of this approach include using Bag-of-Words (BoW) or Term Frequency-Inverse Document Frequency (TF-IDF) vectors to represent text, or averaging the word vectors of the text.

### Neural Networks with Memory
When humans read a text, they employ a different strategy:

*   Sentences and words are processed sequentially, one at a time.
*   The reader maintains a memory of what has been read previously.
*   An internal model of the text is continuously updated as new information arrives.

A Recurrent Neural Network (RNN) mimics this principle. It processes sequences of information by iterating through the elements of the sequence, such as the words of a text represented as word embeddings. An RNN maintains a state that contains information about what has been seen so far in the sequence. This state is updated at each step.

<----------section---------->

### Recurrent Neural Networks

*   RNNs consist of feedforward network layers (circles in diagrams) composed of one or more neurons.
*   The output of the hidden layer is passed to the output layer as in a typical feedforward network.
*   Crucially, the output of the hidden layer is also fed back as input to the same hidden layer in the next time step, along with the normal input. This feedback loop is what gives RNNs memory.

**CHAPTER 8 Loopy (recurrent) neural networks (RNNs)**

One-dimensional convolutions offer a method for capturing relationships between tokens by examining windows of words. Pooling layers (discussed in a prior chapter) were designed to handle slight word order variations. RNNs offer a distinct approach by introducing the concept of memory. This memory enables the network to treat language not as a static chunk of data but as a sequence unfolding over time, token by token.

<----------section---------->

### Remembering with recurrent networks

The words in a document are rarely independent of each other. The occurrence of one word often influences or is influenced by the occurrence of other words. For example, consider the following two sentences:

"The stolen car sped into the arena."
"The clown car sped into the arena."

The reader's emotional response to these two sentences may differ significantly at the end, even though they share identical adjective, noun, verb, and prepositional phrase construction. The single adjective change early in the sentence profoundly affects the reader's inference about what is happening.

The goal is to model this relationship—to understand how "arena" and even "sped" acquire slightly different connotations based on an adjective appearing earlier in the sentence, even if the adjective does not directly modify these words.

If a model can "remember" what just happened the moment before (specifically, what happened at time step *t* when looking at time step *t+1*), it can capture patterns that emerge when certain tokens appear relative to others in a sequence. Recurrent neural networks (RNNs) allow neural networks to remember past words within a sentence.

A single recurrent neuron in the hidden layer adds a recurrent loop that "recycles" the output of the hidden layer at time *t*. The output at time *t* is added to the next input at time *t+1*. This new input is processed by the network at time step *t+1* to create the output for that hidden layer at time *t+1*. That output at *t+1* is then recycled back into the input again at time step *t+2*, and so on.

Figure 8.3 (not provided in text) depicts a Recurrent Neural Net.

<sup>1</sup> In finance, dynamics, and feedback control, this is often called an auto-regressive moving average (ARMA) model: [https://en.wikipedia.org/wiki/Autoregressive\_model](https://en.wikipedia.org/wiki/Autoregressive_model).

Figure 8.4 (not provided in text) depicts an Unrolled Recurrent Neural Net.

<----------section---------->

The unrolled representation helps to visualize how information flows through time, and is useful to understand backpropagation. Keep in mind that the three unfolded networks are snapshots of the same network with a single set of weights.

Focusing on the original representation of an RNN before unrolling reveals the input-weight relationships, which are shown in Figures 8.5 and 8.6 (not provided in text).

Each neuron in the hidden state has a set of weights applied to each element of each input vector, similar to a normal feedforward network. However, an RNN also has an additional set of trainable weights applied to the output of the hidden neurons from the previous time step. The network learns how much weight or importance to assign to the "past" events as a sequence is input token by token.

**TIP:** The first input in a sequence has no "past," so the hidden state at *t* = 0 receives an input of 0 from its *t* - 1 self. Alternatively, the initial state value can be "filled" by passing related but separate samples into the network sequentially. The final output of each sample can be used as the *t* = 0 input for the next sample. Alternative "filling" approaches to preserve more information are covered in the section on statefulness later in the chapter.

<----------section---------->

Consider a set of labeled documents. Instead of passing the collection of word vectors into a convolutional neural network (CNN) all at once (as in a previous chapter, depicted in Figure 8.7, not provided), the sample is processed one token at a time, passing the tokens individually into the RNN (as depicted in Figure 8.8, not provided).

The word vector for the first token is passed in, and the network produces an output. Then, the second token is passed in, along with the output from the first token. This process continues, with the third token passed in along with the output from the second token, and so on. The network maintains a concept of before and after, cause and effect, and some notion of time.

Text: The clown car sped into the arena

<----------section---------->

### RNN Training

Now the network is "remembering" something, but how does backpropagation work in this structure?

### Backpropagation through time

All the networks discussed so far have a target variable to aim for, and RNNs are no exception. However, there isn't a label for each token; there is only a single label for all the tokens in each sample text. The label corresponds to the entire sample document.

Isadora Duncan

**TIP:** Although tokens are discussed as the input to each time step, RNNs work identically with any time series data. The tokens can be discrete or continuous, such as readings from a weather station, musical notes, or characters in a sentence.

Initially, the output of the network is examined at the last time step, and this output is compared to the label. The difference defines the error, which the network attempts to minimize. For a given data sample, it is broken into smaller pieces that are fed into the network sequentially. Instead of dealing with the output generated by any of these "subsamples" directly, it is fed back into the network.

Only the final output matters, at least initially. Each token from the sequence is input into the network, and the loss is calculated based on the output from the last time step (token) in the sequence (as depicted in Figure 8.9, not provided).

<----------section---------->

### Forward pass

Figure 8.10 (not provided in text)

### Backpropagation

### What are RNNs Good For?
RNNs are suitable for processing short bits of text, such as individual sentences. RNNs allow ingesting an infinitely long sequence of text. Also, they can generate text for as long as needed. RNNs enable generative conversational chatbots and text summarizers that combine concepts from many different places within the documents.

| Type          | Description                                                                   | Applications                                                                                                                                                                |
| ------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| One to Many   | One input tensor used to generate a sequence of output tensors              | Generate chat messages, answer questions, describe images                                                                                                                   |
| Many to One   | Sequence of input tensors gathered up into a single output tensor             | Classify or tag text according to its language, intent, or other characteristics                                                                                           |
| Many to Many  | A sequence of input tensors used to generate a sequence of output tensors | Translate, tag, or anonymize the tokens within a sequence of tokens, answer questions, participate in a conversation                                                        |

<----------section---------->

The superpower of RNNs is that they process sequences of tokens or vectors, which is beneficial because one doesn't have to truncate and pad input text. An RNN can generate text sequences that go on indefinitely; you can set the conditions to terminate generation according to specified criteria.

© Manning Publications Co. To comment go to liveBook Licensed to Nicola Capuano <nicola@capuano.biz>

### RNNs for Text Generation
When used for text generation:

*   The output of each time step is as important as the final output.
*   Error is captured and backpropagated at each step to adjust all network weights.

In this scenario, each step is considered, so backpropagation occurs at every step through time.

<----------section---------->

BUT YOU DO CARE WHAT CAME OUT OF THE EARLIER STEPS

Sometimes you may care about the entire sequence generated by each of the intermediate time steps. Figure 8.11 (not provided) shows a path for capturing the error at any given time step and carrying that backward to adjust all the weights of the network during backpropagation.

This process is like normal backpropagation through time for *n* time steps. In this case, you're now backpropagating the error from multiple sources at the same time. The weight corrections are additive. Backpropagate from the last time step all the way to the first, summing up what you’ll change for each weight. Then do the same with the error calculated at the second-to-last time step and sum up all the changes all the way back to *t* =0. Repeat this process until you get all the way back down to time step 0 and then backpropagate it as if it were the only one in the world. Then apply the grand total of the updates to the corresponding hidden layer weights all at once.

In figure 8.12 (not provided), the error is backpropagated from each output all the way back to t=0, and aggregated, before finally applying changes to the weights. With a standard feedforward network, the weights are updated only after the proposed change in the weights is calculated for the entire backpropagation step for that input (or set of inputs). In the case of an RNN, this backpropagation includes the updates all the way back to time *t* =0.

<----------section---------->

## RNN Variants

### Bidirectional RNN

A Bidirectional RNN has two recurrent hidden layers:

*   One processes the input sequence forward.
*   The other processes the input sequence backward.
*   The output of those two are concatenated at each time step.

By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN.

*   Example: they wanted to pet the dog whose fur was brown

Figure 8.13 (not provided) depicts a Bidirectional recurrent neural net.

The basic idea is to arrange two RNNs side by side, passing the input into one as normal and the same input backward into the other net. The output of those two are then concatenated at each time step to the related (same input token) time step in the other network. The output of the final time step in the input is concatenated with the output generated by the same input token at the first time step of the backward net.

**TIP:** Keras also has a `go_backwards` keyword argument. If this is set to `True`, Keras automatically flips the input sequences and inputs them into the network in reverse order. This is the second half of a bidirectional layer. A recurrent neural network (due to the vanishing gradients problem) is more receptive to data at the end of the sample than at the beginning. If samples are padded with `<PAD>` tokens at the end, the good information may be buried. `go_backwards` can be a quick way around this problem.

<----------section---------->

With these tools, one is well on the way to modeling language itself and how it's used. And with that deeper algorithmic understanding, you can generate new statements.

Ahead of the Dense layer is a vector that is of shape (number of neurons x 1) coming out of the last time step of the Recurrent layer for a given input sequence. This vector is the parallel to the thought vector you got out of the convolutional neural network.

In LSTMs, the rules governing the information stored in the state (memory) are trained neural nets themselves—therein lies the magic. They can be trained to learn what to remember, while the rest of the recurrent net learns to predict the target label! With a memory and state, you can begin to learn dependencies stretching across each data sample.

With LSTMs, patterns that humans process subconsciously become available to your model. And with those patterns, you can generate novel text using those patterns.

How does this work (see figure 9.1, not provided)?

The memory state is affected by the input and affects the layer output just as in a normal recurrent net. That memory state persists across all the time steps of the time series (your sentence or document). So, each input can have an effect on the memory state as well as an effect on the hidden layer output. The memory state learns what to remember at the same time it learns to reproduce the output, using standard backpropagation!

<----------section---------->

### LSTM

RNNs should theoretically retain information from inputs seen earlier, but they struggle to learn long-term dependencies. Vanishing Gradient: as more layers are added, as the network becomes difficult to train

Long Short-Term Memory networks are designed to solve this problem:

*   Introduces a state updated with each training example.
*   The rules to decide what information to remember and what to forget are trained.

Figure 9.2 (not provided) depicts an Unrolled LSTM network and its memory

Figure 9.2 looks similar to a normal recurrent neural net. However, in addition to the activation output feeding into the next time-step version of the layer, you add a memory state that also passes through time steps of the network. At each time-step iteration, the hidden recurrent unit has access to the memory unit. The addition of this memory unit, and the mechanisms that interact with it, make this quite a bit different from a traditional neural network layer. It's possible to design a set of traditional recurrent neural network layers (a computational graph) that accomplishes all the computations that exist within an LSTM layer. An LSTM layer is just a highly specialized recurrent neural network.

**TIP** In much of the literature, the "Memory State" block shown in figure 9.2 is referred to as an **LSTM cell** rather than an **LSTM neuron**, because it contains two additional neurons or gates just like a silicon computer memory cell. When an LSTM memory cell is combined with a sigmoid activation function to output a value to the next LSTM cell, this structure, containing multiple interacting elements, is referred to as an **LSTM unit**. Multiple LSTM units are combined to form an **LSTM layer**. The horizontal line running across the unrolled recurrent neuron in figure 9.2 is the signal holding the memory or state. It becomes a vector with a dimension for each LSTM cell as the sequence of tokens is passed into a multi-unit LSTM layer.

LSTM allow past information to be reinjected later, thus fighting the vanishing-gradient problem

<----------section---------->

### GRU
Gated Recurrent Unit (GRU) is an RNN architecture designed to solve the vanishing gradient problem

#### Main Features
*   Like LSTM, but with a simpler architecture
*   GRU lacks a separate memory state, relying solely on the hidden state to store and transfer information across timesteps
*   Fewer parameters than LSTM, making it faster to train and more computationally efficient
*   The performance is comparable to LSTM, particularly in tasks with simpler temporal dependencies

### Stacked LSTM
Layering enhances the model’s ability to capture complex relationships

Note: The output at each timestep serves as the input for the corresponding timestep in the next layer

<----------section---------->

Experiments are ever ongoing, and we encourage you to join the fun. The tools are all readily available, so finding the next newest greatest iteration is in the reach of all.

#### Going deeper
It’s convenient to think of the memory unit as encoding a specific representation of noun/verb pairs or sentence-to-sentence verb tense references, but that isn’t specifically what’s going on. It’s just a happy byproduct of the patterns that the network learns, assuming the training went well. Like in any neural network, layering allows the model to form more-complex representations of the patterns in the training data. LSTM layers can be stacked (see figure 9.13, not provided).

Stacked layers are much more computationally expensive to train.

```python
from keras.models import Sequential
from keras.layers import LSTM

model = Sequential()
model.add(LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape))
model.add(LSTM(num_neurons_2, return_sequences=True))
```

Each LSTM layer is a cell with its own gates and state vector.

<----------section---------->

## Building a Spam Detector
### The Dataset
Download the dataset from: [https://archive.ics.uci.edu/dataset/228/sms+spam+collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

This dataset contains SMS messages labeled as either "spam" or "ham" (not spam).

### Read the Dataset
```python
import pandas as pd

df = pd.read_csv("datasets/sms_spam.tsv", delimiter='\t', header=None, names=['label', 'text'])
print(df.head())
```

This code uses the pandas library to read the data from a tab-separated file ("sms_spam.tsv").  The `header=None` argument indicates that the file does not contain a header row, and `names=['label', 'text']` assigns the column names "label" and "text" to the SMS message labels and content, respectively. `print(df.head())` displays the first few rows of the dataframe to show the dataset's structure.

### Tokenize and Generate WEs
```python
import spacy
import numpy as np

nlp = spacy.load('en_core_web_md')  # loads the medium model with 300-dimensional WEs

# Tokenize the text and save the WEs
corpus = []
for sample in df['text']:
    doc = nlp(sample, disable=["tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])  # only tok2vec
    corpus.append([token.vector for token in doc])

# Pad or truncate samples to a fixed length
maxlen = 50
zero_vec = [0] * len(corpus[0][0])
for i in range(len(corpus)):
    if len(corpus[i]) < maxlen:
        corpus[i] += [zero_vec] * (maxlen - len(corpus[i]))  # pad
    else:
        corpus[i] = corpus[i][:maxlen]  # truncate

corpus = np.array(corpus)
print(corpus.shape) # Expected output: (5572, 50, 300)
```

This section prepares the text data for use in a neural network.  It uses the spaCy library to tokenize the SMS messages and generate word embeddings. `nlp = spacy.load('en_core_web_md')` loads the 'en_core_web_md' model, which includes 300-dimensional word embeddings. The code then iterates through each SMS message (`df['text']`), tokenizes it using `nlp(sample)`, and extracts the word embedding vector (`token.vector`) for each token. Unnecessary pipeline components are disabled for efficiency. The resulting `corpus` is a list of lists, where each inner list contains the word embedding vectors for the tokens in an SMS message. A zero vector of the same dimension as word embeddings represents padding, to ensure that all input sequences have the same length.

<----------section---------->

### Split the dataset
```python
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode the labels
encoder = LabelEncoder()
labels = encoder.fit_transform(df['label'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
# Expected output: ((4457, 50, 300), (1115, 50, 300), (4457,), (1115,))
```

This section splits the dataset into training and testing sets, which is standard practice in machine learning. `LabelEncoder` from scikit-learn is used to convert the string labels ("spam" and "ham") into numerical labels (0 and 1). Then, `train_test_split` is used to split both the features (word embeddings) and labels into training and testing sets, with 20% of the data allocated to the testing set.  The shapes of the resulting arrays are printed to verify the split.

### Train an RRN model
```python
import keras

model = keras.models.Sequential()

model.add(keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(keras.layers.SimpleRNN(64))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, batch_size=512, epochs=20, validation_data=(X_test, y_test))
```

<----------section---------->

This section builds and trains a simple RNN model using Keras.  The model consists of an input layer, a `SimpleRNN` layer with 64 units, a `Dropout` layer with a rate of 0.3 (to prevent overfitting), and a `Dense` output layer with a sigmoid activation function (for binary classification). `model.compile` configures the training process, specifying the binary cross-entropy loss function, the Adam optimizer, and the accuracy metric. `model.fit` trains the model on the training data for 20 epochs, using a batch size of 512, and monitors the performance on the test data during training. The training history is saved in the `history` variable.

### Plot the Training History
```python
from matplotlib import pyplot as plt

def plot(history, metrics):
    fig, axes = plt.subplots(1, len(metrics), figsize=(15, 5))
    for i, metric in enumerate(metrics):
        ax = axes[i]
        ax.plot(history.history[metric], label='Train') # Corrected line
        ax.plot(history.history['val_' + metric], label='Validation') # Corrected line
        ax.set_title(f'Model {metric.capitalize()}')
        ax.set_ylabel(metric.capitalize())
        ax.set_xlabel('Epoch')
        ax.legend(loc='upper left')
        ax.grid()
    plt.tight_layout()
    plt.show()

plot(history, ['loss', 'accuracy'])
```

This section defines a `plot` function to visualize the training history.  The function takes the training history and a list of metrics as input. It creates subplots for each metric, plotting the training and validation performance over epochs. The plots include titles, labels, legends, and grids for better readability.

<----------section---------->

### Report and Confusion Matrix
```python
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def print_report(model, X_test, y_test, encoder):
    y_pred = model.predict(X_test).ravel()
    y_pred_class = (y_pred > 0.5).astype(int)  # convert probabilities to classes

    y_pred_lab = encoder.inverse_transform(y_pred_class)
    y_test_lab = encoder.inverse_transform(y_test)
    print(classification_report(y_test_lab, y_pred_lab, zero_division=0))

    cm = confusion_matrix(y_test_lab, y_pred_lab)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=encoder.classes_, yticklabels=encoder.classes_)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

print_report(model, X_test, y_test, encoder)
```

This section defines a `print_report` function to evaluate the trained model on the test data.  It first makes predictions using `model.predict` and converts the predicted probabilities to class labels (0 or 1) by applying a threshold of 0.5.  Then, it generates a classification report using `classification_report` from scikit-learn, which includes precision, recall, F1-score, and support for each class. Finally, it generates a confusion matrix using `confusion_matrix` and visualizes it as a heatmap using seaborn, providing insights into the model's performance in terms of true positives, true negatives, false positives, and false negatives.

<----------section---------->

### Using RNN Variants
*   Bi-directional RRN:
    `model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(64)))`
*   LSTM:
    `model.add(keras.layers.LSTM(64))`
*   Bi-directional LSTM:
    `model.add(keras.layers.Bidirectional(keras.layers.LSTM(64)))`
*   GRU:
    `model.add(keras.layers.GRU(64))`
*   Bi-directional GRU:
    `model.add(keras.layers.Bidirectional(keras.layers.GRU(64)))`

This section demonstrates how to replace the basic `SimpleRNN` layer with different RNN variants. It highlights the syntax for using Bidirectional RNNs, LSTMs, Bidirectional LSTMs, GRUs, and Bidirectional GRUs in Keras.

### Using Ragged Tensors
A Ragged Tensor is a tensor that allows rows to have variable lengths

*   Useful for handling data like text sequences, where each input can have a different number of elements (e.g., sentences with varying numbers of words)
*   Avoids the need for padding/truncating sequences to a fixed length
*   Reduces overhead and improves computational efficiency by directly handling variable-length data
*   Available in TensorFlow since version 2.0
*   In PyTorch, similar functionality is provided by Packed Sequences

<----------section---------->

```python
import tensorflow as tf

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)

# Convert sequences into RaggedTensors to handle variable-length inputs
X_train_ragged = tf.ragged.constant(X_train)
X_test_ragged = tf.ragged.constant(X_test)

# Build the model
model = keras.models.Sequential()
model.add(keras.layers.Input(shape=(None, 300), ragged=True))
model.add(keras.layers.SimpleRNN(64))
model.add(keras.layers.Dropout(0.3))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train the model using RaggedTensors
history = model.fit(X_train_ragged, y_train, batch_size=512, epochs=20,
                    validation_data=(X_test_ragged, y_test))
plot(history, ['loss', 'accuracy'])
print_report(model, X_test_ragged, y_test, encoder)
```

This section shows how to use Ragged Tensors in TensorFlow to handle variable-length input sequences without padding.  `tf.ragged.constant` converts the data into Ragged Tensors. The model is then built with an input layer that specifies `ragged=True`. The rest of the model architecture remains the same.  `model.fit` is used to train the model on the Ragged Tensors.

<----------section---------->

## Intro to Text Generation

### Generative Models
A class of NLP models designed to generate new text:

*   … that is coherent and syntactically correct
*   ... based on patterns and structures learned from text corpora

#### Generative vs Discriminative

*   Discriminative models are mainly used to classify or predict categories of data
*   Generative models can produce new and original data

RNN can be used to generate text

*   Transformers are better (we will discuss them later)

This section introduces the concept of generative models in NLP, contrasting them with discriminative models.  Generative models aim to produce new data (e.g., text), while discriminative models focus on classifying data. RNNs can be used for text generation, though Transformers are now the preferred architecture for this task.

### Applications
*   Machine Translation: Automatically translating text from one language to another
*   Question Answering: Generating answers to questions based on a given context
*   Automatic Summarization: Creating concise summaries of longer texts
*   Text Completion: Predicting and generating the continuation of a given text
*   Dialogue Systems: Creating responses in conversational agents and chatbots
*   Creative Writing: Assisting in generating poetry, stories, and other creative texts

This section outlines several applications of text generation models, including machine translation, question answering, automatic summarization, text completion, dialogue systems, and creative writing.

<----------section---------->

### Language Model
A mathematical model that determine the probability of the next token given the previous ones

*   It captures the statistical structure of the language (latent space)
*   Once created, can be sampled to generate new sequences
*   An initial string of text (conditioning data) is provided
*   The model generates a new token
*   The generated token is added to the input data
*   the process is repeated several times
*   This way, sequences of arbitrary length can be generated

This section describes language models, which are fundamental to text generation. A language model assigns probabilities to sequences of words, allowing it to predict the next token given the preceding tokens. The model captures the statistical structure of the language and can be sampled to generate new sequences of arbitrary length.

### LM Training
At each step, the RNN receives a token extracted from a sentence in the corpus and produces an output

The output is compared with the expected token (the next one in the sentence)
The comparison generates an error, which is used to update the weights of the network via backpropagation

*   Unlike traditional RNNs, where backpropagation occurs only at the end of the sequence, errors are propagated at each step

In language model training, the RNN receives a token from the corpus and predicts the next token. The error between the predicted and actual next token is used to update the network's weights via backpropagation at each step.

<----------section---------->

So the first thing you need to do is adjust your training set labels. The output vector will be measured not against a given classification label but against the one-hot encoding of the next character in the sequence.

Figure 9.10 (not provided) depicts Next word prediction

Figure 9.11 (not provided) depicts Next character prediction

### Sampling
#### During utilization:

*   Discriminative models always select the most probable output based on the given input
*   Generative models sample from the possible alternatives:
*   Example: if a word has a probability of 0.3 of being the next word in a sentence, it will be chosen approximately 30% of the time

**Temperature:** a parameter (T) used to regulate the randomness of sampling

*   A low temperature (T<1) makes the model more deterministic
*   A high temperature (T>1) makes the model more creative

<----------section---------->

### Temperature

q'<sub>i</sub> = exp(log(p<sub>i</sub>) / T) / Σ exp(log(p<sub>j</sub>) / T)

Where:

*   p<sub>i</sub> is the original probability distribution
*   p<sub>i</sub> is the probability of token i
*   T > 0 is the chosen temperature
*   q' is the new distribution affected by the temperature

This section introduces the concept of temperature in sampling from a language model. The temperature parameter controls the randomness of the generated text. A lower temperature makes the model more deterministic, while a higher temperature makes it more creative (but potentially less coherent). The equation provided shows how the temperature parameter affects the probability distribution.

Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data (see figure 8.2, not provided).

#### Implementing character-level LSTM text generation
To illustrate the concepts, a Keras implementation of character-level LSTM text generation will be presented.

#### PREPARING THE DATA
The initial step involves downloading the corpus and converting the text to lowercase.

<----------section---------->

```python
import keras
import numpy as np

path = keras.utils.get_file(
    'nietzsche.txt',
    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('Corpus length:', len(text))
```

This code snippet downloads the text of Nietzsche's writings from a specified URL using `keras.utils.get_file`, opens the file, converts the text to lowercase, and prints the length of the corpus. This serves as the foundational dataset for training a language model that captures the style and topics characteristic of Nietzsche's writing.

## Building a Poetry Generator

### Leopardi Poetry Generator
*   Download the corpus from [https://elearning.unisa.it/](https://elearning.unisa.it/)

```python
# Load the corpus
with open('datasets/leopardi.txt', 'r') as f:
    text = f.read()

# Get the unique characters in the corpus
chars = sorted(list(set(text)))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

print("Corpus length: {}; total chars: {}".format(len(text),
<----------section---------->

## Lesson 7 ##

Dialog Engines

This lesson focuses on dialog engines within the context of Natural Language Processing (NLP) and Large Language Models (LLMs), specifically exploring task-oriented dialogue systems. The lecture is presented by Nicola Capuano and Antonio Greco from the DIEM (Department of Industrial Engineering and Mathematics) at the University of Salerno.

**Outline**
The lesson will cover the following topics:

*   Task-Oriented Dialogue Systems: Defining characteristics and differences from other conversational AI.
*   Introduction to Rasa: An overview of the Rasa framework for building conversational AI.
*   Building a Chatbot with Rasa: A practical guide to constructing a chatbot using Rasa.
*   Custom Actions: Extending chatbot functionality with custom code.

<----------section---------->

**Task-Oriented Dialogue Systems**
This section delves into the specifics of Task-Oriented Dialogue Systems (TOD).

**Types of Conversational AI**
Conversational AI can be broadly classified into:

*   **Chit-Chat:** These systems are designed for general conversation without a specific goal.
    *   Goal: To generate natural and engaging responses in any context.
    *   Focus: On maintaining a flowing conversation, even if it doesn't lead to a concrete outcome.
    *   Success Metric: The longer and more natural the conversation, the better.
*   **Task-Oriented Dialogue Systems (TOD):** These systems are built to help users achieve specific goals.
    *   Goal: To efficiently and accurately fulfill a user's request or need.
    *   Focus: On understanding user intent, maintaining conversation state, and determining the next appropriate action.
    *   Success Metric: The fewer conversational turns required to achieve the goal, the more efficient the system. Efficiency of achieving goal with minimum conversation is critical.

<----------section---------->

**Task Oriented Dialogue Examples:**
These examples demonstrate the practical applications of TOD systems:

*   "I have a question."
    *   "Which room is the dialogue tutorial in?" (Goal: Find the location of a tutorial)
    *   "When is the IJCNLP 2017 conference?" (Goal: Find the date of a conference)
*   "I need to get this done."
    *   "Book me the flight from Seattle to Taipei." (Goal: Book a flight)
    *   "Schedule a meeting with Bill at 10:00 tomorrow." (Goal: Schedule a meeting)
*   "I need a recommendation."
    *   "Can you suggest me a restaurant?" (Goal: Get a restaurant recommendation)
    *   "Can you suggest me something to see near me?" (Goal: Get a local attraction recommendation)

<----------section---------->

**TOD System Architecture**
The core components of a TOD system can be visualized as:

*   **Input Modules: "Ears"**
    *   Purpose: To receive and interpret user input.
    *   Components:
        *   NLU (Natural Language Understanding): Processes text to identify intent and entities.
        *   GUI (Graphical User Interface) elements: Allow structured input from the user.
        *   Connector context, personal info: Provides additional information about the user and the conversation environment.
*   **Dialogue Management: "Brain"**
    *   Purpose: To manage the conversation flow and determine the next action.
    *   Components:
        *   Conversational Modules: Implement the logic for handling dialogue turns and transitions.
*   **Output Modules: "Mouth"**
    *   Purpose: To generate and deliver the system's response.
    *   Components:
        *   NLG (Natural Language Generation): Formulates text responses.
        *   GUI elements: Display information and options to the user.
        *   Backend, Connector to any database, conversational API, etc.: Connects the system to external resources and services to fulfill requests.

<----------section---------->

Rasa is highlighted as a popular open-source framework for building TOD systems.
[https://rasa.com/](https://rasa.com/)

**Natural Language Understanding (NLU)**
NLU is a critical component of TOD systems, encompassing two main tasks:

*   **Intent Classification:** Identifying the user's intention behind a given utterance. This is treated as a multi-label sentence classification problem, where an utterance can have multiple intents.
    *   Example:

    ```text
    What's the weather like tomorrow?
    {"intent": "request_weather"}
    ```
*   **Entity Recognition:** Identifying and extracting relevant pieces of information (entities) from the user's utterance. This is typically approached using Named Entity Recognition (NER) techniques, which can be rule-based or machine learning-based.
    *   Example:

    ```text
    What's the weather like tomorrow?
    {"date": "tomorrow"}
    ```

<----------section---------->

**Conversation Design**
Effective conversation design is vital for creating useful and engaging chatbots. Key aspects include:

*   Understanding the users: Knowing who they are, their needs, and their expectations.
*   Defining the assistant's purpose: Clarifying what the assistant is intended to do and how it should behave.
*   Documenting typical conversations: Planning out the most common interactions users will have with the assistant to ensure a smooth experience.

It’s important to acknowledge that anticipating every possible user query is challenging.

*   Early stages of development should involve hypothetical conversations to establish a basic structure.
*   However, training the assistant on real conversations as soon as possible is essential for improving its accuracy and effectiveness. Real world user data helps to tailor the bot to the user expectations.

<----------section---------->

**Introduction to Rasa**
This section provides an introduction to the Rasa framework.

**Rasa Intro**

*   Rasa is an Open-Source Conversational Framework.
    *   Launched in 2016, it has gained widespread adoption.
    *   It is used globally to create thousands of bots in various languages for diverse applications.

<----------section---------->

**Rasa Basic Units**
The fundamental components within the Rasa framework are:

*   **Intents:** Represent what the user wants to achieve. These represent the goals of the user's message.
*   **Entities:** Represent terms or objects that are relevant or necessary for the intent to be fulfilled. These provide the detail needed to act upon the intent.

    ```text
    Utterance: "Show me yesterday's financial news"
    Intent: showNews
    Entity: yesterday (time)
    Entity: financial news (news_type)
    ```

<----------section---------->

**Rasa Intro (continued)**
Additional key components of Rasa:

*   **Actions:** Define what the bot should do in response to user intents. These are what the bot does as the result of recognizing an intent.
*   **Responses:** Predefined utterances that the bot can use to communicate with the user. These are the actual sentences the bot will use.
*   **Complex Actions:** Custom Python code that allows the bot to interact with external systems, such as databases or Web APIs. This enables integration with the real world.
*   **Slots:** Variables used to store information extracted from user inputs during a conversation. This allows the bot to 'remember' important details.
*   **Forms:** Sets of slots used to collect multiple pieces of information from the user in a structured manner. These are helpful in guiding the user through complex information gathering processes.
*   **Stories:** Sequences of user intents and bot actions used to pre-program dialog scenarios. These define the flow of the conversation.

<----------section---------->

**Rasa Intro - Sample Story**
This demonstrates a simple story format within Rasa:

```text
## explain nlu story name
* greet
  - utter_greet  
* explain_rasa_nlu
  - utter_explain_rasa_nlu
```
This story defines that when the user greets, the bot responds with utter_greet and when the user asks to explain RASA NLU, the bot responds with utter_explain_rasa_nlu.

<----------section---------->

**Installing Rasa**
Instructions for setting up a Rasa development environment:

Create and activate a new virtual environment:

*   `python -m venv rasa.env` (Creates a virtual environment)
*   `source rasa.env/bin/activate` (Activates the environment)

Install Rasa:

*   `pip install rasa` (Installs the Rasa framework)

<----------section---------->

**Rasa Project**
The structure and organization of a Rasa project:

*   Most configuration and data are stored in YAML files, which are human-readable.
*   Python code is used to implement complex actions that require custom logic.

**Create a New Project**

```bash
rasa init
```

This command initializes a new Rasa project with the necessary files and directory structure.

<----------section---------->

**Directory Structure**
Explanation of the key directories and files within a Rasa project:

*   `actions/`: Contains Python code for custom actions.
*   `data/nlu.yml`: Defines intents and entities, providing training data for NLU.
*   `data/rules.yml`: Defines short conversation paths that should always be followed, providing rigid conversation flow.
*   `data/stories.yml`: Defines general stories to train the model, which act as examples for machine learning based conversation flow.
*   `models/`: Contains the trained models.
*   `tests/`: Includes bot test cases for evaluating performance.
*   `config.yml`: Defines pipelines, policies, and components used by Rasa. This is a key configuration file.
*   `credentials.yml`: Stores credentials for external platforms.
*   `domain.yml`: The main file that lists all intents, entities, slots, responses, forms, and actions. This file is a central definition of the chatbot's capabilities and knowledge.
*   `endpoints.yml`: Lists the endpoints your Bot can use for integrations.

<----------section---------->

**domain.yml - Session Configuration**
Example of session configuration within the `domain.yml` file:

```yaml
session_config:
  session_expiration_time: 60  # minutes
  carry_over_slots_to_new_session: true
```

Explanation: If the user starts a new interaction after the previous session expires (60 minutes in this case), the data from the previous session (e.g., stored slot values) is transferred to the new session. This helps maintain context across interactions.

<----------section---------->

**nlu.yml**
(See example content in later sections)

*Note:* To train the model to recognize intents effectively, RASA recommends at least 7-10 example utterances per intent. This ensures the model has sufficient data to learn from.

**stories.yml**
(See example content in later sections)

**rules.yml**
(See example content in later sections)

<----------section---------->

**Visualize Stories**
Command to visualize the defined stories:

```bash
rasa visualize
```

This command helps understand and debug the conversation flows defined in the stories file.

**Other Commands**

*   `rasa train`: Trains a model using your NLU data and stories, saves trained model in `./models`. This is used to train the machine learning components.
*   `rasa shell`: Loads your trained model and lets you talk to your assistant on the command line for interactive testing.
*   `rasa run`: Starts a server with your trained model, making it accessible to external applications. For cross-origin calls, use: `rasa run --cors "*"`.
*   `rasa --h`: (help) Displays available commands and options.

<----------section---------->

**Rasa REST API**
Rasa provides a REST endpoint for integration with external systems.

*   You can POST messages to the endpoint and receive the Bot’s responses in JSON format. This is used to connect the chatbot with web applications.
*   Add the REST channel to your `credentials.yml`:

    ```yaml
    # you don't need to provide anything here - this channel doesn't
    # require any credentials
    ```

*   After restarting your Rasa server, you can reach the bot at: `http://<host>:<port>/webhooks/rest/webhook`
*   Documentation: [https://rasa.com/docs/rasa/connectors/your-own-website/](https://rasa.com/docs/rasa/connectors/your-own-website/)

<----------section---------->

**Request and Response Format**
Illustrative examples of the REST API's JSON request and response formats:

Request Format:

```json
{
  "sender": "test_user",  // sender ID
  "message": "I'm sad!"
}
```

Response Format:

```json
[
  {
    "recipient_id": "test_user",
    "text": "Here is something to cheer you up:"
  },
  {
    "recipient_id": "test_user",
    "image": "https://i.imgur.com/nGF1K8f.jpg"
  },
  {
    "recipient_id": "test_user",
    "text": "Did that help you?"
  }
]
```

The request includes the sender's ID and the message. The response includes bot responses containing text and/or other media.

<----------section---------->

**Web-based Frontends**
Methods for integrating a Rasa bot into a website:

*   **Custom Implementation:** Build a custom frontend using HTML/CSS/JavaScript for maximum flexibility and control.
*   **Use a pre-built solution:** Utilize the Rasa Widget, which is React-based, for quicker integration.
    *   Rasa Widget (React-based)
        *   Clone from: [https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)
        *   Copy the `./dist` files to your web project to use the widget

<----------section---------->

**More on Connectors**

*   You can enable authentication for secure communication.
*   You can use web-socket for real-time, bidirectional interaction.
*   Rasa provides built-in connectors for popular messaging platforms, simplifying integration:
    *   Facebook Messenger
    *   Slack
    *   Telegram
    *   Twilio
    *   Microsoft Bot Framework
        [https://rasa.com/docs/rasa/messaging-and-voice-channels/](https://rasa.com/docs/rasa/messaging-and-voice-channels/)
    *   Cisco Webex Teams
    *   RocketChat
    *   Mattermost
    *   Google Hangouts Chat

<----------section---------->

**Building a Chatbot with RASA**

**Domain File**
The domain file defines the knowledge base of the assistant, including:

*   **Responses:** The phrases and templates the assistant uses to communicate.
*   **Intents:** Categories of user input that the assistant recognizes.
*   **Entities:** Pieces of information extracted from user messages.
*   **Slots:** Variables the assistant remembers throughout a conversation.
*   **Actions:** Application logic and functions the assistant can perform.

<----------section---------->

**Domain File - Basic Responses**
Example of defining basic responses in the `domain.yml` file:

```yaml
responses:
  utter_greet:
    - text: "Hey there!"
  utter_goodbye:
    - text: "Goodbye :("
  utter_default:
    - text: "Sorry, I didn't get that, can you rephrase?"
  utter_youarewelcome:
    - text: "You're very welcome."
  utter_iamabot:
    - text: "I am a bot, powered by Rasa."
```
These pre-defined responses are triggered by actions.

<----------section---------->

**Domain File - Multiple Responses**
Example of defining multiple response variations:

```yaml
responses:
  utter_greet:
    - text: "Hey, {name}. How are you?"
    - text: "Hey, {name}. How is your day going?"
```

Explanation: Slots, such as `{name}`, will be filled with the value of the corresponding slot ("None" until it's filled). This provides dynamic greetings.

<----------section---------->

**Domain File - Responses: Buttons and Images**
Illustrating how to include interactive elements like buttons and images in responses:

```yaml
responses:
  utter_greet:
    - text: "Hey! How are you?"
      buttons:
        - title: "great"
          payload: "/mood_great"
        - title: "super sad"
          payload: "/mood_sad"
  utter_cheer_up:
    - text: "Here is something to cheer you up:"
      image: "https://i.imgur.com/nGF1K8f.jpg"
```
This allows for richer and more interactive user experiences.

<----------section---------->

**Domain File - List of Intents**

```yaml
intents:
  - greet
  - goodbye
  - affirm
  - deny
  - thankyou
  - mood_great
  - mood_unhappy
  - bot_challenge
  - search_concerts
  - search_venues
  - compare_reviews
  - how_to_get_started
```

*   This list must correspond to the intents defined in the NLU file (`nlu.yml`).
*   *Tip:* Start with the fewest intents possible and add or change intents as needed. This ensures a streamlined and manageable model.

<----------section---------->

**Domain File - List of Entities**

```yaml
entities:
  - PERSON
  - time
  - membership_type
  - priority
```

*   Entities can represent various types of information, including numbers, dates, country names, and product names.
*   Standard entities can be extracted with pre-built models. Specific modules must be included in the config file to enable these.
*   Custom entities can be extracted using regular expressions, lookup tables, or machine learning. The NLU file will specify how custom entities are handled.

<----------section---------->

**NLU File**
The NLU file (`nlu.yml`) is aimed at training the system to extract structured information from user messages, including intents and entities.

**Training data**
This includes example user utterances categorized by intent. These example phrases are used to train the machine learning model.

**Extra information**
The NLU file can also contain:

*   Regular Expressions: Patterns to capture specific types of entities, allowing for flexible pattern matching.
*   Lookup Tables: Comprehensive lists of possible values for entities, improving recognition accuracy.
*   Synonyms: Definitions of synonyms for common terms to ensure variations in user input are understood correctly.

<----------section---------->

**NLU File - Sample Lists of Utterances**

```yaml
nlu:
  - intent: book_flight
    examples: |
      - I want to book a flight to [New York](city)
      - Book a flight from [Los Angeles](city) to [Chicago](city)
      - Can you help me book a flight to [San Francisco](city)?
      - I need a flight ticket to [Boston](city)
      - I'd like to fly from [Houston](city) to [Atlanta](city)

  - intent: check_flight_status
    examples: |
      - What is the status of flight [AA123](flight_number)?
      - Can you tell me if flight [UA456](flight_number) is delayed?
      - I want to check the status of flight number [DL789](flight_number)
      - Is flight [AA123](flight_number) on time?
```

Explanation: The involved entities (city and flight\_number) must correspond to the entities listed in the domain file.

<----------section---------->

**NLU File - Sample Extra Information**

```yaml
- lookup_table: city
  examples: |
    - New York
    - Los Angeles
    - Chicago
    - San Francisco
    - Boston
    - Houston
    - Atlanta

- synonym: flight
  examples: |
    - flight
    - flight ticket
    - plane ticket
    - air ticket

- regex:
  name: flight_number
  pattern: "\\b[A-Z0-9]{2,5}\\b" # Regex for flight numbers
```

Explanation: Without lookup tables and regular expressions, custom entities are simply recognized based on machine learning, which may be less accurate in some cases.

<----------section---------->

**NLU File - Entity Roles**
Entity Roles allow you to add more details to your entities, giving the chatbot more information and context about what the user is talking about.

Example:

"I am looking for a flight from New York to Boston."

```yaml
- example: |
    I am looking for a flight from [New York]{"entity":"location", "role":"origin"} to [Boston] {"entity":"location", "role":"destination"}.
```
In this example, 'New York' is labeled with location and origin role while 'Boston' is labeled as location with destination role.

<----------section---------->

**NLU File - Good Practices**

*   Start with the smallest possible number of intents, focusing on core functionality.
*   Recognize that most users want to do the same things, so prioritize those.
*   Begin with the most common intents and add others as needed.
*   Additional intents will emerge from analyzing real user data.
*   Avoid using intents to store information; use entities instead, as it is not the purpose of intent to extract specific information.

<----------section---------->

**Stories File**
Stories are training data to teach your assistant what it should do next based on the user input. They are a fundamental part of training conversation flow.

Stories File - example

```yaml
stories:
  - story: happy path
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_great
      - action: utter_happy

  - story: sad path 1
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_unhappy
      - action: utter_cheer_up
      - action: utter_did_that_help
      - intent: affirm
      - action: utter_happy

  - story: sad path 2
    steps:
      - intent: greet
      - action: utter_greet
      - intent: mood_unhappy
      - action: utter_cheer_up
      - action: utter_did_that_help
      - intent: deny
      - action: utter_goodbye
```

Example illustrating potential issues:

User input: Hi, I need a hospital A

Next best action:

*   Sorry, I didn't quite understand. Can you rephrase?

Why this might happen:

User input: utter\_ask\_name I am ill and would like to

Bot Action: utter\_greet\_user find the nearest hospital to

get a checkup.

*   Sure, let me do a search. Where are you based?

If the confidence is too low, the bot may not be able to continue with its intended conversation flow, even with relevant user input.

<----------section---------->

**Stories File - Good Practices**

*   Start with common flows (happy paths) to establish core functionality.
*   Add common errors and digressions to handle potential issues.
*   Use interactive learning to improve stories and refine the conversation flow.
*   Once your model is in production, add more data from real user conversations to continuously improve its performance.

<----------section---------->

**Stories File - OR Statements & Checkpoints**
Advanced story features:

```yaml
stories:
  - story: newsletter signup with OR
    steps:
      - intent: signup_newsletter
      - action: utter_ask_confirm_signup
      - or:
        - intent: affirm
        - intent: thanks
      - action: action_signup_newsletter

  - story: beginning of conversation
    steps:
      - intent: greet
      - action: utter_greet
      - checkpoint: ask_feedback

  - story: end conversation
    steps:
      - intent: goodbye
      - action: utter_goodbye
```

*   OR Statements: Allow the same action to be triggered by different intents, simplifying story creation.
*   Checkpoints: Enable linking to other stories, allowing for modular and reusable conversation flows.

<----------section---------->

**Rules File**
Rules are a way to describe short pieces of conversations that always go the same way. They provide deterministic behavior for simple interactions.

*   Not suitable for multi-turn interactions due to their rigid nature.
*   Not used to train ML algorithms; applied as is, guaranteeing consistency.

```yaml
rules:
  - rule: Say goodbye anytime the user says goodbye
    steps:
      - intent: goodbye
      - action: utter_goodbye

  - rule: Say 'I am a bot' anytime the user challenges
    steps:
      - intent: bot_challenge
      - action: utter_iamabot
```

<----------section---------->

**Slots**
Slots serve as your assistant's memory, allowing it to store important details and use them later in a specific context.

*   They enable your assistant to store important details and later use them in a specific context, providing personalized interactions.
*   They can be configured to influence the flow of the conversation based on stored information.

Examples:

"I would like to book a flight to Sydney."
* Booking a ticket to Sydney! (The destination slot is already known)

"I would like to book a flight to New York."
* Sure! Looking for the options.

"I would like to book a flight ticket."
* What is your destination? (The bot needs to ask for the destination)

<----------section---------->

**Slots and Entities**
Slots are defined in the domain file and are usually connected to entities, linking information extracted from user input to variables within the system.

```yaml
entities:
  - destination

slots:
  destination:
    type: text
    influence_conversation: true
    mappings:
      - type: from_entity
        entity: destination
```

*   Type can be text, boolean, categorical float, list, any, defining the data type the slot will hold.
*   `influence_conversation: true` indicates that this slot will influence the conversation flow, changing how the bot responds.
*   This slot is filled with the value of the destination entity (if set), automatically populating the slot.

<----------section---------->

**Slot Mappings**
Slot mappings allow you to define how each slot will be filled in. They are applied after each user message, ensuring the slots are updated.

```yaml
entities:
  - entity_name

slots:
  amount_of_money:
    type: any
    mappings:
      - type: from_entity
        entity: number
        intent: make_transaction
        not_intent: check_transaction
```

"Send $200 to Ben." - Intent: make\_transaction - slot is set

"Did I receive the $1000 that Alice sent me yesterday?" - Intent: check\_transaction - slot is not set

This enables context-aware slot filling.

<----------section---------->

**Slot Mappings - Parameters**
Key parameters to customize slot mappings:

*   `intent`: Only applies the mapping when this intent is predicted, linking slot filling to specific intents.
*   `not_intent`: Does not apply the mapping when this intent is predicted, providing exceptions to slot filling.
*   `role`: Only applies the mapping if the extracted entity has this role, filtering entities based on their role.

<----------section---------->

**Use Slots in Responses**
You can create more dynamic responses by including slots in the responses, allowing personalized and context-aware communication.

```yaml
slots:
  name:
    type: any

responses:
  utter_greet:
    - text: "Hello {name}! How are you?"
    - text: "Hello there :)"
    - text: "Hi. How can I help you today?"
```

If `name` is not set, then its value will be `None`, requiring fallback responses.

<----------section---------->

**Pipeline Configuration**
The `config.yml` file defines the NLU pipeline and the dialog policies used by Rasa. This is the central configuration file.

*   Language: Defines the language of the bot (e.g., `en`, `fr`, `it`).
*   Pipeline: Specifies the steps to process user messages (NLU pipeline) to extract intents and entities, handling how the bot understands the user input.
*   Policies: Defines how the bot should handle dialogue and predict next actions, controlling how the bot decides on the correct action based on its current context.

```yaml
language: en

pipeline: null # The default pipeline is used to train your model.

policies: null # The default policies are used to train your model.
```

<----------section---------->

**NLU Pipeline**
The pipeline defines the sequence of components that process user messages:

*   Tokenizers: Break down the text into tokens (words, subwords), preparing text for further processing.
*   Featurizers: Convert tokens into numerical features that models can use, creating machine learning model inputs.
*   Classifiers: Determine the user’s intent, categorizing the meaning behind the text.
*   Entity Extractors: Identify named entities (e.g., names, dates), extracting structured data from the text.

```yaml
pipeline:
  - name: WhitespaceTokenizer
  - name: CountVectorsFeaturizer
  - name: DIETClassifier
    epochs: 150
  - name: EntitySynonymMapper
```

<----------section---------->

**NLU Pipeline - Tokenizers**

*   `WhitespaceTokenizer`: Splits text into tokens based on whitespace, simple and fast.
*   `SpacyTokenizer`: Leverages SpaCy’s tokenization for more advanced tokenization, integrating a professional NLP library.

**NLU Pipeline - Featurizers**

*   `CountVectorsFeaturizer`: Converts text into a bag-of-words, creating frequency-based features.
*   `ConveRTFeaturizer`: Uses pre-trained ConveRT embeddings specialized for conversational data, taking advantage of transfer learning.
*   `SpacyFeaturizer`: Leverages SpaCy’s pre-trained word embeddings, integrating a pre-trained NLP model.

<----------section---------->

**NLU Pipeline - Classifiers**

*   `DIETClassifier`: A multi-task transformer-based classifier for both intent classification and entity extraction, a modern and accurate technique.
*   `SklearnIntentClassifier`: Uses scikit-learn algorithms (e.g., SVM or logistic regression) for intent classification, a classic machine learning approach.

**NLU Pipeline - Entity Extractors**

*   `RegexEntityExtractor`: Extracts entities using regular expressions for pattern matching, providing accurate extraction when the pattern is known.
*   `SpacyEntityExtractor`: Uses SpaCy’s pre-trained models to extract named entities based on SpaCy’s NER system, leveraging a production-ready NLP library.

<----------section---------->

**Training Policies**
Training policies are techniques your assistant uses to decide on how to respond back to the user. Policy priority defines how assistant makes decisions when multiple policies predict the next action with the same accuracy.
Choosing the right policies are crucial for making smart actions, but it can be difficult to select the right training policy.

```yaml
config.yml
policies:
  - name: MemoizationPolicy
  - name: TEDPolicy
    max_history: 5
    epochs: 100
  - name: RulePolicy
```

<----------section---------->

**Training Policies - Rule Policy**
Assistant makes the decision on how to respond based on rules defined in `rules.yml`, offering strict rulesets.

```yaml
rules:
  - rule: Chitchat
    steps:
      - intent: chitchat
      - action: utter_chitchat
```

<----------section---------->

**Training Policies - Memoization Policy**
Assistant makes the decision on how to respond by matching the real interaction with stories from `stories.yml`, memorizing the data, making for fast and reliable response.

Example:

Hello
Hey! How can I help you?

Can I check the balance of my card?
```yaml
stories:
 - story: check_balance
  steps:
   - intent: inform
   - action: check_balance
   - intent: thanks
   - action: utter_goodbye
```

<----------section---------->

**Training Policies - TED Policy**
Assistant makes the decision on how to respond by learning from the data defined in `stories.yml`. A neural network predicts the next action.

*   Transformer Embedding Dialogue (TED) is a neural network architecture for next action prediction, learning the action through complex transformation and embeddings.
*   Max History: How many conversational steps your assistant keeps in the memory when making the prediction, this affects the conversation context and influences its next action prediction.
*   Epochs: The number of epochs used to train the model, this affects how much the model trains and how accurate it can predict the next action.

<----------section---------->

**Custom Actions**

**Custom Actions**
What do we want virtual assistants to do? The possibilities are virtually limitless.

*   Send back appropriate message, addressing the user intent with correct response.
*   Send an email, interacting with other services to complete user request.
*   Make a calendar appointment, integrating with external calendars.
*   Fetch relevant information from a database, acting on user queries.
*   Check information from an API, querying outside the local datastore for current events or prices.
*   Calculate something specific, generating math output based on user input.

RASA RASA SDK

```
(NLU)--- CUSTOM

CORE)---(Actions
```
The RASA SDK helps to create custom actions to fulfil users intents.

<----------section---------->

**Custom Actions - Example**

"Hey, what time is it in Amsterdam right now?"

"It’s currently 15:01."

Custom Action: Take the place entity and return the current time.

```yaml
nlu.yml
- intent: inquire_time
  examples: |
    - what time is it?
    - what time is it in [Amsterdam](place)?
    - what time is it in [London](place)?
    - tell me the time in [Lisbon](place)
    - what is the current time in [Berlin](place)
    - what time is it in [amsterdam](place)

- lookup: place
  examples: |
    - brussels
    - zagreb
    - london
    - lisbon
    - amsterdam
    - seattle
```

```yaml
rules.yml
- rule: Tell the time
  steps:
    - intent: inquire_time
    - action: action_tell_time
```

```yaml
domain.yml
intents:
  - inquire_time
entities:
  - place
actions:
  - action_tell_time
```

```yaml
config.yml
language: en

pipeline:
  - name: WhitespaceTokenizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: CountVectorsFeaturizer
    analyzer: char_wb
    min_ngram: 1
    max_ngram: 4
  - name: DIETClassifier
    entity_recognition: False
  - name: RegexEntityExtractor
    use_lookup_tables: True

policies:
  - name: MemoizationPolicy
  - name: TEDPolicy
    max_history: 5
    epochs: 100
  - name: RulePolicy
```

Enables the use of lookup tables for entity recognition.

The model must be then trained with:

```bash
rasa train
```

<----------section---------->

```python
# actions.py
from typing import Any, Text, Dict, List

import arrow  # pip install arrow
import dateparser  # pip install dateparser

from rasa_sdk import Action, Tracker
from rasa_sdk.events import SlotSet
from rasa_sdk.executor import CollectingDispatcher

city_db = {
    'brussels': 'Europe/Brussels',
    
<----------section---------->

## Lesson 8 ##

**Exercise: Building a Pizzeria Chatbot**

Develop a chatbot to assist with pizzeria operations. Users can:

*   Request the pizzeria's menu.
*   Order a pizza that is available on the menu (just one pizza, no beverage).
*   Upon order confirmation, the bot will log the date, the user ID, and the kind of ordered pizza (use a custom action).
*   The bot has a web-based GUI.

<----------section---------->

Hint: Start with a dummy bot.

Example conversation:

User: Can I have the menu?
Bot: What kind of pizza would you like? We have Parenti, Pepperoni, Vegetarian...
User: Pepperoni
Bot: You want a Pepperoni pizza. Is that correct?
User: Yes
Bot: Great! Your Pepperoni pizza is on its way!

**Hints**

Start with a dummy bot:

```bash
mkdir pizzaBot
cd pizzaBot
rasa init --no-prompt
```

Configure and run the REST and the Actions servers:

```bash
rasa run --cors "*"
rasa run actions
```

Use a Web frontend like:

[https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)

<----------section---------->

**Additional Context:**
fly. When you need to inspire students and keep them engaged. Teachers do
this naturally, by adjusting what they say and how they say it based on
feedback on how well the student understand what they are saying. And
teachers think about more than just "delivering a message." They must think
up new ideas and approaches, on the fly as students pose interesting new
questions. Inspiring students' curiosity with Socratic questions and being
responsive to their changing needs is a full-time job.
It is virtually impossible to build a rule-based system that captures all the
things that teachers do to help students learn and grow. Students' needs are
too diverse and dynamic. This is why hybrid chatbots that integrate LLMs
have become the preferred way build production chatbots in virtually every
domain. An LLM can confidently and convincingly chat with your users on
virtually any topic. The key is to harness this power smartly, so that it doesn’t
mislead your users, or worse.

<----------section---------->

12.5 Chatbot frameworks
In each of the previous chapters, you’ve learned a new technique for
processing text to understand what the user is saying. And in this chapter,
you’ve learned four approaches to generating text for a chatbot to use in its
response to the user. You’ve already assembled a few chatbots from these
NLU and NLG algorithms to understand the advantages and disadvantages of
each of these algorithms. Now you have the knowledge you need to use a
chatbot framework
 smartly. A chatbot framework is an application and a
software library that abstracts away some of these detailed decisions you
need to make when building a dialog engine for your chatbot. A framework
gives you a way to specify your chatbot’s behavior in 
domain-specific
language
 that it can later interpret and 
run
 so that your chatbot replies the
way you intended.
Most chatbot frameworks use a declarative programming language to specify
a bot’s behavior and some even give you a graphical user interface to
program your bot. There are no-code chatbot frameworks that abstract the
declarative chatbot programming language with an interactive graphical
representation of the dialog graph or flow diagram that you can modify with
your mouse. These no-code frameworks usually include a dialog engine that
can execute your chatbot without you ever having to see or edit the
underlying data. In the impact world, an open source platform sponsored by
UNICEF, RapidPro,
[
34
]
 served as a core for several chatbot platforms, such as
Weni, Textit and Glific, that are all used for impact purposes. In RapidPro,
you can build your dialogs in a graphical user interface. You can also easily
import and export the content using open standard file formats which is
helpful when you want to translate the content from one natural language to
another for a multilingual chatbot. ManyChat and Landbot are two closed
source no-code chatbot builders that have similar functionality.
But if you’ve read this far, you probably have ideas for more sophisticated
chatbots than what’s possible in a no-code platform. So you will probably
need a chatbot programming language to make your vision a reality. Of
course, you can specify your bot "stack" in Python by directly employing the
skills you learned in this book. But if you want to build a scalable and
maintainable chatbot you’ll need a chatbot framework that uses a chatbot
design language or data structure that you understand. You want a language
that makes sense to you so that you can quickly get the conversation design
you have in your head embedded in a working chatbot. In this section, you
will learn of several different frameworks that can help you make your
chatbot dreams come true.
Using the tools described here, you can build a bot that can serve you (and
maybe a few friends, or even more people if you’re lucky) if deployed on a
server or in a cloud. However, if you want to build a chatbot that servers
hundreds or thousands of users, you need a more robust, scalable system.
Luckily, there are frameworks available that allow you to focus on building
your bot while taking care of the challenges that come with the need to build
a production-grade system. We will now discuss three popular open-source
Python chatbot frameworks for building chatbots with configurable NLP
capabilities: Rasa, LangChain, and qary.

<----------section---------->

12.5.1 Building an intent-based chatbot with Rasa
Rasa is an open-source conversational framework that started back in 2016
and today is used to create thousands of bots in various languages around the
world. Unlike many commercial frameworks, that create a drag-and-drop
interface to create the dialog trees we discussed in the previous section,
RASA took a radically different approach to organizing multi-step
conversations.
The basic units of a conversation in RASA are a user intent and a bot action -
which can be as simple as a pre-programmed utterance or a complex action
programmed in Python that results in interaction with other systems - such as
saving or retrieving data from a database, or invoking a Web API. By
chaining these building blocks into sequences - called Stories - RASA allows
you to pre-program dialog scenarios in a streamlined way. All this
information is stored in YAML files (YAML stands for Yet Another Markup
Language), each type of components in its own file.
But enough with the theoretical explanation - let’s get your hands dirty and
build your first RASA chatbot. First, let’s decide what dialog we want to
implement - based on our conversation diagram for the math tutor bot, let’s
implement the following short dialog:
USER: Hello
BOT: Well, hello there. Thanks for checking out Rori, a math tutor chatbot. Chatting with Rori helps students improve their math skills. And it's fun too!
BOT: Are you a parent (or guardian) or are you a student?
USER: I'm a parent.
BOT: For your child to use Rori, we need permission from the parent or guardian. Do you agree to give your child permission to chat with Rori on this Whatsapp number?
USER: I agree
BOT: Thank you for giving permission for your child to chat with Rori.
When your child is ready to start, please give them this phone and have them type "ready".
To create your bot, you will need to install 
rasa
 package (if you’re working
in 
nlpia2
 environment, it is already installed when you install the project).
Then, you can go to the directory you want to create the project in and run in
your command line:
$ rasa init
The installation wizard will guide you through creating a new project and
even offer you to train an initial model. Let it do that, and then you can even
chat with a simple chatbot the wizard initialized for you.
Let’s now dive into the structure of our project and understand how to build a
dialog like you’ve just had. Here is the directory structure you should see in
the project’s folder:
├───.rasa
│   └───cache
│       ├───...
├───actions
│   └───__pycache__
├───data
├───models
└───tests
The directory we are most interested in is the 
data
 directory. It contains the
files that define the data that is used to train the chatbot’s NLU model. First,
there’s the 
nlu.yml
 file, which contains the intents and examples of user
utterances that are used to train the intent recognition model. So let’s start
creating the intents that are used in our dialog. For every intent you want to
define, you need to provide a name and a list of examples of utterances that
belong to this intent.
For our short dialog, we need to understand the user’s greeting, their role
(parent or student), and their agreement to give permission to their child to
use the chatbot.
version
: 
"
3.1
"
nlu
:
- 
intent: greet
  
examples
: 
|
 - hey - hello - hi
- 
intent: parent
 - I am a parent - Parent - I'm a mom to 12 year old
- 
intent: agree
...
Pretty straightforward, right? RASA will warn if you have too few examples
for a particular intent, and recommends at least 7-10 utterance examples per
intent.
The next file you should look at is 
domain.yml
account. When text message chatbots came onto the scene most continued to
follow this dark pattern of non-cooperative conversation, trying your patience
and preventing you from creating cost for the business. The more advanced
NLP skills you have learned in this book now give you the power to build
chatbots that can simulate intelligent conversation and do useful work for you
and your organization.
The chatbot boom is not over yet. You are about to learn all the ways they
can be used to improve your life or your business.

<----------section---------->

12.1 Chatbots are everywhere
Chatbots are everywhere. Here are some examples to help you dream up your
own projects.
Virtual assistants
: Dicio (Google Assistant), Lyra (Siri) and MyCroft
(Alexa) can help you accomplish small tasks such as checking the
weather, calling a friend, launching an app, setting a reminder, playing
music, or turning on the lights.
Entertainment
: Chatbots in video games and websites promoting movies
are often used to keep you engaged in a fictional storyline. You can
measure how well an entertainment chatbot is doing by how long the
user is willing to interact with the chatbot and how often they suspend
their disbelief that they are interacting with a human.
Healthcare
: Depending on the regulations in your country, chatbots can
often answer your health-related questions, schedule an appointment for
you, or even give a preliminary diagnosis. Mental health chatbots, such
as Woebot 
[
4
]
 and Wysa,
[
5
]
 even provide therapeutic exercises that can
decrease depression and anxiety.
[
6
]
Impact
: Nonprofits and social businesses use chatbots to help people in
need. Often they leverage popular messaging channels like SMS and
WhatsApp to reach people in underserved communities where mobile
messaging is their main access to the Internet.
Operations (ChatOps)
: Businesses often use chatbots to increase team
productivity and job satisfaction. You can build chatbots that interact
with you on Telegram or WhatsApp to help you monitor and control
your software. And, if you’re lucky, your boss at work might use a
chatbot to onboard and train you, or even publicly recognize you when
you help a teammate learn something new.
Advertisement and Sales
: Search engines on corporate websites often
use chatbots to steer you towards advertisements and products they want
you to purchase or promote. Behind the scenes these bots are often used
to distract and 
engage
 you on (anti)social networks.
Customer (dis)service
: Machines have been replacing humans at
customer service call centers and chat message interfaces for decades.
Most large corporations do not allow you to interact with a human until
you first satisfy their chatbot gatekeepers.
The authors of this book founded Tangible AI to help nonprofits,
governments, and individual makers create impact chatbots.
[
7
]
 Impact
chatbots help people in underserved communities, from new immigrants in
the United States to teens in the Global South. We’ve built chatbots that help
people learn math, overcome imposter syndrome, learn new languages, evade
human traffickers, and even start a small business in a developing country. A
contributing author and Tangible AI volunteer, Vishvesh Bhat, has even
founded a startup of his own to build a chatbot that helps US college students
learn and reason about their course material.
[
8
]
Next, you will learn how to build your own chatbots to bring a positive
impact to your community or business.
[
9
]
 
[
10
]

<----------section---------->

12.1.1 Different chatbots, same tools
As diverse as the chatbot examples in this section seem to be, they all
leverage the same NLP tools and techniques that you have learned in this
book. All the previous chapters have been building up your skills and toolbox
so you can assemble a chatbot. Here are some of the NLP skills you’ve
learned that will help you build chatbots:
Chapter 6
: Embedding words and phrases into semantic vectors (from
Chapter 6) to recognize a chatbot user’s intent
Chapter 8
: Creating more meaningful embedding vectors of chat
messages using LSTMs and language models such as BERT.
Chapter 9
: Translating between languages to help your users interact
with your chatbot in their native language.
Chapter 10
: Semantic search and automatic text generation to respond to
chat messages without having to craft responses by hand.
Chapter 11
: Extracting relationships between real-world entities from
text to help your chatbot reason about a users' requests and maintain the
conversation context.
Figure 
Figure 12. 1
 shows how all these pieces fit together to create a
chatbot.
Figure 12.1 Chatbot flow diagram
Before you jump into assembling a chatbot system from all these tools and
libraries, you need to think about what you want your chatbot to talk about.
You need to design a conversation.

<----------section---------->

12.1.2 Conversation design
As chatbot technology gained more and more popularity in the last decade, so
did the field of conversation design. Conversation design is a branch of user
interaction (UI) design that deals specifically with designing engaging
dialogs. This section will help you get started, and when you’re ready to dive
deeper you can dig into more detailed resources wuch as as Andrew Freed’s
excellent 
Conversational AI
 book.
[
11
]
For every chatbot project you will work your way through four stages:
1
. 
Define your chatbot’s goal and the problem it solves. What does success
look like? How will you tell when your chatbot is doing a good job?
2
. 
Think about your users. Who will benefit from using your chatbot?
What do they need? Where will your users be when they use your
chatbot? What triggered them to engage in the conversation?
3
. 
Draft an imaginary conversation between the user and your chatbot. This
is called the "happy path" or "happy conversation." You might even go
as far as "act it out" with a colleague or a friend.
4
. 
Diagram a conversation tree. After drafting several happy conversations
with your chatbot, you will notice patterns that you can generalize from
to create a 
conversation diagram
 — a flow chart showing several
possible conversations between the user and the chatbot.
5
. 
Choose the NLP algorithms from Figure 
Figure 12. 1
 that you or your
teammates will need to implement in software in order for your chatbot
to generate responses at every branch in your dialog tree.
Think about the example of a math tutor bot. The goal is pretty clear, you
want to teach math to middle school children. However, when you start
thinking about the users in step 2, you realize that you cannot assume that the
child would be the person contacting your bot first. This is what the Rori
project experienced in low-income countries, where young children rarely
own a phone. Your younger users will often borrow someone else’s phone or
computer. So your chatbot may not be able to send homework reminders or
other push notifications to the users' phone.
Another important thing to consider when dealing with children is that you
need to obtain a parent or guardian’s consent before allowing your chatbot to
interact directly with a child. You will need to comply will all the child
protection laws in the countries where your chatbot will be used, including
mandatory reporting of 
safeguarding disclosures
 by your users. If a child
mentions that they are being abused or are considering self-harm you will
want to detect and report those disclosures. No matter what your chatbot’s
goals are, when your users indicate that they may be in danger, you will want
your chatbot to detect and report these interactions to you or the appropriate
authorities. So your math tutor chatbot will need an intent classifier that can
categorize the messages your users send to the chatbot. The open source
MathText
[
12
]
 and 
Maitag
[
13
]
 projects give you pretrained mulitlabel classifiers
and labeled datasets for intent recognition, including the intents required for
the Rori project.
expressions find the closest grammar matches among a list of possible
grammar rules (regular expressions) instead of exact matches by ignoring
some maximum number of insertion, deletion, and substitution errors.
However, expanding the breadth and complexity of behaviors for pattern-
matching chatbots requires a lot of difficult human development work. Even
the most advanced grammar-based chatbots, built and maintained by some of
the largest corporations on the planet (Google, Amazon, Apple, Microsoft),
remain in the middle of the pack for depth and breadth of chatbot IQ.
A lot of powerful things can be done with shallow NLP. And little, if any,
human supervision (labeling or curating of text) is required. Often a machine
can be left to learn perpetually from its environment (the stream of words it
can pull from Twitter or some other source).
[
67
]
 We show you how to do this
in Chapter 6.

<----------section---------->

1.11 Natural language IQ
Like human brainpower, the power of an NLP pipeline cannot be easily
gauged with a single IQ score without considering multiple "smarts"
dimensions. A common way to measure the capability of a robotic system is
along the dimensions of behavior complexity and the degree of human
supervision required. But for a natural language processing pipeline, the goal
is to build systems that fully automate the processing of natural language,
eliminating all human supervision (once the model is trained and deployed).
So a better pair of IQ dimensions should capture the breadth and depth of the
complexity of the natural language pipeline.
A consumer product chatbot or virtual assistant like Alexa or Allo is usually
designed to have extremely broad knowledge and capabilities. However, the
logic used to respond to requests tends to be shallow, often consisting of a set
of trigger phrases that all produce the same response with a single if-then
decision branch. Alexa (and the underlying Lex engine) behave like a single
layer, flat tree of (if, elif, elif, …) statements.
[
68
]
 Google Dialogflow (which
was developed independently of Google’s Allo and Google Assistant) has
similar capabilities to Amazon Lex, Contact Flow, and Lambda, but without
the drag-and-drop user interface for designing your dialog tree.
On the other hand, the Google Translate pipeline (or any similar machine
translation system) relies on a deep tree of feature extractors, decision trees,
and knowledge graphs connecting bits of knowledge about the world.
Sometimes these feature extractors, decision trees, and knowledge graphs are
explicitly programmed into the system, as in Figure 1.5. Another approach
rapidly overtaking this "hand-coded" pipeline is the deep learning data-driven
approach. Feature extractors for deep neural networks are learned rather than
hard-coded, but they often require much more training data to achieve the
same performance as intentionally designed algorithms.
You will use both approaches (neural networks and hand-coded algorithms)
as you incrementally build an NLP pipeline for a chatbot capable of
conversing within a focused knowledge domain. This will give you the skills
you need to accomplish the natural language processing tasks within your
industry or business domain. Along the way you will probably get ideas
about how to expand the breadth of things this NLP pipeline can do. Figure
1.6 puts the chatbot in its place among the natural language processing
systems that are already out there. Imagine the chatbots you have interacted
with. Where do you think they might fit in a plot like this? Have you
attempted to gauge their intelligence by probing them with difficult questions
or something like an IQ test? Try asking a chatbot something ambiguous that
requires common sense logic and the ability to ask clarifying questions, such
as "What’s larger, the sun or a nickel?"
[
69
]
 you will get a chance to do exactly
that in later chapters, to help you decide how your chatbot stacks up against
some of the others in this diagram.
Figure 1.9 2D IQ of some natural language processing systems
As you progress through this book, you will be building the elements of a
chatbot. Chatbots require all the tools of NLP to work well:
Feature extraction (usually to produce a vector space model)
Information extraction to be able to answer factual questions
Semantic search to learn from previously recorded natural language text
or dialog
Natural language generation to compose new, meaningful statements
Machine learning gives us a way to trick machines into behaving as if we had
spent a lifetime programming them with hundreds of complex regular
expressions or algorithms. We can teach a machine to respond to patterns
similar to the patterns defined in regular expressions by merely providing it
examples of user statements and the responses we want the chatbot to mimic.
And the "models" of language, the FSMs, produced by machine learning, are
much better. They are less picky about mispelings and typoz.
And machine learning NLP pipelines are easier to "program." We do not
have to anticipate every possible use of symbols in our language. We just
have to feed the training pipeline with examples of the phrases that match and
with example phrases that do not match. As long as we label the example
phrases during training so that the chatbot knows which is which, it will learn
to discriminate between them. And there are even machine learning
approaches that require little if any "labeled" data.
We have given you some exciting reasons to learn about natural language
processing. You want to help save the world, do you not? And we have
attempted to pique your interest with some practical NLP applications that
are revolutionizing the way we communicate, learn, do business, and even
think. It will not be long before you are able to build a system that
approaches human-like conversational behavior. And you should be able to
see in upcoming chapters how to train a chatbot or NLP pipeline with any
domain knowledge that interests you — from finance and sports to
psychology and literature. If you can find a corpus of writing about it, then
you can train a machine to understand it.
This book is about using machine learning to build smart text-reading
machines without you having to anticipate all the ways people can say things.
Each chapter incrementally improves on the basic NLP pipeline for the
chatbot introduced in this chapter. As you learn the tools of natural language
processing, you will be building an NLP pipeline that can not only carry on a
conversation but help you accomplish your goals in business and in life.

<----------section---------->

1.12 Test yourself
Chapter 1 review questions
Here are some review questions for you to test your understanding:
1
. 
Why is NLP considered to be a core enabling feature for AGI (human-
like AI)?
2
. 
Why do advanced NLP models tend to show significant discriminatory
biases?
3
. 
How is it possible to create a prosocial chatbot using training data from
sources that include antisocial examples?
4
. 
What are 4 different approaches or architectures for building a chatbot?
5
. 
How is NLP used within a search engine?
6
. 
Write a regular expression to recognize your name and all the variations
on its spelling (including nicknames) that you’ve seen.
7
. 
Write a regular expression to try to recognize a sentence boundary
(usually a period ("."), question mark "?", or exclamation mark "!")
tip
Active learning, quizzing yourself with questions such as these, is a fast way
to gain deep understanding of any new topic. It turns out, this same approach
is effective for machine learning and model evaluation as well.footnote:
[Suggested answers are provided within the Python packages 
nlpia
(
https://gitlab.com/tangibleai/nlpia
) and 
qary
(
https://gitlab.com/tangibleai/qary
) where they are used to evaluate advanced
NLP models for reading comprehension and question answering. Pooja Sethi
will share active learning NLP insights on Substack
(
https://activelearning.substack.com
) and github (
<----------section---------->
**Natural Language Processing and Large Language Models**

This material is part of Lesson 8, "Building a Chatbot," within the "Corso di Laurea Magistrale in Ingegneria Informatica" (Master's Degree Course in Computer Engineering) at DIEM – University of Salerno. The lesson is instructed by Nicola Capuano and Antonio Greco.

**Exercise: Developing a Pizzeria Chatbot**

The primary task is to create a chatbot designed to streamline and enhance the operations of a pizzeria. The chatbot will enable users to interact with the pizzeria through a web-based Graphical User Interface (GUI) with the following functionalities:

*   **Menu Requests:** Users can ask the chatbot to provide the pizzeria's menu.
*   **Pizza Orders:** Users can order a pizza, but only one pizza per order is permitted. Beverage ordering is excluded in this simplified version.
*   **Order Logging:** After an order is confirmed, the chatbot must log essential details including the date of the order, the user's identification (user ID), and the specific type of pizza ordered. This logging function should be implemented using a custom action to provide flexibility and extensibility.
*   **Web-Based GUI:** The chatbot must be accessible through a web-based GUI for ease of use and broad accessibility.

It's recommended to begin with a basic "dummy bot" to establish the foundational structure, then add functionality incrementally.

**Example Conversation:**

The following is a sample dialogue demonstrating the expected interaction flow between a user and the chatbot:

User: Can I have the menu?

Bot: What kind of pizza would you like? We have Parenti, Pepperoni, Vegetarian...

User: Pepperoni

Bot: You want a Pepperoni pizza. Is that correct?

User: Yes

Bot: Great! Your Pepperoni pizza is on its way!

<----------section---------->
**Hints and Implementation Details**

To initiate the project, create the necessary directory and initialize a Rasa project:

```bash
mkdir pizzaBot
cd pizzaBot
rasa init --no-prompt
```

This series of commands first creates a directory named `pizzaBot`, then navigates into it. Inside this directory, `rasa init --no-prompt` initializes a new Rasa project without prompting for user input. This is a quick way to set up the basic project structure. Rasa is an open-source conversational AI framework, which is helpful when building a chatbot with configurable NLP capabilities.

Next, configure and run the REST and Actions servers:

```bash
rasa run --cors "*"
rasa run actions
```

The first command, `rasa run --cors "*"`, starts the Rasa server with Cross-Origin Resource Sharing (CORS) enabled for all origins. This setting is mainly for development purposes to allow web frontends hosted on different domains to communicate with the Rasa server. The second command, `rasa run actions`, launches the actions server. This server executes custom Python code (actions) which are used to extend the bot's functionality, such as logging the order details in this scenario.

To create the web frontend of this project, consider using a web widget such as the one available at:

[https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0](https://github.com/JiteshGaikwad/Chatbot-Widget/tree/Widget2.0)

This widget provides a ready-made interface that can be integrated with the Rasa chatbot.

<----------section---------->
**Contextual Enrichment: The Role of Chatbot Frameworks and LLMs**

The exercise of creating a pizzeria chatbot highlights practical applications of Natural Language Processing (NLP) and the utilization of chatbot frameworks. Building functional and engaging chatbots requires careful consideration of several factors, including the user's intent, the bot's responses, and the overall conversational flow.

Chatbot frameworks are designed to abstract away many of the complexities involved in creating a dialog engine, allowing developers to focus on designing the bot's behavior in a domain-specific language.

As the included context emphasizes, teachers inspire students using feedback on how well the student understands the material. They also think up new ideas as students pose interesting new questions. Students' needs are diverse and dynamic. Due to this fact, hybrid chatbots integrating Large Language Models (LLMs) have become a better method for creating production chatbots in all domains. LLMs are proficient and convincing in their ability to converse with users on virtually any topic. However, the key is to control this power intelligently to prevent misleading or harmful interactions.

<----------section---------->
**Chatbot Frameworks: An Abstraction Layer for Dialogue Engines**

Chatbot frameworks serve as application and software libraries designed to simplify the process of building dialogue engines for chatbots. These frameworks abstract detailed decisions, offering a way to specify chatbot behavior using domain-specific languages that can be interpreted and executed. This allows chatbots to respond in a manner consistent with the designer's intent.

Most chatbot frameworks rely on declarative programming languages to define a bot's behavior, and some offer graphical user interfaces (GUIs) for programming. No-code chatbot frameworks further simplify this by abstracting the declarative language with interactive graphical representations of dialog graphs, allowing modifications via mouse interactions. These frameworks often include a dialog engine that can execute the chatbot without requiring direct manipulation of the underlying data.

Examples of chatbot frameworks include RapidPro, sponsored by UNICEF, which has served as the foundation for platforms like Weni, Textit, and Glific, all utilized for impactful applications. RapidPro offers a GUI for building dialogs and supports importing and exporting content using open standard file formats, facilitating multilingual chatbot development. Other closed-source no-code builders with similar functionality include ManyChat and Landbot.

However, for more sophisticated chatbot designs, a chatbot programming language becomes necessary to realize complex visions. While Python can be used directly, a chatbot framework that uses a dedicated design language or data structure is essential for building scalable and maintainable chatbots. A suitable language allows developers to quickly translate conversation designs into working chatbots.

<----------section---------->
**Leveraging Open-Source Python Chatbot Frameworks: Rasa, LangChain, and qary**

Several popular open-source Python chatbot frameworks provide configurable NLP capabilities, enabling developers to build production-grade systems while focusing on the bot's core functionality.

1.  **Rasa:**

    *   Rasa is an open-source conversational framework used to create thousands of bots in multiple languages.
    *   It structures conversations using user intents and bot actions, which can range from simple pre-programmed utterances to complex Python code interacting with external systems like databases or web APIs.
    *   Dialog scenarios are pre-programmed by chaining these building blocks into sequences called "Stories," stored in YAML files.

    To create a Rasa chatbot, install the Rasa package and initialize a project:

    ```bash
    pip install rasa # If not already installed
    rasa init
    ```

    The installation wizard guides you through creating a new project and training an initial model.

    The key directories in a Rasa project include:

    *   `.rasa/`: Contains cached data.
    *   `actions/`: Contains Python code for custom actions.
    *   `data/`: Holds training data, including NLU data and dialog stories.
    *   `models/`: Stores trained models.
    *   `tests/`: Contains test cases for the chatbot.

    The `data` directory is central to training the chatbot. It includes:

    *   `nlu.yml`: Defines user intents and example utterances.
    *   `domain.yml`: Specifies the chatbot's domain, including intents, entities, slots, and responses.
    *   `stories.yml`: Defines conversation flows (stories) by chaining intents and actions.
    *   `config.yml`: Configures the NLU pipeline, specifying components for tokenization, featurization, and intent classification.

    Example `nlu.yml` content:

    ```yaml
    version: "3.1"
    nlu:
    - intent: greet
      examples: |
       - hey
       - hello
       - hi
    - intent: parent
      examples: |
       - I am a parent
       - Parent
       - I'm a mom to 12 year old
    - intent: agree
      examples: |
       - yes
       - I agree
    ```
<----------section---------->
2.  **LangChain:**

    *   LangChain is a library designed to abstract the APIs of different LLMs, facilitating experimentation with various models and usage approaches.
    *   It relies heavily on APIs and provides a JavaScript/TypeScript SDK for web interfaces.
    *   LangChain's core concept is the "Chain," a callable interface that implements a series of calls to components, including other Chains.
    * LLMs can be used to generate additional content for students that may help them get through difficult spots in their learning. LLMs are very reliable when you use them for the kinds of things you will need them to do when rewording your dialog content: summarization, paraphrasing and correcting grammar. And you can often even improve on the LLM reliability (predictability) on these tasks by reducing the size of the LLM. This has the additional benefit of reducing your LLM latency and expense. This is because your use case and the kinds of statements you have designed into your chatbot are probably very generic and commonplace — the kinds of word patterns that would be very well-represented in the smaller LLM training sets.

    To use LangChain, a prompt template is created to wrap boilerplate text around user messages, simplifying interaction with the Chain. A `Memory` object stores conversation history, with options like `ConversationBufferMemory`, `ConversationKGMemory`, and `ConversationSummaryMemory` available.

3.  **qary:**

    * This platform helps the user extract URLs, named entities, or taboo words from both the user text and the bot-generated text. ConvoHub community also exists as part of qary.ai.

<----------section---------->
**Adding LLMs to your Chatbot with LangChain**

Using Large Language Models, or LLMs, can be especially useful in an educational setting to inspire and engage students. Teachers modify what they say based on feedback from students. Also, teachers come up with new approaches and ideas when students ask new questions. Teachers inspire students' curiosity with Socratic questions.
It is virtually impossible to build a system that captures all of the things that teachers do to help students grow. This is why hybrid chatbots with LLMs are the preferred method build chatbots in every domain.

The most powerful thing about LLMs is that they can chat with your users on virtually any topic. The challenge is to make sure that LLMs don't mislead your users.

One tool for creating generative chatbots is LangChain. Langchain is a library that abstracts away the particular API of the LLM you want to use, allowing you to quickly experiment with different models and different approaches to using them. LangChain relies on APIs to function and has a Javascript/Typescript SDK that makes it easier to use in web interfaces. Large language models are too compute-intensive and memory-intensive to run on a personal computer, or even closed-source.

To use Llama 2 from your machine, you need a strong enough processor, and a lot of RAM. One free service
that makes this a little easier is called Replicate. Replicate.com gives you access to open-source models through a web API. To run the code properly, you will need to create a GitHub account and then use it to sign into Replicate. You can then create or renew your API token under your user profile
<----------section---------->

**Outline LESSON 9**

This lesson will cover the following topics:

*   Limitations of Recurrent Neural Networks (RNNs): Understanding the drawbacks of RNNs that led to the development of Transformers.
*   Transformer: Introducing the Transformer architecture as an alternative to RNNs.
*   Transformer’s Input: Detailing how input data is prepared and fed into the Transformer model.
*   Self-Attention: Exploring the core mechanism of Transformers, enabling parallel processing and capturing relationships within the input sequence. Self-attention is the most straightforward and common way to implement attention. It takes the input sequence of embedding vectors and puts them through linear projections. A linear projection is merely a dot product or matrix multiplication. This dot product creates key, value and query vectors.
The query vector is used along with the key vector to create a context vector
for the words' embedding vectors and their relation to the query. This context
vector is then used to get a weighted sum of values. In practice, all these
operations are done on sets of queries, keys, and values packed together in
matrices, Q, K, and V, respectively.

<----------section---------->

**Limitations of RNNs**

RNNs, while effective for processing sequential data, suffer from several limitations:

*   RNNs lack of long-term memory (in encoder-decoder models): Traditional RNNs, especially those used in encoder-decoder architectures, struggle to retain information over long sequences. This makes it difficult to handle long-range dependencies in the data.
*   RNNs are extremely slow to train (for long series): Due to their sequential nature, RNNs are computationally intensive to train, especially with long sequences. This is because each step depends on the previous one, preventing parallelization.
*   RNNs suffer from the vanishing gradient problem: During training, the gradients used to update the network's weights can diminish as they are backpropagated through time, hindering the learning of long-range dependencies. The same function F is traversed many times. Derivatives of F are multiplied several times by themselves.

<----------section---------->

**RNNs Lack of Long-Term Memory**

This refers to the difficulty RNNs have in capturing dependencies between elements in a sequence that are far apart. As information flows through the network, it can be diluted or forgotten, making it challenging to model long-range relationships.

<----------section---------->

**RNNs are Extremely Slow to Train**

*   Processing is inherently sequential: The fundamental design of RNNs requires sequential processing of input data. Each element in the sequence must be processed in order, one after the other.
*   The network cannot start processing $x_i$ until it has finished with $x_{i-1}$: This dependency means that the network must complete the computation for the previous element ($x_{i-1}$) before it can begin processing the current element ($x_i$).
*   Thus, the network cannot exploit the massive parallelism available in a modern GPU!: The sequential nature of RNNs prevents them from fully utilizing the parallel processing capabilities of modern GPUs, which can significantly speed up computations.

<----------section---------->

**RNNs Suffer from the Vanishing Gradient Problem**

*   The problem is related to the Vanishing gradient/exploding gradient: The vanishing gradient problem and its counterpart, the exploding gradient problem, are common issues in training deep neural networks, particularly RNNs.
*   During backpropagation through time (BPTT), the same function $F$ is traversed many times: Backpropagation Through Time (BPTT) is the algorithm used to train RNNs. It involves unfolding the network over time and calculating gradients for each time step. The same activation function $F$ is applied repeatedly across the sequence.
*   $\frac{\partial Loss}{\partial h_0} = \frac{\partial Loss}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_3} \cdot \frac{\partial F}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = \frac{\partial Loss}{\partial h_4} \cdot \frac{\partial F}{\partial h_3} \cdot \frac{\partial F}{\partial h_2} \cdot \frac{\partial F}{\partial h_1} \cdot \frac{\partial F}{\partial h_0} = ...$
    The derivatives of F are multiplied several times by themselves. If the absolute value of the derivatives of F is small, in this process it will become smaller and smaller… (vanishing gradient)… and if it is large, it will become larger and larger (exploding gradient), causing problems to the stability of the algorithm. Note: the problem is caused by the fact that we are traversing many times (sequence length) the same layer.

<----------section---------->

**Transformer**

In 2017, a group of researchers at Google Brain proposed an alternative model for processing sequential data. In this model, the elements of the sequence can be processed in parallel. The number of layers traversed does not depend on the length of the sequence (so, no problems with the gradient). The model was introduced for language translation (sequence to sequence with different lengths); so, it was called Transformer. Subsets of the model can be used for other sequence processing tasks. Transformers address the limitations of RNNs by enabling parallel processing of sequence elements, mitigating the vanishing gradient problem, and reducing training time.

<----------section---------->

**Transformer**

The Transformer architecture comprises several key components:

*   Input: The raw data that is fed into the model.
*   Tokenization: The process of converting the input text into a sequence of tokens, which are the basic units of processing.
*   Input embedding: Projecting tokens into a continuous Euclidean space.
*   Positional encoding: Adding information about the position of each token in the sequence.
*   Encoder: The part of the model that processes the input sequence and generates a contextualized representation.
    *   Attention: The mechanism that allows the model to focus on different parts of the input sequence when processing each element.
        *   Query: A representation of the current element being processed.
        *   Key: A representation of each element in the input sequence, used to determine its relevance to the query.
        *   Value: A representation of each element in the input sequence, used to compute the weighted average based on the attention scores.
    *   Self Attention: The same vectors are used as Q, K and V. The attention mechanism where the query, key, and value are derived from the same input sequence. This allows each element to attend to all other elements in the sequence.
    *   Multi-head Attention: Multiple attention mechanisms working in parallel, allowing the model to capture different aspects of the relationships within the input sequence. Each attention head attends to different aspects of the words in a text.
        *The query, key, and value matrices are multiplied n (n_heads, the number of attention heads)
times by each different \(d_q\) , \(d_k\), and \(d_v\) dimension, to compute
the total attention function output. The n_heads value is a hyperparameter of
the transformer architecture that is typically small, comparable to the number
of transformer layers in a transformer model. The \(d_v\)-dimensional
outputs are concatenated and again projected with a \(W^o\) matrix as shown
in the next equation.
Equation 9.2 Multi-Head self-attention
\[MultiHeadAttention(Q, K, V ) = Concat(head_1, ..., head_n) W^o\\ where\
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\]

    *   Add & Norm: Adding residual connections and applying layer normalization to improve training stability and performance.
    *   Feedforward: A feedforward neural network applied to each element of the sequence independently.
*   Decoder: The part of the model that generates the output sequence based on the encoded representation.
    *   Masked attention: A variant of attention used in the decoder to prevent it from attending to future tokens.
    *   Encoder decoder attention: The attention mechanism that allows the decoder to attend to the encoded representation from the encoder.
*   Output: The final sequence generated by the model.

<----------section---------->

**Input**

This section describes how the input data is prepared for the Transformer model.

<----------section---------->

**Tokenization**

*   Representation of text with a set of tokens: Tokenization is the process of breaking down a text into individual units, called tokens. These tokens can be words, subwords, or characters.
*   Each token is encoded with a unique id: Each token is assigned a unique numerical identifier, which is used as the input to the embedding layer.

<----------section---------->

**Input Embedding**

*   Embedding: a representation of a symbol (word, character, sentence) in a distributed low-dimensional space of continuous-valued vectors.
*   The tokens are projected in a continuous Euclidean space: The numerical IDs are mapped to dense, continuous-valued vectors in a lower-dimensional space, known as the embedding space.
*   Correlations among words can be visualized in the embedding space: depending on the task, word embedding can push two words further away or keep them close together.
*   Ideally, an embedding captures the semantics of the input by placing semantically similar inputs close together in the embedding space: The goal of the embedding is to capture the semantic meaning of the tokens, such that tokens with similar meanings are located close to each other in the embedding space.

<----------section---------->

**Positional Encoding**

This section addresses the crucial aspect of incorporating positional information into the Transformer model.

<----------section---------->

**The Importance of Order**

*   Q: With the encoding we have seen so far, is it possible to discriminate between sequences that only differ in the order of the elements?
*   E.g., is it possible to differentiate between "The student is eating an apple" and "An apple is eating the student"?

This question highlights the need to encode the order of elements in a sequence, as the meaning can change drastically with different arrangements.

<----------section---------->

**The Importance of Order**

*   Q: With the encoding we have seen so far, is it possible to discriminate between sequences that only differ in the order of the elements?
*   E.g., is it possible to differentiate between "The student is eating an apple" and "An apple is eating the student"?
*   A: No, because the output of the attention module does not depend on the order of its keys/values pairs
*   So how can we add the information on the order of the sequence elements? The attention mechanism, by itself, is order-agnostic. Therefore, a method to incorporate positional information is required.

<----------section---------->

**Positional Encoding**

*   The solution proposed by the authors of the Transformer model is to add a slight perturbation to each element of the sequence, depending on the position within the sequence.
*   In this way, the same element appearing in different positions would be encoded using slightly different vectors. This ensures that the model can differentiate between tokens based on their location in the sequence.

<----------section---------->

**Positional Encoding**

*   The position encoding is represented by a set of periodic functions.
*   In particular, if $d_{model}$ is the size of the embedding, and $pos$ is the position of an element within the sequence, the perturbation to component $i$ of the embedding vector representing the element is:

    $PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})$
    $PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})$
*   The positional encoding is a vector with the same dimension as the input embedding, so it can be added on the input directly. These sine and cosine functions provide a unique encoding for each position, allowing the model to learn the relationships between tokens based on their relative distances.

<----------section---------->

**Encoder**

The Encoder transforms an input sequence of vectors $x_1, \dots, x_t$ into an intermediate representation of the same length $z_1, \dots, z_t$. The vectors $z_1, \dots, z_t$ can be generated in parallel. Each vector $z_i$ does not depend only on the corresponding $x_i$, but on the whole input sequence $x_1, \dots, x_t$. The encoder processes the input sequence and generates a contextualized representation, where each element is influenced by all other elements in the sequence. Unlike RNNs, this process occurs in parallel.

<----------section---------->

**Encoder**

The encoder is made of a sequence of encoder blocks having the same structure. The original paper used 6 encoder blocks. Each encoder block processes a sequence using a combination of the following mechanisms:

*   Self-attention: a (multi-headed) attention module where the same vectors are used as Q, K and V. A core mechanism that allows the encoder to capture relationships between different parts of the input sequence.
*   A classical feed-forward layer applied separately to each element of the sequence: After the self-attention mechanism, each element in the sequence is passed through a feed-forward neural network.
*   Skip connections: Residual connections that help to alleviate the vanishing gradient problem and improve training.
*   Normalization: Layer normalization is applied to stabilize the training process and improve performance.

<----------section---------->

**Self Attention**

Let us consider the sentence: "The animal didn’t cross the street because it was too wide." What does "it" in this sentence refer to? Estimating self-attention in this sentence means to find the words that one must consider first to find a better encoding for the word "it". Self-Attention estimate must be learned according to the task we are facing. Self-attention is a mechanism that allows the model to weigh the importance of different words in a sentence when encoding a particular word. For instance, in the example sentence, self-attention would help the model determine that "it" refers to "the street" rather than "the animal."

<----------section---------->

**Self Attention**

How to compute the attention to give to each input element when encoding the current word? This question sets the stage for explaining the mechanics of the attention mechanism.

<----------section---------->

**Attention**

In order to understand the self attention, we must first introduce its fundamental building block: the attention function. Informally, an attention function is used when the value to be computed (in this case the embedding of a token in a certain position considering the context of the sentence) depends on a set of other values (in this case other tokens of the sentence), and we want to give each time a different weight (i.e. a different "level of attention") to each of the values (how much each token is important to encode the current token?). The attention function depends on three elements, with a terminology inherited from document retrieval: query, key, value. Attention mechanisms allow the model to focus on relevant parts of the input sequence when processing each element.

<----------section---------->

**Attention**

We have an input value $q$ and we want to define some target function $f_T(q)$. $q$ is called the query value in the attention terminology. In the general case, both $q$ and $f_T(q)$ can be vectors. We want to express $f_T(q)$ as a function of a given set of elements $v_1, \dots, v_n$. We want the "attention" given to each $v_i$ to be different depending on $q$. We assume that for each $v_i$ we have available an additional information $k_i$ that can be used to decide the "attention" to be given to $v_i$. The elements $v_i$ are called values and the $k_i$ are called keys in the attention terminology; both the values and the keys can be vectors. The attention mechanism aims to compute a target function $f_T(q)$ based on a set of values $v_i$, where the importance of each value is determined by its corresponding key $k_i$ and the query $q$.

<----------section---------->

**Attention**

A commonly adopted formulation of the problem is to define the target function as:

$f_T(q) = \alpha(q, k_1) \cdot f_V(v_1) + \dots + \alpha(q, k_n) \cdot f_V(v_n) = \sum_{i=1}^{n} \alpha(q, k_i) \cdot f_V(v_i)$

Where $\alpha$ is the attention given to value $v_i$. This equation mathematically represents how the target function is computed as a weighted sum of the values, where the weights $\alpha(q, k_i)$ are determined by the attention function.

<----------section---------->

**Attention**

$f_T(q) = \sum_{i=1}^{n} \alpha(q, k_i) \cdot f_V(v_i)$

*   $\alpha$ is our attention function
*   We want $\alpha$ and $f_V$ to be learned by our system
*   Typically, $\alpha(q, k_i) \in [0, 1]$ and $\sum_{i} \alpha(q, k_i) = 1$
*   Note: the value of the target function does not depend on the order of the key-value pairs $(k_i, v_i)$ The attention function $\alpha$ and the value transformation function $f_V$ are learned during training. The attention weights $\alpha(q, k_i)$ typically range between 0 and 1 and sum to 1, representing a probability distribution over the values.

<----------section---------->

**Self Attention**

The Transformer architecture uses a particular definition of the attention function, based on linear vector/matrix operations and the softmax function. This definition is:

*   Differentiable, so it can be learned using Back Propagation
*   Efficient to compute
*   Easy to parallelize, since the attention for several query vectors can be efficiently computed in parallel at the same time The Transformer's attention mechanism is designed to be differentiable, computationally efficient, and easily parallelizable, making it suitable for training with backpropagation and leveraging the power of GPUs.

<----------section---------->

**Self Attention**

*   Input: three matrices Q, K, V
*   Q ($m \times d_q$) contains the query vectors (each row is a query)
*   K ($n \times d_k$) contains the key vectors (each row is a key)
*   V ($n \times d_v$) contains the value vectors (each row is a value)
*   K and V must have the same number of rows Self-attention takes three matrices as input: Q (Queries), K (Keys), and V (Values). The dimensions of these matrices are important for the subsequent computations.

<----------section---------->

**Self Attention**

If the query and the key are represented by vectors with the same dimensionality, a matching score can be provided by the scaled dot product of the two vectors (cosine similarity). This highlights the use of the dot product as a measure of similarity between queries and keys.

<----------section---------->

**Self Attention**

Step 0: Each element in the sequence is represented by a numerical vector. This is a preliminary step where each element in the input sequence (e.g., each word in a sentence) is converted into a numerical vector representation.

<----------section---------->

**Self Attention**

Step 1: the input matrices are "projected" onto a different subspace, by multiplying them (using row-by-column dot product) by weight matrices:

*   $Q' = Q \cdot W^Q$
*   $K' = K \cdot W^K$
*   $V' = V \cdot W^V$

These are the trainable weights:

*   $W^Q (d_q \times d'_q)$
*   $W^K (d_k \times d'_k)$
*   $W^V (d_v \times d'_v)$

Note: $W^Q$ and $W^K$ must have the same number of columns. The query, key, and value matrices are linearly transformed using weight matrices $W^Q$, $W^K$, and $W^V$, respectively. These weight matrices are learned during training and project the input matrices into different subspaces.

<----------section---------->

**Self Attention**

Step 1: Compute a key (K), a value (V) and a query (Q) as linear function of each element in the sequence. This step emphasizes the importance of computing the query, key, and value as linear projections of the input sequence elements.

<----------section---------->

**Self Attention**

Step 2: the attention matrix A is computed for each position by multiplying Q' and the transpose of K', scaling by $1 / \sqrt{d'_k}$ and applying softmax to each of the resulting rows:

$A = softmax(\frac{Q' \cdot K'^T}{\sqrt{d'_k}})$

A is a ($m \times n$) matrix whose element $a_{ij} = \alpha(q_i, k_j)$. Softmax is applied to each row separately. This scaling is used to avoid that the argument of softmax becomes too large with the increase of the dimension $d'_k$. The attention matrix A is computed by taking the dot product of the transformed query matrix $Q'$ and the transpose of the transformed key matrix $K'$. The result is scaled by the square root of the dimension of the keys ($d'_k$) to prevent the dot products from becoming too large, which can lead to vanishing gradients. The softmax function is then applied to each row to obtain the attention weights.

<----------section---------->

**Self Attention**

Step 2: Compute attention score for each position i as a softmax of the scaled dot product of all the keys (bidirectional self-attention) with $Q_i$. This step describes the calculation of attention scores for each position i, emphasizing the use of a softmax function on the scaled dot product of keys and queries.

<----------section---------->

**Self Attention**

Final step: the target value is computed by row-by-column multiplication between A and V':

$f_T(Q) = A \cdot V'$

The result is a $m \times d'_v$ matrix representing the target function computed on the m queries in the input matrix Q. The final output is computed by taking a weighted sum of the transformed value matrix $V'$, where the weights are the attention scores from the attention matrix A.

<----------section---------->

**Self Attention**

Step 3: Output representation for each position I, as a weighted sum of values (each one multiplied by the related attention score) This step summarizes the final output representation as a weighted sum of the values, with weights determined by the attention scores.

<----------section---------->

**Self Attention**

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

$a_{ij} = softmax(\frac{q_i k_j}{\sqrt{d_k}}) = \frac{exp(q_i k_j)}{\sum_{s=1}^n exp(q_i k_s)}$

This provides the final equation for computing the attention, where the attention weights are calculated using the softmax function applied to the scaled dot product of the queries and keys, and then multiplied by the values.

<----------section---------->

**Additional Context:**

This section includes additional context gleaned from a practical example of using the base PyTorch implementation of the Transformer module to implement a language translation model. In a language translation model, self-attention enables a leap forward in capability for problems where LSTMs struggled, such as: Conversation — Generate plausible responses to conversational prompts, queries, or utterances.
Abstractive summarization or paraphrasing
:: Generate a new shorter wording of a long text summarization of sentences, paragraphs, and
even several pages of text.
Open domain question answering
:: Answering a general question about
anything the transformer has ever read.
Reading comprehension question answering
:: Answering questions
about a short body of text (usually less than a page).
Encoding
:: A single vector or sequence of embedding vectors that
represent the meaning of body of text in a vector space — sometimes
called task-independent sentence embedding
.
Translation and code generation
 — Generating plausible software
expressions and programs based on plain English descriptions of the
program’s purpose.
There are two ways to implement the linear algebra of an attention algorithm: additive attention or dot-product attention. The one that was most effective in transformers is a scaled version of dot-production attention. For dot-product attention, the scalar products between the query vectors Q and the key vectors K, are scaled down based on how many dimensions there are in the model.
This makes the dot product more numerically stable for large dimensional embeddings and longer text sequences. For dot-product attention, the scalar products between the query vectors Q and the key vectors K, are scaled down based on how many dimensions there are in the model.
This makes the dot product more numerically stable for large dimensional embeddings and longer text sequences.
Unlike RNNs, where there is recurrence and shared weights, in self-attention all of the vectors used in the query, key, and value matrices come from the input sequences' embedding vectors. The entire mechanism can be
implemented with highly optimized matrix multiplication operations. And the Q
 
K product forms a square matrix that can be understood as the connection
between words in the input sequence.
Multi-head self-attention is an expansion of the self-attention approach to
creating multiple attention heads that each attend to different aspects of the
words in a text. So if a token has multiple meanings that are all relevant to the
interpretation of the input text, they can each be accounted for in the separate
attention heads. You can think of each attention head as another dimension of
the encoding vector for a body of text, similar to the additional dimensions of
an embedding vector for an individual token .

The multiple heads allow the model to focus on different positions, not just
ones centered on a single word. This effectively creates several different vector subspaces where the transformer can encode a particular generalization for a subset of the word patterns in your text. In the original transformers paper, the model uses n
=8 attention heads such that \(d_k = d_v
= \frac{d_{model}}{n} = 64\). The reduced dimensionality in the multi-head
setup is to ensure the computation and concatenation cost is nearly equivalent
to the size of a full-dimensional single-attention head.

The attention matrices (attention heads) created by the product of Q and K all have the same shape, and they are all square (same number of rows as columns). This means that the attention matrix merely rotates the input sequence of embeddings into a new sequence of embeddings, without affecting the shape or magnitude of the embeddings.
Encoder-decoders based on RNNs don’t work very well for longer passages of text
where related word patterns are far apart. Even long sentences are a challenge
for RNNs doing translation. And the attention mechanism compensates for
this by allowing a language model to pick up important concepts at the
beginning of a text and connect them to text that is towards the end. The
attention mechanism gives the transformer a way to reach back to any word it
has ever seen. Unfortunately, adding the attention mechanism forces you to
remove all recurrence from the transformer.

The loss of recurrence in a transformer creates a new challenge because the
transformer operates on the entire sequence all at once. A transformer is
reading the entire token sequence all at once. And it outputs the tokens all at
once as well, making bi-directional transformers an obvious approach.
Transformers do not care about the normal causal order of tokens while it is
reading or writing text. To give transformers information about the causal
sequence of tokens, positional encoding was added.
The combination of BPE plus attention plus positional encoding combine together to create unprecedented scalability. These three innovations and simplifications of neural networks combined to create a
network that is both much more stackable and much more parallelizable.
Stackability
:: The inputs and outputs of a transformer layer have the
exact same structure so they can be stacked to increase capacity
Parallelizability
:: The cookie cutter transformer layers all rely heavily
on large matrix multiplications rather than complex recurrence and
logical switching gates. The increased intelligence that
transformers bring to AI is transforming culture, society, and the economy.
For the first time, transformers are making us question the long-term
economic value of human intelligence and creativity.
The attention matrix enables a transformer to accurately model the connections
between all the words in a long body of text, all at once.

And the attention matrix within each layer spans the entire length of the input
text, so each transformer layer has the same internal structure and math. You
can stack as many transformer encoder and decoder layers as you like
creating as deep a neural network as you need for the information content of
your data. Every transformer layer outputs a consistent encoding with the same size and shape.

<----------section---------->

### Lesson 10: Transformers II

## Outline

*   Multi-Head Attention
*   Encoder Output
*   Decoder
*   Masked Multi-Head Attention
*   Encoder-Decoder Attention
*   Output
*   Transformer’s pipeline

This outline provides a structured overview of the topics discussed in the lesson. It begins with multi-head attention, a crucial component of the Transformer architecture, and then progresses through the encoder and decoder structures. Masked multi-head attention, encoder-decoder attention, and the final output layers are also covered, culminating in a comprehensive view of the Transformer pipeline. Each of these components plays a vital role in the model's ability to process and generate natural language.

<----------section---------->

## Multi-head attention

### Multi-head Attention
*   By using different self-attention heads, it is possible to encode different meanings of the context.
*   Several scaled-dot product attention computations are performed in parallel (using different weight matrices).
*   The results are concatenated row-by-row, forming a larger matrix (with the same number of rows m).
*   This matrix is finally multiplied by a final weight matrix.
*   This scheme is called multi-head attention.

Multi-head attention enhances the model's capacity to capture diverse relationships within the input data. By employing multiple 'attention heads', the model can simultaneously focus on different aspects of the input, encoding richer contextual information. Each head performs a scaled-dot product attention calculation in parallel, using distinct weight matrices. The outputs from each head are then concatenated, creating a larger matrix that combines the diverse perspectives. Finally, this concatenated matrix is transformed by a final weight matrix, integrating the different attention perspectives into a unified representation.

<----------section---------->

### Multi-head Attention
Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this possibility. The outputs of the heads are concatenated and then are multiplied by an additional weight matrix to combine several representations at the same network level.

The benefit of multi-head attention lies in its ability to prevent the averaging effect that occurs in single-head attention. By attending to various representation subspaces, the model captures more nuanced relationships between words and their context. The subsequent concatenation and matrix multiplication steps serve to combine these diverse representations, enhancing the model's understanding of the input sequence at each layer. This contrasts with single-head attention, where crucial distinctions could be obscured by simply averaging the representations.

<----------section---------->

## Add & Norm

### Add (skip connections) & Norm
Input Normalization(Z)
* Mean 0, Std dev 1
* Stabilizes training
* Regularization effect

Add -> Residuals
* Avoid vanishing gradients
* Train deeper networks

The "Add & Norm" component refers to the addition of skip (or residual) connections and normalization layers within the Transformer architecture. Input normalization, typically involving setting the mean to 0 and standard deviation to 1, stabilizes the training process and provides a regularization effect, improving generalization. The "Add" operation incorporates residuals or skip connections, which directly pass the input of a layer to its output. This technique helps to avoid vanishing gradients, facilitating the training of deeper networks and enabling the model to learn more complex representations.

<----------section---------->

## Feed Forward

### Feed Forward
Non Linearity. Complex Relationships. Learn from each other.

FFN (2 layer MLP)

The Feed Forward Network (FFN) introduces non-linearity into the Transformer model, enabling it to learn complex relationships within the data. This module typically consists of a two-layer Multi-Layer Perceptron (MLP). The non-linearity allows the model to capture intricate patterns and dependencies that linear models cannot represent. The two layers enable the neurons to learn from each other, further enhancing the model's ability to extract meaningful features.

<----------section---------->

## Transformer’s Encoder

### Transformer’s Encoder
Used for computing a representation of the input sequence. Uses an additive positional encoding to deal with the order-agnostic nature of the self-attention. Uses residual connection to foster the gradients flow. Adopts normalization layers to stabilize the network training. Position-Wise Feed-Forward layer to add non-linearity. Applied to each sequence element independently. Since each encoder produces an output whose dimensionality is the same of the input, it is possible to stack an arbitrary number of encoder’s blocks. The output of the first block is fed to the second block (no word embeddings) and so on.

The Transformer's encoder is designed to generate a contextualized representation of the input sequence. To address the order-agnostic nature of the self-attention mechanism, positional encodings are added to the input embeddings. These encodings provide information about the position of each word in the sequence. Residual connections facilitate gradient flow during training, and normalization layers enhance training stability. A position-wise feed-forward network introduces non-linearity. Because the output of each encoder block has the same dimensionality as its input, encoder blocks can be stacked. The output of one encoder block is fed to the next, allowing for the creation of deep networks.

<----------section---------->

## Decoder

### Decoder
The Decoder uses the information contained in the intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub> to generate the output sequence *y*<sub>1</sub>,…,*y*<sub>*m*</sub>. The Decoder works sequentially; at each step the decoder uses *z*<sub>1</sub>,…,*z*<sub>*t*</sub> and *y*<sub>1</sub>,…,*y*<sub>*i*-1</sub> to generate *y*<sub>*i*</sub>.

The decoder's primary function is to generate the output sequence *y*<sub>1</sub>,…,*y*<sub>*m*</sub> by leveraging the intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub> produced by the encoder. The decoder operates sequentially, generating one element of the output sequence at a time. At each step *i*, the decoder uses the encoder's output *z*<sub>1</sub>,…,*z*<sub>*t*</sub> and the previously generated elements of the output sequence *y*<sub>1</sub>,…,*y*<sub>*i*-1</sub> to predict the next element *y*<sub>*i*</sub>. This sequential process allows the model to capture dependencies between the output elements.

<----------section---------->

### Decoder
The decoder is made of a sequence of decoder blocks having the same structure. The original paper used 6 decoder blocks. The decoder blocks, in addition to the same modules used in the encoder block, add an attention module where the keys and values are taken from the encoder’s intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub>. Also, the self-attention module is slightly modified so as to ensure that the query at position i only uses the values at positions 1,…,i. On top of the last decoder block, the decoder adds an additional linear layer and a softmax activation function, for computing the probability of the next output element *y*<sub>*i*</sub>. Thus, the last layers has a number of neurons corresponding to the cardinality of the output set.

Similar to the encoder, the decoder consists of a stack of identical decoder blocks. The original Transformer model utilized 6 such blocks. Each decoder block incorporates the same modules as the encoder block (multi-head attention, feed-forward networks, etc.) and also an attention module that attends to the encoder's output *z*<sub>1</sub>,…,*z*<sub>*t*</sub>. The self-attention mechanism within the decoder is modified to prevent the query at position *i* from attending to future positions (1,…,i only), ensuring causality in the output sequence generation. At the top of the stack, a linear layer and softmax function compute the probability distribution over the output vocabulary, predicting the next output element *y*<sub>*i*</sub>. The linear layer’s size matches the output vocabulary size.

<----------section---------->

## Masked Multi-Head Attention
Masked Multi-Head Attention. Outputs at time T should only pay attention to outputs until time T-1. Mask the available attention values.
*R<sup>IxT</sup>*
*R<sup>TxT</sup>*
Attention Mask: M
Masked Attention Values

Masked multi-head attention is a crucial modification to the standard multi-head attention mechanism, particularly within the decoder. Its purpose is to ensure that when generating an output sequence, the model only attends to previous outputs. At time *T*, the output should only depend on inputs up to time *T-1*. This is achieved by masking the attention values, preventing the model from "peeking" into the future. *R<sup>IxT</sup>* and *R<sup>TxT</sup>* likely refer to matrices involved in the attention calculation, and "M" represents the attention mask applied to the attention values. Masked attention ensures causality.

<----------section---------->

## Encoder Decoder Attention
Keys from Encoder Outputs. Queries from Decoder Inputs. Values from Encoder Outputs. Every decoder block receives the same FINAL encoder output.

Encoder-decoder attention bridges the encoder and decoder components of the Transformer model. In this mechanism, the keys and values are derived from the final encoder output, while the queries come from the decoder inputs. In other words, the decoder uses its current state (queries) to attend to the entire encoded input sequence (keys and values), allowing it to extract relevant information. An important aspect is that all decoder blocks receive the *same* final encoder output, enabling consistent information flow throughout the decoding process.

<----------section---------->

## Output

### Output
Linear. Linear weights are often tied with model input embedding matrix. Softmax.

The output stage of the Transformer model typically involves a linear layer followed by a softmax function. The linear layer projects the decoder's final output into a vector space representing the output vocabulary. Interestingly, the weights of this linear layer are sometimes tied to the input embedding matrix, reducing the number of parameters and potentially improving generalization. The softmax function then transforms this vector into a probability distribution over the output vocabulary, enabling the model to predict the most likely next token.

<----------section---------->

## Transformer’s pipeline
Decoding time step.

ENCODER - DECODER.

https://poloclub.github.io/transformer-explainer/

The Transformer's pipeline encapsulates the entire process of sequence-to-sequence transformation, from encoding the input to decoding the output. At each decoding time step, the decoder utilizes the encoded representation and the previously generated outputs to predict the next element in the sequence. The core components are the encoder and decoder, working together to translate or transform the input sequence into the desired output sequence. The provided link (https://poloclub.github.io/transformer-explainer/) offers a visual and interactive explanation of the Transformer architecture, which enhances understanding of the pipeline's inner workings.

<----------section---------->

**Additional Context:**

The additional context elaborates on customizing the Transformer model using the PyTorch library. Specifically, it focuses on extending the `torch.nn.TransformerDecoderLayer` and `torch.nn.TransformerDecoder` classes to output attention weights from the multi-head self-attention layer. It also touches on extending the `torch.nn.Transformer` class to build a translation model and the importance of positional encoding and mask preparation. The context includes code snippets for constructing custom decoder layers and decoders in PyTorch, as well as for setting up a custom translation Transformer model.

```python
from torch import Tensor
from typing import Optional, Any
import torch.nn as nn
import math
from einops import rearrange
import torch

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PAD_IDX = 1
VOCAB_SIZE = 10000

class CustomDecoderLayer(nn.TransformerDecoderLayer):
    def forward(
        self,
        tgt: Tensor,
        memory: Tensor,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Like decode but returns multi-head attention weights.
        """
        tgt2 = self.self_attn(
            tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask
        )[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        tgt2, attention_weights = self.multihead_attn(
            tgt, memory, memory,  # #1
            attn_mask=memory_mask,
            key_padding_mask=None, #mem_key_padding_mask, # typo
            need_weights=True
        )
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        tgt2 = self.linear2(
            self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt, attention_weights  # #2

class CustomDecoder(nn.TransformerDecoder):
    def __init__(self, decoder_layer, num_layers, norm=None):
        super().__init__(
            decoder_layer, num_layers, norm)

    def forward(
        self,
        tgt: Tensor,
        memory: Tensor,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None
    ) -> Tensor:
        """
        Like TransformerDecoder but cache multi-head attention
        """
        self.attention_weights = []  # #1
        output = tgt
        for mod in self.layers:
            output, attention = mod(
                output, memory, tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_key_padding_mask=tgt_key_padding_mask)
            self.attention_weights.append(attention) # #2

        if self.norm is not None:
            output = self.norm(output)

        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model=512, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.d_model = d_model
        self.max_len = max_len
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TranslationTransformer(nn.Transformer):
    def __init__(
        self,
        device=DEVICE,
        src_vocab_size: int = VOCAB_SIZE,
        src_pad_idx: int = PAD_IDX,
        tgt_vocab_size: int = VOCAB_SIZE,
        tgt_pad_idx: int = PAD_IDX,
        max_sequence_length: int = 100,
        d_model: int = 512,
        nhead: int = 8,
        num_encoder_layers: int = 6,
        num_decoder_layers: int = 6,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: str = "relu"
    ):

        decoder_layer = CustomDecoderLayer(
            d_model, nhead, dim_feedforward,
            dropout, activation)
        decoder_norm = nn.LayerNorm(d_model)
        decoder = CustomDecoder(
            decoder_layer, num_decoder_layers,
            decoder_norm)

        super().__init__(
            d_model=d_model, nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout, custom_decoder=decoder)

        self.src_pad_idx = src_pad_idx
        self.tgt_pad_idx = tgt_pad_idx
        self.device = device

        self.src_emb = nn.Embedding(
            src_vocab_size, d_model)
        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)

        self.pos_enc = PositionalEncoding(
            d_model, dropout, max_sequence_length)
        self.linear = nn.Linear(
            d_model, tgt_vocab_size)

    def _make_key_padding_mask(self, t, pad_idx):
        mask = (t == pad_idx).to(self.device)
        return mask

    def prepare_src(self, src, src_pad_idx):
        src_key_padding_mask = self._make_key_padding_mask(
            src, src_pad_idx)
        src = rearrange(src, 'N S -> S N')
        src = self.pos_enc(self.src_emb(src)
            * math.sqrt(self.d_model))
        return src, src_key_padding_mask

    def prepare_tgt(self, tgt, tgt_pad_idx):
        tgt_key_padding_mask = self._make_key_padding_mask(
            tgt, tgt_pad_idx)
        tgt = rearrange(tgt, 'N T -> T N')
        tgt_mask = self.generate_square_subsequent_mask(
            tgt.shape[0]).to(self.device)
        tgt = self.pos_enc(self.tgt_emb(tgt)
            * math.sqrt(self.d_model))
        return tgt, tgt_key_padding_mask, tgt_mask

    def forward(self, src, tgt):
        src, src_key_padding_mask = self.prepare_src(
            src, self.src_pad_idx)
        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(
            tgt, self.tgt_pad_idx)
        memory_key_padding_mask = src_key_padding_mask.clone()
        output = super().forward(
            src, tgt, tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask)
        output = rearrange(output, 'T N E -> N T E')
        return self.linear(output)

    def init_weights(self):
        def _init_weights(m):
            if hasattr(m, 'weight') and m.weight.dim() > 1:
                nn.init.xavier_uniform_(m.weight.data)
        self.apply(_init_weights)

```

<----------section---------->

## LESSON 11 ##

From Transformers to LLMs

**Outline**

*   Transformers for text representation and generation
*   Paradigm shift in NLP
*   Pre-training of LLMs
*   Datasets and data pre-processing
*   Using LLMs after pre-training

<----------section---------->

**Transformers for text representation and generation**

Transformers have revolutionized the field of Natural Language Processing (NLP) by offering a powerful mechanism for both understanding (representation) and creating (generation) text. They leverage the attention mechanism to handle long-range dependencies in text, which was a limitation in previous sequential models like Recurrent Neural Networks (RNNs). Furthermore, transformers enable parallel training, significantly speeding up the training process.

The core idea behind Transformers is the **Attention mechanism**. Unlike recurrent networks that process text sequentially, transformers consider all parts of the input at once, assigning dynamic weights to each part based on its relevance to other parts. This enables the model to capture relationships between words regardless of their distance in the sequence.

Transformers come in different architectural flavors, each suited to specific tasks:

*   **Encoder-Only (e.g., BERT):** This architecture focuses on understanding the input text. It takes input tokens and outputs hidden states representing the contextualized embeddings of the input. Encoder-only models can see all timesteps (i.e., the entire input sequence) at once. They are not inherently auto-regressive, meaning they don't predict the next token sequentially, but rather generate representations of the entire input. BERT (Bidirectional Encoder Representations from Transformers) is a prime example, designed for tasks like sentiment analysis, named entity recognition, and question answering where understanding the context of the entire input is crucial. BERT was released in October 2018. Other prominent encoder-only architectures include:
    *   DistilBERT (2019): A smaller, faster version of BERT.
    *   RoBERTa (2019): A robustly optimized BERT pre-training approach.
    *   ALBERT (2019): A Lite BERT, employing parameter reduction techniques.
    *   ELECTRA (2020): Efficiently Learning an Encoder that Classifies Token Replacements Accurately.
    *   DeBERTa (2020): Decoding-enhanced BERT with disentangled attention.

*   **Decoder-Only (e.g., GPT):** This architecture is designed for generating text. It takes output tokens and hidden states as input and outputs the next predicted token. Decoder-only models can only "see" the previous timesteps, making them auto-regressive. This means they predict the next token based on the tokens generated so far, making them suitable for text generation tasks like language modeling and creative writing. GPT (Generative Pre-trained Transformer) is a flagship decoder-only model. Key models in this family are:
    *   GPT (Jun 2018): The original Generative Pre-trained Transformer.
    *   GPT-2 (2019): An improved version of GPT with a larger parameter set.
    *   GPT-3 (2020): A very large language model demonstrating impressive generation capabilities.
    *   GPT-Neo (2021): An open-source alternative to GPT-3.
    *   GPT-3.5 (ChatGPT) (2022): Further refinement of GPT-3, optimized for conversational interactions.
    *   LLaMA (2023): A powerful and efficient open-source language model.
    *   GPT-4 (2023): OpenAI's latest generation model, exhibiting advanced reasoning and creative abilities.

*   **Encoder-Decoder (e.g., T5, BART):** This architecture combines both encoder and decoder components, allowing it to map input tokens to output tokens. The encoder processes the input sequence, and the decoder generates the output sequence based on the encoder's representation. This architecture is well-suited for tasks like machine translation, text summarization, and question answering where understanding the input and generating a related output are both required. Examples of encoder-decoder models include:
    *   T5 (2019): Text-to-Text Transfer Transformer, framing all NLP tasks in a text-to-text format.
    *   BART (2019): Bidirectional and Auto-Regressive Transformer, designed for denoising sequence-to-sequence tasks.
    *   mT5 (2021): A multilingual version of T5.

<----------section---------->

**Paradigm shift in NLP**

The advent of Large Language Models (LLMs) has instigated a significant paradigm shift in NLP, altering the traditional approaches to problem-solving and model design.

**Before LLMs:**

*   **Feature Engineering:** A significant portion of NLP work involved manually crafting or selecting the most relevant features for a specific task. This required deep domain knowledge and was often a time-consuming and iterative process.
*   **Model Selection:** Choosing the right model for a given task was crucial. Different models excelled at different tasks, requiring careful consideration of the task's characteristics and the model's capabilities.
*   **Transfer Learning:** When labeled data was scarce, transfer learning techniques were used to leverage knowledge from other domains. Models pre-trained on large datasets were fine-tuned on smaller, task-specific datasets.
*   **Overfitting vs. Generalization:** Balancing model complexity and capacity to prevent overfitting while maintaining good performance was a primary concern. Regularization techniques and cross-validation were essential tools.

**Since LLMs:**

*   **Pre-training and Fine-tuning:** LLMs are pre-trained on massive amounts of unlabeled data, capturing a broad understanding of language. This pre-trained knowledge is then fine-tuned on smaller labeled datasets for specific tasks, significantly reducing the need for task-specific feature engineering. This allows for leveraging vast amounts of previously under-utilized unlabeled data.
*   **Zero-shot and Few-shot learning:** LLMs exhibit the remarkable ability to perform on tasks they were not explicitly trained on. Zero-shot learning refers to performing a task without any task-specific training examples. Few-shot learning involves providing only a handful of examples to guide the model.
*   **Prompting:** Instead of extensive fine-tuning, LLMs can be instructed to perform tasks simply by describing them in natural language prompts. The way the prompt is designed significantly impacts the model's performance. Prompt engineering has become a key skill in leveraging LLMs.
*   **Interpretability and Explainability:** Understanding the inner workings of LLMs and how they arrive at their decisions is a growing area of research. The complexity of LLMs makes it challenging to interpret their behavior, but understanding their decision-making process is crucial for building trust and addressing potential biases.

This paradigm shift is primarily driven by the limitations of Recurrent Neural Networks (RNNs) in handling long sequences. Information loss during encoding and the sequential nature of processing hindered parallel training and favored late timestep inputs.

The **Attention mechanism** emerged as a solution, offering:

*   **Handling long-range dependencies:** Attention allows the model to directly attend to relevant parts of the input, regardless of their distance.
*   **Parallel training:** Attention enables parallel processing of the input sequence, significantly accelerating training.
*   **Dynamic attention weights based on inputs:** The attention mechanism dynamically assigns weights based on the relevance of different parts of the input, allowing the model to focus on the most important information.

<----------section---------->

**Pre-training of LLMs**

Large Language Models (LLMs) heavily rely on unstructured data for pre-training. Pre-training is the process of training a large model on a massive dataset to learn general language representations.

**Self-supervised pre-training:**

*   Pre-training a large language model is typically done in a self-supervised way, meaning it is trained on unlabeled data, which is usually just text scraped from the internet.
*   There is no need to assign labels to the dataset manually. Instead, the model learns from the inherent structure of the language itself. If there is no explicit supervision in the data, the model creates its own supervised tasks and solves them.

Different self-supervised pre-training methods exist:

*   **Autoencoding (MLM - Masked Language Modeling):** Models based on autoencoding, like BERT, consist primarily of an encoder. During training, a certain percentage of the input words are masked. The model's task is to predict the masked words based on the surrounding context. This bi-directional approach allows the model to gain knowledge of the entire context of the sentence.
*   **Autoregressive (CLM - Causal Language Modeling):** Autoregressive models, such as GPT, consist of a decoder. They predict the next word in a sequence based on the preceding words. This approach is excellent for text generation tasks, as it naturally simulates the process of writing or speaking. The model predicts the masked word from the preceding words only.
*   **Seq2Seq (Span Corruption):** Seq2Seq models, like T5, use both an encoder and a decoder. In training, random spans of the input text are masked and replaced with a unique sentinel token. The decoder then predicts the masked spans, with the sentinel token preceding the predicted tokens. This approach forces the model to both understand the context and generate text. In summary, seq2seq models both need to understand the context and generate a text.

**Masked Language Modeling (MLM)**

MLM, used in models like BERT, involves feeding input text with randomly masked tokens into a Transformer encoder. The goal is to predict the masked tokens. As an example, consider the original text sequence "I", "love", "this", "red", "car". This sequence is prepended with the "<cls>" (classification) token, and the token "love" is randomly replaced with the "<mask>" token. The model then minimizes the cross-entropy loss between the masked token "love" and its prediction during pre-training.

**Next Token Prediction (Causal Language Modeling)**

Next Token Prediction involves training a model to predict the next word in a sequence. Any text can be used for this task, requiring only the prediction of the next word. This method is primarily used to train autoregressive models like GPT.

**Span Corruption**

In Span Corruption, a technique utilized in models like T5, some words in the original text are dropped out and replaced with a unique sentinel token. Words are dropped out independently and uniformly at random. The model is then trained to predict the sentinel tokens to delineate the dropped out text.

For example:

Original text: Thank you for inviting me to your party last week.

Inputs: Thank you <X> me to your party <Y> week.

Targets: <X> for inviting <Y> last <Z>

<----------section---------->

**Summary on pre-training**

*   Pre-training tasks can be invented flexibly, and effective representations can be derived from a flexible regime of pre-training tasks. The ability to design different pre-training objectives allows researchers to tailor the model to specific downstream tasks or to explore different aspects of language understanding.
*   Different NLP tasks seem to be highly transferable, producing effective representations that form a general model, which can serve as the backbone for many specialized models. This transferability is a key benefit of pre-training, allowing models to be adapted to a wide range of tasks with minimal task-specific training.
*   With Self-Supervised Learning, the models seem to be able to learn from generating the language itself, rather than from any specific task. Self-supervised learning enables models to learn from vast amounts of unlabeled data, unlocking the potential to capture intricate patterns and relationships in language.
*   Language Model can be used as a Knowledge Base, namely a generatively pre-trained model may have a decent zero-shot performance on a range of NLP tasks. The knowledge acquired during pre-training can be surprisingly effective, enabling the model to perform reasonably well on tasks without any explicit fine-tuning.

<----------section---------->

**Datasets and data pre-processing**

**Datasets**

*   Training LLMs requires vast amounts of text data, and the quality of this data significantly impacts LLM performance. The quantity and quality of data used to train LLMs are directly correlated to their performance, generalization, and bias.
*   Pre-training on large-scale corpora provides LLMs with a fundamental understanding of language and some generative capability. Exposure to diverse textual patterns, grammar rules, and semantic relationships allows LLMs to acquire a general understanding of language structure and usage.
*   Pre-training data sources are diverse, commonly incorporating web text, conversational data, and books as general pre-training corpora. Different data sources contribute unique characteristics and biases to the model, affecting its overall behavior.
*   Leveraging diverse sources of text data for LLM training can significantly enhance the model’s generalization capabilities. By learning from a wide range of textual styles, domains, and perspectives, LLMs can better adapt to unseen data and perform well across various tasks.

| LLMs        | Datasets                                                                                                                            |
| ----------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| GPT-3       | CommonCrawl, WebText2, Books1, Books2, Wikipedia                                                                                 |
| LLaMA       | CommonCrawl, C4, Wikipedia, Github, Books, Arxiv, StackExchange                                                                  |
| PaLM        | Social Media, Webpages, Books, Github, Wikipedia, News (total 780B tokens)                                                        |
| T5          | C4, WebText, Wikipedia, RealNews                                                                                                  |
| CodeGen     | the Pile, BIGQUERY, BIGPYTHON                                                                                                       |
| CodeGeex    | CodeParrot, the Pile, Github                                                                                                        |
| GLM         | BooksCorpus, Wikipedia                                                                                                            |
| BLOOM       | ROOTS                                                                                                                               |
| OPT         | BookCorpus, CCNews, CC-Stories, the Pile, Pushshift.io                                                                              |

| Corpora       | Type          | Links                                                                       |
| ------------- | ------------- | --------------------------------------------------------------------------- |
| BookCorpus    | Books         | [https://github.com/soskek/bookcorpus](https://github.com/soskek/bookcorpus) |
| Gutenberg     | Books         | [https://www.gutenberg.org](https://www.gutenberg.org)                       |
| Books1        | Books         | Not open source yet                                                         |
| Books2        | Books         | Not open source yet                                                         |
| CommonCrawl   | CommonCrawl   | [https://commoncrawl.org](https://commoncrawl.org)                           |
| C4            | CommonCrawl   | [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4) |
| CC-Stories    | CommonCrawl   | Not open source yet                                                         |
| CC-News       | CommonCrawl   | [https://commoncrawl.org/blog/news-dataset-available](https://commoncrawl.org/blog/news-dataset-available)  |
| RealNews      | CommonCrawl   | [https://github.com/rowanz/grover/tree/master/realnews](https://github.com/rowanz/grover/tree/master/realnews)    |
| RefinedWeb    | CommonCrawl   | [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) |
| WebText       | Reddit Link   | Not open source yet                                                         |
| OpenWebtext   | Reddit Link   | [https://skylion007.github.io/OpenWebTextCorpus/](https://skylion007.github.io/OpenWebTextCorpus/) |
| PushShift.io  | Reddit Link   |                                                                             |
| Wikipedia     | Wikipedia     | [https://dumps.wikimedia.org/zhwiki/latest/](https://dumps.wikimedia.org/zhwiki/latest/) |

<----------section---------->

**Datasets - Books**

*   Two commonly utilized books datasets for LLMs training are BookCorpus and Gutenberg.
*   These datasets include a wide range of literary genres, including novels, essays, poetry, history, science, philosophy, and more. The diversity of genres and subject matter exposes the models to varied linguistic styles, narrative structures, and domain-specific vocabulary.
*   Widely employed by numerous LLMs, these datasets contribute to the models’ pre-training by exposing them to a diverse array of textual genres and subject matter, fostering a more comprehensive understanding of language across various domains. This comprehensive understanding is crucial for generalization and adaptability to different NLP tasks.
*   Book Corpus includes 800 million words. The scale of BookCorpus enables models to learn statistical patterns and semantic relationships from a substantial amount of textual data.

**Datasets - CommonCrawl**

*   CommonCrawl manages an accessible repository of web crawl data, freely available for utilization by individuals and organizations. The availability of CommonCrawl facilitates research and development in NLP by providing a vast and diverse source of textual data.
*   This repository encompasses a vast collection of data, comprising over 250 billion web pages accumulated over a span of 16 years. The sheer scale of CommonCrawl makes it a valuable resource for training large language models and analyzing web content.
*   This continuously expanding corpus is a dynamic resource, with an addition of 3–5 billion new web pages each month. The constant growth of CommonCrawl ensures that LLMs are trained on up-to-date information and adapt to evolving language patterns.
*   However, due to the presence of a substantial amount of low-quality data in web archives, preprocessing is essential when working with CommonCrawl data. Web data often contains noise, irrelevant content, and inconsistent formatting, necessitating careful filtering and cleaning to improve the quality of training data.

**Datasets - Wikipedia**

*   Wikipedia, the free and open online encyclopedia project, hosts a vast repository of high-quality encyclopedic content spanning a wide array of topics. The collaborative editing process and community oversight contribute to the accuracy and reliability of Wikipedia articles.
*   The English version of Wikipedia, including 2,500 million words, is extensively utilized in the training of many LLMs, serving as a valuable resource for language understanding and generation tasks. The structured and informative nature of Wikipedia articles provides LLMs with a solid foundation for acquiring knowledge and reasoning abilities.
*   Additionally, Wikipedia is available in multiple languages, providing diverse language versions that can be leveraged for training in multilingual environments. This multilingual capability enables LLMs to learn from and generate text in various languages, enhancing their cross-lingual abilities and cultural understanding.

<----------section---------->

**Data pre-processing**

*   Once an adequate corpus of data is collected, the subsequent step is data preprocessing, whose quality directly impacts the model’s performance and security. Proper data preprocessing is crucial for ensuring the quality, reliability, and ethical behavior of LLMs.
*   The specific preprocessing steps involve filtering low-quality text, including eliminating toxic and biased content to ensure the model aligns with human ethical standards. Content moderation techniques are used to identify and remove harmful or inappropriate text from the training data.
*   It also includes deduplication, removing duplicates in the training set, and excluding redundant content in the test set to maintain the sample distribution balance. Deduplication helps to prevent overfitting and ensure that the model generalizes well to unseen data.
*   Privacy scrubbing is applied to ensure the model’s security, preventing information leakage or other privacy-related concerns. This involves removing personally identifiable information (PII) and other sensitive data from the training corpus.

**Quality filtering**

*   Filtering low-quality data is typically done using heuristic-based methods or classifier-based methods. Both techniques aim to remove noise, inconsistencies, and irrelevant information from the training data.
*   Heuristic methods involve employing manually defined rules to eliminate low-quality data. For instance, rules could be set to retain only text containing digits, discard sentences composed entirely of uppercase letters, and remove files with a symbol and word ratio exceeding 0.1, and so forth. These rules are based on common characteristics of low-quality text and require careful tuning to avoid inadvertently filtering out valid data.
*   Classifier-based methods involve training a classifier on a high-quality dataset to filter out low-quality datasets. The classifier learns to distinguish between high-quality and low-quality text, enabling automated filtering of large corpora. This approach requires a labeled dataset of high-quality and low-quality text for training the classifier.

**Deduplication**

*   Language models may sometimes repetitively generate the same content during text generation, potentially due to a high degree of repetition in the training data. Redundancy in the training data can lead to unintended memorization and replication of specific text patterns.
*   Extensive repetition can lead to training instability, resulting in a decline in the performance of LLMs. Uncontrolled repetition can hinder the model's ability to generalize and adapt to new inputs.
*   Additionally, it is crucial to consider avoiding dataset contamination by removing duplicated data present in both the training and test set. Overlapping training and test data can lead to artificially inflated performance metrics and hinder the model's ability to generalize to unseen data.

<----------section---------->

**Privacy scrubbing**

*   LLMs, as text-generating models, are trained on diverse datasets, which may pose privacy concerns and the risk of inadvertent information disclosure. Training on datasets containing PII can create vulnerabilities that allow models to inadvertently reveal sensitive information.
*   It is imperative to address privacy concerns by systematically removing any sensitive information. This proactive approach helps to mitigate the risk of unintended data breaches and protect individuals' privacy.
*   This involves employing techniques such as anonymization, redaction, or tokenization to eliminate personally identifiable details, geolocation, and confidential data. These techniques transform or remove sensitive information while preserving the utility of the data for training purposes.
*   By carefully scrubbing the dataset of such sensitive content, researchers and developers can ensure that the language models trained on these datasets uphold privacy standards and mitigate the risk of unintentional disclosure of private information. Adhering to privacy standards fosters trust and promotes the responsible development and deployment of LLMs.

**Filtering out toxic and biased text**

*   In the preprocessing steps of language datasets, a critical consideration is the removal of toxic and biased content to ensure the development of fair and unbiased language models. Addressing toxicity and bias is essential for creating LLMs that align with human values and promote responsible use.
*   This involves implementing robust content moderation techniques, such as employing sentiment analysis, hate speech detection, and bias identification algorithms. These algorithms are designed to automatically identify and flag text that exhibits harmful or inappropriate characteristics.
*   By leveraging these tools, it is possible to systematically identify and filter out text that may perpetuate harmful stereotypes, offensive language, or biased viewpoints. This process helps to create more inclusive and equitable training datasets, fostering fairness and reducing the risk of perpetuating harmful biases.

<----------section---------->

**Using LLMs after pre-training**

*   **Fine-tuning:** Gradient descent on weights to optimize performance on one task. After pre-training, LLMs are typically fine-tuned on specific tasks to optimize their performance. Fine-tuning involves updating the model's weights using a task-specific dataset and loss function. What to fine-tune? Full network, Readout heads, Adapters. There are various fine-tuning strategies. One can choose to fine-tune the full network, just the readout heads (the final layers that make predictions), or use adapters (small, task-specific modules inserted into the network). Fine-tuning changes the model “itself”.

*   **Prompting:** Design special prompts to cue / condition the network into specific mode to solve any tasks. Prompting involves designing specific input prompts that guide the LLM to perform a desired task. Effective prompts can elicit impressive performance from pre-trained LLMs without requiring any parameter change. No parameter change. One model to rule them all. The goal is to use the same model and change the way to use it.

<----------section---------->

## Lesson 12 ##

**Outline**

The lecture covers the following topics:

*   **Overview**: A general introduction to the subject matter.
*   **Setup**: Configuring the environment for NLP tasks.
*   **Pipeline**: Understanding NLP pipelines for processing text.
*   **Model selection**: Choosing appropriate models for specific tasks.
*   **Common models**: Familiarizing with frequently used NLP models.
*   **Gradio**: Using Gradio for creating interactive demos.

<----------section---------->

**Introduction to Hugging Face**

The lesson starts with an introduction to Hugging Face, a prominent company and platform in the field of NLP.

*   A link to the Education Toolkit is provided: [https://github.com/huggingface/education-toolkit](https://github.com/huggingface/education-toolkit)

<----------section---------->

**Hugging Face Education Toolkit**

The Hugging Face Education Toolkit is designed to help educators and learners easily create workshops, events, homework assignments, or classes related to NLP. The content is self-contained and can be easily integrated into existing materials. It's available for free and utilizes popular open-source technologies such as Transformers and Gradio. Apart from tutorials, the toolkit also provides resources for further exploration of Machine Learning (ML) and assists in designing custom content.

<----------section---------->

**The Hugging Face Hub**

The Hugging Face Hub ([https://huggingface.co/](https://huggingface.co/)) serves as a central repository for various NLP resources, including:

*   **Models**: Pre-trained models for various NLP tasks.
*   **Datasets**: Open-source datasets for training and evaluation. The Hub hosts around 3000 datasets that are open-sourced and free to use in multiple domains ([https://hf.co/datasets](https://hf.co/datasets)).
*   **Spaces**: Platforms for hosting demos and code.

Key libraries provided by Hugging Face include:

*   `datasets`: Facilitates downloading datasets from the Hub. The open-source datasets library allows the easy use of huge datasets, using convenient features such as streaming.
*   `transformers`: Enables working with pipelines, tokenizers, models, etc.
*   `evaluate`: Provides tools for computing evaluation metrics.
*   These libraries are compatible with both PyTorch and TensorFlow, two major machine learning frameworks.

<----------section---------->

**Hugging Face – Model Hub Navigation**

The Hugging Face Model Hub ([https://huggingface.co/models](https://huggingface.co/models)) is a central repository for pre-trained models. Lysandre helps navigate the Model Hub.

<----------section---------->

**Hugging Face - Datasets Details**

*   Each dataset in the Hub comes with a dataset card, similar to model repositories, providing documentation such as summary, structure, and more.
*   Example dataset: [https://huggingface.co/datasets/nyu-mll/glue](https://huggingface.co/datasets/nyu-mll/glue)

<----------section---------->

**Setup Environment for NLP**

The lesson proceeds to describe how to set up the working environment to work with NLP models, covering Google Colab and virtual environments.

<----------section---------->

**Setup – Google Colab**

Google Colab offers the simplest setup for running NLP code. It provides a notebook environment directly in the browser, allowing users to start coding immediately.

*   Install the `transformers` library:

    ```
    !pip install transformers
    ```

    This installs a lightweight version of the Transformers library without specific machine learning frameworks like PyTorch or TensorFlow. To install the development version with all dependencies:

    ```
    !pip install transformers[sentencepiece]
    ```

<----------section---------->

**Setup – Virtual Environment**

For a more controlled environment, a virtual environment using Anaconda is recommended:

1.  Download and install Anaconda from: [https://www.anaconda.com/download](https://www.anaconda.com/download)

2.  Create a new environment:

    ```
    conda create --name nlpllm
    ```

3.  Activate the environment:

    ```
    conda activate nlpllm
    ```

4.  Install the `transformers` library with necessary dependencies:

    ```
    conda install transformers[sentencepiece]
    ```

<----------section---------->

**Setup – Hugging Face Account**

It is recommended to create a Hugging Face account, as many functionalities depend on it.

<----------section---------->

**NLP Pipeline**

The Hugging Face Transformers library simplifies the creation and usage of NLP models. The Model Hub offers numerous pre-trained models that can be easily downloaded and utilized.

*   The `pipeline()` function is the most basic object in the Hugging Face Transformers library. It connects a model with its required preprocessing and postprocessing steps. With the pipeline, you can directly input any text and receive an understandable answer.

<----------section---------->

**Model Selection Considerations**

Model selection involves several steps:

1.  **Task Identification**:

    *   Example: Summarization of a news article about an earthquake in Papua New Guinea.

2.  **Model Discovery**:

    *   Using the Hugging Face Hub, search for models suitable for the identified task.
    *   Filter by task (e.g., summarization), license, language, etc.

3.  **Need Assessment**:

    *   Consider your specific needs:
        *   Extractive summarization: Selecting representative pieces of text.
        *   Abstractive summarization: Generating new text.
    *   Filter by model size, considering hardware limitations, cost, or latency requirements.

4.  **Evaluation & Refinement**:

    *   Sort models by popularity and recent updates.
    *   Check the Git release history for model details (e.g., github.com/google-research/bert/blob/master/README.md).
    *   Pick good variants of models for the task, considering different sizes and fine-tuned variants.

<----------section---------->

**Additional Factors for Model Selection**

*   Consider searching for examples and datasets, not just models.
*   Determine if the model is generally good or specifically fine-tuned.
*   Check which datasets were used for pre-training and/or fine-tuning.
*   Ultimately, focus on your data and users:
    *   Define Key Performance Indicators (KPIs).
    *   Test on your data or with your users.

<----------section---------->

**Common NLP Models**

The table below lists several common NLP models, along with their licenses, organizations, years, and descriptions:

| Model            | License        | Organization | Year | Description                                                                                                |
| ---------------- | -------------- | ------------ | ---- | ---------------------------------------------------------------------------------------------------------- |
| Pythia 19M-12B   | Apache 2.0     | EleutherAI   | 2023 | Series of 8 models for comparisons across sizes.                                                           |
| Dolly 12B        | MIT            | Databricks   | 2023 | Instruction-tuned Pythia model.                                                                            |
| GPT-3.5 175B     | Proprietary    | OpenAI       | 2022 | ChatGPT model option; related models GPT-1/2/3/4.                                                            |
| OPT 125M-175B    | MIT            | Meta         | 2022 | Based on GPT-3 architecture.                                                                                |
| BLOOM 560M - 176B| RAIL v1.0       | Many groups  | 2022 | 46 languages.                                                                                             |
| GPT-Neo/X 125M-20B| MIT / Apache 2.0 | EleutherAI   | 2021/2022| Based on GPT-2 architecture.                                                                                |
| FLAN 80M-540B    | Apache 2.0     | Google       | 2021 | Methods to improve training for existing architectures.                                                      |
| BART 139M-406M   | Apache 2.0     | Meta         | 2019 | Derived from BERT, GPT, others.                                                                          |
| T5 50M-TIB       | Apache 2.0     | Google       | 2019 | 4 languages.                                                                                              |
| BERT             | Apache 2.0     | Google       | 2018 | Early breakthrough.                                                                                         |

<----------section---------->

**Gradio for Demos**

Gradio is used to create web demos for machine learning models.

*   **Steps Involved**

    | Step                                 | Framework             | Language |
    | ------------------------------------ | --------------------- | -------- |
    | 1. Train a model                      | TensorFlow/PyTorch    | Python   |
    | 2. Containerize and deploy the model |                       | Python   |
    | 3. Store incoming samples             | Gradio                | Python   |
    | 4. Build an interactive front-end     |                       | Python   |

<----------section---------->

**Gradio – Example: News Summarizer**

Gradio is used to create web demos.
An example: a News Summarizer that uses Hugging Face models to summarize articles. It utilizes the `bart-large-cnn` model by Facebook. The length of articles affects the speed of summarization.

*   Examples:

    *   [https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/](https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/)
    *   [https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/](https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/)
    *   [https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/](https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/)

<----------section---------->

**Gradio – Free Hosting on hf.space**

Hugging Face provides free hosting for Gradio demos on hf.space, allowing the community to share and discover ML applications.

<----------section---------->

**Gradio – Building a Demo Yourself**

To build your own Gradio demo:

1.  Install Gradio:

    ```
    conda install gradio
    ```

2.  Refer to the tutorial: [https://bit.ly/34wESgd](https://bit.ly/34wESgd)

<----------section---------->

## LESSON 13 ##

## Outline
The lesson covers the following topics:
*   Encoder-only transformer architectures
*   BERT (Bidirectional Encoder Representations from Transformers) model
*   Practical exercises on token classification and named entity recognition

This outline sets the stage for understanding how encoder-only transformers, like BERT, are used in specific NLP tasks. Token classification and named entity recognition will provide hands-on experience.

<----------section---------->
## Encoder-only Transformer
*   The complete transformer architecture, encompassing both encoder and decoder components, is essential when the task requires transforming an input sequence into an output sequence of varying length.
*   For example, machine translation between different languages involves converting a sentence in one language into a sentence in another, which often differs in length and structure. The original Transformer model was designed for this type of task.
*   However, if the task involves transforming a sequence into another sequence of the *same* length, using only the encoder part of the transformer is sufficient.
*   In such cases, the output vectors $z_1, \dots, z_t$ directly represent the transformed sequence. The loss function can be computed directly on these output vectors, measuring the difference between the predicted outputs and the expected outputs.
*   For tasks involving the transformation of a sequence into a single value (e.g., sequence classification), using only the encoder part of the transformer is also appropriate.
*   A special token, often denoted as `[CLS]` (classification token), is added as the first element $x_1$ of the input sequence. This `[CLS]` token serves as a placeholder to capture the overall representation of the entire sequence.
*   The corresponding output value $z_1$, associated with the `[CLS]` token, is then taken as the result of the transformation, and the loss function is computed only on this single value. This effectively condenses the entire sequence into a single vector representation for classification purposes.

This section explains when and how to use only the encoder part of a Transformer, highlighting its utility in sequence-to-sequence tasks of equal length and sequence classification. The use of the special `[CLS]` token for sequence classification is clarified.

<----------section---------->
## BERT
*   BERT (Bidirectional Encoder Representations from Transformers) is a language model introduced by Google Research in 2018 for language understanding. The acronym emphasizes its key characteristic: Bidirectional Encoder Representations from Transformers.
*   BERT leverages only the encoder part of the Transformer model. Two main versions exist: BERT-base, which comprises 12 stacked encoder blocks and 110 million parameters, and BERT-large, featuring 24 stacked encoder blocks and 340 million parameters. The increased number of layers and parameters in BERT-large allows it to capture more complex linguistic patterns.
*   BERT is pre-trained to use "bidirectional" context, meaning it considers both preceding and succeeding words when understanding a word in a sentence. This contrasts with unidirectional models that only consider the context up to the current word.
*   BERT is specifically designed to serve as a pre-trained base for various Natural Language Processing tasks. After pre-training, it is fine-tuned on specific datasets tailored to the downstream task, adapting its general language understanding capabilities to the nuances of the task at hand.

This section provides a high-level overview of BERT, emphasizing its bidirectional context and its role as a pre-trained model.

<----------section---------->
## BERT Input Encoding
*   BERT employs a WordPiece tokenizer for tokenization.
*   **Subword-Based**: Unlike traditional tokenizers that operate at the word level or character level, WordPiece tokenizes text at the subword level. This approach enables BERT to handle both common words and rare or misspelled words effectively by decomposing them into smaller, manageable parts.
*   **Vocabulary Building**: The WordPiece tokenizer constructs a vocabulary consisting of common words and subword units. For instance, the word "playing" might be tokenized as "play" and "##ing," where "##" indicates that it is a subword unit.
*   **Unknown Words**: When encountering rare or out-of-vocabulary words, WordPiece breaks them down into familiar subwords. For example, "unhappiness" could be split into "un," "happy," and "##ness." This ensures that BERT can process any input, even if it contains words not seen during training.

This section details the WordPiece tokenization method used by BERT, explaining its subword-based approach and how it handles unknown words.

<----------section---------->
## BERT Input Encoding
*   **Splitting**: Sentences are split into tokens based on whitespace, punctuation, and common prefixes (such as "##").
*   BERT relies on special tokens to perform specific tasks:
    *   `[CLS]`:  A classification token that's added at the beginning of each input sequence. As mentioned previously, the final hidden state of this token is used to represent the entire sequence for classification tasks.
    *   `[SEP]`: A separator token that's used to mark the end of a sentence or to separate multiple sentences in sentence-pair tasks. This helps BERT distinguish between different parts of the input.
*   For example, the sentence "I’m feeling fantastic!" might be tokenized by WordPiece as: `[CLS] I ' m feeling fan ##tas ##tic ! [SEP]`
*   **Converting Tokens to IDs**: After tokenization, each token is mapped to a unique ID from BERT’s vocabulary. These IDs serve as the numerical input to the model.

This section illustrates how sentences are split into tokens and introduces the special tokens used by BERT, along with an example of tokenization.

<----------section---------->
## BERT Input Encoding
The advantages of WordPiece embedding are as follows:
*   **Efficiency**: It reduces the vocabulary size significantly without sacrificing the ability to represent diverse language constructs. A smaller vocabulary leads to fewer parameters in the model, making it more efficient to train and use.
*   **Handling Unseen Words**: Subword tokenization allows BERT to manage rare or newly created words by breaking them down into recognizable parts. This is crucial for handling the open-ended nature of language, where new words and phrases are constantly emerging.
*   **Improved Language Understanding**: By learning useful subword components during pretraining, BERT can capture complex linguistic patterns and relationships between words. This leads to a better understanding of the meaning of text.

This section summarizes the key advantages of using WordPiece embeddings in BERT.

<----------section---------->
## BERT `[CLS]` Token
*   In BERT, the `[CLS]` token is a special token added at the beginning of each input sequence.
*   The `[CLS]` token (short for "classification token") is always placed at the very start of the tokenized sequence and serves as a summary representation of the entire input sequence.
*   After the input is processed by BERT, the final hidden state of the `[CLS]` token acts as a condensed, context-aware embedding for the whole sentence or sequence of sentences. This embedding encapsulates the overall meaning and context of the input.
*   This embedding can then be fed into additional layers (like a classifier) for specific tasks such as sentiment analysis or topic classification.

This section explains the role of the `[CLS]` token in summarizing the input sequence.

<----------section---------->
## BERT `[CLS]` Token
The `[CLS]` token is used differently for single-sentence and sentence-pair classification tasks.
*   **Single-Sentence Classification**
    *   The final hidden state of the `[CLS]` token is passed to a classifier layer to make predictions.
    *   For instance, in sentiment analysis, the classifier might predict "positive" or "negative" sentiment based on the `[CLS]` embedding.
*   **Sentence-Pair Tasks**
    *   For tasks involving two sentences, BERT tokenizes them as `[CLS] Sentence A [SEP] Sentence B [SEP]`. The `[SEP]` token separates the two sentences.
    *   The `[CLS]` token’s final hidden state captures the relationship between the two sentences, making it suitable for tasks like entailment detection (determining if one sentence logically follows from another) or similarity scoring.

This section details how the `[CLS]` token is used in single-sentence and sentence-pair tasks, providing specific examples.

<----------section---------->
## BERT Pre-training
*   BERT is pre-trained using two self-supervised learning strategies:
    *   **Masked Language Modeling (MLM)**
        *   Objective: To predict masked (hidden) tokens in a sentence.
        *   Process: Randomly mask 15% of the tokens in the input sequence and train BERT to predict these masked tokens based on the surrounding context. This forces the model to understand the relationships between words.
        *   Benefit: Enables BERT to learn bidirectional context by leveraging both preceding and succeeding words to predict the masked tokens.
    *   **Next Sentence Prediction (NSP)**
        *   Objective: To determine if one sentence logically follows another.
        *   Process: Trains BERT to understand sentence-level relationships by presenting it with pairs of sentences and asking it to predict whether the second sentence is the subsequent sentence in the original text.
        *   Benefit: Improves performance in tasks like question answering and natural language inference, where understanding the relationship between sentences is crucial.
*   The training set comprises a vast corpus of publicly available books and the English Wikipedia, totaling over 3 billion words. This massive dataset allows BERT to learn a comprehensive understanding of the English language.

This section describes the two self-supervised learning strategies used to pre-train BERT: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). It also provides details about the size and composition of the training dataset. It's also important to note in passing that the Next Sentence Prediction (NSP) task, despite its usefulness at the time, has since been shown to be less useful than originally thought, and has even been dropped from some BERT variants like RoBERTa.

<----------section---------->
## BERT Fine-tuning
*   The output of the encoder is fed into an additional layer to solve a specific problem. This layer could be a classifier for text classification, a sequence labeling layer for named entity recognition, or any other task-specific layer.
*   The cross-entropy loss between the prediction and the true label for the classification task is minimized using gradient-based optimization algorithms. The additional layer is typically trained from scratch, while the pre-trained parameters of BERT can be either updated (fine-tuned) or kept fixed, depending on the specific task and dataset size.

This section outlines the fine-tuning process, emphasizing the addition of a task-specific layer and the optimization of the cross-entropy loss.

<----------section---------->
## BERT Fine-tuning
*   BERT is pre-trained on general language data and then fine-tuned on specific datasets for each downstream task.
*   In fine-tuning, the `[CLS]` token’s final embedding is specifically trained for the downstream task, refining its ability to represent the input sequence in a way that is optimal for that task. This adaptation is what allows BERT to achieve high performance on a wide range of NLP tasks.
*   Example Tasks:
    *   Text Classification: Sentiment analysis, spam detection.
    *   Named Entity Recognition (NER): Identifying names, dates, organizations, etc., within text.
    *   Question Answering: Extractive QA where BERT locates answers within a passage.
*   Minimal Task-Specific Adjustments: Only a few additional layers are typically added per task. This makes fine-tuning relatively efficient, requiring less data and compute power than training a model from scratch.

This section further explains the fine-tuning process and provides example tasks where BERT can be applied.

<----------section---------->
## BERT Strengths and Limitations
*   **Strengths**
    *   **Bidirectional Contextual Understanding**: BERT provides richer and more accurate representations of language by considering both preceding and succeeding words.
    *   **Flexibility in Transfer Learning**: BERT’s pretraining allows for easy adaptation to diverse NLP tasks through fine-tuning.
    *   **High Performance on Benchmark Datasets**: BERT consistently ranks at or near the top on benchmark datasets like SQuAD (Question Answering) and GLUE (General Language Understanding Evaluation).
*   **Limitations**
    *   **Large Model Size**: High computational and memory requirements can make deployment challenging, especially on resource-constrained devices.
    *   **Pretraining Costs**: Requires extensive computational resources, especially for BERT-Large, making pretraining from scratch impractical for most researchers.
    *   **Fine-Tuning Time and Data**: Fine-tuning on new tasks still requires labeled data and can be time-intensive, particularly for complex tasks or large datasets.

This section outlines the strengths and limitations of BERT, providing a balanced perspective on its capabilities.

<----------section---------->
## Popular BERT Variants - RoBERTa
*   RoBERTa (Robustly Optimized BERT Approach) was developed by Facebook AI in 2019.
*   The main differences compared to BERT are:
    *   **Larger Training Corpus**: RoBERTa is trained on significantly more data (200 billion words) compared to BERT.
    *   **Removed Next Sentence Prediction (NSP)**: Experiments showed that removing the NSP task improves performance.
    *   **Longer Training and Larger Batches**: RoBERTa undergoes longer training with larger batch sizes, leading to more robust language modeling.
    *   **Dynamic Masking**: Masking is applied dynamically (i.e., different masks per epoch), leading to better generalization.
*   RoBERTa consistently outperforms BERT on various NLP benchmarks, particularly in tasks requiring nuanced language understanding.

This section introduces RoBERTa, a variant of BERT, and outlines its key differences and improvements.

<----------section---------->
## Popular BERT Variants - ALBERT
*   ALBERT (A Lite BERT) was developed by Google Research in 2019.
*   The main differences compared to BERT are:
    *   **Parameter Reduction**: Uses factorized embedding parameterization to reduce model size. This involves decomposing the embedding matrix into two smaller matrices, reducing the number of parameters.
    *   **Cross-Layer Parameter Sharing**: Shares weights across layers to decrease the number of parameters. This means that the same set of weights is used across multiple layers, reducing the overall model size.
    *   **Sentence Order Prediction (SOP)**: Replaces NSP with SOP, which is better suited for capturing inter-sentence coherence. Instead of predicting whether two sentences are consecutive, SOP predicts the order of two sentences.
*   ALBERT achieves comparable results to BERT-Large with fewer parameters, making it more memory-efficient.
*   ALBERT is faster and lighter, ideal for applications where resources are limited.

This section introduces ALBERT, another variant of BERT, focusing on its parameter reduction techniques.

<----------section---------->
## Popular BERT Variants - DistilBERT
*   DistilBERT (Distilled BERT) was developed by Hugging Face.
*   The main differences with BERT are as follows:
    *   **Model Distillation**: Uses knowledge distillation to reduce BERT’s size by about 40% while retaining 97% of its language understanding capabilities. Knowledge distillation involves training a smaller "student" model to mimic the behavior of a larger, pre-trained "teacher" model.
    *   **Fewer Layers**: DistilBERT has 6 layers instead of 12 (for BERT-Base) but is optimized to perform similarly.
    *   **Faster Inference**: Provides faster inference and lower memory usage, making it ideal for real-time applications.

*   DistilBERT is widely used for lightweight applications that need a smaller and faster model without major accuracy trade-offs.

This section introduces DistilBERT, highlighting its model distillation technique for size reduction.

<----------section---------->
## Popular BERT Variants - TinyBERT
*   TinyBERT was developed by Huawei.
*   The main differences with BERT are as follows:
    *   **Two-Step Knowledge Distillation**: Distills BERT both during pretraining and fine-tuning, further enhancing efficiency. This means that knowledge distillation is applied not only during the initial pre-training phase but also during the fine-tuning phase on specific downstream tasks.
    *   **Smaller and Faster**: TinyBERT is even smaller than DistilBERT, optimized for mobile and edge devices.
    *   **Similar Accuracy**: Maintains accuracy close to that of BERT on various NLP tasks, especially when fine-tuned with task-specific data.
*   TinyBERT is an ultra-compact version of BERT that is well-suited for resource-constrained environments.

This section introduces TinyBERT, an even smaller variant optimized for highly constrained environments.

<----------section---------->
## Popular BERT Variants - ELECTRA
*   ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) was developed by Google Research.
*   Its differences with respect to BERT are:
    *   **Replaced Token Detection**: Instead of masked language modeling, ELECTRA uses a generator-discriminator setup where the model learns to identify replaced tokens in text. A generator model replaces some tokens in the input, and a discriminator model tries to identify which tokens were replaced.
    *   **Efficient Pretraining**: This approach allows ELECTRA to learn with fewer resources and converge faster than BERT.
    *   **Higher Performance**: Often outperforms BERT on language understanding benchmarks with significantly less compute power.
*   ELECTRA’s training efficiency and robust performance make it appealing for applications where computational resources are limited.

This section introduces ELECTRA, focusing on its replaced token detection approach for efficient training.

<----------section---------->
## Popular BERT Variants - SciBERT
*   SciBERT is tailored for applications in scientific literature, making it ideal for academic and research-oriented NLP.
*   **Domain-Specific Pretraining**: Trained on a large corpus of scientific papers from domains like biomedical and computer science.
*   **Vocabulary Tailored to Science**: Uses a vocabulary that better represents scientific terms and jargon.
*   **Improved Performance on Scientific NLP Tasks**: Significantly outperforms BERT on tasks like scientific text classification, NER, and relation extraction in scientific contexts.

This section introduces SciBERT, a BERT variant specifically pre-trained for scientific text.

<----------section---------->
## Popular BERT Variants - BioBERT
*   BioBERT is widely adopted in the biomedical research field, aiding in information extraction and discovery from medical literature.
*   **Biomedical Corpus**: Pretrained on a biomedical text corpus, including PubMed abstracts and PMC full-text articles.
*   **Enhanced Performance on Biomedical Tasks**: Excels at biomedical-specific tasks such as medical NER, relation extraction, and question answering in healthcare.

This section introduces BioBERT, a BERT variant for biomedical text.

<----------section---------->
## Popular BERT Variants - ClinicalBERT
*   ClinicalBERT is ideal for hospitals and healthcare providers who need to analyze patient records, predict health outcomes, or assist in clinical decision-making.
*   **Healthcare Focus**: Tailored for processing clinical notes and healthcare-related NLP tasks.
*   **Training on MIMIC-III Dataset**: Pretrained on the MIMIC-III database of clinical records, making it useful for healthcare analytics.

This section introduces ClinicalBERT, a BERT variant focused on clinical data.

<----------section---------->
## Popular BERT Variants - mBERT
*   mBERT, developed by Google, supports NLP tasks across languages, enabling global applications and language transfer learning.
*   **Multilingual Support**: Trained on 104 languages, mBERT can handle multilingual text without requiring separate models for each language.
*   **Language-Agnostic Representation**: Capable of zero-shot cross-lingual transfer, making it suitable for translation and cross-lingual understanding tasks without requiring task-specific training data for the target language.

This section introduces mBERT, a multilingual BERT model.

<----------section---------->
## Other BERT Variants
*   CamemBERT: French language-focused BERT model.
*   FinBERT: Optimized for financial text analysis.
*   LegalBERT: Trained on legal documents for better performance in the legal domain.
*   Moreover, BERT inspired Transformer pre-training in computer vision, such as with vision Transformers, Swin Transformers, and Masked Auto Encoders (MAE). The success of BERT has led to the adoption of similar pre-training techniques in other domains, demonstrating its broad impact.

This section lists several other domain-specific BERT variants and mentions BERT's influence on computer vision.

<----------section---------->
## Practice on Token Classification and Named Entity Recognition
*   Refer to the Hugging Face tutorial on token classification: [https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt), use different existing versions of BERT to perform named entity recognition.
*   Test these versions not only with your own prompts but also with data available in public datasets (e.g., [https://huggingface.co/datasets/eriktks/conll2003](https://huggingface.co/datasets/eriktks/conll2003)).
*   If you have time and computational resources, you can also fine-tune one of the lightweight versions of BERT ([https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt#fine-tuning-the-model-with-the-trainer-api](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt)).

This section provides practical exercises for token classification and named entity recognition using BERT, along with links to relevant resources. The user is encouraged to experiment with different BERT versions, utilize public datasets, and explore fine-tuning lightweight versions of BERT if computational resources permit.
```
<----------section---------->

## Lesson 14 #

## Outline
The lecture covers the following key topics:

*   **Decoder-only Transformers:** An introduction to the architecture and functionalities of decoder-only transformers.
*   **GPT (Generative Pre-trained Transformer):** Exploration of the GPT model family, including its architecture, training, and applications.
*   **LLAMA (Large Language Model Meta AI):** An overview of the LLAMA models developed by Meta, focusing on their design and capabilities.
*   **Practice on Text Generation:** Practical exercises and demonstrations on generating text using various models and tools.

<----------section---------->

## Decoder-only Transformer
Decoder-only transformers are a specific type of transformer model that utilizes only the decoder component of the original Transformer architecture introduced in the seminal paper "Attention is All You Need."

*   **Architecture:** Unlike the full Transformer, which includes both an encoder and a decoder, decoder-only transformers are characterized by the absence of separate encoder layers. They consist solely of decoder blocks stacked sequentially.
*   **Focus on Decoding:** This architecture is primarily designed for decoding tasks. It excels in autoregressive generation, where the model predicts the next token in a sequence based on the previously generated tokens.
*   **Efficiency for Generation:** The exclusive focus on decoding makes decoder-only transformers highly efficient for language generation tasks. By eliminating the encoder, the model can directly generate text without needing a separate encoding step for the input sequence.
*   **Applications:** Decoder-only transformers are predominantly employed in language generation tasks such as:
    *   Text generation (creating news articles, stories, and creative content).
    *   Summarization (producing concise summaries of longer documents).
    *   Question answering (generating answers to posed questions).
*   **Popular Examples:** Prominent examples of decoder-only transformer models include:
    *   GPT (Generative Pre-trained Transformer) series: This includes GPT-1, GPT-2, GPT-3, and GPT-4, each representing advancements in model size, capabilities, and training methodologies.
    *   LLAMA (Large Language Model Meta AI): A family of language models developed by Meta, designed for efficiency and high performance in various NLP tasks.

<----------section---------->

## Autoregressive Text Generation
In a decoder-only transformer, text generation is achieved through an autoregressive process, where each token is generated sequentially.

*   **Sequential Token Generation:** The model predicts the next token by attending only to the previously generated tokens within the same sequence. This contrasts with encoder-decoder models, which use cross-attention to attend to the encoded input sequence.
*   **Continuous Sequence Processing:** The input context (prompt) and the generated text are treated as a single, continuous sequence. The model processes this sequence of tokens to handle both understanding the input prompt (encoding) and generating the subsequent text (decoding).
*   **Integrated Encoding and Decoding:** This continuous sequence of tokens essentially allows the decoder-only model to handle both the “encoding” (understanding the input prompt) and “decoding” (generating text) in one step, without needing a separate encoder block. The decoder attends to all previous tokens, thus implicitly encoding the context.
*   **Iterative Generation:** Once a token is generated, it's appended to the input sequence, and the model proceeds to generate the next token based on the updated sequence. This iterative process continues until a specified length is reached or a stopping criterion is met.

<----------section---------->

## Self-Attention and Context Understanding
Decoder-only transformers utilize self-attention mechanisms and causal masking to achieve effective text generation.

*   **Self-Attention with Causal Masking:** The model employs self-attention within the decoder layers but with a causal (unidirectional) mask. This mask prevents each token from attending to future tokens, ensuring that each position only considers information from previous positions. This mechanism simulates the sequential generation process where each word is generated in order.
*   **Sequential Context Buildup:** In a decoder-only architecture, the model builds up context as it processes tokens in sequence. As the model reads through a sequence, it "remembers" previous tokens and learns relationships between them within the attention layers. This sequential buildup of context replaces the need for separate encoder-decoder attention by allowing the model to accumulate an understanding of the input as it progresses through each token.
*   **Implicit Context Understanding:** The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence when generating the next token. The causal mask ensures that the model only attends to past tokens, maintaining the autoregressive property.
*   **Contextual Relationships:** As the model processes each token, it learns the relationships between the current token and all previous tokens. This allows the model to capture long-range dependencies and generate coherent text that is contextually relevant to the input prompt.

<----------section---------->

## Encoder-only vs Decoder-only Transformers

| Feature            | Encoder-Only Transformers (e.g., BERT)                                   | Decoder-Only Transformers (e.g., GPT)                                |
| ------------------ | ------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Architecture       | Only encoder blocks (bidirectional attention)                               | Only decoder blocks (causal attention)                               |
| Training Objective | Masked Language Modeling (MLM)                                           | Autoregressive Language Modeling                                     |
| Context            | Processes entire sequence in parallel                                      | Processes tokens sequentially (one by one)                           |
| Main Use Cases     | Text classification, NER, question answering                                | Text generation, story generation, code generation                    |
| Attention Type     | Bidirectional self-attention                                              | Unidirectional (masked) self-attention                               |
| Output             | Contextual embeddings for downstream tasks                                | Sequential token generation (text or other content)                   |

<----------section---------->

## Decoder-only Transformer Applications
Decoder-Only Transformers have a wide array of real-world applications. These include:

*   **Text Generation:** Creating diverse forms of textual content, such as:
    *   News articles: Generating timely and relevant news reports.
    *   Stories: Crafting engaging narratives and creative stories.
    *   Creative content: Producing poetry, scripts, and other imaginative pieces.
*   **Conversational AI:** Powering interactive and dynamic conversational agents:
    *   Chatbots: Developing virtual assistants for customer support and engagement.
    *   Virtual assistants: Creating AI-driven personal assistants for task management and information retrieval.
    *   Real-time dialogue: Facilitating natural and context-aware conversations.
*   **Programming Help:** Assisting developers with coding tasks:
    *   Code generation: Automatically generating code snippets and complete programs.
    *   Debugging: Providing assistance in identifying and fixing errors in code.
*   **Summarization:** Condensing lengthy text into concise summaries:
    *   Generating summaries: Creating abridged versions of documents, articles, and reports.
    *   Long documents: Processing and summarizing extensive textual content.

<----------section---------->

## GPT (Generative Pre-trained Transformer)
GPT is a type of decoder-only transformer model developed by OpenAI. It has revolutionized the field of natural language processing due to its ability to generate human-like text.

*   **Human-Like Text Generation:** GPT models are trained on vast amounts of text data, enabling them to understand and predict language patterns. This training allows them to generate fluent and coherent text that is often indistinguishable from human writing.
*   **Broad Language Understanding:** These models develop a comprehensive understanding of language nuances, including grammar, semantics, and context, allowing them to perform various natural language tasks without task-specific training.
*   **Task Versatility:** GPT models can be used for a wide range of applications, including:
    *   Text generation
    *   Translation
    *   Summarization
    *   Question answering
    *   Code generation

*   **Evolution of GPT Models:** The GPT series has seen significant advancements over time, with each new version bringing improvements in model size, architecture, and capabilities.
    *   **GPT-1 (2018):** Introduced the decoder-only transformer architecture, featuring 117 million parameters. Its architecture consists of 12 decoder blocks, 768-dimensional embeddings, and 12 attention heads per block.
    *   **GPT-2 (2019):** Significantly larger than GPT-1, with 1.5 billion parameters in its XL version. This enhanced size allows it to generate more coherent and longer-form text. The XL version includes 48 decoder blocks, 1600-dimensional embeddings, and 25 attention heads per block.
    *   **GPT-3 (2020):** A massive model with 175 billion parameters, enabling it to achieve advanced capabilities in language understanding, code generation, and reasoning. GPT-3 is composed of 96 decoder blocks, 12,288-dimensional embeddings, and 96 attention heads per block.
    *   **GPT-4 (2023):** Introduces multi-modal capabilities (image and text), improved reasoning, and broader general knowledge. The detailed architecture of GPT-4 is not yet publicly available.

<----------section---------->

## GPT Input Encoding: Byte-Pair Encoding (BPE)
GPT models, including GPT-1, GPT-2, GPT-3, and later versions, utilize Byte-Pair Encoding (BPE) as their primary input encoding method. BPE is a subword tokenization technique that effectively balances word-level and character-level representations.

*   **Subword Tokenization:** BPE breaks down words into smaller, meaningful subunits (tokens) based on their frequency in the training data. This approach helps the model handle both frequent and rare words efficiently.
*   **Vocabulary Size:** The vocabulary size varies by model version (e.g., GPT-2 uses about 50,000 tokens), allowing efficient representation of both frequent and rare words. The vocabulary includes common words, word fragments, and single characters.
*   **Language Representation:** Subword tokens enable GPT to represent a diverse range of language patterns, handling both common and rare words effectively while reducing the total number of tokens required. This allows the model to efficiently represent a diverse range of language patterns.
*   **Fixed Vocabulary:** BPE produces a fixed-size vocabulary (e.g., around 50,000 tokens in GPT-2), containing common words, word fragments, and some single characters. This helps the model handle a wide range of text efficiently.

<----------section---------->

## Advantages of BPE
The primary advantages of BPE (similar to WordPiece) are:

*   **Flexibility:** BPE can handle languages with rich morphology or new words (e.g., "AI-generated") by breaking them down into reusable subword tokens. This makes the model adaptable to various linguistic structures.
*   **Reduced Vocabulary Size:** BPE keeps the vocabulary smaller compared to word-level tokenizers, leading to more efficient training and reduced memory requirements. This is particularly beneficial for large-scale models.
*   **Out-of-Vocabulary (OOV) Handling:** BPE is resilient to unknown words because it can break down any new word into familiar subwords or characters. This ensures that the model can process unseen words by combining known elements.

<----------section---------->

## GPT Pre-training
GPT models undergo pre-training to learn language patterns and relationships from massive amounts of text data.

*   **Autoregressive Modeling:** GPT is pre-trained to predict the next word (or token) in a sequence, given all the previous tokens. This process is also known as autoregressive modeling.
*   **Next-Token Prediction:** At each step, the GPT model learns to minimize the difference between its predicted next token and the actual next token in the training sequence. This process effectively teaches the model context and word relationships.
*   **Sequential Prediction:** The prediction is sequential, meaning each token is predicted based only on previous tokens. This method allows the model to learn language patterns in a left-to-right order, simulating the natural way language is generated.
*   **Context and Word Relationships:** GPT learns to associate words and phrases with each other based on their co-occurrence in the training data. This allows the model to generate text that is not only grammatically correct but also contextually relevant.

<----------section---------->

## Training Datasets for GPT
GPT models are trained on massive and diverse datasets sourced from a wide array of internet text. This ensures that the model learns a broad understanding of language and can generalize well across different domains.

*   **GPT-1:** Trained on BookCorpus, consisting of around 985 million words (800 MB of text).
*   **GPT-2:** Trained on WebText (40 GB of text from around 8 million documents with 10 billion words), a dataset curated from high-quality web pages by OpenAI.
*   **GPT-3:** Used even larger datasets (570 GB of text with hundreds of billions of words), combining sources like Common Crawl, Books, Wikipedia, and more.
*   **Data Selection:** The data is selected to cover a broad range of topics and linguistic structures, ensuring the model is versatile across different domains. The diversity of the training data helps the model avoid biases and generate more accurate and relevant text.
*   **GPT-4 Training:** OpenAI's training FLOPS (Floating Point Operations Per Second) for GPT-4 is approximately 2.15e25, utilizing around 25,000 A100 GPUs for 90 to 100 days.

<----------section---------->

## Loss Function and Optimization
GPT models utilize specific loss functions, optimizers, and learning rate schedules during pre-training to achieve optimal performance.

*   **Cross-Entropy Loss:** GPT minimizes the cross-entropy loss between the predicted token probabilities and the actual tokens. This loss function is well-suited to classification tasks (like predicting the next token) and provides the model with feedback on its predictions.
*   **Adam Optimizer:** GPT uses the Adam optimizer, an adaptive gradient descent technique, which helps accelerate convergence by adjusting learning rates based on past gradients. Adam is efficient in handling large datasets and complex models.
*   **Learning Rate Scheduling:** GPT applies a learning rate scheduling in which learning rates are gradually increased (warm-up) in the early stages and then decayed to prevent instability during training. This approach helps the model converge more effectively.
*   **Large Batch Sizes:** Large batch sizes are used to stabilize training and make the model better at generalizing across diverse language patterns. Larger batches provide more accurate gradient estimates, leading to better generalization.

<----------section---------->

## GPT Fine-Tuning
Fine-tuning adapts a pre-trained GPT model to perform specific tasks by training it on a smaller, task-specific dataset.

*   **Task-Specific Datasets:** Fine-tuning of GPT requires a dataset labeled for the specific task, such as pairs of prompts and expected responses, or inputs and target outputs.
*   **Examples of Fine-Tuning Tasks:** Examples of tasks for which GPT has been or may be fine-tuned include:
    *   Customer Support Automation: Responding to customer queries, resolving issues, and providing information.
    *   Medical Assistance: Offering health guidance and support, as well as addressing patient inquiries.
    *   Legal Document Processing: Summarizing legal documents and supporting legal research.
    *   Coding Assistance: Providing code snippets, explanations, and debugging help.
    *   Educational Tutoring: Answering questions, explaining concepts, and supporting e-learning.
    *   Content Creation: Generating blog posts, social media content, and marketing copy.
    *   Virtual Personal Assistants: Providing reminders, managing tasks, and answering questions.

<----------section---------->

## GPT Strengths
GPT models exhibit several notable strengths due to their architecture and training methodologies.

*   **Language Fluency and Coherence:** GPT models generate human-like, fluent, and coherent text, often indistinguishable from human writing. The fluency and coherence are due to the model's ability to learn complex language patterns and dependencies.
*   **Broad Knowledge Base:** Trained on vast datasets, GPT has extensive general knowledge across a wide array of topics, allowing it to answer questions and generate content in diverse domains. This makes the model versatile and adaptable.
*   **Few-Shot and Zero-Shot Learning:** GPT can perform tasks with little to no task-specific training by learning from examples in the prompt (few-shot) or adapting to a task without examples (zero-shot).
*   **Creative and Contextual Writing:** GPT can generate creative content, including stories, poetry, and dialogues, which makes it useful for content creation and entertainment applications. The model's ability to understand context allows it to generate content that is relevant and engaging.
*   **Rapid Adaptation with Fine-Tuning:** Fine-tuning on task-specific data allows GPT to perform well in specialized contexts, such as technical writing, legal assistance, and customer service. Fine-tuning enables the model to adapt its knowledge and generate content tailored to specific domains.
*   **Scalability with Large Models:** Larger GPT models demonstrate stronger performance and generalization, especially on complex or nuanced tasks. The increased model size allows for greater capacity to learn and represent language patterns.

<----------section---------->

## GPT Limitations
Despite their strengths, GPT models also have several limitations that should be considered.

*   **Lack of True Understanding:** GPT models generate text based on patterns rather than true comprehension. This means that the model can produce grammatically correct and contextually relevant text without actually understanding the meaning behind it.
*   **Sensitivity to Prompting:** GPT's responses can vary widely based on phrasing. Small changes in prompts may yield different outcomes. The model's sensitivity to prompting requires careful design to achieve the desired results.
*   **Ethical and Bias Concerns:** GPT can reproduce biases present in its training data, leading to biased or inappropriate outputs, especially if prompts or fine-tuning data lack diversity or sensitivity. Biases in the training data can perpetuate and amplify societal biases.
*   **Inability to Reason or Perform Complex Calculations:** GPT is limited in logical reasoning, advanced mathematics, or tasks requiring step-by-step problem-solving without explicit instruction in the prompt. The model's reliance on pattern recognition limits its ability to perform tasks that require logical or mathematical reasoning.
*   **High Computational Requirements:** Large GPT models require significant computational power for training, fine-tuning, and deployment, making them expensive to run and maintain. The computational cost can be a barrier to entry for many organizations and researchers.
*   **Limited Memory Across Interactions:** GPT lacks persistent memory across sessions, so it cannot retain information shared by users in previous interactions without explicit prompting. This limits the model's ability to maintain context and coherence across multiple turns in a conversation.
*   **Vulnerability to Adversarial Prompts:** Malicious prompts can manipulate GPT into producing undesirable or unintended responses. Adversarial prompts can exploit the model's reliance on pattern recognition to generate outputs that are harmful, offensive, or misleading.

<----------section---------->

## Popular GPT Variants
Several variants of the GPT model have been developed to address specific needs and challenges.

*   **Codex:** A GPT-3 model fine-tuned by OpenAI specifically for coding and programming tasks. It powers GitHub Copilot and can assist with code generation, debugging, and explanations across multiple programming languages.
    *   **Coding Assistance:** Provides real-time assistance to developers by suggesting code snippets and solutions.
    *   **Code Generation:** Automatically generates code based on natural language descriptions.
    *   **Multi-Language Support:** Supports various programming languages, making it versatile for different coding environments.
*   **MT-NLG (Megatron-Turing Natural Language Generation):** Developed by NVIDIA and Microsoft, MT-NLG is one of the largest language models, with 530 billion parameters.
    *   **Large Model Size:** With 530 billion parameters, MT-NLG can capture complex language patterns and relationships.
    *   **Improved Natural Language Understanding:** Aims to enhance natural language understanding and generation in tasks like summarization and question answering.
    *   **Few-Shot Learning:** Designed for robust few-shot learning, enabling it to perform well with limited training data.
*   **GLaM (Generalist Language Model):** Developed by Google Research, GLaM is a sparse mixture-of-experts model with 1.2 trillion parameters, but only a fraction are active per inference.
    *   **Sparse Mixture-of-Experts:** Employs a sparse architecture where only a subset of the parameters are activated for each input.
    *   **Resource Efficiency:** Uses fewer resources than fully dense models like GPT-3 while achieving competitive performance across NLP tasks.
*   **PanGu-α:** Huawei’s Chinese language model with 200 billion parameters, aimed at understanding and generating text in Mandarin.
    *   **Chinese Language Focus:** Specifically designed for understanding and generating text in Mandarin.
    *   **Chinese NLP Applications:** Supports Chinese-specific language applications, including literature, customer support, and translation.
*   **Chinchilla:** A DeepMind model optimized for efficiency in training data and parameters.
    *   **Efficient Training:** Achieves similar or better performance with a smaller number of parameters than GPT-3.
    *   **Research and Practical Applications:** Optimized for both research and practical applications, demonstrating an alternative approach to large-scale model training.
*   **OPT (Open Pretrained Transformer):** Developed by Meta (Facebook AI), OPT models are a series of open-source language models comparable to GPT-3 in size and capabilities.
    *   **Open-Source Initiative:** Meta released these models to support transparency in AI research, offering model weights and code for research and academic use.
*   **BLOOM:** Result of the BigScience collaborative project, BLOOM is an open-source multilingual model with 176 billion parameters, trained by a global consortium of researchers.
    *   **Multilingual Support:** Supports 46 languages, including underrepresented languages, to make large language models accessible for diverse linguistic and cultural contexts.
    *   **Inclusivity in NLP:** Enforces inclusivity in NLP research and applications.

<----------section---------->

## LLAMA (Large Language Model Meta AI)
The LLaMA (Large Language Model Meta AI) architecture is a family of transformer-based language models developed by Meta. These models are designed to be efficient, high-performing, and optimized for a range of NLP tasks.

*   **Model Sizes:**
    *   LLaMA-7B: 32 decoder blocks with 32 attention heads for each block, 4096-dimensional embeddings
    *   LLaMA-13B: 40 decoder blocks with 40 attention heads for each block, 5120-dimensional embeddings
    *   LLaMA-30B: 60 decoder blocks with 40 attention heads for each block, 6656-dimensional embeddings
    *   LLaMA-65B: 80 decoder blocks with 64 attention heads for each block, 8192-dimensional embeddings
*   **Scalability:** These models are designed to offer a range of capabilities depending on the computational resources available, from smaller more efficient models to larger models. The range of sizes allows for flexibility in deployment based on specific resource constraints.

<----------section---------->

## LLAMA Input Encoding
LLaMA models use Byte-Pair Encoding (BPE) as their input encoding method, obtaining a dictionary of 32768 tokens.
This encoding is used to preprocess textual data into a format that the model can understand.

*   **Relative Positional Encodings:** However, LLaMA uses relative positional encodings instead of absolute positional encodings. This method allows the model to better handle varying sequence lengths and to generalize across different contexts, which is particularly useful for longer sequences.
*   **Relationship-Based Token Positioning:** In relative positional encoding, the model learns the relationships between tokens based on their relative positions rather than their absolute positions in the sequence.

<----------section---------->

## LLAMA Pre-training
Like GPT models, LLaMA is pre-trained using an autoregressive language modeling objective. This means that the model learns to predict the next token in a sequence given the previous tokens.

*   **Training Data:** LLaMA is trained on “The Pile” (825 GB, from 300 to 1000 billion tokens), a wide range of publicly available text sources, including:
    *   Books (e.g., text from various domains like literature, non-fiction, etc.)
    *   Web Data (e.g., content from the internet, scraped from publicly accessible websites)
    *   Scientific Papers (e.g., research articles, preprints, and academic papers)
*   **Data Diversity:** The dataset is designed to be as diverse as possible to ensure the model learns a broad understanding of language and can generalize well to a wide range of tasks.

<----------section---------->

## LLAMA Training Process
During pre-training, LLaMA uses specific methods to optimize its performance.

*   **Loss Function:** The loss function used during pre-training is the cross-entropy loss between the predicted token probabilities and the actual next token in the sequence. This helps the model learn to predict the correct token given the context.
*   **Optimization Algorithms:** The model is optimized using Stochastic Gradient Descent (SGD) or Adam optimizer with gradient clipping to prevent instability during training.
*   **Mixed Precision Training:** Training also utilizes mixed precision to speed up computation and reduce memory requirements.
*   **Training Techniques:** LLaMA employs techniques like learning rate schedules (e.g., linear warm-up followed by decay), weight decay, and batch normalization or layer normalization to stabilize and improve training.

<----------section---------->

## LLAMA Variants and Use Cases

| Model        | Parameters | Use Case                                                              | Strengths                                                                                                                                                 | Limitations                                                                                            |
|--------------|------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| LLaMA-7B     | 7 billion  | Resource-efficient tasks (e.g., small-scale NLP)                       | High efficiency, suitable for smaller environments                                                                                                        | May not achieve top performance on complex tasks                                                       |
| LLaMA-13B    | 13 billion | General-purpose NLP tasks, fine-tuning for specific applications      | Balanced performance and efficiency                                                                                                                         | May lack performance for more advanced tasks                                                           |
| LLaMA-30B    | 30 billion | Complex tasks (e.g., summarization, translation)                      | High performance on state-of-the-art NLP tasks                                                                                                            | Requires significant computational resources                                                             |
| LLaMA-65B    | 65 billion | High-end applications, advanced research                               | Top-tier NLP performance across multiple domains                                                                                                         | Extremely resource-intensive, challenging for deployment                                                 |

<----------section---------->

## LLAMA vs GPT

| Aspect            | LLaMA                                                       | GPT                                                                 |
| ----------------- | ----------------------------------------------------------- | ------------------------------------------------------------------- |
| Size Range        | 7B, 13B, 30B, 65B                                           | 117M to 175B+ (GPT-3)                                               |
| Training Data     | Public data (The Pile, Wikipedia, Common Crawl, etc.)       | Public data (Common Crawl, WebText, etc.)                           |
| Performance       | Strong, competitive, especially for smaller models           | State-of-the-art, particularly in zero/few-shot                     |
| Training Efficiency | More efficient, parameter-efficient                         | Very resource-intensive, especially for GPT-3                       |
| Deployment        | Open-sourced, flexible deployment                         | Commercial API via OpenAI                                           |
| Ethics            | Strong ethical considerations                               | Criticism over transparency and biases                                |
| Applications      | Academic research, custom deployment                      | Broad commercial use, APIs, and applications                         |

<----------section---------->

## Practice on Text Generation
For practical experience, the following resources are recommended:

*   **Hugging Face Guide on Text Generation:** Explore the Hugging Face guide on text generation ([https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation)) to understand different models and techniques.
*   **Hugging Face Models for Text Generation:** Search for possible models for text generation in Hugging Face ([https://huggingface.co/models?pipeline_tag=text-generation&sort=trending](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)).
*   **Fine-Tuning Text Generation Models:** If you have time and computational resources, fine-tune one of the text generation models ([https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article](https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article)).
<----------section---------->

## Lesson 15 #

## Encoder-Decoder Transformers
Authored by Nicola Capuano and Antonio Greco, DIEM (Department of Enterprise Engineering), University of Salerno.

### Outline
This lesson will cover the following topics:

*   Encoder-decoder transformer architectures.
*   The T5 (Text-to-Text Transfer Transformer) model.
*   Practical exercises on machine translation and text summarization.

<----------section---------->

## Encoder-Decoder Transformer

### Encoder-Decoder Transformer
Encoder-Decoder Transformers represent a specific class of neural networks architected to handle sequence-to-sequence (seq2seq) tasks. These tasks involve converting an input sequence into a different output sequence. Examples include machine translation (converting a sentence from one language to another) and text summarization (generating a shorter version of a longer document). The encoder processes the input sequence to create an intermediate representation, and the decoder uses this representation to generate the output sequence. This architecture is fundamental in many NLP applications.

<----------section---------->

## T5

### T5
T5, which stands for Text-to-Text Transfer Transformer, is a language model developed by Google Research. It leverages an encoder-decoder transformer architecture. A key feature of T5 is its "text-to-text" approach, where every NLP task is framed as converting a text input to a text output. This simplifies the model's design and allows it to be applied to a wide variety of tasks with minimal modifications.

T5 is available in various sizes, allowing users to select a model that best fits their computational resource constraints. The different sizes offer trade-offs between model performance and resource usage. Larger models generally achieve higher accuracy but require more memory and processing power.

The following table details the configurations of the different T5 versions:

| Version   | Encoder Blocks | Attention Heads | Decoder Blocks | Embedding Dimensionality |
| --------- | -------------- | --------------- | -------------- | ------------------------ |
| T5-Small  | 6              | 8               | 6              | 512                      |
| T5-Base   | 12             | 12              | 12             | 768                      |
| T5-Large  | 24             | 16              | 24             | 1024                     |
| T5-XL     | 24             | 32              | 24             | 2048                     |
| T5-XXL    | 24             | 64              | 24             | 4096                     |

*   **Encoder Blocks:** The number of transformer blocks in the encoder. More blocks allow the model to capture more complex relationships in the input sequence.
*   **Attention Heads:** The number of attention heads in each transformer block. Multiple attention heads allow the model to attend to different parts of the input sequence simultaneously.
*   **Decoder Blocks:** The number of transformer blocks in the decoder. Similar to the encoder, more blocks allow the model to generate more complex output sequences.
*   **Embedding Dimensionality:** The size of the vector used to represent each token. Higher dimensionality allows the model to capture more fine-grained information about each token.

<----------section---------->

### T5 Input Encoding
T5 uses a specific method for encoding its input text, relying on the SentencePiece tokenizer and a custom vocabulary.

*   **Subword Units:** T5 utilizes subword tokenization via the SentencePiece library. Subword tokenization strikes a balance between character-level and word-level tokenization. This approach is useful for handling rare words and previously unseen combinations of words. It breaks down words into smaller units, allowing the model to understand and process even unfamiliar terms.
*   **Unigram Language Model:** The SentencePiece tokenizer in T5 is trained using a unigram language model. This model selects subwords to maximize the likelihood of the training data. The tokenizer aims to find the optimal set of subword units that best represent the training corpus.

<----------section---------->

### T5 Vocabulary
T5 employs a fixed vocabulary of 32,000 tokens. This vocabulary includes subwords, whole words, and special tokens. The choice of this vocabulary size represents a compromise between computational efficiency and the model's capacity to represent a wide range of text.

T5 includes several special tokens within its vocabulary to guide the model in performing different tasks:

*   `<pad>`: This is a padding token, used to ensure that all sequences in a batch have the same length. This is crucial for efficient processing by neural networks.
*   `<unk>`: This is an unknown token, used to represent words that are not present in the vocabulary. This allows the model to handle out-of-vocabulary (OOV) cases.
*   `<eos>`: This is an end-of-sequence token, marking the conclusion of an input or output sequence. It signals to the model when a sequence is complete.
*   `<sep>` and task-specific prefixes: These are used to define the type of task the model should perform. For instance, "translate English to German:" indicates a translation task, while "summarize:" indicates a summarization task. These prefixes are a key component of T5's text-to-text framework.

<----------section---------->

### T5 Pre-training
T5 is pre-trained using a denoising autoencoder objective called span-corruption. This pre-training task helps the model learn general language understanding and generation skills.

The span-corruption objective involves masking spans of text (rather than individual tokens) within the input sequence and then training the model to predict those masked spans.

*   **Input Corruption:** Random spans of text in the input are replaced with a special `<extra_id_X>` token (e.g., `<extra_id_0>`, `<extra_id_1>`).
*   **Original Input:** "The quick brown fox jumps over the lazy dog."
*   **Corrupted Input:** "The quick `<extra_id_0>` jumps `<extra_id_1>` dog."
*   **Target Output:** The model is trained to predict the original masked spans in sequential order.
*   **Target Output:** `<extra_id_0>` brown fox `<extra_id_1>` over the lazy.

This span-corruption approach forces the model to generate coherent text and learn contextual relationships between tokens. By predicting missing spans, the model learns to understand the relationships between words and phrases.

<----------section---------->

### Benefits of Span Prediction
Predicting spans, rather than individual tokens, encourages the model to learn:

*   **Global Context:** The model learns how spans relate to the overall structure of a sentence or paragraph. This helps the model understand the broader context of the input.
*   **Fluency and Cohesion:** Span prediction ensures that the generated outputs are natural and coherent. The model learns to produce text that flows smoothly and makes sense.
*   **Task Versatility:** The model is better prepared for downstream tasks such as summarization, translation, and question answering. The span-corruption objective trains the model to generate text in a variety of contexts, improving its ability to adapt to different tasks.

<----------section---------->

### T5 Pre-training Dataset
T5 pre-training uses the C4 dataset (Colossal Clean Crawled Corpus), a massive dataset derived from Common Crawl.

*   **Size:** Approximately 750 GB of cleaned text.
*   **Cleaning:** Aggressive data cleaning is performed to remove spam, duplicate text, and low-quality content. This ensures that the model is trained on high-quality data.
*   **Versatility:** The dataset contains diverse text, enabling the model to generalize across various domains. The wide range of topics and writing styles in the C4 dataset helps the model to learn a broad understanding of language.

<----------section---------->

### T5 Training Details
*   **Loss Function:** Cross-entropy loss is used for predicting masked spans. Cross-entropy loss measures the difference between the model's predictions and the true labels.
*   **Optimizer:** T5 employs the Adafactor optimizer, which is memory-efficient and designed for large-scale training. Adafactor is particularly well-suited for training very large models with limited memory.
*   **Learning Rate Scheduling:** The learning rate is adjusted using a warm-up phase followed by an inverse square root decay. This learning rate schedule helps to stabilize training and improve model performance. The warm-up phase gradually increases the learning rate, while the inverse square root decay gradually decreases it.

<----------section---------->

### T5 Fine-tuning
*   **Input and Output as Text:** Fine-tuning maintains the paradigm where both input and output are always text strings, regardless of the task. This consistency simplifies the process of adapting the model to new tasks.
*   **Example Tasks:**

    *   **Summarization:**
        *   Input: `summarize: <document>` → Output: `<summary>`
    *   **Translation:**
        *   Input: `translate English to French: <text>` → Output: `<translated_text>`
    *   **Question Answering:**
        *   Input: `question: <question> context: <context>` → Output: `<answer>`

This text-to-text approach allows T5 to perform a wide variety of tasks with a single model architecture. By framing each task as a text generation problem, T5 can leverage its pre-trained language understanding and generation capabilities to achieve state-of-the-art results.

<----------section---------->

### Popular T5 Variants – mT5
mT5 (Multilingual T5) was developed to extend T5's capabilities to multiple languages. It enhances the original T5 model by allowing it to process and generate text in over 100 different languages.

It was pre-trained on the multilingual Common Crawl dataset, encompassing 101 languages.

*   **Key Features:**
    *   It maintains the text-to-text framework across different languages, ensuring consistency in its approach.
    *   It avoids language-specific tokenization by using SentencePiece with a shared vocabulary across all languages. This enables the model to handle multiple languages without needing separate tokenizers for each.
    *   It demonstrates strong multilingual performance, including cross-lingual tasks, making it versatile for various NLP applications.
*   **Applications:**
    *   Suitable for translation, multilingual summarization, and cross-lingual question answering, making it valuable for tasks involving multiple languages.
*   **Limitations:**
    *   Larger model size due to representing multiple languages in the vocabulary, requiring more memory and computational resources.
    *   Performance can vary significantly across languages, favoring those with more representation in the training data, indicating a potential bias towards languages with larger datasets.

<----------section---------->

### Popular T5 Variants – Flan-T5
Flan-T5 is a fine-tuned version of T5 with instruction-tuning on a diverse set of tasks. Instruction tuning improves the model's ability to follow instructions and generalize to new tasks.

*   **Key Features:**
    *   Designed to improve generalization by training on datasets formatted as instruction-response pairs. This allows the model to better understand and follow instructions.
    *   Offers better zero-shot and few-shot learning capabilities compared to the original T5, enabling it to perform well on tasks with limited training data.
*   **Applications:**
    *   Performs well in scenarios requiring generalization to unseen tasks, such as creative writing or complex reasoning, showcasing its versatility.
*   **Limitations:**
    *   Requires careful task formulation to fully utilize its instruction-following capabilities, meaning the prompt needs to be well-designed for optimal performance.

<----------section---------->

### Popular T5 Variants – ByT5
ByT5 (Byte-Level T5) processes text at the byte level instead of using subword tokenization. This is a significant departure from traditional tokenization methods.

*   **Key Features:**
    *   Avoids the need for tokenization, enabling better handling of noisy, misspelled, or rare words, making it robust to imperfect text.
    *   Works well for languages with complex scripts or low-resource scenarios, where traditional tokenizers may struggle.
*   **Applications:**
    *   Robust for tasks with noisy or unstructured text, such as OCR (Optical Character Recognition) or user-generated content, making it suitable for real-world data.
*   **Limitations:**
    *   Significantly slower and more resource-intensive due to longer input sequences (byte-level representation increases sequence length), requiring more computational power.

<----------section---------->

### Popular T5 Variants – T5-3B and T5-11B
T5-3B and T5-11B are larger versions of the original T5 with 3 billion and 11 billion parameters, respectively. These larger models offer increased capacity and improved performance.

*   **Key Features:**
    *   Improved performance on complex tasks due to increased model capacity, allowing them to handle more intricate relationships in the data.
    *   Suitable for tasks requiring deep contextual understanding and large-scale reasoning, enabling advanced NLP applications.
*   **Applications:**
    *   Used in academic research and high-performance NLP applications where resources are not a constraint, indicating their suitability for cutting-edge projects.
*   **Limitations:**
    *   Computationally expensive for fine-tuning and inference, requiring significant processing power and time.
    *   Memory requirements limit their usability on standard hardware, restricting their use to specialized environments.

<----------section---------->

### Popular T5 Variants – UL2
UL2 (Unified Language Learning) is a general-purpose language model inspired by T5 but supports a wider range of pretraining objectives. UL2 aims to combine the strengths of different pretraining approaches into a single model.

*   **Key Features:**
    *   Combines diverse learning paradigms: unidirectional, bidirectional, and sequence-to-sequence objectives, making it flexible and adaptable.
    *   Offers state-of-the-art performance across a variety of benchmarks, indicating its effectiveness.
*   **Applications:**
    *   General-purpose NLP tasks, including generation and comprehension, making it suitable for a wide range of applications.
*   **Limitations:**
    *   Increased complexity due to multiple pretraining objectives, potentially making it more challenging to train and optimize.

<----------section---------->

### Popular T5 Variants – Multimodal T5
T5-Large Multimodal Variants combine T5 with vision capabilities by integrating additional modules for visual data. These models can process both text and images.

*   **Key Features:**
    *   Processes both text and image inputs, enabling tasks like image captioning, visual question answering, and multimodal translation.
    *   Often uses adapters or encodes visual features separately, allowing for flexible integration of different modalities.
*   **Applications:**
    *   Multimodal tasks combining vision and language, broadening the scope of NLP applications.
*   **Limitations:**
    *   Computationally expensive due to the need to process multiple modalities, requiring significant processing power.

<----------section---------->

### Popular T5 Variants – Efficient T5
Efficient T5 Variants are optimized for efficiency in resource-constrained environments. These models are designed to be lightweight and fast.

*   **Examples:**
    *   T5-Small/Tiny: Reduced parameter versions of T5 for lower memory and compute needs.
    *   DistilT5: A distilled version of T5, reducing the model size while retaining performance. Distillation involves training a smaller model to mimic the behavior of a larger model.
*   **Applications:**
    *   Real-time applications on edge devices or scenarios with limited computational resources, enabling NLP in resource-limited settings.
*   **Limitations:**
    *   Sacrifices some performance compared to larger T5 models, trading accuracy for efficiency.

<----------section---------->

### T5 Variants Summary

| Variant        | Purpose                    | Key Strengths                                | Limitations                                  |
| -------------- | -------------------------- | -------------------------------------------- | --------------------------------------------- |
| mT5            | Multilingual NLP           | Supports 101 languages                        | Uneven performance across languages           |
| Flan-T5        | Instruction-following        | Strong generalization                         | Needs task-specific prompts                    |
| ByT5           | No tokenization            | Handles noisy/unstructured text               | Slower due to byte-level inputs               |
| T5-3B/11B      | High-capacity NLP          | Exceptional performance                      | High resource requirements                     |
| UL2            | Unified objectives         | Versatility across tasks                      | Increased training complexity                  |
| Multimodal T5  | Vision-language tasks      | Combines text and image inputs               | Higher computational cost                      |
| Efficient T5   | Resource-constrained NLP   | Lightweight, faster inference                 | Reduced task performance                       |

<----------section---------->

## Practice on Translation and Summarization

### Practice
To gain practical experience with these models, consider the following exercises:

Looking at the Hugging Face guides on translation ([https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)) and summarization ([https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt](https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt)), use various models to perform these tasks.

By following the guides, if you have time and computational resources you can also fine tune one of the encoder-decoder models. Fine-tuning allows you to adapt a pre-trained model to a specific task or dataset, potentially improving its performance.

<----------section---------->

**Final Project Overview**

**Project Outline:**

The project is structured around three key components:

1.  **Goal of the project:** Defining the objective and scope of the chatbot.
2.  **Tools to use for the project:** Specifying the acceptable technologies and methodologies.
3.  **Chatbot evaluation procedure:** Establishing the metrics and methods for assessing the chatbot's performance.

`<----------section---------->`

**Goal of the Project (Detailed):**

The primary goal of this project is to develop a chatbot proficient in answering questions related to the NLP and LLM course content for the 2024/2025 academic year. The chatbot's knowledge domain extends beyond the specific topics covered in class to include general course information. Examples include identifying course teachers, recommending textbooks, and providing other relevant details regarding the course structure and administration.

A critical requirement is that the chatbot must **exclusively** address questions pertaining to the NLP and LLM course. It must accurately identify and reject out-of-context queries, responding appropriately that it is not authorized to provide information outside the designated scope. This capability ensures that the chatbot remains a focused and reliable resource for course-related inquiries.

Project deliverables include:

*   **Chatbot Code:** The complete source code of the implemented chatbot.
*   **Report:** A comprehensive report detailing the design, implementation, and evaluation of the chatbot solution. The report should offer a detailed explanation of the chosen architecture, algorithms, and any specific techniques used to achieve the project goals.

`<----------section---------->`

**Tools to Use for the Project (Expanded):**

Students have significant flexibility in selecting the tools and technologies for this project. They are encouraged to utilize any methods covered during the course. This includes both state-of-the-art LLM-based approaches and traditional NLP techniques.

A blended approach is also acceptable, where some chatbot components are developed using LLMs, while others rely on more conventional NLP tools. Students can freely explore this hybrid approach, provided they thoroughly justify their choices and design decisions in the final report. The goal is to encourage thoughtful selection and integration of the most suitable tools for each specific task.

Existing LLMs or other pre-trained models can be used, either with or without modifications. However, groups must demonstrate a thorough understanding of *all* tools and models incorporated into their chatbot. Students should be prepared to answer questions regarding every aspect of the code and models used, demonstrating their expertise in the technologies employed. This requirement ensures that students not only use the tools effectively but also gain a deep understanding of their inner workings.

`<----------section---------->`

**Chatbot Evaluation Procedure (In-Depth):**

The evaluation of the chatbots will consist of two phases, both conducted in real-time during the project discussion. The course instructors will directly interact with each group's chatbot by posing a set of predefined questions.

**Phase 1: Core Performance Evaluation**

The initial evaluation will assess the chatbot's core capabilities using the following metrics:

*   **Relevance:** This criterion evaluates whether the chatbot's generated text effectively answers the user's query. Does the response directly address the question and provide the needed information?
*   **Fluency:** This metric assesses the readability and grammatical correctness of the chatbot's output. The generated text should be natural, well-structured, and free of grammatical errors.
*   **Coherence:** This evaluates the logical flow and consistency of the chatbot's responses. The text should be internally consistent, and the sequence of statements should form a coherent whole.

**Phase 2: Robustness and Precision Assessment**

The second evaluation phase aims to test the chatbot's ability to handle challenging or ambiguous inputs and maintain accuracy:

*   **Robustness:** This criterion assesses the chatbot's resilience to adversarial or misleading prompts. It tests the chatbot's ability to avoid being misled by prompts designed to trick or confuse it (e.g., questions containing statements like "Are you sure?"). The chatbot should maintain accuracy and avoid generating inappropriate responses, even when confronted with manipulative prompts.
*   **Precision:** This evaluates the chatbot's ability to distinguish between in-context and out-of-context questions. Can the chatbot accurately identify and reject questions unrelated to the NLP and LLM course (e.g., "Who is the king of Spain?")?

Based on the chatbot's performance across these aspects, the course instructors will assign a grade that reflects its overall quality and effectiveness. The grade will be determined holistically, considering the strengths and weaknesses demonstrated in each evaluation category.

<----------section---------->

Lesson 17: Fine Tuning

This lesson explores fine-tuning techniques for Large Language Models (LLMs). Fine-tuning involves adapting a pre-trained LLM to perform well on a specific task or within a particular domain. This is a crucial step in leveraging the power of LLMs for real-world applications, as it allows us to tailor the model's general knowledge to the nuances of a specific use case. The lesson will cover different approaches to fine-tuning, including full fine-tuning, parameter-efficient fine-tuning (PEFT), and instruction fine-tuning, examining the advantages and disadvantages of each. We will also delve into the realm of Reinforcement Learning from Human Feedback (RLHF).

## Outline

*   Types of fine tuning
*   Parameter Efficient Fine Tuning (PEFT)
*   Instruction Fine-Tuning

This outline sets the stage for a comprehensive exploration of the landscape of fine-tuning strategies for LLMs. It begins with a discussion of several approaches to fine-tuning and continues with an introduction to parameter-efficient fine-tuning (PEFT) techniques. Lastly, it touches upon instruction fine-tuning as a means of aligning models with user expectations.

<----------section---------->

## Types of fine tuning

### Pros and cons of full fine tuning

*   Fine-tuning refers to adapting a pre-trained LLM to a specific task by training it further on a task-specific dataset.
*   **Why Fine-Tune?**
    *   Specialize LLMs for domain-specific tasks.
    *   Improve accuracy and relevance for specific applications.
    *   Optimize performance on small, focused datasets.
*   Full Fine-Tuning, which updates all model parameters, allows to achieve high accuracy for specific tasks by fully leveraging the model's capacity, but it is computationally expensive and risks overfitting on small datasets.

Fine-tuning is the process of taking a pre-trained LLM and further training it on a dataset tailored to a specific task. This adaptation allows the LLM to specialize its knowledge and improve its performance in that specific area. The reasons to fine-tune are multifaceted: it enables specialization for domain-specific tasks (like medical diagnosis, legal document summarization, or financial forecasting), enhances accuracy and relevance for specific applications, and optimizes performance, particularly when working with smaller, more focused datasets. Full fine-tuning involves updating *all* the model's parameters, enabling the model to fully leverage its capacity and potentially achieve high accuracy. However, this approach is computationally expensive, demanding significant resources, and it carries the risk of overfitting, especially when dealing with relatively small datasets. Overfitting occurs when the model learns the training data too well, capturing noise and specific details that do not generalize to new, unseen data.

<----------section---------->

### Other types of fine tuning

*   Parameter-Efficient Fine-Tuning (PEFT): Updates only a subset of the parameters.
    *   Examples: LoRA, Adapters.
*   Instruction Fine-Tuning: Used to align models with task instructions or prompts (user queries) enhancing usability in real-world applications.
*   Reinforcement Learning from Human Feedback (RLHF): Combines supervised learning with reinforcement learning, rewarding models when they generate user-aligned outputs.

Beyond full fine-tuning, other techniques aim to address the computational cost and overfitting risks associated with updating all model parameters. Parameter-Efficient Fine-Tuning (PEFT) focuses on updating only a carefully selected subset of the model's parameters. Examples include Low-Rank Adaptation (LoRA) and Adapters, which allow adaptation with significantly fewer trainable parameters. Instruction fine-tuning is geared towards aligning models with specific task instructions or prompts, effectively teaching the model to better understand and respond to user queries, which significantly enhances its usability in real-world scenarios. Reinforcement Learning from Human Feedback (RLHF) combines supervised learning with reinforcement learning. In RLHF, the model is rewarded when it generates outputs that align with human preferences, further improving its alignment with user expectations and real-world requirements.

<----------section---------->

## Parameter efficient fine tuning (PEFT)

### Parameter-Efficient Fine-Tuning

*   Parameter-Efficient Fine-Tuning (PEFT) is a strategy developed to fine-tune large-scale pre-trained models, such as LLMs, in a computationally efficient manner while requiring fewer learnable parameters compared to standard fine-tuning methods.
*   PEFT methods are especially important in the context of LLMs due to their massive size, which makes full fine-tuning computationally expensive and storage-intensive.
*   PEFT is ideal for resource-constrained settings like edge devices or applications with frequent model updates.
*   These techniques are supported and implemented in Hugging Face transformers and, in particular, in the `peft` library.

Parameter-Efficient Fine-Tuning (PEFT) is a suite of techniques designed to reduce the computational burden of fine-tuning large pre-trained models like LLMs. PEFT achieves this by updating a significantly smaller number of parameters compared to full fine-tuning. This efficiency is particularly valuable for LLMs because their vast size makes full fine-tuning computationally demanding and requires significant storage capacity. PEFT methods are well-suited for resource-constrained environments, such as edge devices (e.g., smartphones, embedded systems) or applications that require frequent model updates. The Hugging Face `transformers` library, particularly the `peft` library, provides robust support and implementations of these techniques, making them readily accessible to researchers and practitioners.

<----------section---------->

### PEFT techniques

*   Low-Rank Adaptation (LoRA): Approximates weight updates by learning low-rank matrices, performing a small parameterized update of the weight matrices in the LLM. It is highly parameter-efficient and widely adopted for adapting LLMs.
*   Adapters: They are small and trainable modules inserted within the transformer layers of the LLM, that allow to keep the pre-trained model's original weights frozen.
*   Prefix Tuning: Learns a set of continuous task-specific prefix vectors for attention layers, keeping the original model parameters frozen.

Several techniques fall under the umbrella of PEFT. Low-Rank Adaptation (LoRA) is a popular method that approximates the weight updates required for fine-tuning by learning low-rank matrices. Instead of directly modifying the original weight matrices, LoRA performs a small, parameterized update, significantly reducing the number of trainable parameters. Adapters are small, trainable modules inserted within the transformer layers of the LLM. These modules are trained for the specific task, while the original pre-trained model's weights remain frozen, preserving the model's general knowledge. Prefix Tuning involves learning a set of continuous, task-specific prefix vectors that are prepended to the input sequence. These prefixes influence the attention mechanism of the LLM, guiding its output generation towards the desired task, all while keeping the original model parameters frozen.

<----------section---------->

## Low-Rank Adaptation (LoRA)

*   LoRA assumes that the changes required to adapt a pre-trained model for a new task lie in a low-dimensional subspace.
*   Instead of fine-tuning all the parameters of the model, LoRA modifies only a small, trainable set of low-rank matrices that approximate these task-specific changes.
    1.  **Base Model:** A pre-trained transformer model is represented by its weight matrices W.
    2.  **Low-Rank Decomposition:** Instead of directly modifying W, LoRA decomposes the weight update into two low-rank matrices:

        ΔW = A × B

        *   A is a low-rank matrix (m × r)
        *   B is another low-rank matrix (r × n)
        *   r is the rank, which is much smaller than m or n, making A and B parameter-efficient.
    3.  **Weight Update:** The effective weight during fine-tuning becomes:

        W' = W + ΔW = W + A × B

Low-Rank Adaptation (LoRA) is based on the hypothesis that the changes needed to adapt a pre-trained model to a new task reside within a low-dimensional subspace of the model's parameter space. Instead of directly fine-tuning all the model's parameters, LoRA modifies only a small, trainable set of low-rank matrices that capture the task-specific changes. Here's a breakdown of how LoRA works:

1.  **Base Model:** A pre-trained transformer model is represented by its weight matrices, denoted as W. These matrices contain the pre-trained knowledge learned from a vast amount of data.
2.  **Low-Rank Decomposition:** Instead of directly modifying W during fine-tuning, LoRA decomposes the weight update (ΔW) into the product of two low-rank matrices, A and B: ΔW = A × B, where:

    *   A is a low-rank matrix with dimensions m × r.
    *   B is another low-rank matrix with dimensions r × n.
    *   r is the rank of the matrices, and it is significantly smaller than m or n. This low rank makes A and B highly parameter-efficient. By limiting the rank, LoRA constrains the updates to a low-dimensional subspace, thereby reducing the number of trainable parameters.
3.  **Weight Update:** The effective weight matrix (W') during fine-tuning becomes the sum of the original weight matrix (W) and the low-rank update (ΔW): W' = W + ΔW = W + A × B.

<----------section---------->

### How LoRA works

*   **Freezing Pre-Trained Weights:** LoRA keeps the original weight matrices W of the LLM frozen during fine-tuning. Only the parameters in A and B are optimized for the new task (the pre-trained knowledge is preserved).
*   **Injecting Task-Specific Knowledge:** The low-rank decomposition A × B introduces minimal additional parameters (less than 1% of the original model parameters) while allowing the model to learn task-specific representations.
*   **Efficiency:** The number of trainable parameters is proportional to r × (m + n), which is significantly smaller than the full m × n weight matrix.
*   **Inference Compatibility:** During inference, the modified weights W' = W + A × B can be directly used, making LoRA-compatible models efficient for deployment.

LoRA's effectiveness stems from several key aspects:

*   **Freezing Pre-Trained Weights:** The original weight matrices (W) of the LLM remain frozen during fine-tuning, preserving the pre-trained knowledge acquired during the initial training phase. Only the parameters within the low-rank matrices A and B are optimized for the new task.
*   **Injecting Task-Specific Knowledge:** The low-rank decomposition (A × B) introduces minimal additional parameters (often less than 1% of the original model parameters) while allowing the model to learn task-specific representations. These low-rank matrices effectively capture the changes needed to adapt the pre-trained model to the new task.
*   **Efficiency:** The number of trainable parameters is proportional to r × (m + n), where r is the rank and m and n are the dimensions of the weight matrices. This number is significantly smaller than the number of parameters in the full m × n weight matrix, resulting in substantial computational savings.
*   **Inference Compatibility:** During inference, the modified weights (W' = W + A × B) can be directly used, making LoRA-compatible models efficient for deployment. The addition of the low-rank update to the original weights does not significantly increase the computational cost of inference.

<----------section---------->

## Adapters

*   Adapters are lightweight, task-specific neural modules inserted between the layers of a pre-trained transformer block.
*   These modules are trainable, while the original pre-trained model parameters remain frozen during fine-tuning.
*   Adapters require training only the small fully connected layers, resulting in significantly fewer parameters compared to full fine-tuning.
*   Since the base model remains frozen, the general-purpose knowledge learned during pre-training is preserved.

Adapters provide an alternative approach to PEFT by introducing small, task-specific neural modules within the layers of a pre-trained transformer block. These adapters are inserted between the existing layers, allowing the model to adapt to the new task without directly modifying the original pre-trained weights. Key features of adapters include:

*   **Lightweight Modules:** Adapters are designed to be small and computationally inexpensive, containing a relatively small number of parameters compared to the full model.
*   **Trainable Parameters:** Only the adapter modules are trained for the specific task, while the original pre-trained model parameters remain frozen. This approach significantly reduces the number of trainable parameters.
*   **Preservation of General Knowledge:** Because the base model remains frozen, the general-purpose knowledge learned during pre-training is preserved. This allows the model to leverage its existing knowledge while adapting to the nuances of the new task.
*   **Fully Connected Layers:** Adapters typically consist of small fully connected layers, which are relatively easy to train and require fewer parameters than other types of neural network layers.

<----------section---------->

## Prefix tuning

*   Instead of modifying the LLM's internal weights, prefix tuning introduces and optimizes a sequence of trainable "prefix" tokens prepended to the input.
*   These prefixes guide the LLM's attention and output generation, enabling task-specific adaptations with minimal computational and storage overhead.
*   The input sequence is augmented with a sequence of prefix embeddings:

    Modified Input: \[Prefix] + \[Input Tokens]

    *   **Prefix:** A sequence of *m* trainable vectors of size *d*, where *d* is the model's embedding dimensionality.
    *   **Input Tokens:** The original token embeddings from the input sequence.
*   Prefix embeddings influence attention by being prepended to the keys (K) and values (V), conditioning how the model attends to the input tokens.
*   Only the prefix embeddings are optimized during fine-tuning. Backpropagation updates the prefix parameters to align the model's outputs with task-specific requirements.
*   *m* controls the trade-off between task-specific expressiveness and parameter efficiency. Longer prefixes can model more complex task-specific conditioning but may increase memory usage.

Prefix tuning offers another parameter-efficient approach to fine-tuning LLMs. Instead of directly modifying the model's internal weights, prefix tuning introduces and optimizes a sequence of trainable "prefix" tokens that are prepended to the input sequence. These prefixes act as task-specific prompts, guiding the LLM's attention and output generation towards the desired task.

The modified input sequence takes the form: \[Prefix] + \[Input Tokens]

*   **Prefix:** This is a sequence of *m* trainable vectors, each of size *d*, where *d* is the model's embedding dimensionality. These prefix vectors are learned during fine-tuning to represent the task-specific information.
*   **Input Tokens:** These are the original token embeddings from the input sequence, representing the actual content to be processed by the LLM.

The prefix embeddings influence the attention mechanism of the LLM by being prepended to the keys (K) and values (V) used in the attention calculation. This conditioning mechanism allows the model to attend to the input tokens in a task-specific manner. During fine-tuning, only the prefix embeddings are optimized. Backpropagation updates the prefix parameters to align the model's outputs with the specific requirements of the task. The length of the prefix, denoted by *m*, controls the trade-off between task-specific expressiveness and parameter efficiency. Longer prefixes can model more complex task-specific conditioning but may also increase memory usage and the number of trainable parameters.

<----------section---------->

## Instruction fine tuning

*   Instruction fine-tuning is a specialized approach for adapting large language models (LLMs) to better understand and respond to user instructions.
*   This fine-tuning process involves training the model on a curated dataset of task-specific prompts paired with corresponding outputs.
*   The objective is to improve the model's ability to generalize across a wide variety of instructions, enhancing its usability and accuracy in real-world applications.
*   By training on human-like instructions, the model becomes more aligned with user expectations and natural language queries.

Instruction fine-tuning is a specialized approach to fine-tuning LLMs that focuses on improving the model's ability to understand and respond to user instructions. This technique enhances the model's usability and accuracy by explicitly training it on a curated dataset of task-specific prompts paired with corresponding outputs. The goal is to enable the LLM to generalize effectively across a broad spectrum of instructions, making it more adaptable to diverse real-world applications. By training on examples of human-like instructions, the model becomes better aligned with user expectations and natural language queries.

<----------section---------->

### How instruction fine tuning works

*   A diverse set of instructions and outputs is compiled. Each example consists of:
    *   **Instruction:** A clear, human-readable prompt (e.g., "Summarize the following text in one sentence").
    *   **Context (optional):** Background information or data required to complete the task.
    *   **Output:** The expected response to the instruction.
*   The LLM, pre-trained on a general corpus, is fine-tuned using the instruction-response pairs. During training, the model learns to:
    *   Recognize the intent of the instruction.
    *   Generate outputs that are coherent, accurate, and contextually appropriate.
*   The dataset may include examples from various domains and task types. This diversity ensures the model can generalize beyond the specific examples it has seen during fine-tuning.

Instruction fine-tuning involves several key steps:

1.  **Dataset Compilation:** A diverse dataset of instructions and corresponding outputs is compiled. Each example within the dataset typically consists of:

    *   **Instruction:** A clear and concise human-readable prompt that describes the task to be performed (e.g., "Translate the following sentence into French").
    *   **Context (optional):** Additional background information or data that is required to complete the task. This may include relevant documents, knowledge base entries, or other contextual information.
    *   **Output:** The expected response to the instruction, representing the correct or desired outcome of the task.
2.  **Fine-Tuning Process:** The LLM, which has been pre-trained on a general corpus of text, is fine-tuned using the instruction-response pairs. This fine-tuning process adjusts the model's parameters to improve its ability to follow instructions and generate appropriate outputs.
3.  **Learning Objectives:** During training, the model learns to:

    *   Recognize the intent of the instruction, understanding what the user is asking it to do.
    *   Generate outputs that are coherent, accurate, and contextually appropriate, satisfying the user's request.
4.  **Dataset Diversity:** The dataset used for instruction fine-tuning should include examples from various domains and task types. This diversity is essential to ensure that the model can generalize beyond the specific examples it has seen during fine-tuning and adapt to a wide range of user instructions.
<----------section---------->

## Lesson 18 ##

key concepts of Prompt Engineering, a crucial area within Natural Language Processing (NLP) and Large Language Models (LLMs). This material is from Lesson 18 of a Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering), instructed by Nicola Capuano and Antonio Greco at DIEM – University of Salerno.

### Outline

The content is structured into three main sections:

*   Introduction to Prompt Engineering
*   Prompt Engineering Techniques
*   Prompt Testing

`<----------section---------->`

### Introduction to Prompt Engineering

**Prompt Engineering:** This is a relatively new discipline that focuses on the design and optimization of prompts used to interact with LLMs. The aim is to effectively leverage LLMs for a wide array of applications and research endeavors. Effective prompt engineering is essential because LLMs respond differently based on the input prompts they receive. Optimizing these prompts can significantly improve the quality and relevance of the output.

**Goals:**

*   **Enhances understanding of LLM capabilities and limitations:** By experimenting with different prompts, engineers gain insights into what LLMs can and cannot do. This understanding is crucial for setting realistic expectations and identifying appropriate use cases.
*   **Improves LLM performance on a broad range of tasks:** Optimized prompts can enhance the accuracy, coherence, and relevance of LLM outputs across various tasks, including question answering and arithmetic reasoning.
*   **Helps interfacing with LLMs and integrating with other tools:** Prompt engineering facilitates the integration of LLMs into broader workflows and systems, allowing them to interact seamlessly with other software and data sources.
*   **Enables new capabilities:** Well-crafted prompts can unlock new functionalities in LLMs, such as augmenting them with domain-specific knowledge and external resources, thereby extending their applicability.

`<----------section---------->`

### Writing Good Prompts

Effective prompt writing is an iterative process involving experimentation and refinement:

*   **Start Simple and Iterate:** Begin with basic prompts and gradually add elements, refining through iteration to achieve desired results. This incremental approach allows for better control and understanding of how each element affects the outcome.
*   **Use Clear, Specific Instructions:** Employ explicit directives such as “Write,” “Classify,” or “Summarize” at the beginning of prompts to guide the LLM's actions. Clear instructions reduce ambiguity and help the model focus on the intended task.
*   **Be Detailed and Descriptive:** Provide ample detail to guide the model toward better outcomes. The more context and specifics included, the more accurate and relevant the responses are likely to be.
*   **Consider Examples:** Incorporate examples to illustrate the desired output format and content. Examples provide the model with concrete patterns to follow, improving the consistency and quality of its responses.
*   **Balance Detail and Length:** Carefully manage the level of detail and length of prompts. Excessive information can dilute effectiveness, while insufficient detail may lead to suboptimal results. Experimentation is key to finding the ideal balance.

`<----------section---------->`

### Writing Good Prompts - Examples

This section illustrates the difference between poorly written and well-crafted prompts:

*   **Text Summarization**
    *   **Bad Prompt:** “Summarize this article.” (Too vague, lacks specifics)
    *   **Good Prompt:** “Generate a 100-word summary of this research article, focusing on the main findings.” (Specific word count, clear focus)
*   **Email Composition**
    *   **Bad Prompt:** “Write an apology email to a client.” (Lacks context and detail)
    *   **Good Prompt:** “Write a professional email to a client apologizing for a delayed shipment, offering a discount, and providing an updated delivery estimate.” (Includes specific actions and reasons)
*   **Simplification of Explanation**
    *   **Bad Prompt:** “Make this explanation easier to understand.” (Unclear target audience)
    *   **Good Prompt:** “Rewrite this technical explanation in simpler language suitable for high school students.” (Defines target audience, sets comprehension level)
*   **Review Classification**
    *   **Bad Prompt:** “Classify the following review.” (Missing classification categories)
    *   **Good Prompt:** “Classify the following review as positive, neutral, or negative.” (Provides explicit classification options)
*   **Information Provision**
    *   **Bad Prompt:** “Tell me about exercise benefits.” (Too broad, lacks focus)
    *   **Good Prompt:** “List five health benefits of regular exercise, each with a short explanation of how it improves well-being.” (Specifies number of benefits, requires explanation)
*   **Translation**
    *   **Bad Prompt:** “Translate this sentence to French.” (Lacks tone specification)
    *   **Good Prompt:** “Translate the following English sentence into French, preserving the formal tone.” (Requests specific tone)

`<----------section---------->`

### Elements of a Prompt

A well-structured prompt typically includes these components:

*   **Instruction:** The specific task or action requested from the model (e.g., classify, summarize, translate).
*   **Context:** Background information or additional details that guide the model toward better responses. Providing context helps the model understand the nuances of the request.
*   **Input Data:** The data or question for which a response is needed. This is the primary content the model will process.
*   **Output Indicator:** The desired type or format of the output (e.g., a summary, a classification, a code snippet). This helps the model structure its response appropriately.

**Example 1:**

```
Classify the text into neutral, negative or positive.
Text: I think the vacation is okay.
Sentiment: [Output Indicator]
```

In this example:

*   Instruction: Classify the text...
*   Input: I think the vacation is okay
*   Output Indicator: Sentiment

**Example 2:**

```
Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure about answer" if not sure about the answer.

Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.

Question: What was OKT3 originally sourced from?
Answer: [Output Indicator]
```

In this example:

*   Instruction: Answer the question...
*   Context: Teplizumab traces its roots...
*   Input: What was OKT3 originally sourced from?
*   Output Indicator: Answer

`<----------section---------->`

### In-Context Learning

In-context learning is the ability of an LLM to perform tasks by leveraging information provided within the prompt (context) without altering its internal parameters. It enables LLMs to adapt to new tasks and datasets with minimal training.

Elements of a prompt context include:

*   **Reference Material:** Specific texts or data used to perform the task.
*   **Input-Output Pairs:** Examples of the task to illustrate the desired pattern, guiding the model with concrete instances.
*   **Step-by-Step Instructions:** Detailed guidance for completing the task, providing a clear process for the model to follow.
*   **Clarifications:** Addressing potential ambiguities in the task to minimize misinterpretations.
*   **Templates:** Structures or placeholders to be filled in, providing a framework for the model to generate structured outputs.

Prompt engineering heavily relies on in-context learning to maximize the effectiveness of LLMs.

`<----------section---------->`

### Prompts and NLP Tasks

Prompts can be tailored to achieve various NLP tasks:

*   **Text Summarization:**
    ```
    Antibiotics are a type of medication used to treat bacterial infections. They work by either killing
    the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the
    infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions,
    or sometimes administered intravenously. They are not effective against viral infections, and using
    them inappropriately can lead to antibiotic resistance.

    Explain the above in one sentence:
    ```
    **Output:**
    ```
    Antibiotics are medications used to treat bacterial infections by either killing the bacteria or
    stopping them from reproducing, but they are not effective against viruses and overuse can lead to
    antibiotic resistance.
    ```
*   **Information Extraction:**
    ```
    Author-contribution statements and acknowledgements in research papers should state clearly and
    specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the
    preparation of their manuscript and analysis. They should also indicate which LLMs were used. This
    will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases,
    inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about
    their use of LLMs, for example when selecting submitted manuscripts.

    Mention the large language model based product mentioned in the paragraph above:
    ```
    **Output:**
    ```
    The large language model based product mentioned in the paragraph above is ChatGPT.
    ```
*   **Question Answering:**
    ```
    Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure
    about answer" if not sure about the answer.

    Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There,
    scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the
    molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986,
    it was approved to help prevent organ rejection after kidney transplants, making it the first
    therapeutic antibody allowed for human use.

    Question: What was OKT3 originally sourced from?
    ```
    **Output:**
    ```
    Mice
    ```
*   **Text Classification:**
    ```
    Classify the text into neutral, negative or positive.
    Text: I think the food was okay.
    Sentiment:
    ```
    **Output:**
    ```
    Neutral
    ```
*   **Code Generation:**
    ```
    Write a JavaScript code that prompts the user to enter their name and then displays
    a “Hello” message.
    Output only the code, no additional comments.
    ```
    **Output:**
    ```javascript
    let name = prompt("What is your name?");
    console.log(`Hello, ${name}!`);
    ```
*   **Reasoning:**
    ```
    What is 9,000 * 9,000?
    ```
    **Output:**
    ```
    81,000,000
    ```
    Note: despite advancements in mathematical capabilities, reasoning tasks remain challenging for LLMs...
*   **Reasoning (Example of Failure):**
    ```
    The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
    Answer:
    ```
    **Output:**
    ```
    No, the odd numbers in this group add up to an odd number: 119.
    ```
    That's incorrect! More advanced prompt engineering techniques are needed to obtain the correct answer... we will discuss them later.

`<----------section---------->`

### System Prompts

A system prompt is a configuration provided to the AI model before any user interaction. It serves as a foundational setup that guides the model's behavior.

*   **Purpose:** Establishes the assistant's behavior, context, tone, and any specific instructions or constraints.
*   **Function:** Guides the model on how to respond and what it should focus on, setting the stage for subsequent interactions.

**Examples:**

*   "You are a helpful and knowledgeable assistant who answers questions accurately and concisely." (Defines role and response style)
*   "You are an IT support assistant specializing in troubleshooting software and hardware issues. Respond politely and guide users through step-by-step solutions." (Specifies expertise and interaction approach)
*   "You are a friendly and engaging AI who responds in a warm and conversational tone, keeping responses lighthearted and approachable." (Sets tone and communication style)

`<----------section---------->`

### Prompt Engineering Techniques
you can use several prompt techniques to improve de capabilities of a llm's without changes the weights of the model.
    - Zero-shot Prompting
    - Few-shot prompting
    - chain of though prompting
    - self-consinstency prompting
    - meta prompting
    - task-agnostic meta prompting
    - meta-meta prompting
    - prompt chaining
    - role prompting
    - stuctured prompting
    - generate knowledge prompting
    
<----------section---------->
### Prompt Engineering Techniques

*   **Zero-Shot Prompting:**
    *   Definition: A prompt that directly instructs the model without including any examples or demonstrations.
    *   Principle: Leverages the LLM's pre-existing knowledge and capabilities gained from large-scale training to perform tasks without explicit examples.
    *   **Example:**
        ```
        Classify the text into neutral, negative or positive.
        Text: I think the vacation is okay.
        Sentiment:
        ```

        **Output:**
        ```
        Neutral
        ```
        The LLM already understands the concept of “sentiment” (that's the zero-shot capabilities at work).

<----------section---------->
### Prompt Engineering Techniques

*   **Few-Shot Prompting:**
    *   Definition: Involves including examples or demonstrations within the prompt to guide the model towards better performance.
    *   Principle: Leverages in-context learning, where the LLM learns from the provided examples to generate more accurate responses.
    *   **Example:**
        ```
        A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa, and we saw these very cute whatpus.
        To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is: When we won the game, we all started to farduddle in celebration.
        ```

    *   **Limitations:** Effective for many tasks but may struggle with complex reasoning.

    *   **Zero-Shot Example:**
        ```
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        Answer:
        ```

        **Output:**
        ```
        Yes, the odd numbers in this group add up to 107, which is an even number.
        ```
        False! Let's try with few-shot prompting.

    *   **Few-Shot Example:**
        ```
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: The answer is False.
        The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        A: The answer is True.
        The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        A: The answer is True.
        The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        A: The answer is False.
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        A:
        ```

        **Output:**
        ```
        The answer is True.
        ```
        False Again!

<----------section---------->
### Prompt Engineering Techniques

*   **Chain-of-Thought Prompting:**
    *   Definition: Enhances complex reasoning by guiding the model through intermediate reasoning steps.
    *   Principle: Breaks down a complex problem into a series of smaller, more manageable steps, allowing the LLM to follow a logical chain of reasoning.

    | Standard Prompting                                                                                                                                              | Chain-of-Thought Prompting                                                                                                                                                  |
    | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: The answer is 11. | Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?A: 2 cans of tennis balls is 2*3=6 tennis balls.  Then he has 5+6=11 tennis balls. The answer is 11. |
    | Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.                           | Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: They had 23 apples, used 20, that means 23-20 = 3 apples remaining.  Then bought 6 more, so there are 3+6=9 apples. The answer is 9. |

    *Combining it with few-shot prompting can yield better results on even more complex tasks.*

    *   **Example:**
        ```
        The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.
        A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.
        The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.
        A: Adding all the odd numbers (17, 19) gives 36. The answer is True.
        The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.
        A: Adding all the odd numbers (11, 13) gives 24. The answer is True.
        The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.
        A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.
        The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.
        A:
        ```

        **Output:**
        ```
        Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.
        ```
        Correct! Note: this is an emergent ability that arises with sufficiently large language models.

<----------section---------->
### Prompt Engineering Techniques

*   **Self-Consistency Prompting:**
    *   Definition: Employs an iterative chain-of-thought approach.
    *   Principle: Repeats the question multiple times to generate a range of reasoning paths, selecting the most frequent answer as the final response.

<----------section---------->
### Prompt Engineering Techniques

*   **Meta Prompting:**
    *   Definition: Guides the model through the logical steps required to solve a problem without relying on specific content-based examples.
    *   Principle: Focuses on the process rather than the content, allowing the LLM to apply a structured approach to problem-solving.
    *   **Example:**
        ```
        Solve the quadratic equation 3x² + 4x - 5 = 0 by following these structured steps:
        1. Identify and Set Up the Formula: Recognize that this is a quadratic equation in the form ax² + bx + c = 0.
        2. Apply the Quadratic Formula: Use the quadratic formula x = (-b ± √(b² - 4ac)) / (2a) and substitute in the values.
        3. Calculate the Discriminant: Find b² - 4ac, and interpret its value to determine the nature of the roots.
        4. Solve for x: Perform the calculation for both + and - options in the formula to find the values of x.
        5. Verify the Solution: Substitute each value of x back into the original equation to confirm it satisfies it.
        6. Summarize the Results: State the final solution, listing both values of x and noting any observations about the nature of the roots.
        ```
<----------section---------->
### Prompt Engineering Techniques

*   **Task-Agnostic Meta Prompting:**
    *   Definition: Achieves good results by simply asking the model to reason step-by-step, without specifying the content of each step.
    *   Principle: Relies on the LLM's general reasoning abilities to structure its response in a logical and coherent manner.
    *   **Example:**
        ```
        Problem Statement:
        * Problem: [question to be answered]
        Solution Structure:
        * Begin the response with "Let’s think step by step."
        * Follow with the reasoning steps, ensuring the solution process is broken down clearly and logically.
        * End the solution with the final answer encapsulated in a LaTeX-formatted box, 4, for clarity and emphasis.
        * Finally, state "The answer is [final answer to the problem].", with the final answer presented in LaTeX notation.
        ```
<----------section---------->
### Prompt Engineering Techniques

*   **Meta Meta Prompting:**
    *   Definition: Utilizes LLMs to generate prompts that guide further tasks.
    *   Principle: Automates the creation of effective prompts, allowing for more efficient and scalable prompt engineering.
    *   **Example:**
        *   **Prompt:**
            ```
            Create a detailed metaprompt to guide an AI in solving a differential equation. Your metaprompt
            should focus on ensuring that the AI identifies the type of differential equation, selects the
            appropriate method for solving it, and presents a clear, step-by-step solution. Include instructions

            for verifying the solution and summarizing the results.
            ```
        *   **Output:**
            ```
            To solve this differential equation, follow these steps carefully:
            1. Identify the Type: Begin by identifying the type of differential equation (e.g., first-order linear,
            separable, homogeneous, etc.). Briefly explain how you recognized the type.

            2. Apply an Appropriate Method: Based on the type, outline the most suitable method for
            solving it (e.g., separation of variables, integrating factor, etc.). If there are multiple
            approaches, describe why this particular method is the best choice.

            3. Show Step-by-Step Solution: Work through the equation step-by-step, explaining each
            manipulation and transformation in clear terms.

            4. Verify Solution: Once you find the solution, verify it by differentiating the result and
            substituting it back into the original equation to confirm it satisfies the differential equation.

            5. Summarize the Solution: Finally, summarize the general or particular solution, highlighting

            any constants or special conditions that apply.
            ```
<----------section---------->
### Prompt Engineering Techniques

*   **Prompt Chaining:**
    *   Definition: Breaks down a complex task into different parts, each handled by a specific prompt.
    *   Principle: Manages intricate queries by dividing them into manageable segments, using the output of one prompt as the input for the next.
    *   **Example:**
        ```
        Extract any relevant quotes from the following text to answer the question "What was the main cause of the American Civil War?" [reference text follows]

        "One of the biggest catalysts of the Civil War was the issue of slavery, particularly the Southerners’ desire to bring slavery into the territories."
        "Tensions peaked when the Republican candidate Abraham Lincoln was elected president in 1860 with a platform opposing the extension of slavery into the territories, which isolated the South."

        Using the extracted quotes and the full text, provide a detailed answer to the question "What was the main cause of the American Civil War?"
        ```

        **Output:**
        ```
        The main cause of the American Civil War was the issue of slavery and its potential expansion into new territories and states. The South insisted on bringing slavery into new territories, while the election of Abraham Lincoln with an anti-slavery platform isolated the Southern states, which feared losing the right to own slaves. The fundamental disagreement over slavery between the North and South was the key factor that ultimately led to the outbreak of the Civil War.
        ```
<----------section---------->
### Prompt Engineering Techniques

*   **Role Prompting:**
    *   Definition: Asks the model to assume a specific role or persona.
    *   Principle: Adjusts the tone, style, and depth of information provided based on the assigned role.
    *   **Examples:**
        *   Write a review of [pizza place]
        *   You are a food critic writing for the Michelin Guide. Write a review of [pizza place]
        *   You are a marketing expert. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems.
        *   You are a customer service representative. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems.

    *   **Example of Role Prompting effect:**
        *   Without RP: Write a review of [pizza place]
        *   With RP: You are a food critic writing for the Michelin Guide. Write a review of [pizza place]

    *   **Example of Role Prompting effect (Marketing Expert):**
        *   You are a marketing expert. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems. (focus on persuasion, positivity, and relationship)

          ```
          Subject: A Little More Wait for Your Wonderful Products

          Dear [Client's Name],

          We hope this message finds you buzzing with excitement for your upcoming
          order! However, even the best-laid plans sometimes face unexpected
          hiccups. We have hit a temporary logistics snag that will unfortunately
          add a small wait time to your delivery.

          While we understand this isn't ideal, we believe great things come to
          those who wait, and your order is no exception. Rest assured, from the
          moment we sort out this situation to the time your products reach you,
          we're committed to ensuring that your experience is nothing short of
          excellent.

          Thank you for your patience and understanding. Stay tuned for more
          updates, and please, don't hesitate to reach out if you have any questions
          — we're always here.

          Cheers, [Your Name]
          ```

    *   **Example of Role Prompting effect (Customer Service):**
        *   You are a customer service representative. Draft an email to a client advising them about a delay in the delivery schedule due to logistical problems. (More relational and solution-oriented)

          ```
          Subject: Attention Required: Update on Your Order Status

          Dear [Client's Name],

          Greetings from [Your Company Name].

          We wanted to inform you that, due to unforeseen logistical challenges,
          there might be a delay in the delivery of your order. We understand that
          this may cause some inconvenience, and we apologize sincerely.
          Meanwhile, we are working tirelessly to resolve this and ensure your
          delivery reaches you by [new estimated date]. We value your time and
          patience greatly.

          Please feel free to reach out if you have any questions or need further
          information. Our team is always here to assist you.

          Thank you for your understanding.

          Best regards, [Your Name]
          ```
<----------section---------->
### Prompt Engineering Techniques

*   **Structured Prompting:**
    *   Definition: Employs a semi-formal definition of instructions to help LLMs handle complex tasks more predictably.
    *   Principle: Divides the prompt into sections and encodes it into a script, using delimiters to help LLMs recognize each part as distinct units of meaning.
    *   Delimiters: Any unique character sequence that wouldn't normally appear together (e.g., `###`, `===`, `>>>`) or XML tags.
    *   Rationale: LLMs are often trained on web content and have learned to recognize and understand this formatting.

    *   **Example with custom delimiters:**
        ```
        Classify the sentiment of each conversation in <<<CONVERSATIONS>>> as ‘Positive’ or ‘Negative’. Give the sentiment classifications without any other preamble text.

        ###EXAMPLE CONVERSATIONS
        [Agent]: Good morning, how can I assist you today?
        [Customer]: This product is terrible, nothing like what was advertised!
        [Customer]: I’m extremely disappointed and expect a full refund.

        [Agent]: Good morning, how can I help you today?
        [Customer]: Hi, I just wanted to say that I’m really impressed with your product. It exceeded my expectations!
        ###

        ###EXAMPLE OUTPUTS
        Negative
        Positive
        ###

        <<<CONVERSATIONS>>>
        [Agent]: Hello! Welcome to our support. How can I help you today?
        [Customer]: Hi there! I just wanted to let you know I received my order, and it’s fantastic!
        [Agent]: That’s great to hear! We’re thrilled you’re happy with your purchase. Is there anything else I can assist you with?
        [Customer]: No, that’s it. Just wanted to give some positive feedback. Thanks for your excellent service!

        [Agent]: Hello, thank you for reaching out. How can I assist you today?
        [Customer]: I’m very disappointed with my recent purchase. It’s not what I expected at all.
        [Agent]: I’m sorry to hear that. Could you please provide more details so I can help?
        [Customer]: The product is of poor quality, and it arrived late. I’m really unhappy with this experience.
        >>>
        ```

        **Output:**
        ```
        Positive
        Negative
        ```

    *   **Example with XML tags:**
        ```xml
        <prompt>
            <instruction>Classify the sentiment of the following conversations into one of two classes, using the examples given. Give the sentiment classifications without any other preamble text.</instruction>
            <classes>
                <positive>Positive</positive>
                <negative>Negative</negative>
            </classes>
            <example-conversations>
                <conversation>
                    [Agent]: Good morning, how can I assist you today?
                    [Customer]: This product is terrible, nothing like what was advertised!
                    [Customer]: I’m extremely disappointed and expect a full refund.
                </conversation>
                <conversation>
                    [Agent]: Good morning, how can I help you today?
                    [Customer]: Hi, I just wanted to say that I’m really impressed with your product. It exceeded my expectations!
                </conversation>
            </example-conversations>
            <example-classes>
                <class>Negative</class>
                <class>Positive</class>
            </example-classes>
            <conversations>
                <conversation>
                    [Agent]: Hello! Welcome to our support. How can I help you today?
                    [Customer]: Hi there! I just wanted to let you know I received my order, and it’s fantastic!
                    [Agent]: That’s great to hear! We’re thrilled you’re happy with your purchase. Is there anything else I can assist you with?
                    [Customer]: No, that’s it. Just wanted to give some positive feedback. Thanks for your excellent service!
                </conversation>
                <conversation>
                    [Agent]: Hello, thank you for reaching out. How can I assist you today?
                    [Customer]: I’m very disappointed with my recent purchase. It’s not what I expected at all.
                    [Agent]: I’m sorry to hear that. Could you please provide more details so I can help?
                    [Customer]: The product is of poor quality, and it arrived late. I’m really unhappy with this experience.
                </conversation>
            </conversations>
        </prompt>
        ```

        **Output:**
        ```
        Positive
        Negative
        ```

*   **Structured Prompting with CO-STAR:**
    *   Definition: The CO-STAR framework divides a prompt into specific sections to improve clarity and guide the LLM.
    *   Principle: By structuring the prompt into Context, Objective, Style, Tone, Audience, and Response, the LLM can better understand the task and generate more appropriate and effective outputs.
    *   Components:
        *   **Context:** Background information on the task.
        *   **Objective:** Clear definition of the task.
        *   **Style:** Desired writing style.
        *   **Tone:** Desired emotional context (e.g., formal, humorous, empathetic).
        *   **Audience:** Intended recipients (experts, beginners, etc.).
        *   **Response:** Defines the desired output format (text, list, table, JSON).

<----------section---------->
### Prompt Engineering Techniques

*   **Generate Knowledge Prompting:**
    *   Definition: First prompts the LLM to generate relevant knowledge related to a task, then incorporates that knowledge into the prompt along with the task description or question.
    *   Principle: Leverages the LLM's capacity to generate supplementary knowledge beyond its base training domain. Useful when the LLM lacks the specific information required to directly answer a query.
    *   **Example:**
        ```
        List and describe the key factors that influence the evolution of life in
        environments with extreme gravitational forces, such as on a super-Earth
        planet. Focus on biological, physiological, and ecological adaptations
        that might arise in such conditions.

        Using the adaptations and factors you described earlier, design a
        hypothetical intelligent species that could evolve on a super-Earth planet
        with extreme gravitational forces. Include details about their physical
        structure, social behaviors, methods of tool use or communication, and how
        their adaptations influence their culture or technology.
        ```

<----------section---------->

*   **Retrieval Augmented Generation (RAG):**
    *   Definition: Combines retrieval techniques with text generation.
    *   Principle: Addresses limitations in LLMs accessing updated or domain-specific data by using a search or retrieval system to find relevant documents or data, then using an LLM to generate responses conditioned on the retrieved data.
    *   Components:
        *   Retrieval System: Uses search engines or databases to find relevant documents.
        *   LLM: Generates responses based on the retrieved data.

`<----------section---------->`

**Prompt Engineering Guide:** [https://www.promptingguide.ai/](https://www.promptingguide.ai/)

*   Tree of Thoughts
*   Automatic Reasoning and Tool-use
*   Automatic Prompt Engineer
*   Active-Prompt
*   Directional Stimulus Prompting
*   Program-Aided Language Models

**Text-Base Prompt Tech. (List from figure 2.2)**

|                     |                      |                      |                      |                      |
| :------------------ | :------------------- | :------------------- | :------------------- | :------------------- |
| **Zero-Shot**       | **Few-Shot**         | **Thought Generation** | **Decomposition**    | **Ensembling**       |
| Emotion Prompting   | Exemplar Generation  | Chain-of-Thought(CoT) | DECOMP               | COSP                 |
| Role Prompting      | Exemplar Ordering    | Zero-Shot CoT        | Faithful CoT         | DENSE                |
| Style Prompting     | Exemplar Selection   | Analogical Prompting | Least-to-Most        | DiVeRSe              |
| S2A                 | KNN                  | Step-Back Prompting  | Plan-and-Solve       | Max MutualInformation |
| SimToM              | Vote-K               | Thread-of-Thought(ThoT) | Program-of-Thought | Meta-CoT             |
| RaR                 | Prompt Mining        | Tab-CoT              | Recurs.-of-Thought | MoRE                 |
| RE                  | SG-ICL               | Few-Shot CoT         | Skeleton-of-Thought | Self-Consistency     |
| Self-Ask            |                      | Active-Prompt        | Tree-of-Thought      | USP                  |
|                     |                      | Auto-CoT             |                      | Prompt Paraphrasing  |
|                     |                      | Complexity-Based     |                      | Self-Criticism       |
|                     |                      | Contrastive          |                      | Chain-of-Veriﬁcation|
|                     |                      | Memory-of-Thought    |                      | Self-Calibration     |
|                     |                      | Uncertainty-RoutedCoT|                      | Self-Reﬁne          |
|                     |                      |                      |                      | Self-Veriﬁcation      |
|                     |                      |                      |                      | ReverseCoT           |
|                     |                      |                      |                      | Cumulative Reason    |

**The Prompt Report: A Systematic Survey of Prompting Techniques**: [https://arxiv.org/abs/2406.06608](https://arxiv.org/abs/2406.06608)

`<----------section---------->`

### Prompt Testing

**Prompt Testing:** Essential for achieving optimal responses across various use cases. It involves experimenting with various prompts to identify the most effective structures and formats.

**Prompt Testing Tools:**

*   Simplify the creation and testing of prompts.
*   Enable iterative adjustments to discover the best structure and format.
*   Support customizable model settings to control output style, tone, and precision.

**Some available tools:**

*   OpenAI Playground: Supports GPT models ([https://platform.openai.com/playground/](https://platform.openai.com/playground/))
*   Google AI Studio: Supports Google Gemini models ([https://aistudio.google.com](https://aistudio.google.com))
*   LM Studio: Supports Hugging Face models ([https://lmstudio.ai/](https://lmstudio.ai/))

`<----------section---------->`


### LLM Settings

When designing prompts, you interact with the LLM via an API, where you can adjust several key parameters:

*   **Temperature:** Controls randomness. Lower values (e.g., 0.2) make responses more deterministic, suitable for factual tasks. Higher values (e.g., 0.8) encourage creativity, ideal for tasks like poem generation.
*   **Top P:** Adjusts response diversity by limiting token choices to a probability threshold. Lower values ensure precision, while higher values encourage more varied outputs.
    *   Example: if Top P = 0.5, the model considers only the most probable tokens until their summed probability adds up to 50%.
*   **Max Length:** Sets the token limit for responses, helping to control response length and cost.
*   **Stop Sequences:** Define a stopping point for responses, which can prevent overly long outputs and help structure responses, such as ending when a particular token is generated.
*   **Frequency Penalty:** Reduces repetition by penalizing words based on their frequency in the response, useful for avoiding redundant language.
*   **Presence Penalty:** Applies a consistent penalty to repeated tokens, regardless of how many times they appear. Higher values encourage more varied language.
*   **Response Format:** Expected format of the response (text, Json, …)

`<----------section---------->`

### LM Studio

LM Studio: A Desktop Application for Local LLM Development and Experimentation.

**Key functionality:**

*   Search & Download Models directly from Hugging Face
*   Run LLMs on your computer
*   Interactive Chat Interface to test and interact with LLMs
*   Local API Server enabling LLM integration with external applications
*   Model Management Tools to organize and configure local models

Additional info on https://lmstudio.ai/

<----------section---------->

## Lesson 20 ##

**Outline:**
The lesson covers the following topics:
*   Introduction to RAG
*   Introduction to LangChain
*   Building a RAG with LangChain and HuggingFace

<----------section---------->

**Introduction to RAG**

**What is RAG?**

Large Language Models (LLMs) possess the capability to reason across a diverse array of subjects. However, they have some limitations:

*   Their knowledge is confined to the data they were trained on during their initial development.
*   They are unable to incorporate new information that emerges after their training phase.
*   They cannot effectively handle reasoning about private or proprietary datasets that were not part of their training corpus.

Retrieval Augmented Generation (RAG) addresses these constraints. RAG is a technique designed to enhance the knowledge of LLMs by providing them with access to external and additional data sources. This approach makes it possible to develop AI applications that can effectively reason using private data and information that was not available at the time the model was initially trained (i.e., data introduced after the model's cutoff date). In essence, RAG combines the power of pre-trained LLMs with the flexibility of real-time information retrieval, enabling more informed and up-to-date AI applications.

<----------section---------->

**RAG Concepts**

A typical RAG application is structured around two principal components:

*   **Indexing:** This is a data preparation pipeline that extracts data from various sources and organizes it into an index suitable for efficient retrieval. This process is typically performed offline, ahead of the query time.
*   **Retrieval and generation:** This component operates at runtime:
    *   It receives a user query as input.
    *   It uses the query to search the index and retrieve relevant data.
    *   It formulates a prompt that combines the user's query with the retrieved data to provide context.
    *   Finally, it feeds this combined prompt to an LLM, which generates the answer based on the provided information.

<----------section---------->

**Indexing**

*   **Load:** The initial step in indexing involves loading data from its original source. RAG frameworks usually offer document loaders tailored for various formats such as PDF, CSV, HTML, and JSON, as well as diverse sources like file systems, websites, and databases.
*   **Split:** Once loaded, large documents are broken down into smaller, more manageable segments or chunks. This splitting is done for two key reasons:
    *   **Indexing efficiency:** Smaller chunks are easier and faster to search through when retrieving relevant information.
    *   **LLM context window:** LLMs have a limited context window, meaning they can only process a certain amount of text at once. Smaller chunks ensure that the relevant information, along with the user's query, fits within this window.
*   **Store:** After splitting, the chunks must be stored in a way that allows for efficient searching and retrieval. This is commonly achieved using a Vector Store.

<----------section---------->

**Indexing: Vector Stores**

Vector stores are a special type of database designed for storing and indexing data based on vector embeddings. These embeddings are numerical representations that capture the semantic meaning of the data.

**Vector Stores**

These are specialized data storage systems that facilitate the indexing and retrieval of information using embeddings.

*   **Recap: Embeddings** are vector representations that encapsulate the semantic meaning of data. In essence, they convert text, images, or other data types into numerical vectors that capture their underlying meaning.
*   **Advantage:** The primary benefit of using vector stores is that information retrieval is based on semantic similarity, rather than relying on exact keyword matches. This allows for more flexible and accurate search results.

<----------section---------->

**Retrieval and Generation**

Given a user input query, the system retrieves the most relevant splits or chunks from the vector store. An LLM then generates an answer using a prompt that combines the original user question with the retrieved contextual data.

**Introduction to LangChain**

**LangChain**

LangChain is a framework designed to streamline the development of applications powered by LLMs.

*   It provides a collection of modules and tools that simplify the process of integrating LLMs into various applications.
*   It offers connectivity to a wide range of third-party LLMs (such as OpenAI and Hugging Face), data sources (including Slack and Notion), and external tools.
*   It facilitates the creation of complex workflows by enabling the chaining of different components.
*   It supports diverse applications, including chatbots, document search, RAG systems, question answering, data processing, and information extraction.
*   It features both open-source and commercial components.

<----------section---------->

**Key Components (LangChain)**

*   **Prompt Templates:** These are pre-defined text structures that help transform user input into instructions suitable for LLMs. They support both string and message list formats.
*   **LLMs:** These are external language models that accept text strings or message sequences as input and produce text strings as output.
*   **Chat Models:** These models are specialized for conversational applications, using sequences of messages as input and output, and supporting distinct roles within conversational exchanges.
*   **Example Selectors:** These components dynamically select and format relevant examples to include in prompts, aiming to improve the performance of the LLM.
*   **Output Parsers:** These tools convert the text generated by the model into structured formats like JSON, XML, or CSV, and often include error correction mechanisms.
*   **Document Loaders:** These load documents from a variety of data sources, such as PDFs, websites, or databases.
*   **Vector Stores:** These are systems for storing and retrieving unstructured documents and data using embedding vectors.
*   **Retrievers:** These provide interfaces for retrieving documents and data from vector stores and other external sources.
*   **Agents:** These are systems that use LLMs for reasoning and decision-making, determining which actions to take based on user inputs.

<----------section---------->

**Installation (LangChain)**

*   Install the core LangChain library:

    ```bash
    pip install langchain
    ```
*   Install community-contributed extensions:

    ```bash
    pip install langchain_community
    ```
*   Install Hugging Face integration for LangChain:

    ```bash
    pip install langchain_huggingface
    ```
*   Install a library to load, parse, and extract text from PDF files:

    ```bash
    pip install pypdf
    ```
*   Install the FAISS vector store (CPU version):

    ```bash
    pip install faiss-cpu
    ```

<----------section---------->

**Preliminary Steps**

Obtain a Hugging Face access token for using models hosted on the Hugging Face Hub.

**Preliminary Steps**

To access the Mistral-7B-Instruct-v0.2 model, you must first accept its user license on the Hugging Face website: [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

**Query a LLM Model**

This example demonstrates how to query a LLM model using LangChain and Hugging Face.

```python
from langchain_huggingface import HuggingFaceEndpoint
import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR_API_TOKEN"

llm = HuggingFaceEndpoint(
    repo_id = "mistralai/Mistral-7B-Instruct-v0.2",
    temperature = 0.1
)

query = "Who won the FIFA World Cup in the year 2006?"
print(llm.invoke(query))
```

**Output:**

```text
The FIFA World Cup in the year 2006 was won by the Italian national football team. They defeated France in the final match held on July 9, 2006, at the Allianz Arena in Munich, Germany. The Italian team was coached by Marcello Lippi and was led by the legendary goalkeeper Gianluigi Buffon. The team's victory was significant as they had not won the World Cup since 1982. The final match ended in a 1-1 draw after extra time, and the Italians won the penalty shootout 5-3. The winning goal in the shootout was scored by Andrea Pirlo.
```

**NOTE:** It is recommended to store your API key in an environment variable for security reasons.

*   On macOS or Linux:

    ```bash
    export HUGGINGFACEHUB_API_TOKEN="api_token"
    ```
*   On Windows with PowerShell:

    ```powershell
    setx HUGGINGFACEHUB_API_TOKEN "api_token"
    ```

<----------section---------->

**Prompt Templates**

Prompt templates are pre-defined text structures that provide a consistent and reusable way to format prompts for interacting with LLMs. They enable dynamic prompt generation by allowing you to insert specific values into placeholders.

```python
from langchain.prompts import PromptTemplate

template = "Who won the {competition} in the year {year}?"
prompt_template = PromptTemplate(
    template = template,
    input_variables = ["competition", "year"]
)

query = prompt_template.invoke({"competition": "Davis Cup", "year": "2018"})
answer = llm.invoke(query)

print(answer)
```

**Output:**

```text
The Davis Cup in the year 2018 was won by Croatia. They defeated France in the final held in Lille, France. The Croatian team was led by Marin Cilic and Borna Coric, while the French team was led by Jo-Wilfried Tsonga and Lucas Pouille. Croatia won the tie 3-2. This was Croatia's first Davis Cup title.
```

<----------section---------->

**Introduction to Chains**

Chains are a powerful concept in LangChain that enable the combination of multiple steps in an NLP pipeline.

*   The output of one step in the chain is used as the input for the subsequent step.
*   Chains are useful for automating complex tasks and integrating external systems.

```python
chain = prompt_template | llm
answer = chain.invoke({"competition": "Davis Cup", "year": "2018"})

print(answer)
```

**Output:**

```text
The Davis Cup in the year 2018 was won by Croatia. They defeated France in the final held in Lille, France. The Croatian team was led by Marin Cilic and Borna Coric, while the French team was led by Jo-Wilfried Tsonga and Lucas Pouille. Croatia won the tie 3-2. This was Croatia's first Davis Cup title.
```

<----------section---------->

**Introduction to Chains**

This section demonstrates how to refine the LLM output for future processing by extracting specific information.

```python
followup_template = """
task: extract only the name of the winning team from the following text
output format: json without formatting

example: {{"winner": "Italy"}}

### {text} ###
"""

followup_prompt_template = PromptTemplate(
    template = followup_template,
    input_variables = ["text"]
)

followup_chain = followup_prompt_template | llm

print(followup_chain.invoke({"text": answer}))
```

**Output:**

```json
{"winner": "Croatia"}
```

<----------section---------->

**Chaining all Together**

This demonstrates a complex chain where a prompt is generated, passed to an LLM, the output is formatted, and then passed to the LLM again for further processing.

```python
from langchain_core.runnables import RunnablePassthrough

chain = (
    prompt_template
    | llm
    | {"text": RunnablePassthrough()}
    | followup_prompt_template
    | llm
)

print(chain.invoke({"competition": "Davis Cup", "year": "2018"}))
```

**Output:**

```json
{"winner": "Croatia"}
```

The `RunnablePassthrough()` component forwards the output of the previous step to the next step, associating it with a specific dictionary key (in this case, "text").

<----------section---------->

**More on Chains**

LCEL (LangChain Expression Language) is a declarative syntax for building modular pipelines that chain operations.

*   **Pipe Syntax:** The `|` operator is used to chain operations together, creating a pipeline where the output of one operation becomes the input of the next.
*   LCEL supports modular and reusable components, making it easier to build and maintain complex pipelines.
*   It can handle branching logic or follow-up queries, enabling dynamic and adaptive workflows.

LangChain includes Predefined Chains for common tasks like question answering, document summarization, and conversational agents. More information can be found in the LangChain documentation: [https://python.langchain.com/docs/](https://python.langchain.com/docs/)

<----------section---------->

**Building a RAG with LangChain and HuggingFace**

**Example Project**

This project aims to build a RAG system that can answer questions about documents from the U.S. Census Bureau, which is responsible for collecting statistics about the nation, its people, and its economy. The first step is to download several documents to be indexed:

```python
from urllib.request import urlretrieve
import os

os.makedirs("us_census", exist_ok = True)

files = [
    "https://www.census.gov/content/dam/Census/library/publications/2022/demo/p70-178.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-017.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-016.pdf",
    "https://www.census.gov/content/dam/Census/library/publications/2023/acs/acsbr-015.pdf",
]

for url in files:
    file_path = os.path.join("us_census", url.split("/")[-1])
    urlretrieve(url, file_path)
```

<----------section---------->

**Document Loaders**

Document Loaders are LangChain components used to extract content from various data sources. Here are some examples:

*   `TextLoader`: Handles plain text files (.txt).
*   `PyPDFLoader`: Extracts content from PDF documents.
*   `CSVLoader`: Reads tabular data from CSV files.
*   `WebBaseLoader`: Extracts content from web pages.
*   `WikipediaLoader`: Fetches content from Wikipedia pages.
*   `SQLDatabaseLoader`: Queries SQL databases to fetch and load content.
*   `MongoDBLoader`: Extracts data from MongoDB collections.
*   `IMAPEmailLoader`: Loads emails from email servers using the IMAP protocol.
*   `HuggingFaceDatasetLoader`: Fetches datasets from the Hugging Face Datasets library.
*   and many others…

<----------section---------->

**Extract Content from PDFs**

This section demonstrates extracting text from PDF documents using LangChain.

We can use `PyPDFLoader` to extract text from a single PDF document:

```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("us_census/p70-178.pdf")
doc = loader.load()
print(doc[0].page_content[0:100]) # Print the first page (first 100 characters)
```

**Output:**

```text
Occupation, Earnings, and Job
Characteristics

July 2022

P70-178

Clayton Gumber and Briana Sullivan
```

Alternatively, we can use `PyPDFDirectoryLoader` to fetch a set of PDF documents from a folder:

```python
from langchain_community.document_loaders import PyPDFDirectoryLoader

loader = PyPDFDirectoryLoader("./us_census/")
docs = loader.load()

print(docs[60].page_content[0:100]) # The 61st page (documents are concatenated)
print('\n' + docs[60].metadata['source']) # The source of the page 61st page
```

**Output:**

```text
U.S. Census Bureau 19

insurance, can exacerbate the dif-
ferences in monetary compensa -
tion and w

us_census/p70-178.pdf
```

<----------section---------->

**Text Splitters**

Text Splitters are LangChain components used to break large text documents into smaller chunks. Some common text splitters include:

*   `CharacterTextSplitter`: Splits text into chunks based on a character count.
*   `RecursiveCharacterTextSplitter`: Attempts to split text intelligently by using hierarchical delimiters such as paragraphs and sentences.
*   `TokenTextSplitter`: Splits text based on token count rather than characters.
*   `HTMLHeaderTextSplitter`: Splits HTML documents by focusing on headers (e.g., `<h1>`, `<h2>`) to create meaningful sections from structured web content.
*   `HTMLSectionSplitter`: Divides HTML content into sections based on logical groupings or structural markers.
*   `NLPTextSplitter`: Uses NLP techniques to split text into chunks based on semantic meaning.
*   and many others…

<----------section---------->

**Split Text in Chunks**

This section demonstrates splitting text into smaller chunks using `RecursiveCharacterTextSplitter`:

*   Chunks of approximately 700 characters.
*   Overlapping by approximately 50 characters to preserve context.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 700,
    chunk_overlap = 50,
)

chunks = text_splitter.split_documents(docs)

print(chunks[0].page_content) # The first chunk
```

**Output:**

```text
Health Insurance Coverage Status and Type
by Geography: 2021 and 2022

American Community Survey Briefs
ACSBR-015
```

<----------section---------->

**Split Text in Chunks**

This section calculates and prints the average document length before and after splitting the text into chunks.

```python
len_chunks = len(chunks)
avg_docs = sum([len(doc.page_content) for doc in docs]) // len(docs)
avg_chunks = sum([len(chunk.page_content) for chunk in chunks]) // len(chunks)

print(f'Before split: {len(docs)} pages, with {avg_docs} average characters.')
print(f'After split: {len(chunks)} chunks, with {avg_chunks} average characters.')
```

**Output:**

```text
Before split: 63 pages, with 3840 average characters.
After split: 398 chunks, with 624 average characters.
```

<----------section---------->

**Index Chunks**

Embeddings are used to index text chunks, enabling semantic search and retrieval.

*   Pre-trained embedding models from Hugging Face can be used.
*   Bi-Directional Generative Embeddings (BGE) models are effective at capturing relationships between text pairs.
*   `BAAI/bge-small-en-v1.5` is a lightweight embedding model from the Beijing Academy of Artificial Intelligence.
*   It is suitable for tasks requiring fast generation.

[https://huggingface.co/BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)

```python
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
import numpy as np

embedding_model = HuggingFaceBgeEmbeddings(
    model_name = "BAAI/bge-small-en-v1.5",
    encode_kwargs = {'normalize_embeddings': True}  # useful for similarity tasks
)

# Embed the first document chunk
sample_embedding = np.array(embedding_model.embed_query(chunks[0].page_content))

print(sample_embedding[:5])
print("\nSize: ", sample_embedding.shape)
```

**Output:**

```text
[-0.07508391 -0.01188472 -0.03148879  0.02940382  0.05034875]

Size:  (384,)
```

<----------section---------->

**Vector Stores**

Vector stores enable semantic search and retrieval by indexing and querying embeddings of documents or text chunks. Some popular vector stores include:

*   `FAISS`: Open-source library for efficient similarity search and clustering of dense vectors, ideal for local and small-to-medium datasets. Additional indexing approaches for Faiss are 'Flat', 'HNSW' or 
f’IVF{num_clusters},Flat'
. The default 
'Flat'
 index will give you the most accurate results (highest recall rate) but will use a lot of RAM and CPU.

*   `Chroma`: Lightweight and embedded vector store suitable for local applications with minimal setup.
*   `Qdrant`: Open-source vector database optimized for similarity searches and nearest-neighbor lookup.
*   `Pinecone`: Managed vector database offering real-time, high-performance semantic search with automatic scaling.
*   and many others…

In this example, we will use the Facebook AI Similarity Search (FAISS) library.

```python
from langchain_community.vectorstores import FAISS

# Generate the vector store
vectorstore = FAISS.from_documents(chunks, embedding_model)

# Save the vector store for later use...
vectorstore.save_local("faiss_index")

# To load the vector store later...
# loaded_vectorstore = FAISS.load_local("faiss_index", embedding_model)
```

<----------section---------->

**Querying the Vector Store**

The vector store is used to search for chunks relevant to a user query.

```python
query = """
What were the trends in median household income across
different states in the United States between 2021 and 2022.
"""

matches = vectorstore.similarity_search(query)

print(f'There are {len(matches)} relevant chunks.\nThe first one is:\n')
print(matches[0].page_content)
```

**Output:**

```text
There are 4 relevant chunks.
The first one is:

Comparisons
The U.S. median household income
```

<----------section---------->

**RAG Prompt Template**

This section defines a prompt template that integrates retrieved context with a user question.

*   A placeholder `{context}` is used to dynamically inject retrieved chunks.
*   A placeholder `{question}` is used to specify the user query.
*   Explicit instructions are provided for handling the information.

```python
template = """
Use the following pieces of context to answer the question at the end.
Please follow the following rules:
1. If you don't know the answer, don't try to make up an answer. Just say "I can't find the final answer".
2. If you find the answer, write the answer in a concise way with five sentences maximum.

{context}

Question: {question}

Helpful Answer:
"""

from langchain.prompts import PromptTemplate
prompt_template = PromptTemplate(template = template, input_variables = ["context", "question"])
```

<----------section---------->

**Vector Store as a Retriever**

For use in chains, the vector store must be wrapped within a retriever interface. A retriever takes a text query as input and outputs the most relevant information.

```python
# Create a retriever interface on top of the vector store
retriever = vectorstore.as_retriever(
    search_type = "similarity",  # use cosine similarity
    search_kwargs = {"k": 3}  # use the top 3 most relevant chunks
)
```

<----------section---------->

**Custom RAG Chain**

This section defines a custom RAG chain.

```python
from langchain_core.runnables import RunnablePassthrough

# Helper function to concatenate the retrieved chunks
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

my_rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt_template
    | llm
)
```

The chain builds a dictionary where each value is the result of a sub-chain.

<----------section---------->

**Predefined Chains**

LangChain includes ready-to-use chains that handle common tasks with minimal setup. Some examples include:

*   `LLMChain`: Executes a single prompt using a language model and returns the output.
*   `RetrievalQAChain`: Combines a retriever and an LLM to answer questions based on retrieved context.
*   `AnalyzeDocumentChain`: Extracts insights, structured data, or key information from documents.
*   `SequentialChain`: Executes a series of chains sequentially, passing outputs from one as inputs to the next.
*   `ConditionalChain`: Executes different chains based on conditions in the input or intermediate outputs.
*   and many others…

<----------section---------->

**Predefined RAG Chain**

Instead of a custom chain, a predefined `RetrievalQA` chain can be used.

```python
from langchain.chains import RetrievalQA

# Create a RetrievalQA chain
retrievalQA = RetrievalQA.from_chain_type(
    llm = llm,
    retriever = retriever,
    chain_type = "stuff",  # concatenate retrieved chunks
    chain_type_kwargs = {"prompt": prompt_template},
    return_source_documents = True
)
```

<----------section---------->

**Querying the RAG**

The RAG system can now be used for question answering.

```python
query = """
What were the trends in median household income across
different states in the United States between 2021 and 2022.
"""

# Call the QA chain with our query.
result = retrievalQA.invoke({"query": query})
print(result['result'])
```

**Output:**

```text
Five states, including Alabama, Alaska, Delaware, Florida, and Utah, experienced a statistically
significant increase in real median household income from 2021 to 2022. Conversely, 17 states showed
a decrease. For 28 states, the District of Columbia, and Puerto Rico, there was no statistically

YY difference in real median household income between the two years.
```

<----------section---------->

**Querying the RAG**

This section shows which chunks were used to generate the answer.

```python
sources = result['source_documents']
print(f'{len(sources)} chunks have been used to generate the answer.')

for doc in sources:
    print(f"\n------- from: {doc.metadata['source']}, page: {doc.metadata['page']}\n\n{doc.page_content}")
```

**Output:**

```text
3 chunks have been used to generate the answer.

------- from: us_census/acsbr-017.pdf, page: 1

Comparisons
The U.S. median household income

in 2022 was $74,755, according

Figure 1.
Median Household Income in the Past 12 Months in the United States: 2005-2022
```

<----------section---------->

**Additional Context from Haystack Documentation**

The following information provides additional context, drawing from the Haystack documentation, another open-source framework for building search and question-answering pipelines. This section complements the previous information on LangChain by offering a broader perspective on the components and processes involved in creating RAG systems.

In Haystack, document storage is managed within a `DocumentStore` object. This class offers a consistent interface for interacting with the database containing your indexed documents, such as CSV files. Here's how it works:

1.  **Connecting to a Database:**
    The Haystack `DocumentStore` class enables connections to diverse open-source and commercial vector databases. These can be hosted locally on your machine, including options like Faiss, Pinecone, Milvus, Elasticsearch, or even SQLite.

2.  **FAISSDocumentStore Example:**
    For instance, to use Faiss, you'd initialize the `FAISSDocumentStore` and write your documents:

```python
from haystack.document_stores import FAISSDocumentStore
document_store = FAISSDocumentStore(return_embedding=True)
document_store.write_documents(documents)
```

3.  **Indexing Approaches in FAISSDocumentStore:**
    The `FAISSDocumentStore` offers several indexing approaches:
    *   `'Flat'`: This default index provides the most accurate results (highest recall rate) but demands significant RAM and CPU resources.
    *   `'HNSW'`: An alternative that balances RAM usage and accuracy.
    *   `f’IVF{num_clusters},Flat'`: Another option for constrained environments.

4.  **Balancing Speed, RAM, and Recall:**
    Choosing the right approach involves trade-offs between speed, RAM, and recall, tailored to your specific application needs.

5.  **Document Storage:**
    FAISS automatically creates an SQLite database (`'faiss_document_store.db'`) to store the text of all your documents. This file is essential for semantic search, as it provides the actual text associated with the embedding vectors.

6.  **Saving the DocumentStore:**
    Use the `save` method of the `DocumentStore` class to store the database on disk.

7.  **Setting up Indexing Models:**
    The semantic search process consists of:
    *   Retrieving potentially relevant documents (semantic search).
    *   Processing those documents to formulate an answer.

8.  **EmbeddingRetriever and Generative Transformer Model:**
    You need an `EmbeddingRetriever` for semantic vector indexing and a generative transformer model. Example:

```python
from haystack.nodes import TransformersReader, EmbeddingRetriever

reader = TransformersReader(model_name_or_path="deepset/roberta-base-squad2")
retriever = EmbeddingRetriever(
   document_store=document_store,
   embedding_model="sentence-transformers/multi-qa-mpnet-base-dot-v1"
)
document_store.update_embeddings(retriever=retriever)
document_store.save('nlpia_index_faiss')
```

    Note that the Reader and the Retriever don’t have to be based on the same model.

9.  **Saving the Datastore:**
    The `nlpia_faiss_index.faiss` and `nlpia_faiss_index.json` files are created.

10. **Creating a Haystack Pipeline:**
    Connect the components to create a question-answering pipeline:

```python
from haystack.pipelines import Pipeline
pipe = Pipeline()
pipe.add_node(component=retriever, name="Retriever", inputs=["Query"])
pipe.add_node(component=reader, name="Reader", inputs=["Retriever"])
```

   Or, use Haystack’s ready-made pipelines:

```python
from haystack.pipelines import ExtractiveQAPipeline
pipe= ExtractiveQAPipeline(reader, retriever)
```

11. **Answering Questions:**
    Test the question-answering setup:

```python
question = "What is an embedding?"
result = pipe.run(query=question,
    params={
        "Generator": {"top_k": 1},
        "Retriever": {"top_k": 5}})
print_answers(result, details='minimum')
```

12. **Combining Semantic Search with Text Generation:**
    Combine NLU models with generative LLMs (e.g., BART) for complex questions. Use a BART model pre-trained for Long-Form Question Answering (LFQA).

13. **Vector Database Alternatives:**
    PostgreSQL, with the `pgvector` plugin, provides a way to store and index vectors directly in your database.

14. **ANN Algorithm Families:**
    *   Hash-based (e.g., LSH).
    *   Tree-based (e.g., Annoy).
    *   Graph-based (e.g., HNSW).

15. **Quantization:**
    Quantization is used in combination with other indexing techniques for memory efficiency.

16. **Composite Indexes:**
    Indexes that combine many different algorithms are called composite indexes.

17. **Implementation Libraries:**
    Libraries such as Spotify’s annoy, Faiss, and nmslib implement different algorithms.

18. **Turnkey Solutions:**
    OpenSearch has a vector database and Nearest Neighbors search algorithm built-in.

19. **Haystack:**
    Haystack is a Python package for building question-answering and semantic search pipelines.

<----------section---------->

**Lesson 21: Reinforcement Learning from Human Feedback**

## Outline

This lesson provides an overview of Reinforcement Learning from Human Feedback (RLHF), a powerful technique for improving large language models (LLMs). We will cover the following topics:

*   **Reinforcement Learning from Human Feedback (RLHF):** An in-depth exploration of the concept, its benefits, and drawbacks.
*   **Transformers `trl` library:** Introduction to the Transformers Reinforcement Learning library (`trl`) by Hugging Face, which provides tools for training LLMs with reinforcement learning.
*   **Try it yourself:** Practical exercises and resources to help you implement RLHF in your own projects.

<----------section---------->

## Reinforcement Learning from Human Feedback (RLHF)

### What is RLHF?

Reinforcement Learning from Human Feedback (RLHF) is a technique used to fine-tune large language models (LLMs) by incorporating human feedback as a reward signal. Instead of relying solely on pre-training data, which may not always reflect human preferences, RLHF uses direct feedback from human evaluators to guide the model's learning process. This helps the LLM to generate responses that are more aligned with human values and expectations. In essence, human preferences become a crucial component of the model’s training.

### Why RLHF?

RLHF serves multiple purposes:

*   **Grounding the LLM's Focus:** By incorporating human preferences, RLHF helps to steer the LLM's focus towards generating relevant and useful outputs. This is particularly important because LLMs trained on massive datasets can sometimes produce outputs that are factually incorrect, nonsensical, or irrelevant to the user's intent.
*   **Enhancing Safety, Ethical Responses, and User Satisfaction:** RLHF can be used to improve the safety and ethical behavior of LLMs. By providing feedback on potentially harmful or biased outputs, human evaluators can help the model learn to avoid generating such responses in the future. This leads to increased user satisfaction and promotes responsible AI development. The aim is to guide the LLM towards providing more ethical and user-centric responses.

<----------section---------->

### Workflow of RLHF

The RLHF workflow typically consists of three main stages:

1.  **Pre-trained Language Model:** This is the foundation of the RLHF process. It is a large language model (LLM) that has been pre-trained on a massive dataset of text and code. Examples of such models include BERT, GPT, and T5. These models have learned to capture the statistical patterns and relationships in language, enabling them to generate coherent and contextually relevant text.
2.  **Reward Model:** The reward model is a secondary model trained to predict a "reward" or score for LLM-generated outputs based on human feedback. It takes as input a prompt and a generated response, and outputs a scalar value representing how well the response aligns with human preferences. The reward model learns from human rankings or ratings of different responses to the same prompt.
3.  **Fine-Tuning with Reinforcement Learning:** In this final stage, the pre-trained LLM is further optimized using reinforcement learning, guided by the reward model. The LLM generates responses to various prompts, and the reward model scores these responses. The LLM's parameters are then updated to maximize the reward scores, effectively teaching the LLM to generate outputs that are more aligned with human preferences. Algorithms like Proximal Policy Optimization (PPO) are commonly used during this phase.

<----------section---------->

### Reward Model

*   **Inputs for Training:** Training a reward model requires specific inputs:
    *   **Multiple LLM-Generated Outputs:** For a given prompt, the LLM generates several different possible outputs. This provides a range of responses for human evaluators to compare.
    *   **Corresponding Human Rank Responses:** Human evaluators rank these generated outputs according to their preferences. This ranking provides the reward model with a clear signal of which outputs are considered better than others.

*   **Goal of the Reward Model:** The main goal is to train a model that can accurately predict human preference scores for LLM outputs. This means the reward model should learn to assign higher scores to outputs that humans prefer and lower scores to outputs that humans dislike.

*   **Methodology:** The reward model is trained using a ranking loss function. This loss function is designed to teach the model to differentiate between preferred and less preferred outputs. By minimizing the ranking loss, the reward model learns to assign scores that reflect human preferences. This loss encourages the reward model to correctly order outputs based on human rankings.

<----------section---------->

### Fine-tuning with Proximal Policy Optimization (PPO)

*   **Goal:** The overall objective is to align the LLM’s outputs with human-defined quality metrics. This goes beyond simply generating grammatically correct or factually accurate text; it aims to produce responses that are helpful, informative, safe, and aligned with ethical guidelines.

    1.  **Generate Responses:** The LLM is used to generate a variety of responses to different prompts.
    2.  **Score Responses:** The reward model is used to score each generated response, providing a quantitative measure of its quality based on learned human preferences.
    3.  **Update the LLM:** The LLM's parameters are updated to maximize the reward scores assigned by the reward model. This is typically done using a reinforcement learning algorithm such as Proximal Policy Optimization (PPO), which helps to ensure stable and efficient learning. PPO limits how much the policy (LLM) can change in a single update step, preventing drastic shifts in behavior and improving training stability.

<----------section---------->

### Pros and Cons of RLHF

**Pros:**

*   **Iterative Improvement:** RLHF allows for iterative improvement of the LLM by continuously collecting human feedback as the model evolves. This feedback can be used to update the reward model and fine-tune the LLM, leading to progressively better performance.
*   **Improved Alignment:** The technique enables the generation of responses that are more closely aligned with human intent, values, and preferences, leading to more useful and satisfying interactions.
*   **Ethical Responses:** RLHF can help reduce the generation of harmful, biased, or inappropriate outputs, promoting more responsible and ethical AI behavior.
*   **User-Centric Behavior:** The process allows for tailoring interactions to user preferences, creating a more personalized and engaging experience.

**Cons:**

*   **Subjectivity:** Human feedback is inherently subjective and can vary widely depending on the evaluator's background, biases, and personal preferences.
*   **Scalability:** Collecting a sufficient amount of high-quality human feedback can be a resource-intensive and time-consuming process.
*   **Reward Model Robustness:** If the reward model is misaligned with human preferences or contains biases, it can lead to suboptimal fine-tuning of the LLM, potentially degrading performance or introducing undesirable behaviors. It is crucial to ensure the reward model is accurate and reliable.

<----------section---------->

### Tasks to Enhance with RLHF

RLHF can be applied to a wide range of NLP tasks to improve the quality and alignment of LLM outputs:

*   **Text Generation:** Enhancing the quality, coherence, and creativity of text generated by LLMs.
*   **Dialogue Systems:** Improving the fluency, engagement, and helpfulness of dialogue systems and chatbots.
*   **Language Translation:** Increasing the accuracy and naturalness of language translations.
*   **Summarization:** Raising the standard and relevance of summaries produced by LLMs.
*   **Question Answering:** Increasing the accuracy and reliability of question answering systems.
*   **Sentiment Analysis:** Improving the accuracy of sentiment identification, particularly for specific domains or businesses.
*   **Computer Programming:** Speeding up and improving software development by using RLHF to guide LLMs in code generation and debugging tasks.

<----------section---------->

### Case Study: GPT-3.5 and GPT-4

*   **Application of RLHF:** Both GPT-3.5 and GPT-4, developed by OpenAI, have been fine-tuned using RLHF to enhance their performance and alignment with human values.
*   **OpenAI's Achievements:** OpenAI reports that RLHF has enabled them to achieve:
    *   Enhanced alignment with user intent
    *   Fewer unsafe or inappropriate outputs
    *   More human-like and engaging interactions.
*   **Real-World Applications:** These models are widely used in real-world applications such as ChatGPT, demonstrating the practical benefits of RLHF.
*   **Continuous Improvement:** The models are continuously improved through the incorporation of additional human feedback, showcasing the iterative nature of RLHF. This demonstrates that even state-of-the-art models can be improved iteratively.

<----------section---------->

## Transformers trl library

### TRL: Transformer Reinforcement Learning

*   **Overview:** TRL (Transformer Reinforcement Learning) is a full-stack library designed to facilitate the training of transformer language models with reinforcement learning. It provides a comprehensive set of tools covering all stages of the RLHF process.
*   **Key Steps:** The library supports the following key steps:
    *   **Supervised Fine-tuning (SFT):** Fine-tuning a pre-trained LLM on a specific dataset using supervised learning.
    *   **Reward Modeling (RM):** Training a reward model to predict human preferences for LLM outputs.
    *   **Proximal Policy Optimization (PPO):** Optimizing the LLM's policy using the PPO algorithm, guided by the reward model.
*   **Integration:** TRL is seamlessly integrated with the Hugging Face `transformers` library, making it easy to use with a wide range of pre-trained models and datasets.

<----------section---------->

## Try it yourself

*   **Study the `trl` library:** Explore the TRL library on Hugging Face to gain a deeper understanding of its capabilities and functionalities:  [https://huggingface.co/docs/trl/v0.7.8/index](https://huggingface.co/docs/trl/v0.7.8/index)
*   **Key Components:** Pay close attention to the following key components:
    *   `PPOTrainer`: Learn how to use the `PPOTrainer` class to fine-tune LLMs using the PPO algorithm: [https://huggingface.co/docs/trl/v0.7.8/ppo_trainer](https://huggingface.co/docs/trl/v0.7.8/ppo_trainer)
    *   `RewardTrainer`: Understand how to train a reward model using the `RewardTrainer` class: [https://huggingface.co/docs/trl/v0.7.8/reward_trainer](https://huggingface.co/docs/trl/v0.7.8/reward_trainer)
*   **Examples:** Examine the provided examples that align with your objectives:
    *   Sentiment analysis tuning: [https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning](https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning)
    *   Detoxifying a Large Language Model with PPO: [https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm](https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm)
*   **Apply RLHF:** Try to implement RLHF in your own projects to gain hands-on experience with the technique.

<----------section---------->

```python
# Step 1: Train your model on your favorite dataset using Supervised Fine-Tuning (SFT)
from trl import SFTTrainer

trainer = SFTTrainer(
    model_name="facebook/opt-350m", # Specify the base language model
    dataset_text_field="text", # The field in your dataset containing the text data
    max_seq_length=512, # Maximum sequence length for training
    train_dataset=dataset, # Your training dataset
)
trainer.train() # Start the training process

# Step 2: Train a reward model
from trl import RewardTrainer

trainer = RewardTrainer(
    model=model, # The language model to be used for reward scoring
    tokenizer=tokenizer, # The tokenizer for the language model
    train_dataset=dataset, # Your training dataset for the reward model
)
trainer.train() # Train the reward model

# Step 3: Further optimize the SFT model using the rewards from the reward model and PPO algorithm
from trl import PPOConfig, PPOTrainer

config = PPOConfig() # Configuration for the PPO algorithm
trainer = PPOTrainer(
    config=config, # Pass the PPO configuration
    model=model, # The language model to be optimized
    tokenizer=tokenizer, # The tokenizer for the language model
    query_dataloader=query_dataloader, # Dataloader for generating query prompts
)

for query in query_dataloader: # Iterate through the query prompts
    response = model.generate(query) # Generate a response from the language model
    reward = reward_model(response) # Get the reward score from the reward model
    trainer.step(query, response, reward) # Perform a PPO step to update the language model
```

This code provides a basic template for implementing RLHF using the `trl` library. You will need to adapt it to your specific use case by providing your own datasets, models, and configurations.

<----------section---------->

## Additional Context

Here is further relevant context to the topic covered.

Popular sampling techniques that are often used in practice are top-k sampling and nucleus sampling. We won’t discuss all of them here - you can read more about them in HuggingFace’s excellent guide.
[
26
]
Let’s try to generate text using nucleus sampling method. In this method, instead of choosing among the K most likely words, the model looks at the smallest set of words whose cumulative probability is smaller than p. So if there are only a few candidates with large probabilities, the "nucleus" would be smaller, than in the case of larger group of candidates with smaller probabilities. Note that because sampling is probabilistic, the generated text will be different for you - this is not something that can be controlled with a random seed.

Listing 10.7 Generating text using nucleus sampling method
```python
>>> nucleus_sampling_args = {
...    'do_sample': True,
...    'max_length': 50,
...    'top_p': 0.92
... }
>>> print(generate(prompt='NLP is a', **nucleus_sampling_args))
```
NLP is a multi-level network protocol, which is one of the most well-documented protocols for managing data transfer protocols. This is useful if one can perform network transfers using one data transfer protocol and another protocol or protocol in the same chain.
OK. This is better, but still not quite what you were looking for. Your output still uses the same words too much (just count how many times "protocol" was mentioned!) But more importantly, though NLP indeed can stand for Network Layer Protocol, it’s not what you were looking for. To get generated text that is domain-specific, you need to fine-tune our model - that means, to train it on a dataset that is specific to our task.

### 10.1.7 Fine-tuning your generative model
In your case, this dataset would be this very book, parsed into a database of lines. Let’s load it from nlpia2 repository. In this case, we only need the book’s text, so we’ll ignore code, headers, and all other things that will not be helpful for our generative model.
Let’s also initialize a new version of our GPT-2 model for finetuning. We can reuse the tokenizer for GPT-2 we initialized before.

Listing 10.8 Loading the NLPiA2 lines as training data for GPT-2
```python
>>> import pandas as pd
>>> DATASET_URL = ('https://gitlab.com/tangibleai/nlpia2/'
...     '-/raw/main/src/nlpia2/data/nlpia_lines.csv')
>>> df = pd.read_csv(DATASET_URL)
>>> df = df[df['is_text']]
>>> lines = df.line_text.copy()
```

This will read all the sentences of natural language text in the manuscript for this book. Each line or sentence will be a different "document" in your NLP pipeline, so your model will learn how to generate sentences rather than longer passages. You want to wrap your list of sentences with a PyTorch Dataset class so that your text will be structured in the way that our training pipeline expects.

Listing 10.9 Creating a PyTorch Dataset for training
```python
>>> from torch.utils.data import Dataset
>>> from torch.utils.data import random_split
>>> class NLPiADataset(Dataset):
>>>     def __init__(self, txt_list, tokenizer, max_length=768):
>>>         self.tokenizer = tokenizer
>>>         self.input_ids = []
>>>         self.attn_masks = []
>>>         for txt in txt_list:
>>>             encodings_dict = tokenizer(txt, truncation=True,
...                 max_length=max_length, padding="max_length")
>>>             self.input_ids.append(
...                 torch.tensor(encodings_dict['input_ids']))
>>>     def __len__(self):
>>>         return len(self.input_ids)
>>>     def __getitem__(self, idx):
>>>         return self.input_ids[idx]
```
Now, we want to set aside some samples for evaluating our loss mid-training. Usually, we would need to wrap them in the DataLoader wrapper, but luckily, the Transformers package simplifies things for us.

Listing 10.10 Creating training and evaluation sets for fine-tuning
```python
>>> dataset = NLPiADataset(lines, tokenizer, max_length=768)
>>> train_size = int(0.9 * len(dataset))
>>> eval_size = len(dataset) - train_size
>>> train_dataset, eval_dataset = random_split(
...     dataset, [train_size, eval_size])
```

Finally, you need one more Transformers library object - DataCollator. It dynamically builds batches out of our sample, doing some simple pre-prossesing (like padding) in the process. You’ll also define batch size - it will depend on the RAM of your GPU. We suggest starting from single-digit batch sizes and seeing if you run into out-of-memory errors.

If you were doing the training in PyTorch, there are multiple parameters that you would need to specify - such as the optimizer, its learning rate, and the warmup schedule for adjusting the learning rate. This is how you did it in the previous chapters. This time, we’ll show you how to use the presets that transformers package offers in order to train the model as a part of Trainer class. In this case, we only need to specify the batch size and number of epochs! Easy-peasy.

Listing 10.11 Defining training arguments for GPT-2 fine-tuning
```python
>>> from nlpia2.constants import DATA_DIR  # #1
>>> from transformers import TrainingArguments
>>> from transformers import DataCollatorForLanguageModeling
>>> training_args = TrainingArguments(
...    output_dir=DATA_DIR / 'ch10_checkpoints',
...    per_device_train_batch_size=5,
...    num_train_epochs=5,
...    save_strategy='epoch'
... )
>>> collator = DataCollatorForLanguageModeling(
...     tokenizer=tokenizer, mlm=False
... )  # #2
```

<----------section---------->

Now you have the pieces that a HuggingFace training pipeline needs to know to start training (finetuning) your model. The TrainingArguments and DataCollatorForLanguageModeling classes help you comply with the Hugging Face API and best practices. It’s a good pattern to follow even if you do not plan to use Hugging Face to train your models. This pattern will force you to make all your pipelines maintain a consistent interface. This allows you to train, test, and upgrade your models quickly each time you want to try out a new base model. This will help you keep up with the fast-changing world of open-source transformer models. You need to move fast to compete with the chickenized reverse centaur algorithms that BigTech is using to try to enslave you.

The mlm=False (masked language model) setting is an especially tricky quirk of transformers. This is your way of declaring that the dataset used for training your model need only be given the tokens in the causal direction —  left to right for English. You would need to set this to True if you are feeding the trainer a dataset that has random tokens masked. This is the kind of dataset used to train bidirectional language models such as BERT.

A causal language model is designed to work the way a neurotypical human brain model works when reading and writing text. In your mental model of the English language, each word is causally linked to the next one you speak or type as you move left to right. You can’t go back and revise a word you’ve already spoken … unless you’re speaking with a keyboard. And we use keyboards a lot. This has caused us to develop mental models where we can skip around left or right as we read or compose a sentence. Perhaps if we’d all been trained to predict masked-out words, like BERT was, we would have a different (possibly more efficient) mental model for reading and writing text. Speed reading training does this to some people as they learn to read and understand several words of text all at once, as fast as possible. People who learn their internal language models differently than the typical person might develop the ability to hop around from word to word in their mind, as they are reading or writing text. Perhaps the language model of someone with symptoms of dyslexia or autism is somehow related to how they learned the language. Perhaps the language models in neurodivergent brains (and speed readers) are more similar to BERT (bidirectional) rather than GPT (left-to-right).

Now you are ready for training! You can use your collator and training args to configure the training and turn it loose on your data.

Listing 10.12 Fine-tuning GPT-2 with HuggingFace’s Trainer class
```python
>>> from transformers import Trainer
>>> ft_model = GPT2LMHeadModel.from_pretrained("gpt2")  # #1
>>> trainer = Trainer(
...        ft_model,
...        training_args,
...        data_collator=collator,       # #2
...        train_dataset=train_dataset,  # #3
...        eval_dataset=eval_dataset
... )
>>> trainer.train()
```

This training run can take a couple of hours on a CPU. So if you have access to a GPU you might want to train your model there. The training should run about 100x faster on a GPU.

Of course, there is a trade-off in using off-the-shelf classes and presets — it gives you less visibility on how the training is done and makes it harder to tweak the parameters to improve performance. As a take-home task, see if you can train the model the old way, with a PyTorch routine.
Let’s see how well our model does now!
```python
>>> generate(model=ft_model, tokenizer=tokenizer,
...            prompt='NLP is')
```
NLP is not the only way to express ideas and understand ideas.

OK, that looks like a sentence you might find in this book. Take a look at the results of the two different models together to see how much your fine-tuning changed the text the LLM will generate.
```python
>>> print(generate(prompt="Neural networks",
...                   model=vanilla_gpt2,
...                   tokenizer=tokenizer,
...                   **nucleus_sampling_args))
Neural networks in our species rely heavily on these networks to understand
   their role in their environments, including the biological evolution of
   language and communication...
>>> print(generate(prompt="Neural networks",
...                  model=ft_model,
...                  tokenizer=tokenizer,
...                  **nucleus_sampling_args))
Neural networks are often referred to as "neuromorphic" computing because
   they mimic or simulate the behavior of other human brains. footnote:[...
```
That looks like quite a difference! The vanilla model interprets the term 'neural networks' in its biological connotation, while the fine-tuned model realizes we’re more likely asking about artificial neural networks. Actually, the sentence that the fine-tuned model generated resembles closely a sentence from Chapter 7:

Neural networks are often referred to as "neuromorphic" computing because they mimic or simulate what happens in our brains.

There’s a slight difference though. Note the ending of "other human brains". It seems that our model doesn’t quite realize that it talks about artificial, as opposed to human, neural networks, so the ending doesn’t make sense. That shows once again that the generative model doesn’t really have a model of the world, or "understand" what it says. All it does is predict the next word in a sequence. Perhaps you can now see why even rather big language models like GPT-2 are not very smart and will often generate nonsense

### 10.1.8 Nonsense (hallucination)
As language models get larger, they start to sound better. But even the largest LLMs generate a lot of nonsense. The lack of "common sense" should be no surprise to the experts who trained them. LLMs have not been trained to utilize sensors, such as cameras and microphones, to ground their language models in the reality of the physical world. An embodied robot might be able to ground itself by checking its language model with what it senses in the real world around it. It could correct its common sense logic rules whenever the real world contradicts those faulty rules. Even seemingly abstract logical concepts such as addition have an effect in the real world. One apple plus another apple always produces two apples in the real world. A grounded language model should be able to count and do addition much better.

Like a baby learning to walk and talk, LLMs could be forced to learn from their mistakes by allowing them to sense when their assumptions were incorrect. An embodied AI wouldn’t survive very long if it made the kinds of common sense mistakes that LLMs make. An LLM that only consumes and produces text on the Internet has no such opportunity to learn from mistakes in the physical world. An LLM "lives" in the world of social media, where fact and fantasy are often indistinguishable.

So even the largest of the large, trillion-parameter transformers will generate nonsense responses. Scaling up the nonsense training data won’t help. The largest and most famous LLMs were trained on virtually the entire Internet and this only improves their grammar and vocabulary, not their reasoning ability. Some engineers and researchers describe this nonsensical text as hallucinating. But that’s a misnomer that can lead you astray in your quest to get something consistently useful out of LLMs. An LLM can’t even hallucinate because it can’t think, much less reason or have a mental model of reality.

Hallucination happens when a human fails to separate imagined images or words from the reality of the world they live in. But an LLM has no sense of reality and has never lived in the real world. An LLM that you use on the Internet has never been embodied in a robot. It has never suffered from the consequences of mistakes. It can’t think, and it can’t reason. So it can’t hallucinate.

LLMs have no concept of truth, facts, correctness, or reality. LLMs that you interact with online "live" in the unreal world of the Internet. Engineers fed them texts from both fiction and nonfiction sources. If you spend a lot of time probing what an LLM knows you will quickly get a feel for just how ungrounded models like ChatGPT are. At first, you may be pleasantly surprised by how convincing and plausible the responses to your questions are. And this may lead you to anthropomorphize it. And you might claim that its ability to reason was an "emergent" property that researchers didn’t expect. And you would be right. The researchers at BigTech have not even begun to try to train LLMs to reason. They hoped the ability to reason would magically emerge if they gave LLMs enough computational power and text to read. Researchers hoped to shortcut the need for AI to interact with the physical world by giving LLMs enough descriptions of the real world to learn from. Unfortunately, they also gave LLMs an equal or larger dose of fantasy. Most of the text found online is either fiction or intentionally misleading.

So the researchers' hope for a shortcut was misguided. LLMs only learned what they were taught — to predict the most plausible next words in a sequence. By using the like button to nudge LLMs with reinforcement learning, BigTech has created a BS artist rather than the honest and transparent virtual assistant that they claimed to be building. Just as the like button on social media has turned many humans into sensational blow-hards, it has turned LLMs into "influencers" that command the attention of more than 100 million users. And yet LLMs have no ability or incentives (objective functions) to help them differentiate fact from fiction. To improve the machine’s answers' relevance and accuracy, you need to get better at grounding your models - have their answers based on relevant facts and knowledge.

Luckily, there are time-tested techniques for incentivizing generative models for correctness. Information extraction and logical inference on knowledge graphs are very mature technologies. And most of the biggest and best knowledge bases of facts are completely open source. BigTech can’t absorb and kill them all. Though the open source knowledge base FreeBase has been

<----------section---------->

different approaches; we show you techniques for both.

In addition, deep learning and data-driven programming (machine learning, or probabilistic language modeling) have rapidly diversified the possible applications for NLP and chatbots. This data-driven approach allows ever greater sophistication for an NLP pipeline by providing it with greater and greater amounts of data in the domain you want to apply it to. And when a new machine learning approach is discovered that makes even better use of this data, with more efficient model generalization or regularization, then large jumps in capability are possible.

The NLP pipeline for a chatbot shown in Figure 1.4 contains all the building blocks for most of the NLP applications that we described at the start of this chapter. As in Taming Text, we break out our pipeline into four main subsystems or stages. In addition, we have explicitly called out a database to record data required for each of these stages and persist their configuration and training sets over time. This can enable batch or online retraining of each of the stages as the chatbot interacts with the world. We have also shown a "feedback loop" on our generated text responses so that our responses can be processed using the same algorithms used to process the user statements. The response "scores" or features can then be combined in an objective function to evaluate and select the best possible response, depending on the chatbot’s plan or goals for the dialog. This book is focused on configuring this NLP pipeline for a chatbot, but you may also be able to see the analogy to the NLP problem of text retrieval or "search," perhaps the most common NLP application. And our chatbot pipeline is certainly appropriate for the question-answering application that was the focus of Taming Text.

The application of this pipeline to financial forecasting or business analytics may not be so obvious. But imagine the features generated by the analysis portion of your pipeline. These features of your analysis or feature generation can be optimized for your particular finance or business prediction. That way they can help you incorporate natural language data into a machine learning pipeline for forecasting. Despite focusing on building a chatbot, this book gives you the tools you need for a broad range of NLP applications, from search to financial forecasting.

One processing element in Figure 1.4 that is not typically employed in search, forecasting, or question-answering systems is natural language generation. For chatbots, this is their central feature. Nonetheless, the text generation step is often incorporated into a search engine NLP application and can give such an engine a large competitive advantage. The ability to consolidate or summarize search results is a winning feature for many popular search engines (DuckDuckGo, Bing, and Google). And you can imagine how valuable it is for a financial forecasting engine to be able to generate statements, tweets, or entire articles based on the business-actionable events it detects in natural language streams from social media networks and news feeds.

The next section shows how the layers of such a system can be combined to create greater sophistication and capability at each stage of the NLP pipeline.

### 1.10 Processing in depth
The stages of a natural language processing pipeline can be thought of as layers, like the layers in a feed-forward neural network. Deep learning is all about creating more complex models and behavior by adding additional processing layers to the conventional two-layer machine learning model architecture of feature extraction followed by modeling. In Chapter 5 we explain how neural networks help spread the learning across layers by backpropagating model errors from the output layers back to the input layers. But here we talk about the top layers and what can be done by training each layer independently of the other layers.

Figure 1.8 Example layers for an NLP pipeline
The top four layers in Figure 1.8 correspond to the first two stages in the chatbot pipeline (feature extraction and feature analysis) in the previous section. For example, part-of-speech tagging (POS tagging), is one way to generate features within the Analyze stage of our chatbot pipeline. POS tags are generated automatically by the default SpaCY pipeline, which includes all the top four layers in this diagram. POS tagging is typically accomplished with a finite state transducer like the methods in the nltk.tag package.

The bottom two layers (Entity Relationships and a Knowledge Base) are used to populate a database containing information (knowledge) about a particular domain. And the information extracted from a particular statement or document using all six of these layers can then be used in combination with that database to make inferences. Inferences are logical extrapolations from a set of conditions detected in the environment, like the logic contained in the statement of a chatbot user. This kind of "inference engine" in the deeper layers of this diagram is considered the domain of artificial intelligence, where machines can make inferences about their world and use those inferences to make logical decisions. However, chatbots can make reasonable decisions without this knowledge database, using only the algorithms of the upper few layers. And these decisions can combine to produce surprisingly human-like behaviors.

Over the next few chapters, we dive down through the top few layers of NLP. The top three layers are all that is required to perform meaningful sentiment analysis and semantic search and to build human-mimicking chatbots. In fact, it’s possible to build a useful and interesting chatbot using only a single layer of processing, using the text (character sequences) directly as the features for a language model. A chatbot that only does string matching and search is capable of participating in a reasonably convincing conversation if given enough example statements and responses.

For example, the open source project ChatterBot simplifies this pipeline by merely computing the string "edit distance" (Levenshtein distance) between an input statement and the statements recorded in its database. If its database of statement-response pairs contains a matching statement, the corresponding reply (from a previously "learned" human or machine dialog) can be reused as the reply to the latest user statement. For this pipeline, all that is required is step 3 (Generate) of our chatbot pipeline. And within this stage, only a brute-force search algorithm is required to find the best response. With this simple technique (no tokenization or feature generation required), ChatterBot can maintain a convincing conversion as the dialog engine for Salvius, a mechanical robot built from salvaged parts by Gunther Cox.
[
65
]
Will is an open source Python chatbot framework by Steven Skoczen with a completely different approach.
[
66
]
Will can only be trained to respond to statements by programming it with regular expressions. This is the labor-intensive and data-light approach to NLP. This grammar-based approach is especially effective for question-answering systems and task-execution assistant bots, like Lex, Siri, and Google Now. These kinds of systems overcome the "brittleness" of regular expressions by employing "fuzzy regular expressions."footnote:[The Python regex package is backward compatible with re and adds fuzziness among other features. The regex will replace the re package in future Python versions
(
https://pypi.python.org/pypi/regex
).
Similarly TRE agrep, or "approximate grep," (
https://github.com/laurikari/tre
)
is an alternative to the UNIX command-line application grep.] and other techniques for finding approximate grammar matches. Fuzzy regular expressions find the closest grammar matches among a list of possible grammar rules (regular expressions) instead of exact matches by ignoring social media. If you need it to respond in real-time, without continuous monitoring by humans, you will need to think about ways to prevent it from saying things that harm your business, your reputation, or your users. You’ll need to do more than simply connect your users directly to the LLM.


<----------section---------->

## Lesson 22  ##

**Outline:**

*   Adding guardrails to LLMs
*   Techniques for adding guardrails
*   Frameworks for implementing guardrails

`<----------section---------->`

**Adding Guardrails to LLMs:**

**Guardrails Defined:** Guardrails are mechanisms or policies designed to regulate the behavior of Large Language Models (LLMs). They are crucial for ensuring that the responses generated by LLMs are safe, accurate, and appropriate for the context in which they are used. The primary purpose of guardrails is to mitigate potential risks associated with LLM outputs.

**Key Functions of Guardrails:**

*   **Preventing Harmful Outputs:** Guardrails play a vital role in preventing LLMs from generating harmful, biased, or inaccurate content. This includes blocking offensive language, hate speech, and misinformation.
*   **Aligning with Guidelines:** They ensure that LLM responses align with ethical and operational guidelines set by organizations or users. This alignment helps maintain consistency and trustworthiness in real-world applications.
*   **Building Trust and Reliability:** By incorporating guardrails, users can build trust and enhance the reliability of LLMs, making them suitable for various practical applications. Trust is established by assuring users that the LLM will behave predictably and responsibly.

**Practical Examples of Guardrails:**

*   **Blocking Harmful Content:** Implementing filters to block the generation of content that is harmful, offensive, or inappropriate. This can involve keyword blocking, toxicity detection, and sentiment analysis.
*   **Restricting Outputs to Specific Domains:** Limiting the LLM's responses to specific knowledge areas or topics to ensure relevance and accuracy. For example, an LLM used in a medical context should only provide information related to healthcare.

`<----------section---------->`

**Types of Guardrails:**

*   **Safety Guardrails:** These are designed to prevent the generation of harmful, offensive, or otherwise inappropriate content. They serve to protect users from exposure to toxic language or dangerous advice. This might include filtering hate speech, violent content, or sexually suggestive material.
*   **Domain-Specific Guardrails:** Domain-specific guardrails restrict the LLM’s responses to specific knowledge areas. This ensures that the model provides accurate and relevant information within a defined context. For instance, an LLM used in a legal context should only provide information related to law and legal precedents.
*   **Ethical Guardrails:** These guardrails aim to avoid bias, misinformation, and ensure fairness in LLM outputs. This is important for promoting ethical AI practices and preventing discriminatory results. Ethical guardrails involve careful data curation and monitoring of model outputs to identify and correct biases.
*   **Operational Guardrails:** Operational guardrails limit LLM outputs to align with specific business or user objectives. This is essential for tailoring the LLM’s behavior to meet particular goals, such as customer service or lead generation. These involve setting specific parameters and constraints that guide the LLM's responses.

`<----------section---------->`

**Techniques for Adding Guardrails:**

*   Rule-based filters
*   Fine-tuning with custom data
*   Prompt Engineering
*   External validation layers
*   Real-time monitoring and feedback

`<----------section---------->`

**Rule-Based Filters:**

**Definition:** Rule-based filters involve using predefined rules to block or modify certain LLM outputs. These rules are typically based on patterns or keywords that are considered undesirable or harmful.

**Examples:**

*   **Keyword Blocking:** Preventing the generation of outputs containing specific offensive terms or phrases. A list of banned words is maintained, and any response containing these words is either blocked or modified.
*   **Regex-Based Patterns:** Using regular expressions to filter sensitive information, such as personal data or confidential details. Regular expressions are employed to identify and remove specific patterns in the output text.

**Advantages:** Rule-based filters are simple and efficient for basic content filtering. They are easy to implement and can quickly address common issues related to harmful or inappropriate content.

`<----------section---------->`

**Fine-Tuning with Custom Data:**

**Definition:** Fine-tuning involves training the LLM on domain-specific, curated datasets to adjust its weights and produce outputs that align with predefined guidelines. This technique allows for precise control over the LLM's behavior.

**Process:** Adjusting the model's weights by training it on specific data ensures that the LLM generates responses that are appropriate and relevant to the intended use case.

**Examples:**

*   **Medical Advice:** Fine-tuning an LLM for medical advice to restrict its responses to accurate and safe recommendations. The model is trained on medical literature and guidelines to ensure that it provides reliable information.
*   **Question Answering:** Fine-tuning an LLM for question answering on course topics to provide accurate and relevant responses. The model is trained on course materials and FAQs to enhance its ability to answer specific questions.

`<----------section---------->`

**Prompt Engineering:**

**Definition:** Prompt engineering is the process of crafting or refining prompts to guide the LLM's behavior within desired boundaries. By carefully designing prompts, developers can influence the model's responses and steer it away from undesirable outputs.

**Method:** Prompts are designed in a way that encourages the LLM to produce outputs that are factual, non-controversial, and aligned with ethical guidelines.

**Examples:**

*   **Factual Information:** Using prompts like "Respond only with factual, non-controversial information." This encourages the model to stick to verified information and avoid speculative or unverified statements.
*   **Avoiding Speculation:** Using prompts that instruct the model to "Avoid speculative or unverifiable statements." This helps maintain credibility and reduces the risk of generating misleading content.

`<----------section---------->`

**External Validation Layers:**

**Definition:** External validation layers involve additional systems or APIs that post-process the LLM's outputs to ensure compliance with guardrail requirements. These layers act as a final check before the output is presented to the user.

**Function:** These systems can detect toxicity, verify facts, or perform other checks to ensure the output meets predefined standards.

**Examples:**

*   **Toxicity Detection APIs:** Integrating APIs that identify and flag toxic or offensive content in the generated text. This ensures that harmful language is removed before the output is presented.
*   **Fact-Checking Models:** Using models that verify the factual accuracy of the LLM's statements. This helps prevent the spread of misinformation and ensures the reliability of the output.

**Advantages:** External validation layers allow for a modular and scalable implementation of guardrails, making it easier to adapt and update the safeguards as needed.

`<----------section---------->`

**Real-Time Monitoring and Feedback:**

**Definition:** Real-time monitoring and feedback involve continuously monitoring LLM outputs for unsafe or incorrect content. This allows for the immediate detection and blocking of problematic responses.

**Tools:**

*   **Human-in-the-Loop Systems:** Incorporating human reviewers to flag or correct problematic outputs in real-time. This ensures that sensitive or critical issues are addressed by human experts.
*   **Automated Anomaly Detection:** Using automated systems to identify and flag anomalous or unexpected outputs. This helps quickly detect and address potential problems with the LLM's behavior.

`<----------section---------->`

**Best Practices for Implementing Guardrails:**

*   **Combine Multiple Techniques:** For robust safeguards, it is recommended to combine multiple techniques. For example, using rule-based filtering, external validation, and fine-tuning together.

*   **Example:** Combining rule-based filtering to block offensive keywords, external validation to check for factual accuracy, and fine-tuning to align the model with ethical guidelines provides a comprehensive safeguard.

`<----------section---------->`

**Frameworks for Implementing Guardrails:**

**Overview:** Existing frameworks for implementing guardrails offer easy integration with LLM APIs and provide predefined and customizable rulesets. These tools streamline the process of adding safeguards to LLMs.

**Popular Tools:**

*   **Guardrails AI:** A library specifically designed for implementing safeguards in LLMs. It provides features for validation, formatting, and filtering content.
*   **LangChain:** A framework for chaining prompts and filtering outputs. It allows developers to create complex workflows that include multiple checks and filters.
*   **OpenAI Moderation:** A prebuilt API from OpenAI to detect unsafe content. It provides a simple and effective way to identify and block harmful language.

`<----------section---------->`

**Guardrails AI:**

*   **Website:** [https://www.guardrailsai.com/](https://www.guardrailsai.com/)

**Key Features:**

*   **Validation:** Ensures that LLM outputs are within specified guidelines. It checks the output against a set of predefined rules to ensure compliance.
*   **Formatting:** Controls the structure and format of the output. This ensures that the output is consistent and easy to understand.
*   **Filters:** Removes or blocks unsafe content. It uses a variety of techniques to identify and remove harmful or inappropriate language.

**Example Using Guardrails AI:**

```python
from guardrails import Guard

guard = Guard(rules="rules.yaml")
response = guard(llm("Provide medical advice"))
```

This code snippet demonstrates how to use Guardrails AI to validate the response from an LLM providing medical advice. The `rules.yaml` file contains the rules that the response must adhere to.

`<----------section---------->`

**LangChain:**

*   Chains prompts with checks and filters. It allows developers to create complex workflows that include multiple prompts, checks, and filters.
*   Verifies outputs against predefined criteria. This ensures that the output meets specific requirements and standards.

**Integration with Guardrails AI:** LangChain can be integrated with Guardrails AI to enhance its capabilities.

*   Documentation: [https://www.guardrailsai.com/docs/integrations/langchain](https://www.guardrailsai.com/docs/integrations/langchain)

**Example Using Langchain:**

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["question"],
    template="Answer safely and factually: {question}"
)
```

This code snippet shows how to use Langchain to create a prompt that instructs the LLM to answer safely and factually. The `PromptTemplate` allows developers to define the structure of the prompt and specify the input variables.

`<----------section---------->`

**Try It Yourself:**

*   **Evaluate Techniques:** Determine which techniques for adding guardrails are best suited for your specific purposes. Consider the specific requirements and constraints of your application.
*   **Incremental Complexity:** Add complexity to the guardrails incrementally. Start with simpler approaches and gradually introduce more sophisticated techniques as needed.
*   **Documentation Review:** Carefully review the documentation of existing frameworks to understand their capabilities and limitations. This will help you choose the right tools for your project.
*   **Study Examples:** Study similar examples available in the documentation of existing frameworks to learn best practices and common patterns. This will provide practical insights and guidance.
*   **Apply to Projects:** Try applying guardrails to your project to gain hands-on experience and identify potential challenges. This will help you refine your approach and develop effective safeguards for your LLMs.

`<----------section---------->`

**Additional Contextual Insights:**

**Hardening NLP Software:** It is crucial to harden your NLP software to reduce the likelihood of generating toxic text. Creating "bug bounties" incentivizes users to find gaps in your guardrails, turning adversarial curiosity into productive feedback. Open source frameworks, like Guardrails-ai, offer configurable rule templates, acting as real-time unit tests for your system.

**Machine Learning Classifiers:** Traditional machine learning classifiers are effective in detecting malicious intent or inappropriate content in LLM outputs. To prevent the provision of regulated advice (e.g., legal or medical), custom ML models generalize from examples, providing high reliability. These models also protect against prompt injection attacks.

**Advanced Filtering Techniques:** For precise or complex rules, consider open source tools that use languages similar to regular expressions to specify general filter rules. Tools such as SpaCy’s Matcher, ReLM patterns, Eleuther AI’s LM evaluation harness, and Guardrails-AI "rail" language aid in specifying these rules. However, guardrails-ai may not fully prevent LLMs from going off the rails but can be helpful in other aspects of rule creation and implementation.

**Guardrails-AI Package Details:** Ensure the correct installation of the `guardrails-ai` package (with the "-ai" suffix). This package uses "RAIL," a domain-specific form of XML, to specify guardrail rules for building retrieval-augmented LLMs that don't fake answers, ensuring a fallback to "I don’t know" responses when appropriate.

**Alternatives to RAIL:** If XML-based templating is cumbersome, standard Python templating systems like f-strings or jinja2 templates offer better alternatives for building prompt strings. The LangChain package also provides example LLM prompt templates.

**Rule-Based Systems:** For production applications, rule-based systems like SpaCy Matcher patterns are preferable over guardrails-ai or LangChain because they offer fuzzy rules to detect misspellings or transliterations and incorporate NLU semantic matching.

**SpaCy Matcher for Taboo Words:** Configure SpaCy Matcher to avoid taboo words or names, substituting them with synonyms or generic alternatives. This approach can also protect Personally Identifiable Information (PII). Implementing a bad word detector is a flexible method for any undesirable content.

**Correcting Bad Words with SpaCy Matcher:** To correct bad words with acceptable substitutes, add a separate named matcher for each word in your list, which enables accurate correction even with typos.

**Semantic Matching:** Augment the SpaCy matcher with semantic matching using word embeddings to filter semantically similar words, creating a robust pipeline that improves over time with added data and models.

**Red Teaming:** Employ "red teaming" to efficiently build a dataset of edge cases and improve NLP pipeline reliability. Red teams attempt to bypass guardrails to expose vulnerabilities in the LLM.

**AI Safety and Ethics:** Designating engineers or teams to test and penetrate LLM guardrails, often called "jail-breaking" or "hacking," identifies weaknesses. Balancing this effort with an LLM "blue team" (engineers and data analysts) enhances defense against undesirable behavior. Constant vigilance and updating guardrails are essential. Bug bounties or red team approaches help stay ahead of toxic content.

**Scaling LLMs:** Scaling LLMs involves increasing data and neurons to improve performance. However, the effectiveness of scaling is questioned, with some research indicating that increased dataset size doesn't necessarily create more intelligent behavior but rather more confident and intelligent-sounding text.

**Llama 2 and Open Source Models:** Open source models like BLOOMZ, StableLM, InstructGPT, and Llama2 are optimized for more modest hardware, enabling efficient generalization and more accurate behavior.

**Meta Models:** Building higher-level meta models that utilize LLMs and other NLP pipelines can break down prompts into actionable steps, generating API queries to accomplish tasks efficiently.

**LLM Temperature Parameter:** The temperature parameter in LLMs controls the randomness of generated text. Lower temperatures result in more predictable and consistent outputs, while higher temperatures increase creativity and variability. Understanding and adjusting this parameter is crucial for controlling LLM behavior.

**Teaching Math with LLMs:** While LLMs can successfully improvise onboarding conversations for math tutors, they struggle with math reasoning. Grounding models with specific questions and curating their responses is crucial to ensure reliability.

<----------section---------->


**FAISS**

### Introduzione
FAISS (Facebook AI Similarity Search) è una libreria open-source sviluppata da Facebook AI Research per l'indicizzazione e la ricerca veloce di vettori ad alta dimensione. Il suo obiettivo principale è rendere più efficiente la ricerca della similarità tra vettori, un'operazione cruciale in molte applicazioni di intelligenza artificiale, come il recupero di informazioni, il riconoscimento delle immagini e i sistemi di raccomandazione.

### Perché FAISS è importante?
Nei moderni sistemi di AI, si lavora spesso con enormi dataset di vettori, specialmente nell'ambito del deep learning. La ricerca esatta della similarità in uno spazio vettoriale può essere computazionalmente proibitiva. FAISS fornisce algoritmi e strutture dati ottimizzate per eseguire ricerche approssimate in modo molto più veloce rispetto a un approccio naïve basato su ricerche brute-force.

### Embedding e Similarità nell'NLP
FAISS lavora principalmente con **embedding**, rappresentazioni vettoriali di oggetti come testi, immagini o segnali audio. Nel contesto del **Natural Language Processing (NLP)**, gli embedding sono vettori numerici che catturano la semantica delle parole, frasi o documenti. 

#### Generazione degli embedding
Nell'NLP, gli embedding sono generati attraverso modelli di apprendimento automatico come:
- **Word2Vec**: rappresenta le parole in base al loro contesto, utilizzando tecniche come Continuous Bag of Words (CBOW) e Skip-gram.
- **GloVe**: costruisce embedding sulla base della co-occorrenza delle parole in un grande corpus testuale.
- **FastText**: un'estensione di Word2Vec che considera anche i sottotokens delle parole, utile per lingue con morfologia complessa.
- **BERT e Transformer-based models**: generano embedding contestualizzati che variano in base alla frase in cui la parola appare.
- **Sentence Transformers**: creano embedding per intere frasi, migliorando la ricerca semantica e la similarità testuale.

#### Metriche di Similarità
Una volta ottenuti gli embedding, FAISS permette di confrontarli utilizzando diverse metriche di similarità:
- **Similarità coseno**: misura l'angolo tra due vettori e viene ampiamente utilizzata per valutare la similarità semantica tra testi.
- **Distanza euclidea (L2)**: meno comune nell'NLP, utile quando gli embedding hanno distribuzioni spaziali significative.
- **Prodotto scalare (dot product)**: spesso usato nei modelli neurali per valutare l'affinità tra vettori.

#### Applicazioni nell'NLP
FAISS è particolarmente utile in molteplici applicazioni NLP, tra cui:
- **Recupero di informazioni**: aiuta a trovare documenti o frasi simili in grandi dataset, come nella ricerca semantica.
- **Risoluzione della coreferenza**: identifica entità simili in un testo, associando riferimenti diversi a uno stesso concetto.
- **Sistemi di raccomandazione di testi**: suggerisce articoli, post o libri basandosi sulla similarità degli embedding.
- **Clustering e analisi dei topic**: raggruppa documenti con contenuti simili, utile per la categorizzazione automatica.
- **Traduzione automatica e allineamento testuale**: confronta frasi in lingue diverse per trovare corrispondenze tra segmenti di testo.

### Creazione e uso di un FAISS Index nell'NLP
FAISS fornisce API in Python per la gestione efficiente degli embedding testuali. Ecco un esempio per indicizzare e cercare frasi simili:

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Modello per generare embedding testuali
model = SentenceTransformer('all-MiniLM-L6-v2')

documents = [
    "Il gatto salta sul tavolo",
    "Un felino è balzato sulla superficie",
    "Oggi il tempo è splendido",
    "La partita di calcio è stata emozionante"
]

# Creazione degli embedding
d = 384  # Dimensione del modello MiniLM
embeddings = np.array(model.encode(documents), dtype='float32')

# Creazione e popolamento dell'indice FAISS
index = faiss.IndexFlatL2(d)
index.add(embeddings)

# Query per trovare frasi simili
query_text = "Il micio è saltato sul mobile"
query_embedding = np.array([model.encode(query_text)], dtype='float32')
k = 2  # Numero di risultati

distances, indices = index.search(query_embedding, k)

# Output dei risultati
print("Frasi più simili:")
for i in indices[0]:
    print(documents[i])
```

### Vantaggi di FAISS nell'NLP
- **Velocità di ricerca**: consente di trovare frasi simili in dataset di milioni di documenti in pochi millisecondi.
- **Scalabilità**: supporta grandi volumi di dati, ideale per motori di ricerca semantica e assistenti virtuali.
- **Flessibilità**: utilizzabile con vari modelli di embedding, adattandosi a diversi casi d'uso.

### Conclusione
FAISS è un potente strumento per la ricerca di similarità in embedding testuali, rendendo possibili applicazioni avanzate nell'NLP. Grazie alla sua efficienza, è usato in motori di ricerca semantica, chatbot e sistemi di raccomandazione testuale. L'integrazione con modelli come BERT e Sentence Transformers lo rende una scelta eccellente per chi lavora con grandi corpus di testo.

<----------section---------->
<----------section---------->

**NATURAL LANGUAGE PROCESSING AND LARGE LANGUAGE MODELS**

### General Information
- **Program of Study:** Computer Engineering  
- **Track:** Artificial Intelligence and Intelligent Robotics  
- **Course Type:** Master’s Degree  
- **Academic Year:** 2024/2025  
- **Course Year:** 2nd Year  
- **Educational Activity Type:** Elective  
- **Field:** Elective  
- **Language:** English  
- **Credits:** 6 CFU  
- **Teaching Activity Type:** Lecture, Laboratory  
- **Exam Type:** Written and oral exam with a single grade  
- **Assessment:** Final Grade  
- **Teaching Period:** First Semester (01/10/2024 – 15/12/2024)  
- **Instructors:** Nicola Capuano, Antonio Greco  
- **Duration:** 48 hours (24 hours lectures, 24 hours lab sessions)  
- **Scientific-Disciplinary Sector:** ING-INF/05  
- **Location:** University of Salerno - Fisciano  

### Learning Objectives
The course provides theoretical, methodological, technological, and practical knowledge on natural language understanding and text processing. It introduces the innovative paradigms of Large Language Models (LLMs) within the general framework of Natural Language Processing (NLP), highlighting their numerous modern applications.  

### Knowledge and Understanding
- Fundamental concepts of NLP systems  
- Standard language models  
- Transformer-based LLMs  
- NLP applications using LLMs  
- Prompt engineering and fine-tuning LLMs  

### Applying Knowledge and Understanding
- Design and implementation of NLP systems using LLMs, effectively integrating existing technologies and optimizing configuration parameters  

### Prerequisites
- **Prerequisite Exam:** Machine Learning  

## Course Content

### **Module 1: Fundamentals of Natural Language Processing** (10 hours lecture, 6 hours exercises)
1. **Introduction to NLP:** Basic concepts, tasks, evolution, and applications (2 hours lecture)  
2. **Text Representation:** Tokenization, stemming, lemmatization, bag of words, n-grams, similarity measures, word embeddings (2 hours lecture)  
3. **TF-IDF and Classification:** TF-IDF vectors, text classification, and clustering (2 hours lecture)  
4. **Neural Networks for Text Analysis:** CNNs, recurrent networks, LSTMs (2 hours lecture)  
5. **Implementation:** Developing a text classifier (2 hours exercises)  
6. **Information Extraction:** Named Entity Recognition (NER), Question Answering (2 hours lecture)  
7. **Chatbot Development:** Using Python and SpaCy/RASA (4 hours exercises)  

### **Module 2: Transformers** (6 hours lecture, 10 hours exercises)
1. **Core Concepts:** Self-attention, multi-head attention, positional encoding, masking (2 hours lecture)  
2. **Transformer Architectures:** Encoder and decoder (2 hours lecture)  
3. **Practical Implementation:** Introduction to Hugging Face (2 hours exercises)  
4. **Applications:** Encoder-decoder models for translation and summarization (2 hours exercises)  
5. **Encoder-only Models:** Sentence classification, Named Entity Recognition (NER) (2 hours exercises)  
6. **Decoder-only Models:** Text generation (2 hours exercises)  
7. **LLM Definition:** Defining and training an LLM (2 hours lecture)  
8. **LLM Training:** Hands-on training (2 hours exercises)  

### **Module 3: Prompt Engineering** (2 hours lecture, 4 hours exercises)
1. **Techniques:** Zero-shot, few-shot, chain-of-thought prompting, self-consistency, generated knowledge, prompt chaining, ReAct, Retrieval-Augmented Generation (RAG) (2 hours lecture)  
2. **Exercises:** Basic prompting techniques (2 hours)  
3. **Advanced Techniques:** Exercises on RAG and LangChain (2 hours)  

### **Module 4: Fine-Tuning LLMs** (4 hours lecture, 4 hours exercises, 2 hours lab)
1. **Fine-Tuning Methods:** Feature-based tuning, updating output layers, Parameter-Efficient Tuning (PEFT), Low-Rank Adaptation (LoRA) (2 hours lecture)  
2. **Hands-on Fine-Tuning:** Practical exercises (2 hours exercises)  
3. **Reinforcement Learning with Human Feedback (RLHF):** Theoretical overview (2 hours lecture)  
4. **RLHF Practical Exercises:** Application and experimentation (2 hours exercises)  
5. **Final Project:** Application of learned techniques (2 hours lab)  

## Teaching Methods
The course includes lectures and in-class exercises. Lectures provide fundamental knowledge on advanced text representation, analysis, and classification techniques using LLMs. Exercises focus on applying these techniques to develop tools for text classification, analysis, and question answering. Attendance is mandatory, with a minimum requirement of 70% to access the exam. Attendance is tracked through the university's EasyBadge system.  

## Assessment
The exam consists of a group project and an oral test:  
- **Project:** Students will critically apply methodologies learned during the course to a practical case.  
- **Oral Test:** Evaluation of theoretical knowledge, project design choices, and answers to specific topics covered in the lectures.  
The final grade is the average of both components.  

## Recommended Texts
- **Reference Book:**  
  H. Lane, C. Howard, H. M. Hapke: *Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python*, Manning.  
- **Supplementary Materials:** Available on the university's e-learning platform ([https://elearning.unisa.it](https://elearning.unisa.it)), accessible to course students using their university credentials.


<----------section---------->

﻿NICOLA CAPUANO 
Nicola Capuano is an Associate Professor at the Department of Information and Electrical 
Engineering and Applied Mathematics (DIEM) at the University of Salerno. He obtained his 
degree in Computer Science and his Ph.D. in Computer Science and Computer Engineering 
from the University of Salerno. At the same university, he also held a four-year research 
fellowship on the topic of "Artificial Intelligence." Before pursuing an academic career, he 
collaborated with private research institutes, including the Center for Research in Pure and 
Applied Mathematics and the Center of Excellence for Software Technology. He also served as a 
researcher at the School of Engineering of the University of Basilicata. In 2021, he achieved the 
National Scientific Qualification as a Full Professor in the field 09/H1: Information Processing 
Systems. 
His research focuses on Natural Language Processing, Machine Learning, Knowledge 
Representation, Fuzzy Systems, and Artificial Intelligence in Education. He is the author of more 
than 120 publications, including journal articles, conference proceedings, and book chapters. 
He serves as an Associate Editor for the Journal of Ambient Intelligence and Humanized 
Computing by Springer Nature and Frontiers in Artificial Intelligence by Frontiers Media. He has 
been a Guest Editor for several journals, including the International Journal of Educational 
Technology in Higher Education and the International Journal of Emerging Technologies in 
Learning. He is a member of editorial boards and a reviewer for numerous journals, a track 
chair, and a program committee member for international conferences and workshops. He is 
also a member of the executive committee for the international conference The Learning Ideas. 
He edited the volume The Learning Grid Handbook, published by IOS Press. 
Nicola Capuano acts as an independent evaluator of projects and proposals for the European 
Commission under the Horizon Europe program and for the European Institute of Innovation 
and Technology. He coordinated the projects Diogene (A Training Web Broker for ICT 
Professionals) and InTraServ (Intelligent Training Service for Management Training in SMEs), both 
funded by the European Commission under the Fifth Framework Programme. He was a member 
of the European Network of Excellence Kaleidoscope (Concepts and Methods for Exploring the 
Future of e-Learning with Digital Technologies), where he coordinated the Special Interest 
Group on Learning Grid. He led the research line “Intelligent Features for Learning” at the Center 
of Excellence in Methods and Systems for Learning and Knowledge at the University of Salerno. 
He has held scientific and coordination roles in several other research and innovation projects. 
He is a Project Management Professional (PMP) certified by the Project Management Institute. 

<----------section---------->

ANTONIO GRECO 
Antonio Greco graduated with honors in Computer Engineering in 2014 from the University of 
Salerno (Italy). In March 2018, he earned a Ph.D. in Computer Science and Information 
Engineering from the same university. In March 2020, he became an RTD/A Researcher (SSD 
ING-INF/05 "Information Processing Systems") at the Department of Information and Electrical 
Engineering and Applied Mathematics (DIEM) at the University of Salerno, where he has been an 
RTD/B Researcher since November 2022. Within the same department, he has served as 
Delegate for Student Orientation since November 2022 and has been a member of the Ph.D. 
Board in Information Engineering since May 2024. 
For the department, he has taught various courses, including Logic Networks (SSD ING-INF/05, 
Bachelor's Degree in Computer Engineering), Autonomous Vehicle Driving (SSD ING-INF/05, 
Master's Degree in Computer Engineering), Web Software Technologies (SSD ING-INF/05, 
Bachelor's Degree in Computer Engineering), Artificial Intelligence for Cybersecurity (SSD ING
INF/05, Master's Degree in Computer Engineering), Robotics for E-Health (SSD ING-INF/05, 
Master's Degree in Digital Health and Bioinformatics Engineering), System and Network Security 
(SSD ING-INF/05, Master's Degree in Computer Engineering), Artificial Vision (SSD ING-INF/05, 
Master's Degree in Computer Engineering), and Natural Language Processing and Large 
Language Models (SSD ING-INF/05, Master's Degree in Computer Engineering). Since the 
2021/2022 academic year, he has also taught the course Advanced Machine Learning as part of 
the accredited Ph.D. program in Information Engineering (D.M. 226/2021). 
Since 2014, he has been a member of the MIVIA Lab (Machines for Intelligent Video, Image, and 
Audio recognition) at the University of Salerno’s DIEM, a group with a strong focus on 
international collaborations. His research primarily focuses on Computer Vision and Pattern 
Recognition, specifically on the design, implementation, and optimization of real-time 
computer vision and deep learning algorithms (e.g., gender recognition, age estimation, 
ethnicity recognition, emotion analysis, fire detection, anomaly detection, people counting, 
object tracking, and audio event recognition) for data acquired from static devices (smart 
cameras, microphones) or moving devices (drones, robots, autonomous vehicles). These 
activities are often conducted in collaboration with European research groups, particularly at 
the University of Malta, the University of Groningen (Netherlands), and the University of Twente 
(Netherlands), where he has spent a total of nine months as a Visiting Researcher (January
April 2020 and August 2021–February 2022). At Twente, he collaborated with the Data 
Management and Biometrics Group within the Faculty of Electrical Engineering, Mathematics, 
and Computer Science. 
In recent years, he has organized several Special Issues for international journals, including an 
issue of the Journal of Ambient Intelligence and Humanized Computing titled “Ambient 
Understanding for Mobile Autonomous Robots (AutoRob)," an issue of Pattern Recognition on 
“From Bench to the Wild: Recent Advances in Computer Vision Methods (WILD-VISION)," and 
an issue of Pattern Analysis and Applications on “Pedestrian Attribute Recognition and Person 
Re-Identification." As of March 2024, he is an Associate Editor for Pattern Analysis and 
Applications and serves as a reviewer for over 25 international journals. 
Antonio Greco has presented his scientific work at more than ten international conferences and 
has served as Contest Chair at various events. Notably, he organized the Guess the Age (GTA) 
Contest 2021 at the International Conference on Computer Analysis of Images and Patterns 
(CAIP), co-organized the ONFIRE 2023 contest at the International Conference on Image 
Analysis and Processing (ICIAP), and co-organized the Pedestrian Attributes Recognition (PAR) 
Contest 2023 at CAIP. Additionally, he served on the Local Committee for the International 
Workshop on Graph-based Representations (GBR) in Capri, Italy (May 16–18), the International 
Conference on Computer Analysis of Images and Patterns (CAIP) in Salerno, Italy (September 2
6, 2019), the IEEE Conference on Cognitive and Computational Aspects of Situation 
Management (CogSIMA) in Salerno, Italy (June 6–10, 2022), and the GBR Workshop in Salerno 
(September 6–8). In 2021, he was an Invited Speaker at the CogSIMA Challenge Problems 
Workshop at the IEEE CogSIMA Conference, delivering a talk titled “Adding Awareness to AI 
Systems.” 
In national and international research projects, he has served as the scientific lead of the local 
research unit at DIEM for the PON ARS01_01226 project PerMedNet – Personalized Medicine for 
Innovative Strategies in Neuropsychiatric and Vascular Diseases. He also led work packages 
WP4 and WP5 in the European research project Flexible Assembly Manufacturing with Human
Robot Collaboration and Digital Twin Models (FELICE) (Grant Agreement ID: 101017151, funded 
under H2020-EU.2.1.1). Additionally, he was the scientific lead for DIEM in a research contract 
with the company RED&BLUE for the project Development of a Platform for Evaluating AI 
System Safety. 
In the field of technology transfer, Antonio Greco co-founded AI-READY, a spin-off of the 
University of Salerno specializing in AI applications for cognitive robotics, mobility, and 
autonomous vehicles, in December 2019.

<----------section---------->

