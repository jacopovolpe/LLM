## Natural Language Processing and Large Language Models

**Corso di Laurea Magistrale in Ingegneria Informatica**

**Lesson 22: Guardrails for LLMs**

**Nicola Capuano and Antonio Greco**

**DIEM â€“ University of Salerno**

### Outline

* Adding guardrails to LLMs
* Techniques for adding guardrails
* Frameworks for implementing guardrails

### Adding Guardrails to LLMs

**Guardrails**

Guardrails are mechanisms or policies that regulate the behavior of LLMs. They help ensure that responses are safe, accurate, and context-appropriate. They can:

* Prevent harmful, biased, or inaccurate outputs.
* Align responses with ethical and operational guidelines.
* Build trust and reliability for real-world applications.

Examples include:

* Blocking harmful content
* Restricting outputs to specific domains

**Types of Guardrails**

* **Safety Guardrails:** Prevent the generation of harmful or offensive content.
* **Domain-Specific Guardrails:** Restrict responses to specific knowledge areas.
* **Ethical Guardrails:** Avoid bias, misinformation, and ensure fairness.
* **Operational Guardrails:** Limit outputs to align with business or user objectives.

### Techniques for Adding Guardrails

* Rule-based filters
* Fine-tuning with custom data
* Prompt Engineering
* External validation layers
* Real-time monitoring and feedback

**Rule-based Filters**

Predefined rules to block or modify certain outputs. Examples:

* Keyword blocking (e.g., offensive terms)
* Regex-based patterns for filtering sensitive information.
* Simple and efficient for basic content filtering.


**Fine-tuning with Custom Data**

Train the model on domain-specific, curated datasets. Adjust weights to produce outputs aligned with guidelines. Examples:

* Fine-tune for medical advice to restrict responses to accurate and safe recommendations.
* Fine-tune for question answering on the topics of the course.


**Prompt Engineering**

Craft and/or refine prompts to guide the LLM behavior within desired boundaries. Examples:

* "Respond only with factual, non-controversial information."
* "Avoid speculative or unverifiable statements."


**External Validation Layers**

Additional systems or APIs that post-process the model's outputs. Examples:

* Toxicity detection APIs
* Fact-checking models

This allows modular and scalable implementation of guardrails.

**Real-time Monitoring and Feedback**

Monitor outputs continuously for unsafe or incorrect content. Flag or block problematic outputs in real-time. Tools:

* Human-in-the-loop systems
* Automated anomaly detection

**Best Practices**

Combine multiple techniques for robust safeguards. Example: Rule-based filtering + External validation + Fine-tuning.

### Frameworks for Implementing Guardrails

Existing frameworks for implementing guardrails offer:

* Easy integration with LLM APIs
* Predefined and customizable rulesets

Popular tools are:

* **Guardrails AI:** A library for implementing safeguards.
* **LangChain:** For chaining prompts and filtering outputs.
* **OpenAI Moderation:** A prebuilt API to detect unsafe content.

**Guardrails AI (https://www.guardrailsai.com/)**

* Validation: Ensures outputs are within specified guidelines.
* Formatting: Controls the output structure.
* Filters: Removes or blocks unsafe content.

```python
from guardrails import Guard
guard = Guard(rules="rules.yaml")
response = guard(llm("Provide medical advice"))
```

**LangChain**

```python
from langchain.prompts import PromptTemplate
prompt = PromptTemplate(
    input_variables=["question"],
    template="Answer safely and factually: {question}"
)
```

* Chains prompts with checks and filters.
* Verifies outputs against predefined criteria.
* Integrable with Guardrails: https://www.guardrailsai.com/docs/integrations/langchain

### Try it Yourself

* Evaluate which techniques to add guardrails are most suited for your purposes.
* A possible suggestion is to proceed by incrementally adding complexity to the guardrails if you are not able to achieve a satisfying result with a simpler approach.
* Carefully review the documentation of the existing frameworks.
* Study similar examples available in the framework documentation.
* Try to apply guardrails to your project.
