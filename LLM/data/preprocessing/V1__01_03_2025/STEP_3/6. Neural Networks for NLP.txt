## Natural Language Processing and Large Language Models

**Master's Degree in Computer Engineering**

**Lesson 6: Neural Networks for NLP**

*Nicola Capuano and Antonio Greco*

*DIEM â€“ University of Salerno*


### Introduction to Neural Networks for NLP

Natural Language Processing (NLP) leverages computational techniques to enable computers to understand, interpret, and generate human language.  Neural networks play a crucial role in modern NLP, offering powerful tools for various tasks. This lesson explores the application of neural networks, specifically Recurrent Neural Networks (RNNs) and their variants, to NLP tasks such as text classification and text generation.


<----------section---------->

### Recurrent Neural Networks (RNNs)

Traditional feedforward neural networks process each input independently, lacking the ability to retain information from previous inputs.  This "memoryless" nature presents challenges for sequential data like text, where the meaning of a word often depends on the preceding words. To overcome this limitation, Recurrent Neural Networks (RNNs) were developed. RNNs possess a recurrent connection, allowing them to maintain an internal state or "memory" that captures information from past inputs.  This memory enables the network to process sequential data effectively, considering the context of each element in the sequence.

The structure of an RNN involves a hidden layer with a feedback loop. At each time step, the network receives an input (e.g., a word embedding) and combines it with the hidden state from the previous time step. This combination is then processed by the hidden layer, producing a new hidden state and an output. The hidden state acts as a summary of the sequence processed so far.


<----------section---------->

### Unrolling and Understanding RNN Operations

Visualizing an RNN as an "unrolled" network helps clarify its operation over time.  Each time step represents the network processing a new input in the sequence. The weights within the RNN are shared across all time steps, enabling the network to learn general patterns from sequential data regardless of the specific position in the sequence.  The initial hidden state is typically set to zero, as there is no previous context for the first input.

Mathematically, at each time step *t*, the hidden state *h<sub>t</sub>* is calculated as a function of the current input *x<sub>t</sub>* and the previous hidden state *h<sub>t-1</sub>*. This function typically involves matrix multiplications with trainable weight matrices and the application of a non-linear activation function.  The output at each time step can be used for tasks like sequence labeling, while the final hidden state can be used for tasks like text classification.


<----------section---------->

### RNN Applications and Training

RNNs are versatile and can be applied to various sequence processing tasks:

* **One-to-Many:**  Generating a sequence of outputs from a single input (e.g., image captioning).
* **Many-to-One:**  Producing a single output from a sequence of inputs (e.g., sentiment analysis).
* **Many-to-Many:** Mapping a sequence of inputs to a sequence of outputs (e.g., machine translation).

Training RNNs involves Backpropagation Through Time (BPTT), a modified version of the standard backpropagation algorithm. BPTT calculates the error at the final time step and then propagates it back through the network across all time steps, adjusting the weights to minimize the error.


<----------section---------->

### RNN Variants: Addressing Limitations and Enhancing Performance

While powerful, basic RNNs can suffer from the vanishing gradient problem, hindering their ability to learn long-range dependencies in sequences.  To address this and other limitations, several variants have been developed:

* **Long Short-Term Memory (LSTM):** LSTMs introduce a memory cell and gating mechanisms to control the flow of information, enabling them to learn and retain information over longer sequences.
* **Gated Recurrent Unit (GRU):** GRUs offer a simpler alternative to LSTMs, also designed to mitigate the vanishing gradient problem with fewer parameters.
* **Bidirectional RNNs:** These process the sequence in both forward and backward directions, capturing contextual information from both past and future elements.
* **Stacked RNNs:** Multiple RNN layers, often LSTMs, are stacked to create deeper networks capable of learning more complex representations.

These variants offer improved performance and address the limitations of basic RNNs, making them suitable for a wider range of NLP tasks.


<----------section---------->

### Introduction to Text Generation and Language Models

Text generation involves using models to create new text that resembles human-written language.  Generative models, such as RNNs and Transformers, learn the statistical patterns in text data and use these patterns to generate new sequences.  A crucial component of text generation is the language model (LM). An LM predicts the probability of a word given the preceding words in a sequence.

Training an LM involves feeding the model a sequence of words and having it predict the next word. The prediction is compared to the actual next word, and the error is used to update the model's parameters.  This process is repeated over a large corpus of text, enabling the model to learn the statistical structure of the language.


<----------section---------->

### Building a Poetry Generator: A Practical Example

A simple poetry generator can be built using an LSTM network trained on a corpus of poetry.  The model learns the patterns and style of the poetry and can then generate new poems by sampling from its learned distribution.

The process involves:

1. **Data Preparation:**  The poetry corpus is preprocessed and divided into sequences of characters or words.
2. **Model Building:** An LSTM network is constructed, taking a sequence as input and predicting the next character or word.
3. **Training:** The model is trained on the prepared data using BPTT.
4. **Generation:**  New poetry is generated by providing a seed sequence and iteratively sampling from the model's predictions.  A temperature parameter controls the randomness of the generated text, allowing for varying degrees of creativity.


<----------section---------->

### Advanced Concepts: Ragged Tensors and Temperature in Generation

Ragged tensors efficiently handle variable-length sequences, eliminating the need for padding. They are available in TensorFlow and similar functionality exists in PyTorch (Packed Sequences).  In text generation, the temperature parameter influences the randomness of the output. Higher temperatures lead to more diverse and creative text, while lower temperatures produce more predictable and conservative outputs.  The temperature modifies the predicted probability distribution, allowing for control over the generation process.


<----------section---------->

### Conclusion

This lesson provides a comprehensive overview of RNNs and their application to NLP tasks.  By understanding the underlying principles of RNNs and their variants, along with the concepts of language modeling and text generation, one can develop powerful NLP applications.  Practical examples, such as building a poetry generator, illustrate the potential of these techniques in creative text generation.  Further exploration of advanced architectures like Transformers will enhance understanding and capabilities in this rapidly evolving field.
