### Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica Lesson 5
Word Embeddings
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This document outlines a lecture on Word Embeddings within a Natural Language Processing (NLP) course. It covers the limitations of TF-IDF, the concept of word embeddings, methods for learning them, alternatives to Word2Vec, and practical ways to work with word embeddings. Word embeddings are a crucial component in many modern NLP applications, providing a way to represent words as dense vectors in a continuous space, where the position of a word reflects its semantic meaning.

<----------section---------->

### Outline
*   Limitations of TF-IDF
*   Word Embeddings
*   Learning Word Embeddings
*   Word2Vec Alternatives
*   Working with Word Embeddings

This outline presents the core topics covered in the lecture. Starting with the drawbacks of a simpler technique like TF-IDF sets the stage for appreciating the advantages of word embeddings. The following sections build upon this foundation, exploring how word embeddings are learned and utilized, and providing insights into state-of-the-art techniques and their applications.

<----------section---------->

### Limitations of TF-IDF
TF-IDF (Term Frequency-Inverse Document Frequency) is a technique used to quantify the importance of a word in a document relative to a collection of documents (corpus). It counts terms according to their exact spelling, meaning that texts with the same meaning but using different words will have completely different TF-IDF vector representations. This inflexibility presents a challenge when dealing with the nuances of language and semantic understanding.

**Examples:**
*   The movie was amazing and exciting.
*   The film was incredible and thrilling.
*   The team conducted a detailed analysis of the data and found significant correlations between variables.
*   The group performed an in-depth examination of the information and discovered notable relationships among factors.

These examples illustrate how different wording can result in distinct TF-IDF representations, even when the underlying meaning is the same. The sentences in each pair convey similar information but utilize different lexical choices.

<----------section---------->

### Term Normalization
Techniques like stemming and lemmatization are used to normalize terms, collecting words with similar spellings under a single token. Stemming reduces words to their root form (e.g., "running" becomes "run"), while lemmatization reduces words to their dictionary form (lemma) considering context (e.g., "better" becomes "good").

**Disadvantages:**
*   They fail to group most synonyms, which limits the ability to capture semantic similarity.
*   They may group together words with similar or the same spelling but different meanings, leading to ambiguity.
*   She is leading the project vs. The plumber leads the pipe.
*   The bat flew out of the cave vs. He hit the ball with a bat.

These examples highlight the risk of oversimplification in normalization. The word "lead" can mean "to guide" or refer to a type of metal pipe, while "bat" can be a nocturnal animal or a piece of sports equipment. Stemming or lemmatization alone cannot resolve these polysemous distinctions.

<----------section---------->

### TF-IDF Applications
TF-IDF is sufficient for many NLP applications that do not require a deep understanding of text semantics:
*   Information Retrieval (Search Engines)
*   Information Filtering (Document Recommendation)
*   Text Classification

However, other applications require a deeper understanding of text semantics:
*   Text generation (Chatbot)
*   Automatic Translation
*   Question Answering
*   Paraphrasing and Text Rewriting

The first group of applications benefits from TF-IDF's ability to identify important keywords, while the second group requires capturing the relationships between words and the underlying meaning of the text. These advanced tasks necessitate models that can discern context and semantic similarity.

<----------section---------->

### Bag-of-Words (recap)
Each word is assigned an index that represents its position in the vocabulary:
*   the 1st word (e.g., apple) has index 0
*   the 2nd word (e.g., banana) has index 1
*   the 3rd word (e.g., king) has index 2
*   ...

Each word is then represented by a one-hot vector:
*   apple = (1,0,0,0,…,0)
*   banana = (0,1,0,0,…,0)
*   king = (0,0,1,0,…,0)

This section provides a quick review of the Bag-of-Words (BoW) model, a foundational concept in NLP. In BoW, a text is represented as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity. One-hot encoding is a way to represent each word in this bag as a vector, where the dimension of the vector is equal to the size of the vocabulary and only one element in the vector is 1, representing the index of the word.

<----------section---------->

### Bag-of-Words (recap)
With this encoding, the distance between any pair of vectors is always the same. It does not capture the semantics of words. Furthermore, it is not efficient since it uses sparse vectors.

**Note:** The figure shows only three dimensions of a space where dimensions equals the cardinality of the vocabulary

The key limitation of BoW is that it treats all words as independent entities. The Euclidean distance between any two one-hot vectors is the same, regardless of whether the words are semantically similar. This means that the model cannot capture any relationships or similarities between words. Furthermore, one-hot vectors are sparse and high-dimensional, making them computationally inefficient for large vocabularies.

<----------section---------->

### Word Embeddings
Word Embeddings: A technique for representing words with vectors (A.K.A. Word Vectors) that are:
*   Dense
*   With dimensions much smaller than the vocabulary size
*   In a continuous vector space

**Key feature:** Vectors are generated so that words with similar meanings are close to each other. The position in the space represents the semantics of the word.

Word Embeddings:
*   king and queen are close to each other
*   apple and banana are close to each other
The words of the first group are far from those of to the second group.

**Example:**
*   Apple = (0.25,0.16)
*   Banana = (0.33,0.10)
*   King = (0.29,0.68)
*   Queen = (0.51,0.71)

This section introduces the core concept of word embeddings, a significant advancement over BoW.  Word embeddings represent words as dense, low-dimensional vectors in a continuous vector space.  The key advantage is that the spatial arrangement of these vectors reflects the semantic relationships between words. Words with similar meanings are located close to each other in the vector space. The vectors are "dense" because most of the elements of the vector have non-zero values, in contrast to the "sparse" one-hot encoding used in Bag-of-Words.  The dimensionality of word embeddings is also much smaller than the vocabulary size, typically ranging from 100 to 300 dimensions, making them more computationally efficient.

<----------section---------->

### Word Embedding: Properties
Word embeddings enable semantic text reasoning based on vector arithmetic.

**Examples:**
*   Subtracting royal from king we arrive close to man: king – royal ≈ man
*   Subtracting royal from queen we arrive close to woman: queen – royal ≈ woman
*   Subtracting man from king and adding woman we arrive close to queen: king – man + woman ≈ queen

One of the most fascinating properties of word embeddings is their ability to perform analogical reasoning through vector arithmetic. For example, subtracting the vector representing "royal" from "king" results in a vector close to "man." These operations demonstrate that word embeddings capture meaningful semantic relationships between words.

<----------section---------->

### Semantic Queries
Word embeddings allow for searching words or names by interpreting the semantic meaning of a query.

**Examples:**
*   Query: "Famous European woman physicist"
    ```
    wv['famous'] + wv['European'] + wv['woman'] + wv['physicist'] ≈ wv['Marie_Curie'] ≈ wv['Lise_Meitner'] ≈ …
    ```
*   Query: “Popular American rock band”
    ```
    wv['popular'] + wv['American'] + wv['rock'] + wv['band'] ≈ wv['Nirvana'] ≈ wv['Pearl Jam'] ≈ …
    ```

Word embeddings facilitate semantic searches, allowing users to retrieve information based on the meaning of their query rather than just keyword matching. By adding the vectors of individual words in the query, the model can identify words or names that are semantically related to the query as a whole, as shown in the examples above.

<----------section---------->

### Analogies
Word embeddings enable answering analogy questions by leveraging their semantic relationships.

**Examples:**
*   Who is to physics what Louis Pasteur is to germs?
    ```
    wv['Louis_Pasteur'] – wv['germs'] + wv['physics'] ≈ wv['Marie_Curie']
    ```
*   Marie Curie is to science as who is to music?
    ```
    wv['Marie_Curie'] – wv['science'] + wv['music'] ≈ wv['Ludwig_van_Beethoven']
    ```
*   Legs is to walk as mouth is to what?
    ```
    wv['legs'] – wv['walk'] + wv['mouth'] ≈ wv['speak'] or wv['eat']
    ```

Word embeddings can also be used to solve analogy questions by performing vector arithmetic. The examples above show how subtracting the vector representing one term in the analogy from another and adding the vector representing the third term can yield a vector close to the fourth term, thus answering the analogy question.

<----------section---------->

### Visualizing Word Embeddings
Google News Word2vec 300-D vectors projected onto a 2D map using PCA. Semantic proximity approximates geographical proximity.

news corpus, cities that are similar in size and culture are clustered close together despite being far apart geographically, such as San Diego and San Jose, or vacation destinations such as Honolulu and Reno.

Fortunately you can use conventional algebra to add the vectors for cities to the vectors for states and state abbreviations. As you discovered in chapter 4, you can use tools such as principal components analysis to reduce the vector dimensions from your 300 dimensions to a human-understandable 2D representation. PCA enables you to see the projection or “shadow” of these 300-D vectors in a 2D plot. Best of all, the PCA algorithm ensures that this projection is the best possible view of your data, keeping the vectors as far apart as possible. PCA is like a good photographer that looks at something from every possible angle before composing the optimal photograph. You don’t even have to normalize the length of the vectors after summing the city + state + abbrev vectors, because PCA takes care of that for you.

We saved these augmented city word vectors in the nlpia package so you can load them to use in your application. In the following code, you use PCA to project them onto a 2D plot.

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
us_300D = get_data('cities_us_wordvectors')
us_2D = pca.fit_transform(us_300D.iloc[:, :300])
```

Figure 6.8 shows the 2D projection of all these 300-D word vectors for US cities:
Figure 6.8 Google News Word2vec 300-D vectors projected onto a 2D map using PCA

Listing 6.11 Bubble chart of US cities
The 2D vectors producted by PCA are for visualization. Retain the original 300-D Word2vec vectors for any vector reasoning you might want to do.
The last column of this DataFrame contains the city name, which is also stored in the DataFrame index.

Memphis, Nashville,
Charlotte, Raleigh, and Atlanta
Houston and Dallas
nearly coincide.
Ft. Worth
El Paso
San Diego
LA, SF, and San Jose
America/Los_…(0.9647851, –0.7217035)
Portland, OR
Honolulu, Reno,
Mesa, Tempe, and Phoenix

Size: population
Position: semantics
Color: time zone
America/Phoenix
America/New_York
America/America/Anchorage
America/Indiana/Indianapolis
America/Los_Angeles
America/Boise
America/Denver
America/Kentucky/Louisville
America/Chicago
Pacific/Honolulu

This section discusses the visualization of high-dimensional word embeddings. Since word embeddings typically have hundreds of dimensions, it is difficult to visualize them directly. Techniques like Principal Component Analysis (PCA) are used to reduce the dimensionality of the word embeddings to two or three dimensions, allowing them to be plotted on a graph. The resulting plots often reveal clusters of words with similar meanings, confirming that semantic proximity is reflected in the vector space.  The example highlights how city embeddings, when projected onto a 2D map, can exhibit geographical relationships. Cities with similar sizes and cultural aspects are often clustered together. The provided code snippet shows how to use scikit-learn's PCA to reduce the dimensionality of city word vectors. The remark "The 2D vectors producted by PCA are for visualization. Retain the original 300-D Word2vec vectors for any vector reasoning you might want to do" makes clear the visualization is for understanding, but the original high-dimensional vectors are required for advanced analysis.

<----------section---------->

### Learning Word Embeddings
This section introduces methods for learning word embeddings from textual data, focusing on the Word2Vec algorithm. The core idea behind these methods is that words with similar meanings tend to appear in similar contexts.

<----------section---------->

### Word2Vec
Word embeddings were introduced by Google in 2013 in the following paper:
*   T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space in 1st International Conference on Learning Representations, ICLR 2013

The paper defines Word2Vec:
*   A methodology for the generation of word embeddings
*   Based on neural networks
*   Using unsupervised learning on a large unlabeled textual corpus

Word2Vec, a seminal algorithm developed by Google, provides an efficient way to learn word embeddings using neural networks in an unsupervised manner. It leverages a large, unlabeled corpus of text to capture semantic relationships between words. The original paper "Efficient Estimation of Word Representations in Vector Space" by Mikolov et al. (2013) laid the foundation for many subsequent advancements in word embedding techniques.

<----------section---------->

### Word2Vec
Idea: words with similar meanings are often found in similar contexts.
*   Context: a sequence of words in a sentence

**Example:**
*   Consider the sentence Apple juice is delicious.
*   Remove one word.
*   The remaining sentence is ____ juice is delicious.
*   Ask someone to guess the missing word.
*   Terms such as banana, pear or apple would probably be suggested.
*   These terms have similar meanings and used in similar contexts.

The central idea behind Word2Vec is that words appearing in similar contexts tend to have similar meanings. The "context" of a word is defined as the surrounding words in a sentence. By analyzing these contexts, Word2Vec can learn to represent words as vectors in such a way that words with similar meanings are located close to each other in the vector space. This example effectively explains that if someone had to guess the missing word, it would probably be a fruit similar to an apple.

<----------section---------->

### Continuous Bag-of-Word
A neural network is trained to predict the central token of a context of m tokens.
*   **Input:** the bag of words composed of the sum of all one-hot vectors of the surrounding tokens.
*   **Output:** a probability distribution over the vocabulary with its maximum in the most probable missing word.
*Example:* Claude Monet painted the Grand Canal in Venice in 1806.

The Continuous Bag-of-Words (CBOW) model is one of the two main architectures in Word2Vec. In CBOW, a neural network is trained to predict a central (target) word based on the surrounding context words. The input to the network is a "bag of words" composed of the surrounding words, which are represented as one-hot vectors and then summed together. The output is a probability distribution over the entire vocabulary, with the goal being to maximize the probability of the actual target word. The example sentence is used in the next section to further explain the training process.

<----------section---------->

### Continuous Bag-of-Word
|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

CBOW's neural network architecture typically consists of three layers: an input layer, a hidden layer, and an output layer. The input layer has |V| neurons, where |V| is the size of the vocabulary. The hidden layer has 'n' neurons, where 'n' is the desired dimensionality of the word embeddings. The output layer also has |V| neurons, representing a probability distribution over the vocabulary.

<----------section---------->

### SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH
Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

This section provides a guideline on when to choose between the Skip-gram and CBOW architectures. Skip-gram performs well with smaller datasets and for representing rare words. CBOW tends to achieve higher accuracy for frequent words and is faster to train, making it suitable for large datasets.

<----------section---------->

### Continuous bag of words vs. bag of words
In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surrounding words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

**Example:** for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

This clarifies the difference between a simple "bag of words" (BoW) and the "continuous bag of words" (CBOW) used in the Word2Vec model. CBOW utilizes a sliding window to define the context, allowing the model to capture local relationships between words, which is not possible with the traditional BoW approach.

<----------section---------->

### Continuous Bag-of-Word
Ten 5-gram examples from the sentence about Monet

CONTINUOUS BAG-OF-WORDS APPROACH
In the continuous bag-of-words approach, you’re trying to predict the center word based on the surrounding words. Instead of creating pairs of input and output tokens, you’ll create a multi-hot vector of all surrounding terms as an input vector. The multi-hot input vector is the sum of all one-hot vectors of the surrounding tokens to the center, target token.

Based on the training sets, you can create your multi-hot vectors as inputs and map them to the target word as output. The multi-hot vector is the sum of the one-hot vectors of the surrounding words’ training pairs wt-2 + wt-1 + wt+1 + wt+2 . You then build the training pairs with the multi-hot vector as the input and the target word wt as the output. During the training, the output is derived from the softmax of the output node with the highest probability.

| Input word wt-2 | Input word wt-1 | Input word wt+1 | Input word wt+2 | Expected output wt |
|-----------------|-----------------|-----------------|-----------------|--------------------|
| Monet           | painted         | Claude          | Monet             | Claude             |
| Claude          | painted         | the             | Monet             | painted            |
| Claude          | Monet           | the             | Grand             | painted            |
| Monet           | painted         | Grand           | Canal           | the                |
| painted         | the             | Canal           | of              | Grand              |
| the             | Grand           | of              | Venice            | Canal              |
| Grand           | Canal           | Venice          | in              | of               |
| Canal           | of              | in              | 1908            | Venice             |
| of              | Venice          | 1908            |                  | in               |
| Venice          | in              | 1908            |                  |                  |

target word w t = word to be predicted
surrounding words w t-2, w t-1 = input words
surrounding words w t+1, w t+2

painted the Grand Canal of Venice in 1908.

This section elaborates on the training process of the CBOW model. The model takes the surrounding words (context) as input and predicts the central word. The input is constructed as a "multi-hot vector," created by summing the one-hot vectors of the surrounding words. The expected output is the one-hot vector of the target word. The table provides training examples extracted from the sentence "Claude Monet painted the Grand Canal of Venice in 1908".

<----------section---------->

### Continuous Bag-of-Word
After the training is complete the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words.
Similar words are found in similar contexts …
… their weights to the hidden layer adjust in similar ways
… this result in similar vector representations

SKIP-GRAM VS. CBOW: WHEN TO USE WHICH APPROACH
Mikolov highlighted that the skip-gram approach works well with small corpora and rare terms. With the skip-gram approach, you’ll have more examples due to the network structure. But the continuous bag-of-words approach shows higher accuracies for frequent words and is much faster to train.

Continuous bag of words vs. bag of words
In previous chapters, we introduced the concept of a bag of words, but how is it different than a continuous bag of words? To establish the relationships between words in a sentence you slide a rolling window across the sentence to select the surround-ing words for the target word. All words within the sliding window are considered to be the content of the continuous bag of words for the target word at the middle of that window.

Example for a continuous bag of words passing a rolling window of five words over the sentence “Claude Monet painted the Grand Canal of Venice in 1908.” The word painted is the target or center word within a five-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are the four surrounding words for the first CBOW rolling window.

the highest probability will be converted to 1, and all remaining terms will be set to 0.
This simplifies the loss calculation.
 After training of the neural network is completed, you’ll notice that the weights have been trained to represent the semantic meaning. Thanks to the one-hot vector conversion of your tokens, each row in the weight matrix represents each word from the vocabulary for your corpus. After the training, semantically similar words will have similar vectors, because they were trained to predict similar surrounding words. This is purely magical!
 After the training is complete and you decide not to train your word model any further, the output layer of the network can be ignored. Only the weights of the inputs to the hidden layer are used as the embeddings. Or in other words: the weight matrix is your word embedding. The dot product between the one-hot vector representing the input term and the weights then represents the word vector embedding.

Retrieving word vectors with linear algebra
The weights of a hidden layer in a neural network are often represented as a matrix: one column per input neuron, one row per output neuron. This allows the weight matrix to be multiplied by the column vector of inputs coming from the previous layer to generate a column vector of outputs going to the next layer . So if you multiply (dot product) a one-hot row vector by the trained weight matrix, you’ll get a vector that is one weight from each neuron (from each matrix column). This also works if you take the weight matrix and multiply it (dot product) by a one-hot column vector for the word you are interested in.

Of course, the one-hot vector dot product just selects that row from your weight matrix that contains the weights for that word, which is your word vector. So you could easily retrieve that row by just selecting it, using the word’s row number or index num-ber from your vocabulary.

WE of Monet

After training, the output layer of the CBOW network is discarded. The weights connecting the input layer to the hidden layer are retained as the word embeddings. Words that appear in similar contexts will have their weights adjusted in similar ways during training, leading to similar vector representations. This process effectively encodes the semantic meaning of words in the vector space. The passage explains how the hidden layer's weight matrix serves as the word embedding and how it can be retrieved through linear algebra operations.

<----------section---------->

### Skip-Gram
Alternative training method for Word2Vec
*   A neural network is trained to predict a context of m tokens based on the central token
*   **Input:** the one-hot vector of the central token
*   **Output:** the one-hot vector of a surrounding word (one training iteration for each surrounding word)

output example skip-grams are shown in figure 6.3. The predicted words for these skip-grams are the neighboring words “Claude,” “Monet,” “the,” and “Grand.”

WHAT IS A SKIP-GRAM? Skip-grams are n -grams that contain gaps because you skip over intervening tokens. In this example, you’re predicting “Claude” from the input token “painted,” and you skip over the token “Monet.”
The structure of the neural network used to predict the surrounding words is similar to the networks you learned about in chapter 5. As you can see in figure 6.4, the net-work consists of two layers of weights, where the hidden layer consists of n neurons; n is the number of vector dimensions used to represent a word. Both the input and out-put layers contain M neurons, where M is the number of words in the model’s vocabu-lary. The output layer activation function is a softmax, which is commonly used for classification problems.

WHAT IS SOFTMAX ?
The softmax function is often used as the activation function in the output layer of neural networks when the network’s goal is to learn classification problems. The softmax will squash the output results between 0 and 1, and the sum of all outputs will always add up to 1. That way, the results of an output layer with a softmax function can be considered as probabilities.
 For each of the K output nodes, the softmax output value can be calculated using the normalized exponential function:

```
σ(z)j = exp(z_j) / Σ_{k=1}^{K} exp(z_k)
```

**Example 3D vector:**
v = [0.5, 0.9, 0.2]

word w t = input word

painted the Grand Canal of Venice in 1908.
surrounding words w t-2 , w t-1 = words to be predicted
surrounding words w t+1 , w t+2

The Skip-gram model is the second main architecture in Word2Vec, offering an alternative approach to learning word embeddings. Unlike CBOW, Skip-gram predicts the surrounding context words given a central (target) word. The input to the network is the one-hot vector of the central word, and the output is a probability distribution over the vocabulary for each surrounding word. A skip-gram is defined as an n-gram that contains gaps because it "skips over intervening tokens," as in predicting "Claude" from "painted" by skipping over "Monet." The activation function is softmax.

<----------section---------->

### Skip-Gram
|V| input and output neurons where V is the vocabulary
n hidden neurons where n is the word embedding dimension

The “squashed” vector after the softmax activation would look like this:

**Example 3D vector after softmax:**
σ(v) = [0.309, 0.461, 0.229]

Notice that the sum of these values (rounded to three significant digits) is approximately 1.0, like a probability distribution.

Figure 6.4 shows the numerical network input and output for the first two surrounding words. In this case, the input word is “Monet,” and the expected output of the network is either “Claude” or “painted,” depending on the training pair.

The Skip-gram neural network architecture also consists of three layers: an input layer, a hidden layer, and an output layer. As with CBOW, the input layer has |V| neurons, where |V| is the size of the vocabulary, and the hidden layer has 'n' neurons, where 'n' is the desired dimensionality of the word embeddings. The output layer has |V| neurons for each of the surrounding words.

<----------section---------->

### Skip-Gram
Ten 5-gram examples from the sentence about Monet

NOTE When you look at the structure of the neural network for word embedding, you’ll notice that the implementation looks similar to what you discovered in chapter 5.

How does the network learn the vector representations?
To train a Word2vec model, you’re using techniques from chapter 2. For example, in table 6.1, wt represents the one-hot vector for the token at position t. So if you want to train a Word2vec model using a skip-gram window size (radius) of two words, you’re considering the two words before and after each target word. You would then use your 5-gram tokenizer from chapter 2 to turn a sentence like this:

```python
sentence = "Claude Monet painted the Grand Canal of Venice in 1806."
```

into 10 5-grams with the input word at the center, one for each of the 10 words in the original sentence.

The training set consisting of the input word and the surrounding (output) words are now the basis for the training of the neural network. In the case of four surrounding words, you would use four training iterations, where each output word is being pre-dicted based on the input word.

Each of the words are represented as one-hot vectors before they are presented to the network (see chapter 2). The output vector for a neural network doing embedding is similar to a one-hot vector as well. The softmax activation of the output layer nodes (one for each token in the vocabulary) calculates the probability of an output word being found as a surrounding word of the input word. The output vector of word probabilities can then be converted into a one-hot vector where the word with the highest probability will be converted to 1, and all remaining terms will be set to 0.

| Input word wt | Expected output wt-2 | Expected output wt-1 | Expected output wt+1 | Expected output wt+2 |
|---------------|----------------------|----------------------|----------------------|----------------------|
| Claude        |                      |                      | Monet                |                      |
| Monet         |                      | Claude               | painted              |                      |
| painted       | Claude               | Monet                | the                  | Grand                |
| the           | Monet                | painted              | Grand                | Canal                |
| Grand         | painted              | the                  | Canal                | of                   |
| Canal         | the                  | Grand                | of                   | Venice               |
| of            | Grand                | Canal                | Venice               | in                   |
| Venice        | Canal                | of                   | in                   | 1908                 |
| in            | of                   | Venice               | 1908                 |                      |
| 1908          | Venice               | in                   |                      |                      |

This segment describes how a skip-gram model learns word vector representations. It starts by tokenizing the input text into n-grams and then representing each word as a one-hot vector. During the training process, the model aims to predict the surrounding words based on the input word, updating the weights through backpropagation. By performing several training iterations, the model learns the vector representations, which capture the contextual meaning of the word.

<----------section---------->

### Skip-Gram
After the training is complete the output layer of the network is discarded. Only the weights of the inputs to the hidden layer are important. They represent the semantic meaning of words.
Similar words are found in similar contexts …
… their weights to the hidden layer adjust in similar ways
… this result in similar vector representations

### CBOW vs Skip-Gram
**CBOW**
*   Higher accuracies for frequent words, much faster to train, suitable for large datasets

**Skip-Gram**
*   Works well with small corpora and rare terms

**Dimension of Embeddings (n)**
*   Large enough to capture the semantic meaning of tokens for the specific task
*   Not so large that it results in excessive computational expense

Similar to CBOW, after the training phase, the output layer is discarded, and the weights from the input to the hidden layer serve as the word embeddings. This emphasizes that Word2Vec and other techniques learn the word embeddings from the co-occurrence statistics present in a text corpus. CBOW works well with frequent words and large datasets, while Skip-gram is more effective for rare words and smaller corpora.  Choosing the right dimension for word embeddings ('n') is a compromise between capturing semantic meaning and computational efficiency.

<----------section---------->

### Improvements to Word2Vec
This section introduces enhancements to the basic Word2Vec algorithm to improve its performance and effectiveness.

<----------section---------->

### Improvements to Word2Vec
**Frequent Bigrams**
*   Some words often occur in combination
*   Elvis is often followed by Presley forming a bigram
*   Predicting Presley after Elvis doesn't add much value
*   To let the network focus on useful predictions frequent bigrams and trigrams are included as terms in the Word2vec vocabulary
*   Inclusion criteria: co-occurrence frequency greater than a threshold

```
score(wi, wj) = (count(wi, wj) - δ) / (count(wi) * count(wj))
```

*   Examples: Elvis\_Presley, New\_York, Chicago\_Bulls, Los\_Angeles\_Lakers, etc.

One improvement is the inclusion of frequent bigrams and trigrams as single tokens in the vocabulary.  This is because some word combinations, such as "Elvis Presley" or "New York," have a meaning that is different from the individual words. This scoring function evaluates the co-occurrence of two words (wi and wj).

<----------section---------->

### Improvements to Word2Vec
**Subsampling Frequent Tokens**
*   Common words (like stop-words) often don’t carry significant information
*   Being frequent, they have a big influence on the training process

**To reduce their effect...**
*   During training (skip-gram method), words are sampled in inverse proportion to their frequency
*   **Probability of sampling:**

```
P(wi) = 1 - sqrt(t / f(wi))
```
Where ```f(wi)``` is the frequency of a word across the corpus, and ```t``` represents a frequency threshold above which you want to apply the subsampling probability.

*   The effect is like the IDF effect on TF-IDF vectors

Subsampling frequent tokens involves down-sampling common words (like stop words) during training. These words often don't contribute significantly to the semantic meaning of the text and can disproportionately influence the training process. The probability of sampling a word is inversely proportional to its frequency in the corpus, which is similar to the IDF (Inverse Document Frequency) weighting used in TF-IDF.

<----------section---------->

### Improvements to Word2Vec
**Negative Sampling**
*   Each training example causes the network to update all weights
*   With thousands or millions of words in the vocabulary, this makes the process computationally expensive

**Instead of updating all weights...**
*   Select 5 to 20 negative words (words not in the context)
*   Update weights only for the negative words and the target word
*   Negative words are selected based on their frequency
*   Common words are chosen more often than rare words
*   The quality of embeddings in maintained

Negative sampling is a technique used to reduce the computational cost of training Word2Vec models. Instead of updating all the weights in the network for each training example, only a small number of negative examples (words not in the context) are updated, along with the target word. This significantly speeds up the training process while maintaining the quality of the word embeddings.

<----------section---------->

### Word2Vec Alternatives
This section introduces