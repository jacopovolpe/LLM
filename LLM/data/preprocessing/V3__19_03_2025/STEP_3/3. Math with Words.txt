## Natural Language Processing and Large Language Models

This lesson, designed for a Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering), delves into the fundamental concepts of Natural Language Processing (NLP) and their application in Large Language Models (LLMs). The material is presented by Nicola Capuano and Antonio Greco from DIEM – University of Salerno.

## Outline

This lesson will cover the following key topics:

*   **Term Frequency (TF):** Understanding how often words appear in a document and its significance.
*   **Vector Space Model (VSM):** Representing text as vectors in a multi-dimensional space.
*   **TF-IDF (Term Frequency-Inverse Document Frequency):** A weighting scheme that refines term frequency by considering the rarity of words across the entire corpus.
*   **Building a Search Engine:** Applying these concepts to create a basic search engine.

<----------section---------->

## Term Frequency

Term Frequency (TF) is a fundamental concept in NLP that quantifies how often a term appears within a document. It's a simple yet powerful way to gauge the importance of a word in that specific document.

### Bag of Words (BoW)

The Bag of Words model is a simplified representation of text used in NLP. It treats a document as an unordered collection of words, disregarding grammar and word order but keeping track of word counts. It is a type of vector space model.

*   **One-hot Encoding:** Each word in the text is first represented as a one-hot vector, where the vector's length equals the vocabulary size. The index corresponding to the word is set to 1, while all other indices are set to 0.

*   **Combining One-Hot Vectors:** After encoding, the one-hot vectors are combined to represent the entire document.
    *   **Binary BoW:** One-hot vectors are combined using the OR operation. This results in a binary vector indicating the presence (1) or absence (0) of each word in the document, without regard to frequency.
    *   **Standard BoW:** One-hot vectors are summed element-wise. This creates a vector where each element represents the number of times the corresponding word appears in the document. This count is the Term Frequency.

*   **Term Frequency (TF):** The number of times each word occurs in the text. It's a numerical representation of the importance of each word in the document, according to its frequency.

**Assumption:** The core assumption of TF is that the more times a word appears in a document, the more relevant or important it is to that document's content. This is a straightforward heuristic, but as we'll see later, it has limitations.

<----------section---------->

### Calculating TF

The following Python code demonstrates how to calculate Term Frequency using the `spaCy` library.

```python
# Example sentence
sentence = "The faster Harry got to the store, the faster Harry, the faster, would get home."

import spacy
nlp = spacy.load("en_core_web_sm") # load the language model
doc = nlp(sentence)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct]

tokens
```

**Explanation:**

1.  **Tokenization:** The sentence is processed using `spaCy` to extract individual tokens (words).
2.  **Lowercasing:**  Tokens are converted to lowercase to ensure consistency.
3.  **Stop Word Removal:** Common words (e.g., "the," "is," "and") that don't carry significant meaning are removed.
4.  **Punctuation Removal:** Punctuation marks are removed.

**Output:**

```
{'faster', 'harry', 'got', 'store', 'faster', 'harry', 'faster', 'home'}
```

Note that due to the nature of sets, the duplicates are automatically removed, therefore losing the count of each word. Lists must be used to maintain word count.

```python
# Build BoW with word count
import collections

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct]

bag_of_words = collections.Counter(tokens) # counts the elements of a list
bag_of_words
```

**Explanation:**

1.  **`collections.Counter`:** The `Counter` object efficiently counts the occurrences of each unique token in the `tokens` list.

**Output:**

```
Counter({'faster': 3, 'harry': 2, 'got': 1, 'store': 1, 'home': 1})
```

The output shows the term frequency for each word in the sentence. "faster" appears 3 times, "harry" appears 2 times, and the other words appear once.

```python
# Most common words
bag_of_words.most_common(2) # most common 2 words
```

**Explanation:**

1.  **`most_common(2)`:** This method returns the two most frequent words and their counts.

**Output:**

```
(('faster', 3), ('harry', 2))
```

<----------section---------->

### Limits of TF

While term frequency provides a basic measure of word importance, it has limitations:

**Example:**

*   In document A the word "dog" appears 3 times.
*   In document B the word "dog" appears 100 times.

Is the word "dog" more important for document A or B?

Initially, one might assume "dog" is more important for document B due to its higher frequency.

**Additional information:**

*   Document A is a 30-word email to a veterinarian.
*   Document B is the novel War & Peace (approx. 580,000 words).

With this additional context, the importance of "dog" in document A becomes more apparent. A high count in a short document likely signifies a greater relevance.

<----------section---------->

### Normalized TF

To address the limitations of raw term frequency, we use **Normalized TF**. This is the word count divided by the total number of words in the document, making the term frequencies comparable across documents of different lengths.

*   TF (dog, document A) = 3/30 = 0.1
*   TF (dog, document B) = 100/580000 = 0.00017

After normalization, the term frequency for "dog" in document A (0.1) is significantly higher than in document B (0.00017), indicating that the word is indeed more important in document A.

```python
import pandas as pd

counts = pd.Series(bag_of_words) # from dict to Pandas Series
counts / counts.sum() # calculate TF
```

**Explanation:**

1.  **`pd.Series(bag_of_words)`:** Converts the `Counter` object into a Pandas Series for easier manipulation.
2.  **`counts / counts.sum()`:** Divides each word count by the total number of words in the sentence, normalizing the term frequencies.

**Output:**

```
faster    0.375
harry     0.250
got       0.125
store     0.125
home      0.125
dtype: float64
```

The output shows the normalized term frequency for each word in the example sentence.

<----------section---------->

## Vector Space Model (VSM)

The Vector Space Model (VSM) is a fundamental framework in NLP for representing text documents as vectors in a high-dimensional space. This enables mathematical operations to be performed on the documents, facilitating tasks such as similarity comparison and classification.

### NLTK Corpora

The Natural Language Toolkit (NLTK) provides a rich collection of text corpora, which are valuable resources for training and testing NLP algorithms.

*   **Training and Testing:** Corpora can be used to train models and test the accuracy.
*   **Corpus Access:** NLTK provides convenient methods to access data.
*   **Documentation:** Detailed information on using NLTK corpora can be found at: [https://www.nltk.org/howto/corpus.html](https://www.nltk.org/howto/corpus.html)

### Reuters 21578 corpus

The Reuters-21578 corpus is a widely used dataset for NLP tasks, especially text classification.

*   **Use:** Used for NLP and text classification tasks.
*   **Content:** Contains news articles published by Reuters in 1986.
*   **Categorization:** The news articles are categorized into 90 topics.

<----------section---------->

### Using Reuters 21578

The following code demonstrates how to load and explore the Reuters-21578 corpus using NLTK.

```python
import nltk

nltk.download('reuters') # download the reuters corpus
ids = nltk.corpus.reuters.fileids() # ids of the documents
sample = nltk.corpus.reuters.raw(ids[0]) # first document

print(len(ids), "samples.\n") # number of documents
print(sample)
```

**Explanation:**

1.  **`nltk.download('reuters')`:** Downloads the Reuters corpus.
2.  **`nltk.corpus.reuters.fileids()`:** Retrieves a list of document IDs in the corpus.
3.  **`nltk.corpus.reuters.raw(ids[0])`:** Retrieves the raw text of the first document in the corpus.

**Output:**

```
10788 samples.

Taiwan had a trade trade surplus of 15.6 billion dlrs last
year, 95 pct of it with the U.S.

The surplus helped swell Taiwan's foreign exchange reserves
to 53 billion dlrs, among the world's largest.

“We must quickly open our markets, remove trade barriers and
cut import tariffs to allow imports of U.S. Products, if we
want to defuse problems from possible U.S. Retaliation," said
Paul Sheen, chairman of textile exporters <Taiwan Safe Group>.

A senior official of South Korea's trade promotion
association said the trade dispute between the U.S. And Japan
might also lead to pressure on South Korea, whose chief exports
are similar to those of Japan.

Last year South Korea had a trade surplus of 7.1 billion
dirs with the U.S., Up from 4.9 billion dlrs in 1985.

In Malaysia, trade officers and businessmen said tough
curbs against Japan might allow hard-hit producers of
semiconductors in third countries to expand their sales to the
U.S.
```

The output displays the number of documents in the Reuters corpus and the content of the first document.

<----------section---------->

```python
doc = nlp(sample)

tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]

bag_of_words = collections.Counter(tokens)
counts = pd.Series(bag_of_words).sort_values(ascending=False) # sorted series
counts = counts / counts.sum() # calculate TF

print(counts.head(10))
```

**Explanation:**

1.  **Tokenization and Cleaning:** The raw text is processed to remove stop words, punctuation, and spaces and converted to lowercase.
2.  **Term Frequency Calculation:** Term frequencies are computed.
3.  **Sorted Series**: Pandas Series is sorted to display highest frequency words.

**Output:**

```
u.s.          0.039627
said         0.037296
trade        0.034965
japan        0.027972
dlrs         0.013986
exports      0.013986
tariffs      0.011655
imports      0.011655
billion      0.011655
electronics    0.009324
dtype: float64
```

This shows the most relevant words in the first news article, based on term frequency.

<----------section---------->

```python
ids_subset = ids[:100] # to speed-up we consider only the first 100 elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    doc = nlp(sample)
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
```

**Explanation:**

1.  **Subset Selection:** First 100 documents are processed to accelerate processing.
2.  **Iterate Through Documents:** Loops through the document IDs.
3.  **Processing Steps:** Each document is processed to calculate term frequencies and sort the series.
4.  **Progress tracking:** Displays progress on console using a carriage return.
5.  **DataFrame Creation:** Combines the term frequency data into a Pandas DataFrame.
6.  **Handling Missing Values:** Fills NaN values (resulting from words not present in certain documents) with 0.

**Output:**

```
Sample 100 of 100 processed.
```

<----------section---------->

```python
df
```

**Explanation:**

The resulting Pandas DataFrame, `df`, represents a term-document matrix.

*   **Rows:** Each row represents a document from the Reuters corpus.
*   **Columns:** Each column represents a unique term (word) in the vocabulary.
*   **Values:** Each cell contains the normalized term frequency of a word in a specific document.

**Output:**

```
         u.s.      said     trade     japan      dlrs   exports   tariffs   imports   billion  electronics  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  0.011655  0.011655     0.009324  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  0.018349  0.055046     0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  0.000000  0.000000     0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...       ...       ...          ...  ...

[100 rows x 3089 columns]
```

The above code is inefficient because spaCy extracts much more information from the text than tokenization.

spaCy extracts a lot of information from text:

*   POS (Part of Speech) tagging
*   Lemmatization
*   Dependency parsing
*   Named Entity Recognition (NER)
*   Etc.

However, here, we only need tokenization.

<----------section---------->

### Corpus Processing (Optimization)

When you call `nlp` on a text, spaCy:

*   Tokenizes the text to produce a `Doc` object.
*   Then the `Doc` is processed in several steps (pipeline).

```python
# Show the current pipeline components
print(nlp.pipe_names)
```

**Output:**

```
['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']
```

We must remove these steps from the pipeline.

```python
ids_subset = ids # we now consider all elements
counts_list = []

for i, id in enumerate(ids_subset):
    sample = nltk.corpus.reuters.raw(id)
    # disable=["tok2vec", "tagger", “parser", “attribute_ruler", “lemmatizer", "ner"]
    doc = nlp(sample, disable=["tok2vec", "tagger", "parser", "attribute_ruler", "lemmatizer", "ner"])
    tokens = [tok.lower_ for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]
    bag_of_words = collections.Counter(tokens)
    counts = pd.Series(bag_of_words).sort_values(ascending=False)
    counts_list.append(counts / counts.sum())
    print("\rSample {} of {} processed.".format(i + 1, len(ids_subset)), end="") # print the state

df = pd.DataFrame(counts_list) # list of series to dataframe
df = df.fillna(0) # change NaNs to 0
```

**Explanation:**

1.  **Disable Pipeline Components:** The `disable` argument in `nlp(sample, disable=[...])` tells spaCy to only perform tokenization, bypassing the other steps in the pipeline. This significantly speeds up the process.

**Output:**

```
Sample 10788 of 10788 processed.
```

This code now processes the entire Reuters corpus more efficiently.

<----------section---------->

```python
df
```

**Explanation:**
The resulting DataFrame is the term-document matrix with the optimized code.

*   **Rows:** Documents from Reuters corpus
*   **Columns:** Terms from vocabulary
*   **Values:** Normalized TF

**Output:**

```
         u.s.      said     trade     japan      dlrs   exports   tariffs  ...
0   0.039627  0.037296  0.034965  0.027972  0.013986  0.013986  0.011655  ...
1   0.000000  0.042254  0.000000  0.000000  0.000000  0.000000  0.000000  ...
2   0.000000  0.033333  0.008333  0.016667  0.000000  0.000000  0.000000  ...
3   0.000000  0.027523  0.018349  0.000000  0.000000  0.009174  0.000000  ...
4   0.000000  0.028302  0.000000  0.000000  0.018868  0.018868  0.000000  ...
..       ...       ...       ...       ...       ...       ...       ...  ...

[10788 rows x 49827 columns]
```

### Term-Document Matrix

*   **Rows:** Represent documents in the corpus.
*   **Columns:** Represent unique terms (words) from the vocabulary.
*   **Elements:** Contain term frequencies (TF), normalized TF, or other weighting schemes (like TF-IDF, which will be covered later).

<----------section---------->

### Vector Space Model

Mathematical representation of documents as vectors in a multidimensional space:

*   Each dimension of the space represents a word.
*   Built from a term-document matrix.

#### 2D Example: with normalized TF
We are considering a vocabulary of just 2 words (w1 and w2).

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![2d_example_vector_space.png](2d_example_vector_space.png)

This diagram illustrates documents as points in a 2D space, where each axis represents the normalized TF of a specific word. The closer the points, the more similar the documents are considered to be.

<----------section---------->

### Document Similarity

Measuring the similarity between documents is a common task in NLP. Two common methods are Euclidean Distance and Cosine Similarity.

#### Euclidean Distance:

*   Calculates the straight-line distance between two vectors in the vector space.
*   Sensitive to the magnitude of the vectors (i.e., longer documents with higher term frequencies will have greater distances).
*   The direction of the vectors captures the relative importance of terms and is more informative than their magnitude.
*   Less commonly used in NLP because document length can skew the results.

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![euclidean_distance.png](euclidean_distance.png)

The diagram visualizes the Euclidean distances between the document vectors.

<----------section---------->

#### Cosine Similarity:

*   Measures the cosine of the angle between two vectors.
*   Focuses on the direction of the vectors, ignoring their magnitude.
*   Effective for normalized text representations, because normalization removes magnitude as a factor.
*   Widely used in NLP because it captures similarity in term distribution rather than document length.

```
sim(A, B) = cos(θ) = (A · B) / (|A| * |B|)
```

Where:

*   `A · B` is the dot product of vectors A and B
*   `|A|` and `|B|` are the magnitudes (Euclidean norms) of vectors A and B

```
doc_0 ~(0.1, 0.17)
doc_1 ~(0.056, 0.056)
doc_2 ~(0.056, 0)
```

![cosine_similarity.png](cosine_similarity.png)

The diagram shows how the cosine similarity measures the angle between the document vectors.

<----------section---------->

#### Properties of Cosine Similarity

Cosine similarity is a value between -1 and 1:

*   **1:** The vectors point in the same direction across all dimensions.
    *   Documents use the same words in similar proportion, likely talking about the same thing.
*   **0:** The vectors are orthogonal (at a 90-degree angle) in all dimensions.
    *   Documents share no words, likely discussing completely different topics.
*   **-1:** The vectors point in opposite directions across all dimensions.
    *   Impossible with TF (counts of words cannot be negative).
    *   Can happen with other word representations (discussed later, like word embeddings).

<----------section---------->

#### Calculate Cosine Similarity

```python
import numpy as np

def sim(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

# Example:
print(sim(df.iloc[0], df.iloc[1]))
print(sim(df.iloc[0], df.iloc[2]))
print(sim(df.iloc[0], df.iloc[0]))
```

**Explanation:**

1.  **`sim(vec1, vec2)` Function:** This function calculates the cosine similarity between two vectors.
2.  **Dot Product:**  Calculates the dot product of the two vectors using `np.dot()`.
3.  **Vector Norms:** Calculates the Euclidean norms (magnitudes) of the vectors using `np.linalg.norm()`.
4.  **Cosine Similarity:** Calculates cosine similarity by dividing the dot product by the product of the norms.

**Output:**

```
0.14261893769917347
0.2365347461078053
1.0
```

The output displays the cosine similarity between different document vectors. Note that the similarity of a document to itself is 1.

```python
# Compare TF matrix subset (documents 0 and 1)
df.loc[[0, 1], (df.loc[0] > 0) & (df.loc[1] > 0)]
```

**Explanation:**
This code shows the overlap in words between documents 0 and 1.

**Output:**

```
       said      min      year       pct  government
0  0.037296  0.002331  0.009324  0.004662    0.002331
1  0.042254  0.028169  0.014085  0.056338    0.014085
```

```python
# Try also with other documents
```
This is an instruction to the reader to manually explore other document pairs in the DataFrame.

<----------section---------->

## TF-IDF

### Inverse Document Frequency

TF does not consider the uniqueness of the word across the corpus.

*   Common words (the, is, and …) appear frequently in many documents, contributing little to distinguishing one document from another.
*   Specific terms relevant to the subject matter of the corpus may also be frequent but still not useful for differentiating documents.
*   In a corpus about astronomy, terms like planet or star would be common and non-discriminatory.

Inverse Document Frequency (IDF) addresses this by measuring the importance of each word in relation to the entire corpus.

### Inverse Document Frequency
IDF increases the weight of words that are rare across documents and decreases the weight of words that are common.

```
idf(t, D) = log(N / |{d ∈ D: t ∈ d}|)
```

Where:

*   `N`: total number of documents in the corpus `N = |D|`
*   `|{d ∈ D: t ∈ d}|` : number of documents where the term t appears (i.e., tf(t, d) != 0).

**Note:** If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the numerator `1 + N` and denominator to `1 + |{d ∈ D:t ∈ d}|`.

The logarithm in the IDF formula serves to dampen the effect of IDF.

### TF-IDF
Term Frequency – Inverse Document Frequency is the product of TF and IDF.

*   TF gives more importance to the words that are more frequent in the document.
*   IDF gives more weightage to the words that are less frequent in the corpus.

```
tfidf(t, d, D) = tf(t, d) * idf(t, D)
```

*   High TF-IDF indicates a term that is frequent in a document but rare in the corpus, making it significant for that document.
*   Low TF-IDF Indicates a term that is infrequent in the document or common in the corpus, making it less significant for that document.

<----------section---------->

### TF-IDF: Example
*   Document A: Jupiter is the largest planet
*   Document B: Mars is the fourth planet from the sum

| Term      | Document A | Document B | df | idf      | TF-IDF (A) | TF-IDF (B) |
| --------- | ---------- | ---------- | -- | -------- | ---------- | ---------- |
| jupiter   | 1          | 0          | 1  | ln(2/1)=0.69 | 0.138      | 0          |
| largest   | 1          | 0          | 1  | ln(2/1)=0.69 | 0.138      | 0          |
| mars      | 0          | 1          | 1  | ln(2/1)=0.69 | 0          | 0.138      |
| fourth    | 0          | 1          | 1  | ln(2/1)=0.69 | 0          | 0.138      |
| planet    | 1          | 1          | 2  | ln(2/2)=0  | 0          | 0          |

**Explanation:**
*Document Frequency (df):* Number of documents that contain the term. *Inverse Document Frequency (idf):* Calculated as the natural logarithm of the total number of documents divided by the document frequency. *TF-IDF:* Calculated by multiplying the term frequency (TF) by the inverse document frequency (IDF).

In this example, the word "planet" appears in both documents, and has an IDF score of 0, diminishing its importance. "Jupiter," "largest," "mars," and "fourth" are unique to each document, therefore have non-zero TF-IDF scores.

<----------section---------->

### Zipf’s Law
Observes patterns in word frequency distribution in natural languages.

*   Named after linguist George Kingsley Zipf.
*   Frequency of a word is inversely proportional to its rank in a frequency table.

```
f(r) = K / r**α
```

Where:

*   f(r) is the frequency of the word at rank r
*   K is a constant
*   r is the rank of the word
*   α determines the shape of the distribution which is approximately equals to 1

### Zipf’s Law
Meaning:

*   The most common word appears approximately:
    *   Twice as often as the second most common word
    *   Three times as often as the third most common word
    *   And so on …

Implications:

*   A small set of highly frequent words dominate word usage.
*   Majority of words are relatively rare with low frequencies.
*   Using the logarithm in IDF mitigates the influence of rare words.
*   This results in a more uniformly distributed TF-IDF score.

<----------section---------->

### Zipf’s Law: Example

```
Zipf plot for Brown corpus tokens
```

![zipf_example.png](zipf_example.png)

The Brown corpus consists of 1 million words (500 samples of 2000+ words each) of running text of edited English prose printed in the United States during the year 1961 and it was revised and amplified in 1979.

The Zipf's Law distribution is useful for understanding the distribution of words in a large corpus of text. This can be used in feature engineering by setting thresholds to remove tail-end words, or determining an appropriate number of dimensions in topic modeling.

### Calculating TF-IDF

```python
# Calculate IDF vector
n_docs = df.shape[0]
count_docs = (df > 0).sum()
idf = np.log10(n_docs / count_docs)

idf.sort_values(ascending=False)
```

**Explanation:**

1.  **`n_docs = df.shape[0]`:**  Gets the total number of documents (rows) in the DataFrame.
2.  **`count_docs = (df > 0).sum()`:** Calculates the number of documents containing each term. The expression `df > 0` creates a boolean DataFrame, where `True` indicates the term is present in the document.  `.sum()` then counts the `True` values for each column.
3.  **`idf = np.log10(n_docs / count_docs)`:** Calculates the IDF for each term.
4.  **`idf.sort_values(ascending=False)`:** Sorts the IDF values in descending order.

**Output:**

```
asian           4.032221
exaggerated     4.032221
refute          4.032221
man             4.032221
laying          4.032221
                  ...
pet             0.494850
year            0.408935
min             0.366532
>               0.292430
said            0.130768
Length: 49827, dtype: float64
```

The output shows the IDF values for each term, sorted from the highest to the lowest. Terms with higher IDF values are rarer across the corpus.

<----------section---------->

```python
# Calculate TF-IDF matrix
tfidf = df.mul(idf.values, axis='columns')
tfidf
```

**Explanation:**

1.  **`df.mul(idf.values, axis='columns')`:** Multiplies each term frequency (TF) value in the DataFrame by the corresponding IDF value. `axis='columns'` ensures that the multiplication is performed column-wise (i.e., each column is multiplied by the corresponding IDF value).

**Output:**

```
          u.s.      said     trade     japan      dlrs   exports   tariffs  ...
0   1.597892  0.004877  1.129083  0.953560  0.047840  0.047840  0.039616  ...
1   0.000000  0.005525  0.000000  0.000000  0.000000  0.000000  0.000000  ...
2   0.000000  0.004359  0.027888  0.056906  0.000000  0.000000  0.000000  ...
3   0.000000  0.003599  0.062565  0.000000  0.000000  0.031283  0.000000  ...
4   0.000