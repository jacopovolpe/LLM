# Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)

**Lesson 21: Reinforcement Learning from Human Feedback (RLHF)**

Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This lesson explores Reinforcement Learning from Human Feedback (RLHF), a cutting-edge technique used to refine and improve Large Language Models (LLMs). This approach allows LLMs to better align with human values, preferences, and ethical considerations.

<----------section---------->

## Outline

*   Reinforcement Learning from Human Feedback (RLHF): Understanding the concept and its significance in LLM development.
*   Transformers `trl` library: Introduction to the `trl` library provided by Hugging Face, a toolset designed to facilitate the training of Transformer language models using Reinforcement Learning techniques.
*   Try it yourself: Practical guidance and resources to experiment with RLHF and the `trl` library.

<----------section---------->

## Reinforcement Learning from Human Feedback (RLHF)

### What is RLHF?

*   RLHF is a technique that leverages human feedback to fine-tune and improve Large Language Models (LLMs). It involves using human preferences and judgments as a reward signal to guide the learning process of the model.  Instead of solely relying on pre-defined datasets, RLHF incorporates human insights, enabling the model to learn from subjective evaluations.
*   RLHF serves as a strategy to balance the raw performance of an LLM (e.g., accuracy, fluency) with its alignment to human values and preferences. This alignment includes factors like safety, ethics, helpfulness, and overall user satisfaction. It helps ensure that the LLM produces outputs that are not only accurate and coherent but also ethically sound and beneficial to users.

### Why RLHF?

*   RLHF offers a potential strategy to "ground" the focus of an LLM, helping it to generate responses that are more relevant, specific, and in line with the intended purpose.  By incorporating human feedback, the model can learn to distinguish between responses that are merely correct and those that are truly valuable and appropriate in a given context.
*   RLHF can significantly enhance the safety, ethical considerations, and user satisfaction associated with LLMs. It helps to mitigate the risk of generating harmful, biased, or inappropriate content, making the LLM a more reliable and trustworthy tool for various applications. It helps the model avoid generating toxic or offensive content and encourages the model to produce responses that are sensitive to cultural norms and user expectations.

### Workflow of RLHF

The RLHF workflow typically involves three main stages:

1.  **Pre-trained Language Model:** This is the foundation of the RLHF process. It is a base LLM that has been pre-trained on a massive corpus of text data (e.g., BERT, GPT, T5). The pre-training stage equips the model with a broad understanding of language, grammar, and factual knowledge. These models are trained using self-supervised learning techniques on large amounts of text data, and they learn to predict the next word in a sequence.
2.  **Reward Model:** A secondary model is trained to score LLM outputs based on human feedback. Human raters provide preferences or rankings for different responses generated by the base LLM for given prompts. The reward model learns to predict these human preferences, effectively quantifying the quality and desirability of LLM outputs.
3.  **Fine-Tuning with Reinforcement Learning:** The pre-trained LLM is further optimized using reinforcement learning (RL). The reward model serves as the environment's reward function, guiding the LLM to generate responses that maximize the predicted human preference score. This stage utilizes RL algorithms like Proximal Policy Optimization (PPO) to iteratively refine the LLM's policy, leading to improved alignment with human expectations.

<----------section---------->

### Reward Model

*   **Inputs:** Training a reward model requires specific input data:
    *   Multiple LLM-generated outputs for given prompts: This consists of various responses generated by the base LLM for the same input prompt. Having multiple outputs allows the reward model to learn to differentiate between responses of varying quality.
    *   Corresponding human rank responses according to their preferences: Human raters rank these LLM-generated outputs based on their perceived quality, helpfulness, and alignment with human values. These rankings provide the ground truth for training the reward model.
*   The primary goal of training a reward model is to predict human preference scores accurately. This involves learning a function that maps LLM outputs to a numerical score that reflects how much humans would prefer that output over others.
*   The methodology often involves using a ranking loss function. This type of loss function is designed to teach the reward model which outputs humans prefer relative to others. For example, if human raters consistently prefer output A over output B, the ranking loss function will penalize the reward model if it assigns a higher score to output B.

### Fine-tuning with Proximal Policy Optimization (PPO)

*   The overarching goal of fine-tuning with PPO is to align the LLM’s outputs with human-defined quality metrics. This goes beyond simply maximizing accuracy; it involves ensuring that the LLM's responses are helpful, informative, safe, and ethical.

    1.  **Generate responses using the LLM:** The LLM generates responses to a variety of prompts or inputs.
    2.  **Score responses with the reward model:** The reward model evaluates the generated responses and assigns a score based on its learned understanding of human preferences.
    3.  **Update the LLM to maximize reward scores:** The LLM's parameters are adjusted using PPO to encourage the generation of responses that receive higher scores from the reward model. PPO is a policy gradient method, meaning it directly optimizes the LLM's policy (the function that determines its output) to increase the likelihood of generating high-reward responses.

<----------section---------->

### Pros and Cons of RLHF

**Pros:**

*   **Iterative Improvement:** RLHF enables iterative improvement by continuously collecting human feedback as the model evolves. This feedback can then be used to update both the reward model and the LLM through fine-tuning, creating a virtuous cycle of improvement.
*   **Improved Alignment:** RLHF leads to the generation of responses that are closer to human intent. The model learns to understand and respond to user queries in a way that aligns with their expectations and preferences.
*   **Ethical Responses:** RLHF can reduce harmful or biased outputs by training the model to avoid generating content that is offensive, discriminatory, or otherwise unethical.
*   **User-Centric Behavior:** RLHF allows for tailoring interactions to user preferences. The model can learn to adapt its communication style and content to suit individual users, resulting in a more personalized and engaging experience.

**Cons:**

*   **Subjectivity:** Human feedback can vary widely depending on the individual raters and their perspectives. This subjectivity can introduce noise into the training process and make it challenging to develop a robust reward model.
*   **Scalability:** Collecting sufficient, high-quality human feedback is a resource-intensive process. It requires recruiting and training raters, designing evaluation protocols, and managing the feedback collection process.
*   **Reward Model Robustness:** A misaligned reward model can lead to suboptimal fine-tuning. If the reward model does not accurately reflect human preferences, the LLM may be optimized to generate responses that are not actually desirable or helpful. This is known as the "reward hacking" problem.

<----------section---------->

### Tasks to Enhance with RLHF

*   **Text Generation:** RLHF can significantly enhance the quality of text produced by LLMs, making it more coherent, engaging, and informative.
*   **Dialogue Systems:** RLHF is valuable for improving the performance of dialogue systems, enabling them to have more natural, helpful, and engaging conversations with users.
*   **Language Translation:** RLHF can increase the precision of language translation by ensuring that the translated text accurately conveys the meaning and intent of the original text.
*   **Summarization:** RLHF can raise the standard of summaries produced by LLMs, making them more concise, accurate, and informative.
*   **Question Answering:** RLHF can increase the accuracy of question answering by training the model to provide relevant and helpful answers to user queries.
*   **Sentiment Analysis:** RLHF has been used to increase the accuracy of sentiment identification for particular domains or businesses, allowing for more nuanced and accurate analysis of customer feedback and opinions.
*   **Computer Programming:** RLHF can be used to speed up and improve software development by assisting programmers with code generation, debugging, and documentation.

<----------section---------->

### Case Study: GPT-3.5 and GPT-4

*   The pre-trained models GPT-3.5 and GPT-4 have been fine-tuned using RLHF, demonstrating the effectiveness of this technique in improving LLM performance.
*   OpenAI declares that they achieved the following with RLHF:
    *   Enhanced alignment with human values and preferences.
    *   Fewer unsafe or inappropriate outputs.
    *   More human-like interactions, making the model more engaging and natural to interact with.
*   These models were or are widely used in real-world applications like ChatGPT, showcasing the practical impact of RLHF in developing advanced AI systems.
*   The models are still incrementally improved with additional human feedback, highlighting the ongoing nature of the RLHF process and the commitment to continuous improvement.

<----------section---------->

## Transformers `trl` library

### TRL: Transformer Reinforcement Learning

*   TRL is a full-stack library that provides a comprehensive set of tools for training transformer language models with Reinforcement Learning. It covers the entire RLHF pipeline, from Supervised Fine-tuning (SFT) and Reward Modeling (RM) to Proximal Policy Optimization (PPO).
*   The library is seamlessly integrated with Hugging Face transformers, leveraging the popular Transformers library and the Hugging Face ecosystem. This integration simplifies the process of building and training RLHF models.

<----------section---------->

## Try it yourself

*   Study the `trl` library on Hugging Face: [https://huggingface.co/docs/trl/v0.7.8/index](https://huggingface.co/docs/trl/v0.7.8/index)
*   Give a careful look to:
    *   `PPOTrainer`: [https://huggingface.co/docs/trl/v0.7.8/ppo_trainer](https://huggingface.co/docs/trl/v0.7.8/ppo_trainer):  Understand how to use the `PPOTrainer` class for optimizing the language model using the PPO algorithm.
    *   `RewardTrainer`: [https://huggingface.co/docs/trl/v0.7.8/reward_trainer](https://huggingface.co/docs/trl/v0.7.8/reward_trainer): Learn how to use the `RewardTrainer` class for training the reward model based on human feedback.
*   Study the examples that are closer to your purposes:
    *   Sentiment analysis tuning: [https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning](https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning): Explore how to use RLHF for fine-tuning language models to generate text with specific sentiment.
    *   Detoxifying a Large Language Model with PPO: [https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm](https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm): Learn how RLHF can be used to mitigate toxicity and harmful content generated by LLMs.
*   Try to apply RLHF to your project.

```python
# Step 1: Train your model on your favorite dataset using Supervised Fine-Tuning (SFT)
from trl import SFTTrainer

trainer = SFTTrainer(
    model_name="facebook/opt-350m",
    dataset_text_field="text",
    max_seq_length=512,
    train_dataset=dataset,
)
trainer.train()

# Step 2: Train a reward model
from trl import RewardTrainer

trainer = RewardTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
)
trainer.train()

# Step 3: Further optimize the SFT model using the rewards from the reward model and PPO algorithm
from trl import PPOConfig, PPOTrainer

config = PPOConfig()
trainer = PPOTrainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    query_dataloader=query_dataloader,
)

for query in query_dataloader:
    response = model.generate(query)
    reward = reward_model(response)
    trainer.step(query, response, reward)
```

<----------section---------->

This code snippet provides a basic outline of how to implement RLHF using the `trl` library. It involves three main steps: 1) Supervised Fine-Tuning (SFT) of a base LLM, 2) training a reward model based on human feedback, and 3) fine-tuning the SFT model using the PPO algorithm guided by the reward model. The links in this document will provide you with much more detail to achieve the stated goals.
