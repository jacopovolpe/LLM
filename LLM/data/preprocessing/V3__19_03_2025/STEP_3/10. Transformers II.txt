# Natural Language Processing and Large Language Models
## Corso di Laurea Magistrale in Ingegneria Informatica
### Lesson 10: Transformers II
**Nicola Capuano and Antonio Greco**
**DIEM – University of Salerno**

This lecture, "Transformers II," presented by Nicola Capuano and Antonio Greco from DIEM – University of Salerno, delves into the architecture and functionality of Transformers, a pivotal component in modern Natural Language Processing (NLP). This lesson builds upon the foundational concepts of Transformers, focusing on the specific mechanisms that enable them to process and generate language effectively. The course is targeted for students of Ingegneria Informatica (Computer Engineering).

<----------section---------->

## Outline

This lesson covers the following key components of the Transformer architecture:

*   **Multi-Head Attention:** Discussing how this mechanism enhances the model's ability to capture different relationships within the input data.
*   **Encoder Output:** Explaining the representation of the input sequence generated by the encoder.
*   **Decoder:** Detailing the decoder's role in generating the output sequence based on the encoder's output.
*   **Masked Multi-Head Attention:** Describing the function of masking in self-attention within the decoder to prevent peeking at future tokens.
*   **Encoder-Decoder Attention:** Explaining how the decoder uses the encoder's output to focus on relevant parts of the input sequence during generation.
*   **Output:** Describing the final layers of the Transformer that produce the output sequence.
*   **Transformer’s pipeline:** Providing a holistic view of the data flow through the Transformer architecture, from input to output.

<----------section---------->

## Multi-head attention

### Multi-head Attention
*   **Encoding Different Meanings:** By employing multiple self-attention "heads," the model can capture various contextual relationships and nuances within the input sequence. Each head focuses on different aspects of the input, allowing for a more comprehensive understanding.
*   **Parallel Scaled-Dot Product Attention:** Several scaled dot-product attention computations are executed in parallel. Each attention head uses its own set of learned weight matrices to project the input into different representation subspaces.
*   **Concatenation:** The outputs from each of the parallel attention heads are concatenated row-by-row to form a larger matrix. Critically, this matrix retains the same number of rows (m) as the original input, ensuring compatibility with subsequent layers.
*   **Final Weight Matrix:** This concatenated matrix is then multiplied by a final, learned weight matrix. This matrix combines the information from the different attention heads into a unified representation.
*   **Multi-Head Attention Defined:** This entire process, involving parallel attention heads, concatenation, and a final linear transformation, is termed multi-head attention. It's a critical innovation enabling Transformers to model complex relationships in data.

### Multi-head Attention
Multi-head attention offers a significant advantage over single-head attention mechanisms. It allows the model to attend to information from various representation subspaces at different positions. This means the model can simultaneously consider multiple aspects of the input, such as syntactic relationships, semantic meanings, and long-range dependencies. In contrast, a single attention head might average out these different aspects, hindering the model's ability to capture complex relationships. The concatenated outputs of the heads are then multiplied by a weight matrix, which combines the different representations at the same network level, further enriching the model's representation power.

<----------section---------->

## Add & Norm

### Add (skip connections) & Norm
This section highlights two key techniques used within each Transformer block to improve training and performance: Add (skip connections or residual connections) and Layer Normalization.

**Input Normalization (Z):**
*   **Mean 0, Std dev 1:** Input normalization involves scaling the input to have a mean of 0 and a standard deviation of 1.
*   **Stabilizes training:** This normalization process helps stabilize training by preventing activations from becoming too large or too small, which can lead to vanishing or exploding gradients.
*   **Regularization effect:** Input normalization also provides a regularization effect, which helps to prevent overfitting by reducing the sensitivity of the model to individual training examples.

**Add -> Residuals:**
*   **Avoid vanishing gradients:** Skip connections allow gradients to flow more easily through the network, bypassing potential bottlenecks caused by multiple layers.
*   **Train deeper networks:** By mitigating the vanishing gradient problem, skip connections enable the training of much deeper networks. The model can learn more complex relationships, leading to improved performance. The output of Add & Norm block is fed to the next layers to propagate the information through the network.

<----------section---------->

## Feed Forward

### Feed Forward
Non-linearity is introduced with a feed-forward network, comprising two fully connected layers and a non-linear activation function between them, it's also described as a (2-layer MLP). This network is applied position-wise, meaning each position in the sequence is processed independently. This operation allows the Transformer to learn complex relationships between features at each position.

*   **Non-linearity:** The use of non-linear activation functions enables the model to learn complex, non-linear relationships in the data.
*   **Complex Relationships:** Feed Forward Networks (FFNs) help the model to capture relationships between features.

FFN (2 layer MLP)

<----------section---------->

## Transformer’s Encoder

### Transformer’s Encoder
The Transformer's encoder is responsible for generating a representation of the input sequence. The encoder block leverages an additive positional encoding which tackles the order-agnostic nature of the self-attention mechanism. The encoder also uses residual connection to foster the gradients flow and adopts normalization layers to stabilize the network training. Position-Wise Feed-Forward layer to add non-linearity and is applied to each sequence element independently. The dimensionality of each encoder block is the same, of the input, so it is possible to stack an arbitrary number of encoder’s blocks. The output of the first block is fed to the second block (no word embeddings) and so on.

<----------section---------->

## Decoder

### Decoder
The decoder leverages the intermediate representation generated by the encoder, denoted as *z*<sub>1</sub>,…,*z*<sub>*t*</sub> to generate the output sequence *y*<sub>1</sub>,…,*y*<sub>*m*</sub>. The decoding process operates sequentially, generating one element at a time. At each step *i*, the decoder uses the encoder's output (*z*<sub>1</sub>,…,*z*<sub>*t*</sub>) and the previously generated output elements (*y*<sub>1</sub>,…,*y*<sub>*i*-1</sub>) to predict the next output element *y*<sub>*i*</sub>.

### Decoder
The decoder consists of a series of decoder blocks with identical structure. The original Transformer paper used 6 such blocks. The decoder blocks contain the same modules used in the encoder block, plus a module for attention where the keys and values are taken from the encoder's intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub>. Also, the self-attention module is slightly modified so as to ensure that the query at position i only uses the values at positions 1,…,i. The self-attention mechanism is masked to prevent the decoder from attending to future tokens, ensuring that the output is generated sequentially. On top of the last decoder block, the decoder includes an additional linear layer and a softmax activation function, for computing the probability of the next output element *y*<sub>*i*</sub>. The last layers has a number of neurons corresponding to the cardinality of the output set.

<----------section---------->

## Masked Multi-Head Attention
Masked Multi-Head Attention is a crucial component of the decoder. Its purpose is to ensure that, when predicting an output at time *T*, the model only attends to outputs up to time *T-1*. This is achieved by masking the available attention values. The attention mask *M* is applied to the attention values to prevent the model from "peeking" at future tokens in the sequence.

*R<sup>IxT</sup>*:  Representing the attention values.
*R<sup>TxT</sup>*:  Representing the Attention Mask: M
Masked Attention Values.

<----------section---------->

## Encoder Decoder Attention
The encoder-decoder attention mechanism connects the encoder and decoder. The keys and values are derived from the encoder outputs, while the queries come from the decoder inputs. Every decoder block receives the same FINAL encoder output, allowing it to focus on different parts of the input sequence at each step of the decoding process.

<----------section---------->

## Output

### Output
The final output stage of the Transformer involves a linear transformation followed by a softmax function. The linear layer projects the decoder's output into the vocabulary space. In some implementations, the weights of this linear layer are tied to the weights of the model's input embedding matrix, which improves parameter efficiency and generalization. The softmax function then converts these projections into probabilities, indicating the likelihood of each token in the vocabulary being the next token in the output sequence.

*   **Linear:** A linear layer projects the decoder's output into the vocabulary space.
*   **Linear weights tied with model input embedding matrix:** Linear weights are often tied with model input embedding matrix to improve parameter efficiency and generalization.
*   **Softmax:** Softmax function produces the probability distribution over the output vocabulary.

<----------section---------->

## Transformer’s pipeline
Decoding time step.

ENCODER - DECODER.

The Transformer pipeline is an iterative process where the decoder generates one token at a time, conditioned on the encoder's output and the previously generated tokens. It starts by encoding the input sequence with the Encoder and decoding with the Decoder. The next token is generated step by step.

Reference website:

https://poloclub.github.io/transformer-explainer/
