# Natural Language Processing and Large Language Models
Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)

**Lesson 21: Reinforcement Learning from Human Feedback**

Nicola Capuano and Antonio Greco
DIEM â€“ University of Salerno

<----------section---------->

## Outline

This lesson will cover the following topics:

*   Reinforcement Learning from Human Feedback (RLHF): An in-depth explanation of the RLHF technique.
*   Transformers trl library: An introduction to the Transformers Reinforcement Learning (TRL) library.
*   Try it yourself: Resources and guidance for implementing RLHF in your own projects.

<----------section---------->

## Reinforcement Learning from Human Feedback (RLHF)

### What is RLHF?

*   RLHF is a technique used to refine large language models (LLMs) by incorporating direct human feedback into the training process. This feedback acts as a crucial guide, steering the model toward desired behaviors and outputs.
*   It serves as a strategy to strike a balance between maximizing the LLM's raw performance (e.g., fluency, coherence) and aligning its responses with human values, ethical considerations, and individual user preferences. This ensures the model is not only capable but also responsible and useful.

### Why RLHF?

*   RLHF offers a potential method for "grounding" the LLM. Grounding refers to anchoring the model's focus and output in a more concrete, relevant domain or context.  Without grounding, LLMs can sometimes generate outputs that are factually incorrect, nonsensical, or irrelevant.
*   It enhances several key aspects of LLM performance:
    *   **Safety:**  Reduces the likelihood of the model generating harmful, offensive, or dangerous content.
    *   **Ethical Responses:** Encourages the model to provide responses that are aligned with ethical principles and avoid biases.
    *   **User Satisfaction:** Increases user satisfaction by tailoring responses to be more helpful, relevant, and aligned with their specific needs and expectations.

### Workflow of RLHF

The RLHF workflow typically consists of three primary stages:

1.  **Pre-trained Language Model:** This is the foundation of the entire process. It's a large language model (LLM) that has been trained on a massive dataset of text and code. Examples of such models include BERT, GPT, and T5.  These models possess a general understanding of language and the ability to generate text.
2.  **Reward Model:**  A separate, secondary model is trained to evaluate and score the outputs generated by the pre-trained LLM. This reward model learns to predict human preferences based on the human feedback it receives. It assigns a "reward" score to each LLM-generated output, reflecting its perceived quality and alignment with human expectations.
3.  **Fine-Tuning with Reinforcement Learning:** In this stage, the pre-trained LLM is further refined using reinforcement learning techniques.  The reward model provides the signal that guides the optimization process. The goal is to adjust the LLM's parameters so that it consistently generates outputs that receive high reward scores from the reward model.  This process is iterative, allowing the LLM to gradually improve its ability to produce desirable and human-aligned responses.

<----------section---------->

### Reward Model

*   The training data for the reward model is crucial for its success and consists of the following:
    *   **Multiple LLM-generated outputs for given prompts:** For a single input prompt, several different outputs are generated by the LLM. This creates a set of diverse responses for the reward model to evaluate.
    *   **Corresponding human rank responses according to their preferences:** Human evaluators then rank these different LLM outputs based on their subjective assessment of quality, helpfulness, safety, and alignment with the prompt.  These rankings represent human preferences.
*   The primary objective is to train the reward model to accurately predict human preferences. This means the model should learn to assign higher scores to outputs that humans would prefer and lower scores to outputs that humans would find less desirable.
*   The training methodology involves using a ranking loss function.  This type of loss function is specifically designed to teach the reward model to differentiate between preferred and less preferred outputs.  It penalizes the model when it assigns a higher score to an output that humans ranked lower, and vice versa.

<----------section---------->

### Fine-tuning with Proximal Policy Optimization (PPO)

*   The ultimate goal is to align the LLM's outputs with the quality metrics defined by humans. In other words, the LLM should learn to generate responses that are consistently considered high-quality by human evaluators.  Proximal Policy Optimization (PPO) is a reinforcement learning algorithm often used to achieve this.
    1.  **Generate responses using the LLM:**  Given a specific prompt, the LLM generates a response.
    2.  **Score responses with the reward model:** The reward model evaluates the generated response and assigns a score, reflecting how well it aligns with human preferences.
    3.  **Update the LLM to maximize reward scores:**  The PPO algorithm then adjusts the LLM's parameters to increase the likelihood of generating responses that receive higher reward scores in the future. This iterative process gradually improves the LLM's alignment with human-defined quality metrics.

<----------section---------->

### Pros and Cons of RLHF

**Pros:**

*   **Iterative Improvement:** The framework enables a continuous cycle of improvement. Human feedback can be collected as the model evolves, allowing for periodic updates to the reward model and subsequent fine-tuning of the LLM. This iterative approach ensures the model remains aligned with evolving human values and preferences.
*   **Improved Alignment:** RLHF leads to the generation of responses that are more closely aligned with human intent and expectations. This improves the usefulness and relevance of the model's outputs.
*   **Ethical Responses:** By incorporating human feedback, RLHF helps to reduce the generation of harmful, biased, or otherwise undesirable outputs. This enhances the safety and ethical considerations of the LLM.
*   **User-Centric Behavior:** The method facilitates the tailoring of interactions to individual user preferences. This customization leads to a more personalized and satisfying user experience.

**Cons:**

*   **Subjectivity:** Human feedback is inherently subjective and can vary widely depending on the individual evaluator. This variability can introduce noise into the training process and make it challenging to define a consistent objective for the model.
*   **Scalability:** Gathering sufficient, high-quality human feedback is a resource-intensive process, especially for very large language models. The cost and logistical challenges associated with collecting and processing this feedback can be significant.
*   **Reward Model Robustness:** A poorly trained or misaligned reward model can lead to suboptimal fine-tuning of the LLM. If the reward model does not accurately reflect human preferences, the LLM may learn to generate outputs that are technically "high-scoring" but are not actually desirable from a human perspective.

<----------section---------->

### Tasks to Enhance with RLHF

RLHF can be used to improve various NLP tasks:

*   **Text Generation:** Elevate the quality and coherence of the text generated by LLMs.
*   **Dialogue Systems:** Boost the performance of dialogue systems, resulting in more natural and engaging conversations.
*   **Language Translation:** Enhance the precision and accuracy of language translation, yielding more faithful and reliable translations.
*   **Summarization:** Increase the standard of summaries produced by LLMs, ensuring they are concise, informative, and representative of the original text.
*   **Question Answering:** Improve the accuracy and relevance of question answering systems, allowing them to provide more helpful and insightful answers.
*   **Sentiment Analysis:** Increase the precision of sentiment identification for specific domains or businesses. RLHF enables targeted analysis that reflects nuanced sentiment within particular contexts.
*   **Computer Programming:** Expedite and refine software development by using LLMs to generate code, documentation, and other programming-related outputs that align with human expectations.

<----------section---------->

### Case Study: GPT-3.5 and GPT-4

*   These pre-trained models have undergone fine-tuning using RLHF, among other techniques, to improve their overall performance and alignment.
*   OpenAI reports that RLHF has led to several key improvements:
    *   Enhanced alignment with human values and preferences.
    *   Reduced generation of unsafe or harmful outputs.
    *   More human-like and engaging interactions.
*   These models have been, or are currently, widely used in real-world applications such as ChatGPT.
*   The models are continuously refined with ongoing human feedback, enabling incremental improvements in their capabilities and alignment over time.

<----------section---------->

## Transformers trl library

### TRL: Transformer Reinforcement Learning

*   TRL (Transformer Reinforcement Learning) is a comprehensive library that provides a set of tools and modules for training transformer language models using reinforcement learning. It covers the entire training pipeline, from supervised fine-tuning (SFT) and reward modeling (RM) to proximal policy optimization (PPO).
*   The library is seamlessly integrated with the Hugging Face Transformers library, providing a user-friendly and efficient environment for RLHF training.

<----------section---------->

## Try it yourself

*   Explore the TRL library on Hugging Face: [https://huggingface.co/docs/trl/v0.7.8/index](https://huggingface.co/docs/trl/v0.7.8/index)
*   Examine the following components:
    *   `PPOTrainer`: [https://huggingface.co/docs/trl/v0.7.8/ppo_trainer](https://huggingface.co/docs/trl/v0.7.8/ppo_trainer)
    *   `RewardTrainer`: [https://huggingface.co/docs/trl/v0.7.8/reward_trainer](https://huggingface.co/docs/trl/v0.7.8/reward_trainer)
*   Investigate examples relevant to your project:
    *   Sentiment analysis tuning: [https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning](https://huggingface.co/docs/trl/v0.7.8/sentiment_tuning)
    *   Detoxifying a Large Language Model with PPO: [https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm](https://huggingface.co/docs/trl/v0.7.8/detoxifying_a_lm)
*   Implement RLHF in your project.

```python
# Step 1: Train your model on your favorite dataset using Supervised Fine-Tuning (SFT)
from trl import SFTTrainer

trainer = SFTTrainer(
    model_name="facebook/opt-350m",  # Specify the base model
    dataset_text_field="text",        # Indicate the text field in your dataset
    max_seq_length=512,               # Define the maximum sequence length
    train_dataset=dataset,             # Provide your training dataset
)
trainer.train()                       # Start the training process

# Step 2: Train a reward model
from trl import RewardTrainer

trainer = RewardTrainer(
    model=model,                     # Pass your model
    tokenizer=tokenizer,               # Provide your tokenizer
    train_dataset=dataset,             # Provide your training dataset
)
trainer.train()                       # Start the training process

# Step 3: Further optimize the SFT model using the rewards from the reward model and PPO algorithm
from trl import PPOConfig, PPOTrainer

config = PPOConfig()                   # Initialize the PPO configuration
trainer = PPOTrainer(
    config=config,                     # Pass the PPO configuration
    model=model,                     # Pass your model
    tokenizer=tokenizer,               # Provide your tokenizer
    query_dataloader=query_dataloader, # Provide a dataloader for queries
)

for query in query_dataloader:        # Iterate through the queries
    response = model.generate(query)  # Generate a response
    reward = reward_model(response)   # Get the reward score
    trainer.step(query, response, reward) # Perform a PPO training step
```

<----------section---------->

**Additional Context:**

**Popular sampling techniques:** To produce diverse and creative text, LLMs utilize sampling techniques during generation, deviating from simply selecting the most probable word at each step. Two common techniques are:

*   **Top-k sampling:** The model chooses from the K most likely words in its vocabulary. This parameter controls how creative the LLM will be.

*   **Nucleus sampling:** Instead of choosing among the K most likely words, the model looks at the smallest set of words whose cumulative probability is smaller than *p*. So if there are only a few candidates with large probabilities, the "nucleus" would be smaller, than in the case of larger group of candidates with smaller probabilities.

**Example of generating text using nucleus sampling method:**

```python
nucleus_sampling_args = {
    'do_sample': True,  # Enable sampling
    'max_length': 50,    # Maximum length of the generated text
    'top_p': 0.92       # Probability threshold for nucleus sampling
}
print(generate(prompt='NLP is a', **nucleus_sampling_args))
```

The output might be something like:

```
NLP is a multi-level network protocol, which is one of the most
well-documented protocols for managing data transfer protocols. This
is useful if one can perform network transfers using one data transfer
protocol and another protocol or protocol in the same chain.
```

This is better, but still not quite what you were looking for. Your output
still uses the same words too much (just count how many times "protocol"
was mentioned!) But more importantly, though NLP indeed can stand for
Network Layer Protocol, itâ€™s not what you were looking for. To get generated
text that is domain-specific, you need to fine-tune our model - that means, to
train it on a dataset that is specific to our task.

**10.1.7 Fine-tuning your generative model:**

In your case, this dataset would be this very book, parsed into a database of lines. Letâ€™s load it from `nlpia2` repository. In this case, we only need the bookâ€™s text, so weâ€™ll ignore code, headers, and all other things that will not be helpful for our generative model.

Letâ€™s also initialize a new version of our GPT-2 model for finetuning. We can reuse the tokenizer for GPT-2 we initialized before.

**Listing 10.8 Loading the NLPiA2 lines as training data for GPT-2:**

```python
import pandas as pd
DATASET_URL = (
    'https://gitlab.com/tangibleai/nlpia2/'
    '-/raw/main/src/nlpia2/data/nlpia_lines.csv'
)
df = pd.read_csv(DATASET_URL)
df = df[df['is_text']]
lines = df.line_text.copy()
```

This will read all the sentences of natural language text in the manuscript for this book. Each line or sentence will be a different "document" in your NLP pipeline, so your model will learn how to generate sentences rather than longer passages. You want to wrap your list of sentences with a PyTorch Dataset class so that your text will be structured in the way that our training pipeline expects.

**Listing 10.9 Creating a PyTorch Dataset for training:**

```python
from torch.utils.data import Dataset
from torch.utils.data import random_split

class NLPiADataset(Dataset):
    def __init__(self, txt_list, tokenizer, max_length=768):
        self.tokenizer = tokenizer
        self.input_ids = []
        self.attn_masks = []
        for txt in txt_list:
            encodings_dict = tokenizer(txt, truncation=True,
                max_length=max_length, padding="max_length")
            self.input_ids.append(
                torch.tensor(encodings_dict['input_ids']))
    def __len__(self):
        return len(self.input_ids)
    def __getitem__(self, idx):
        return self.input_ids[idx]
```

Now, we want to set aside some samples for evaluating our loss mid-training. Usually, we would need to wrap them in the `DataLoader` wrapper, but luckily, the Transformers package simplifies things for us.

**Listing 10.10 Creating training and evaluation sets for fine-tuning:**

```python
dataset = NLPiADataset(lines, tokenizer, max_length=768)
train_size = int(0.9 * len(dataset))
eval_size = len(dataset) - train_size
train_dataset, eval_dataset = random_split(
    dataset, [train_size, eval_size])
```

Finally, you need one more Transformers library object - DataCollator. It dynamically builds batches out of our sample, doing some simple pre-prossesing (like padding) in the process. Youâ€™ll also define batch size - it will depend on the RAM of your GPU. We suggest starting from single-digit batch sizes and seeing if you run into out-of-memory errors.

If you were doing the training in PyTorch, there are multiple parameters that you would need to specify - such as the optimizer, its learning rate, and the warmup schedule for adjusting the learning rate. This is how you did it in the previous chapters. This time, weâ€™ll show you how to use the presets that transformers package offers in order to train the model as a part of `Trainer` class. In this case, we only need to specify the batch size and number of epochs! Easy-peasy.

**Listing 10.11 Defining training arguments for GPT-2 fine-tuning:**

```python
from nlpia2.constants import DATA_DIR
from transformers import TrainingArguments
from transformers import DataCollatorForLanguageModeling
training_args = TrainingArguments(
   output_dir=DATA_DIR / 'ch10_checkpoints',
   per_device_train_batch_size=5,
   num_train_epochs=5,
   save_strategy='epoch'
)
collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)
```

Now you have the pieces that a HuggingFace training pipeline needs to know to start training (finetuning) your model. The `TrainingArguments` and `DataCollatorForLanguageModeling` classes help you comply with the Hugging Face API and best practices. Itâ€™s a good pattern to follow even if you do not plan to use Hugging Face to train your models. This pattern will force you to make all your pipelines maintain a consistent interface. This allows you to train, test, and upgrade your models quickly each time you want to try out a new base model. This will help you keep up with the fast-changing world of open-source transformer models. You need to move fast to compete with the chickenized reverse centaur algorithms that BigTech is using to try to enslave you.

The `mlm=False` (masked language model) setting is an especially tricky quirk of transformers. This is your way of declaring that the dataset used for training your model need only be given the tokens in the causal direction â€” left to right for English. You would need to set this to True if you are feeding the trainer a dataset that has random tokens masked. This is the kind of dataset used to train bidirectional language models such as BERT.

**tip:**
A causal language model is designed to work the way a neurotypical human brain model works when reading and writing text. In your mental model of the English language, each word is causally linked to the next one you speak or type as you move left to right. You canâ€™t go back and revise a word youâ€™ve already spoken â€¦ unless youâ€™re speaking with a keyboard. And we use keyboards a lot. This has caused us to develop mental models where we can skip around left or right as we read or compose a sentence. Perhaps if weâ€™d all been trained to predict masked-out words, like BERT was, we would have a different (possibly more efficient) mental model for reading and writing text. Speed reading training does this to some people as they learn to read and understand several words of text all at once, as fast as possible. People who learn their internal language models differently than the typical person might develop the ability to hop around from word to word in their mind, as they are reading or writing text. Perhaps the language model of someone with symptoms of dyslexia or autism is somehow related to how they learned the language. Perhaps the language models in neurodivergent brains (and speed readers) are more similar to BERT (bidirectional) rather than GPT (left-to-right).

Now you are ready for training! You can use your collator and training args to configure the training and turn it loose on your data.

**Listing 10.12 Fine-tuning GPT-2 with HuggingFaceâ€™s Trainer class:**

```python
from transformers import Trainer
ft_model = GPT2LMHeadModel.from_pretrained("gpt2")
trainer = Trainer(
       ft_model,
       training_args,
       data_collator=collator,
       train_dataset=train_dataset,
       eval_dataset=eval_dataset)
trainer.train()
```

This training run can take a couple of hours on a CPU. So if you have access to a GPU you might want to train your model there. The training should run about 100x faster on a GPU.

Of course, there is a trade-off in using off-the-shelf classes and presets â€” it gives you less visibility on how the training is done and makes it harder to tweak the parameters to improve performance. As a take-home task, see if you can train the model the old way, with a PyTorch routine.

Letâ€™s see how well our model does now!

```python
generate(model=ft_model, tokenizer=tokenizer,
           prompt='NLP is')
```

NLP is not the only way to express ideas and understand ideas.

OK, that looks like a sentence you might find in this book. Take a look at the results of the two different models together to see how much your fine-tuning changed the text the LLM will generate.

```python
print(generate(prompt="Neural networks",
                   model=vanilla_gpt2,
                   tokenizer=tokenizer,
                   **nucleus_sampling_args))
```

Neural networks in our species rely heavily on these networks to understand
   their role in their environments, including the biological evolution of
   language and communication...

```python
print(generate(prompt="Neural networks",
                  model=ft_model,
                  tokenizer=tokenizer,
                  **nucleus_sampling_args))
```

Neural networks are often referred to as "neuromorphic" computing because
   they mimic or simulate the behavior of other human brains. footnote:[...

That looks like quite a difference! The vanilla model interprets the term 'neural networks' in its biological connotation, while the fine-tuned model realizes weâ€™re more likely asking about artificial neural networks. Actually, the sentence that the fine-tuned model generated resembles closely a sentence from Chapter 7:

Neural networks are often referred to as "neuromorphic" computing because they mimic or simulate what happens in our brains.

Thereâ€™s a slight difference though. Note the ending of "other human brains". It seems that our model doesnâ€™t quite realize that it talks about artificial, as opposed to human, neural networks, so the ending doesnâ€™t make sense. That shows once again that the generative model doesnâ€™t really have a model of the world, or "understand" what it says. All it does is predict the next word in a sequence. Perhaps you can now see why even rather big language models like GPT-2 are not very smart and will often generate nonsense.

**10.1.8 Nonsense (hallucination):**

As language models get larger, they start to sound better. But even the largest LLMs generate a lot of nonsense. The lack of "common sense" should be no surprise to the experts who trained them. LLMs have not been trained to utilize sensors, such as cameras and microphones, to ground their language models in the reality of the physical world. An embodied robot might be able to ground itself by checking its language model with what it senses in the real world around it. It could correct its common sense logic rules whenever the real world contradicts those faulty rules. Even seemingly abstract logical concepts such as addition have an effect in the real world. One apple plus another apple always produces two apples in the real world. A grounded language model should be able to count and do addition much better.

Like a baby learning to walk and talk, LLMs could be forced to learn from their mistakes by allowing them to sense when their assumptions were incorrect. An embodied AI wouldnâ€™t survive very long if it made the kinds of common sense mistakes that LLMs make. An LLM that only consumes and produces text on the Internet has no such opportunity to learn from mistakes in the physical world. An LLM "lives" in the world of social media, where fact and fantasy are often indistinguishable.

So even the largest of the large, trillion-parameter transformers will generate nonsense responses. Scaling up the nonsense training data wonâ€™t help. The largest and most famous LLMs were trained on virtually the entire Internet and this only improves their grammar and vocabulary, not their reasoning ability. Some engineers and researchers describe this nonsensical text as hallucinating. But thatâ€™s a misnomer that can lead you astray in your quest to get something consistently useful out of LLMs. An LLM canâ€™t even hallucinate because it canâ€™t think, much less reason or have a mental model of reality.

Hallucination happens when a human fails to separate imagined images or words from the reality of the world they live in. But an LLM has no sense of reality and has never lived in the real world. An LLM that you use on the Internet has never been embodied in a robot. It has never suffered from the consequences of mistakes. It canâ€™t think, and it canâ€™t reason. So it canâ€™t hallucinate.

LLMs have no concept of truth, facts, correctness, or reality. LLMs that you interact with online "live" in the unreal world of the Internet. Engineers fed them texts from both fiction and nonfiction sources. If you spend a lot of time probing what an LLM knows you will quickly get a feel for just how ungrounded models like ChatGPT are. At first, you may be pleasantly surprised by how convincing and plausible the responses to your questions are. And this may lead you to anthropomorphize it. And you might claim that its ability to reason was an "emergent" property that researchers didnâ€™t expect. And you would be right. The researchers at BigTech have not even begun to try to train LLMs to reason. They hoped the ability to reason would magically emerge if they gave LLMs enough computational power and text to read. Researchers hoped to shortcut the need for AI to interact with the physical world by giving LLMs enough descriptions of the real world to learn from. Unfortunately, they also gave LLMs an equal or larger dose of fantasy. Most of the text found online is either fiction or intentionally misleading.

So the researchers' hope for a shortcut was misguided. LLMs only learned what they were taught â€” to predict the most plausible next words in a sequence. By using the like button to nudge LLMs with reinforcement learning, BigTech has created a BS artist rather than the honest and transparent virtual assistant that they claimed to be building. Just as the like button on social media has turned many humans into sensational blow-hards, it has turned LLMs into "influencers" that command the attention of more than 100 million users. And yet LLMs have no ability or incentives (objective functions) to help them differentiate fact from fiction. To improve the machineâ€™s answers' relevance and accuracy, you need to get better at grounding your models - have their answers based on relevant facts and knowledge.

Luckily, there are time-tested techniques for incentivizing generative models for correctness. Information extraction and logical inference on knowledge graphs are very mature technologies. And most of the biggest and best knowledge bases of facts are completely open source. BigTech canâ€™t absorb and kill them all. Though the open source knowledge base FreeBase has been
different approaches; we show you techniques for both.

In addition, deep learning and data-driven programming (machine learning, or probabilistic language modeling) have rapidly diversified the possible applications for NLP and chatbots. This data-driven approach allows ever greater sophistication for an NLP pipeline by providing it with greater and greater amounts of data in the domain you want to apply it to. And when a new machine learning approach is discovered that makes even better use of this data, with more efficient model generalization or regularization, then large jumps in capability are possible.

The NLP pipeline for a chatbot shown in Figure 1.4 contains all the building blocks for most of the NLP applications that we described at the start of this chapter. As in `Taming Text`, we break out our pipeline into four main subsystems or stages. In addition, we have explicitly called out a database to record data required for each of these stages and persist their configuration and training sets over time. This can enable batch or online retraining of each of the stages as the chatbot interacts with the world. We have also shown a "feedback loop" on our generated text responses so that our responses can be processed using the same algorithms used to process the user statements. The response "scores" or features can then be combined in an objective function to evaluate and select the best possible response, depending on the chatbotâ€™s plan or goals for the dialog. This book is focused on configuring this NLP pipeline for a chatbot, but you may also be able to see the analogy to the NLP problem of text retrieval or "search," perhaps the most common NLP application. And our chatbot pipeline is certainly appropriate for the question-answering application that was the focus of `Taming Text`.

The application of this pipeline to financial forecasting or business analytics may not be so obvious. But imagine the features generated by the analysis portion of your pipeline. These features of your analysis or feature generation can be optimized for your particular finance or business prediction. That way they can help you incorporate natural language data into a machine learning pipeline for forecasting. Despite focusing on building a chatbot, this book gives you the tools you need for a broad range of NLP applications, from search to financial forecasting.

One processing element in Figure 1.4 that is not typically employed in search, forecasting, or question-answering systems is natural language generation. For chatbots, this is their central feature. Nonetheless, the text generation step is often incorporated into a search engine NLP application and can give such an engine a large competitive advantage. The ability to consolidate or summarize search results is a winning feature for many popular search engines (DuckDuckGo, Bing, and Google). And you can imagine how valuable it is for a financial forecasting engine to be able to generate statements, tweets, or entire articles based on the business-actionable events it detects in natural language streams from social media networks and news feeds.

The next section shows how the layers of such a system can be combined to create greater sophistication and capability at each stage of the NLP pipeline.

**1.10 Processing in depth:**

The stages of a natural language processing pipeline can be thought of as layers, like the layers in a feed-forward neural network. Deep learning is all about creating more complex models and behavior by adding additional processing layers to the conventional two-layer machine learning model architecture of feature extraction followed by modeling. In Chapter 5 we explain how neural networks help spread the learning across layers by backpropagating model errors from the output layers back to the input layers. But here we talk about the top layers and what can be done by training each layer independently of the other layers.

The top four layers in Figure 1.8 correspond to the first two stages in the chatbot pipeline (feature extraction and feature analysis) in the previous section. For example, part-of-speech tagging (POS tagging), is one way to generate features within the Analyze stage of our chatbot pipeline. POS tags are generated automatically by the default SpaCY pipeline, which includes all the top four layers in this diagram. POS tagging is typically accomplished with a finite state transducer like the methods in the `nltk.tag` package.

The bottom two layers (Entity Relationships and a Knowledge Base) are used to populate a database containing information (knowledge) about a particular domain. And the information extracted from a particular statement or document using all six of these layers can then be used in combination with that database to make inferences. Inferences are logical extrapolations from a set of conditions detected in the environment, like the logic contained in the statement of a chatbot user. This kind of "inference engine" in the deeper layers of this diagram is considered the domain of artificial intelligence, where machines can make inferences about their world and use those inferences to make logical decisions. However, chatbots can make reasonable decisions without this knowledge database, using only the algorithms of the upper few layers. And these decisions can combine to produce surprisingly human-like behaviors.

Over the next few chapters, we dive down through the top few layers of NLP. The top three layers are all that is required to perform meaningful sentiment analysis and semantic search and to build human-mimicking chatbots. In fact, itâ€™s possible to build a useful and interesting chatbot using only a single layer of processing, using the text (character sequences) directly as the features for a language model. A chatbot that only does string matching and search is capable of participating in a reasonably convincing conversation if given enough example statements and responses.

For example, the open source project `ChatterBot` simplifies this pipeline by merely computing the string "edit distance" (Levenshtein distance) between an input statement and the statements recorded in its database. If its database of statement-response pairs contains a matching statement, the corresponding reply (from a previously "learned" human or machine dialog) can be reused as the reply to the latest user statement. For this pipeline, all that is required is step 3 (Generate) of our chatbot pipeline. And within this stage, only a brute-force search algorithm is required to find the best response. With this simple technique (no tokenization or feature generation required), `ChatterBot` can maintain a convincing conversion as the dialog engine for Salvius, a mechanical robot built from salvaged parts by Gunther Cox.

`Will` is an open source Python chatbot framework by Steven Skoczen with a completely different approach. `Will` can only be trained to respond to statements by programming it with regular expressions. This is the labor-intensive and data-light approach to NLP. This grammar-based approach is especially effective for question-answering systems and task-execution assistant bots, like Lex, Siri, and Google Now. These kinds of systems overcome the "brittleness" of regular expressions by employing "fuzzy regular expressions."

harm your business, your reputation, or your users. Youâ€™ll need to do more than simply connect your users directly to the LLM.

There are three popular approaches to reducing an LLMâ€™s toxicity and reasoning errors:

1. Scaling: Make it bigger (and hopefully smarter)
2. Guardrails: Monitoring it to detect and prevent it from saying bad things
3. Grounding: Augment an LLM with a knowledge base of real-world facts
4. Retrieval: Augment an LLM with a search engine to retrieve text used to generate responses.

The next two sections will explain the advantages and limitations of the scaling and guardrail approaches. You will learn about grounding and retrieval in chapter
**10.1.1 Scaling up:**

One of the attractive aspects of LLMs is that you only need to add data and neurons if you want to improve your bot. You donâ€™t have to handcraft ever more complicated dialog trees and rules. OpenAI placed a billion-dollar bet on the idea that the ability to handle complex dialog and reason about the world would emerge once they added enough data and neurons. It was a good bet. Microsoft invested more than a billion dollars in ChatGPTâ€™s emergent ability to respond plausibly to complex questions.

However many researchers question whether this overwhelming complexity in the model is merely hiding the flaws in ChatGPTâ€™s reasoning. Many researchers believe that increasing the dataset does not create more generally intelligent behavior just more confident and intelligent-sounding text. The authors of this book are not alone in holding this opinion. Way back in 2021, in the paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?" prominent researchers explained how the appearance of understanding in LLMs was an illusion. And they were fired for the sacrilege of questioning the ethics and reasonableness of OpenAIâ€™s "spray and pray" approach to AI â€” relying exclusively on the hope that more data and neural network capacity would be enough to create intelligence.

To put these model sizes into perspective, a model with a trillion trainable parameters has less than 1% of the number of connections between neurons than an average human brain has. This is why researchers and large organizations have been investing millions of dollars in the compute resources required to train the largest language models.

Many researchers and their corporate backers are hopeful that increased size will unlock human-like capabilities. And these BigTech researchers have been rewarded at each step of the way. 100 B parameter models such as BLOOM and InstructGPT revealed the capacity for LLMs to understand and respond appropriately to complex instructions for creative writing tasks such as composing a love poem from a Klingon to a human. And then trillion parameter models such as GPT-4 can perform few-shot learning where the entire machine learning training set is contained within a single conversational prompt