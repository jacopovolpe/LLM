**Natural Language Processing and Large Language Models**

Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
Lesson 12
HuggingFace
Nicola Capuano and Antonio Greco
DIEM – University of Salerno

This lesson provides an introduction to Natural Language Processing (NLP) with a focus on utilizing the Hugging Face ecosystem. Hugging Face is a prominent company and community that provides tools and resources for building, training, and deploying machine learning models, particularly in the field of NLP. This lecture is presented as part of the Master's Degree Course in Computer Engineering at the University of Salerno, delivered by Nicola Capuano and Antonio Greco from the DIEM (Department of Industrial Engineering and Mathematics).

<----------section---------->

**Outline**

This lesson will cover the following topics:

*   **Overview:** A general introduction to Hugging Face and its resources.
*   **Setup:** Instructions for setting up the necessary environment to work with Hugging Face tools.
*   **Pipeline:** An explanation of the Hugging Face `pipeline()` function and how to use it for basic NLP tasks.
*   **Model selection:** Guidance on choosing appropriate pre-trained models from the Hugging Face Model Hub.
*   **Common models:** A review of several widely-used NLP models, their licenses, and organizations.
*   **Gradio:** An introduction to Gradio, a tool for building interactive web demos for machine learning models.

<----------section---------->

**Overview: Hugging Face**

Hugging Face ([https://huggingface.co/](https://huggingface.co/)) is a central hub for NLP resources, providing access to pre-trained models, datasets, and tools that simplify the development and deployment of NLP applications. The Education Toolkit ([https://github.com/huggingface/education-toolkit](https://github.com/huggingface/education-toolkit)) is specifically designed to help educators and learners easily prepare workshops, events, homework, or classes related to NLP.

**Hugging Face - Education Toolkit**

The Education Toolkit offers self-contained content that can be easily incorporated into existing materials. It is free to use and relies on well-known open-source technologies such as `transformers` and `gradio`. In addition to tutorials, the toolkit provides resources for further exploration of Machine Learning (ML) and assistance in designing NLP-related content.

<----------section---------->

**Hugging Face Hub**

The Hugging Face Hub ([https://huggingface.co/](https://huggingface.co/)) is a central repository that hosts a variety of resources:

*   **Models:** Pre-trained models for various NLP tasks.
*   **Datasets:** Open-source datasets for training and evaluating models. The Hub hosts around 3000 datasets that are open-sourced and free to use in multiple domains.
*   **Spaces:** Platforms for hosting demos and code, allowing users to showcase their projects.

Key libraries within the Hugging Face ecosystem include:

*   `datasets`: Facilitates the downloading and management of datasets from the Hub. It allows the easy use of these datasets, including huge ones, using very convenient features such as streaming.
*   `transformers`: Enables working with pipelines, tokenizers, models, and other components necessary for NLP tasks.
*   `evaluate`: Provides tools for computing evaluation metrics to assess model performance.

These libraries are compatible with both PyTorch and TensorFlow, providing flexibility in choosing the preferred machine learning framework.

<----------section---------->

**Hugging Face – Model Hub**

The Model Hub ([https://huggingface.co/models](https://huggingface.co/models)) is a central repository for pre-trained models. Lysandre demonstrates how to navigate the Model Hub.

**Model Cards:** Similar to dataset repositories, models have a model card that documents important details such as the intended use, limitations, and training data.

<----------section---------->

**Hugging Face - Datasets**

The Hub ([https://hf.co/datasets](https://hf.co/datasets)) hosts open-sourced and free-to-use datasets across multiple domains. The open-source `datasets` library enables easy use of these datasets, including very large ones, with convenient streaming features. Each dataset has a dataset card documenting its summary, structure, and other relevant information.

Example: The GLUE dataset ([https://huggingface.co/datasets/nyu-mll/glue](https://huggingface.co/datasets/nyu-mll/glue)) is a popular benchmark for evaluating the performance of NLP models on various tasks.

<----------section---------->

**Setup**

This section details the necessary steps to set up the environment for working with Hugging Face tools.

**Setup – Google Colab**

Using a Google Colab notebook is the simplest way to get started. Colab provides a free, cloud-based environment with pre-installed libraries.

*   Install the `transformers` library:
    *   `!pip install transformers`
    *   This installs a minimal version of Transformers without specific machine learning frameworks like PyTorch or TensorFlow.
*   Import the library:
    *   `import transformers`

To install the development version with all dependencies:
*   `!pip install transformers[sentencepiece]`

<----------section---------->

**Setup – Virtual environment**

A virtual environment isolates project dependencies, preventing conflicts between different projects.

1.  **Download and install Anaconda:** [https://www.anaconda.com/download](https://www.anaconda.com/download)
2.  **Create a virtual environment:**
    *   `conda create --name nlpllm`
3.  **Activate the environment:**
    *   `conda activate nlpllm`
4.  **Install `transformers` with `sentencepiece`:**
    *   `conda install transformers[sentencepiece]`

<----------section---------->

**Setup – Create a Hugging Face account**

Many Hugging Face functionalities require an account. It is recommended to create one.

<----------section---------->

**Pipeline**

This section explains the `pipeline()` function in the Hugging Face Transformers library.

*   The Hugging Face Transformers library provides tools to create and use pre-trained models.
*   The Model Hub contains thousands of pre-trained models that can be downloaded and used.
*   The `pipeline()` function is the most basic object in the Hugging Face Transformers library. It’s a high-level abstraction that simplifies common NLP tasks by bundling a pre-trained model with its associated tokenizer and any necessary pre-processing/post-processing steps.
*   It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.

`pipeline()` function <=

<----------section---------->

**Model selection**

Choosing the right pre-trained model is critical for achieving optimal performance in NLP tasks. This section provides guidance on selecting appropriate models from the Hugging Face Hub, which contains over 176,620 models.

Consider a summarization task for an earthquake news article:

"A magnitude 6.7 earthquake rattled Papua New Guinea early Friday afternoon, according to the U.S. Geological Survey. The quake was centered about 200 miles north-northeast of Port Moresby and had a depth of 28 miles. No tsunami warning was issued..."

To find a suitable model on the Hugging Face Hub:

1.  **Filter by task:** Select the "Summarization" task, reducing the number of models to around 960.
2.  **Consider your needs:**
    *   **Extractive Summarization:** Selects representative pieces of text directly from the original article.
    *   **Abstractive Summarization:** Generates new, concise text that captures the main points of the original article.
3.  **Further Filtering:** You can further refine your model selection based on criteria such as license, language, and other specific requirements.

Hugging Face * Models Datasets Spaces Dxs © Solutions
t Tasks | libraries futesets Languages Licenses Models
bert-base-uncased

Footure Extraction Taxtto-image ® jonatasgrosman/wav2vec2-large-

Filter by model size
(for limits on hardware, cost, or latency)
pytorch_model.bin

<----------section---------->

**Model selection**

Additional factors to consider when selecting a model:

1.  **Sort by popularity and updates:**
    *   **Most Downloads:** Indicates widely-used and potentially well-supported models.
    *   **Recently Updated:** Suggests active maintenance and improvements.
    *   **Most Likes:** Reflects community approval and satisfaction.
2.  **Check Git release history:** Examining the GitHub repository (e.g., github.com/google-research/bert/blob/master/README.md) provides insights into the model's development and updates.

BERT
***** New March 11th, 2020; Smailler BERT Models *****
This is a release of 24 smaller BERT models (Engli

3.  **Pick good variants of models for your task:**
    *   Different sizes of the same base model.
    *   Fine-tuned variants of base models.

Models 5,564 few Fulltext search

t5-base
Updated 11 days ago

t5-small
Updated 11 days ago

§ prithivida/parrot_paraphrasezr_on_T5
Updated May 18, 2021 545k a7

<----------section---------->

**Model selection**

Other considerations:

*   **Search for examples and datasets:** Look for resources that demonstrate the model's performance and suitability for your specific task.
*   **Model specialization:** Determine whether the model is generally capable or fine-tuned for a specific task.
*   **Training data:** Identify the datasets used for pre-training and/or fine-tuning, as this influences the model's bias and performance.

Ultimately, model selection should be data and user-driven:

*   **Define KPIs:** Establish key performance indicators to measure the model's effectiveness.
*   **Test on your data or users:** Evaluate the model's performance on your specific dataset and with your target users to ensure it meets your requirements.

<----------section---------->

**Common models**

The following table provides an overview of common NLP models, their licenses, organizations, years of release, and descriptions.

| Model           | License        | Organization | Year | Description                                                                                                                            |
| :---------------- | :------------- | :------------- | :--- | :------------------------------------------------------------------------------------------------------------------------------------- |
| Pythia 19M-12B    | Apache 2.0     | EleutherAI   | 2023 | Series of 8 models for comparisons across sizes.                                                                                        |
| Dolly 12B       | MIT            | Databricks   | 2023 | Instruction-tuned Pythia model.                                                                                                          |
| GPT-3.5 175B      | Proprietary    | OpenAI       | 2022 | ChatGPT model option; related models GPT-1/2/3/4.                                                                                        |
| OPT 125M-175B     | MIT            | Meta         | 2022 | Based on GPT-3 architecture.                                                                                                               |
| BLOOM 560M - 176B | RAIL v1.0       | Many groups  | 2022 | 46 languages.                                                                                                                          |
| GPT-Neo/X 125M-20B | MIT / Apache 2.0 | EleutherAI   | 2021 / 2022 | Based on GPT-2 architecture.                                                                                                               |
| FLAN 80M-540B     | Apache 2.0     | Google       | 2021 | Methods to improve training for existing architectures.                                                                                    |
| BART 139M-406M    | Apache 2.0     | Meta         | 2019 | Derived from BERT, GPT, others.                                                                                                        |
| T5 50M-TIB        | Apache 2.0     | Google       | 2019 | 4 languages.                                                                                                                             |
| BERT            | Apache 2.0     | Google       | 2018 | Early breakthrough.                                                                                                                    |

<----------section---------->

**Gradio**

Gradio is a Python library that allows you to create customizable UI components and interfaces for your machine learning models, facilitating easy sharing and testing.

**Gradio – Demos on the web**

Gradio simplifies the process of deploying machine learning models for demonstration purposes. It covers steps for training, containerization, front-end building and sample storing. The following table summarizes the typical steps.

| Step                                  | Framework          | Language |
| :------------------------------------ | :----------------- | :------- |
| 1. Train a model                      | TensorFlow/PyTorch | Python   |
| 2. Containerize and deploy the model |                    | Python   |
| 3. Store incoming samples             | Gradio             | Python   |
| 4. Build an interactive front-end     |                    | Python   |

<----------section---------->

**Gradio – Demos on the web**

Example: News Summarizer

A news summarizer app demonstrates how Hugging Face models can summarize articles. It uses the `bart-large-cnn` model by Facebook. The demo displays an URL input and a summarization output. Shorter articles generate faster summaries.

URL
Screenshot Flag
Clear Submit
Examples
[https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/](https://www.technologyreview.com/2021/07/22/1029973/deepmind-alphafold-protein-folding-biology-disease-drugs-proteome/)
[https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/](https://www.technologyreview.com/2021/07/21/1029860/disability-rights-employment-discrimination-ai-hiring/)
[https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/](https://www.technologyreview.com/2021/07/09/1028140/ai-voice-actors-sound-human/)

<----------section---------->

**Gradio – Free hosting on hf.space**

Hugging Face Spaces provides free hosting for Gradio demos, allowing community members to share their ML applications.

Hugging Face

Discover amazing ML apps made by the community!

Create new Space
Models Datasets Spaces Docs

<----------section---------->

**Gradio – Build your demo**

To build your own Gradio demo:

1.  Install Gradio: `conda install gradio`
2.  Refer to the documentation: [https://bit.ly/34wESgd](https://bit.ly/34wESgd)
