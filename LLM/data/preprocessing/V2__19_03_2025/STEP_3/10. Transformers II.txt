# Natural Language Processing and Large Language Models
## Corso di Laurea Magistrale in Ingegneria Informatica (Master's Degree Course in Computer Engineering)
### Lesson 10: Transformers II
**Nicola Capuano and Antonio Greco**
**DIEM – University of Salerno**

This document presents the lecture notes for Lesson 10 of the Natural Language Processing (NLP) and Large Language Models (LLMs) course, part of the Master's Degree program in Computer Engineering at the University of Salerno. The lesson, titled "Transformers II," is delivered by Nicola Capuano and Antonio Greco from DIEM (Department of Industrial Engineering and Mathematics). This lesson builds upon the foundational concepts of Transformers, a revolutionary architecture that has significantly advanced the field of NLP.

<----------section---------->

## Outline

This lesson will cover the following topics:

*   **Multi-Head Attention:** Delving deeper into the mechanism that allows the model to focus on different parts of the input sequence when processing it.
*   **Encoder Output:** Understanding the representation of the input sequence generated by the encoder component of the Transformer.
*   **Decoder:** Examining the role and structure of the decoder, responsible for generating the output sequence.
*   **Masked Multi-Head Attention:** Exploring the specific variant of multi-head attention used in the decoder to prevent it from "peeking" into the future.
*   **Encoder-Decoder Attention:** Analyzing how the decoder attends to the encoder's output to generate contextually relevant output tokens.
*   **Output:** Describing the final layers of the Transformer that produce the predicted output sequence.
*   **Transformer’s pipeline:** Overview of the entire process, highlighting the flow of information through the encoder and decoder.

<----------section---------->

## Multi-head attention

### Multi-head Attention
*   **Encoding Different Meanings:** By employing multiple self-attention heads, the model can capture various nuances and interpretations of the context within the input sequence. Each head learns different weight matrices, allowing it to focus on distinct aspects of the relationships between words.
*   **Parallel Scaled-Dot Product Attention:** The multi-head attention mechanism performs several scaled dot-product attention computations simultaneously. Each computation utilizes different learned weight matrices, increasing the model's capacity to capture diverse relationships. This parallelization significantly speeds up the processing.
*   **Concatenation of Results:** The results from each attention head are concatenated row-by-row, forming a larger matrix. The resulting matrix maintains the same number of rows (*m*) as the input, while the number of columns increases proportionally to the number of attention heads.
*   **Final Weight Matrix Multiplication:** This concatenated matrix is then multiplied by a final weight matrix. This multiplication projects the concatenated representations into a final output space, combining the information learned by each head.
*   **Definition:** This entire process – parallel scaled dot-product attention computations with concatenation and final projection – is termed multi-head attention.

### Multi-head Attention
Multi-head attention empowers the model to attend jointly to information from various representation subspaces across different positions within the input. This is crucial because a single attention head, by averaging the attention weights, might inhibit the model's ability to distinguish between different types of relationships.

The outputs from the various attention heads are concatenated to create a comprehensive representation. Subsequently, an additional weight matrix is applied to combine these representations at the same network level. This allows the model to integrate diverse perspectives on the input sequence at each layer of the Transformer. In effect, the multi-head attention allows the transformer to focus on different positions, it creates several different vector subspaces where the transformer can encode a particular generalization for a subset of the word patterns in your text.

<----------section---------->

## Add & Norm

### Add (skip connections) & Norm
This section focuses on two crucial components of the Transformer architecture: Add (skip connections or residual connections) and Norm (Layer Normalization).

**Input Normalization (Z)**
*   **Mean 0, Std dev 1:** The input to each layer is normalized to have a mean of 0 and a standard deviation of 1. This is a common technique in deep learning.
*   **Stabilizes training:** Normalization helps stabilize the training process by preventing the activations from becoming too large or too small, a phenomenon known as the exploding/vanishing gradients problem.
*   **Regularization effect:** Input normalization introduces a subtle regularization effect, which can improve the generalization performance of the model by preventing overfitting to the training data.

**Add -> Residuals**
*   **Avoid vanishing gradients:** Residual connections create a direct path for the gradient to flow through the network, mitigating the vanishing gradient problem. This is achieved by adding the original input to the output of a layer.
*   **Train deeper networks:** Residual connections enable the training of much deeper networks. Without them, the gradients would diminish exponentially as they propagate through the layers, making it difficult for the network to learn.

<----------section---------->

## Feed Forward

### Feed Forward
This section briefly describes the Feed Forward Network (FFN) component of the Transformer architecture.

*   **Non Linearity:** The feed-forward network introduces non-linearity into the model, allowing it to learn complex relationships between the input and output.
*   **Complex Relationships:** Non-linearities are essential for modeling intricate patterns and dependencies in the data.
*   **Learn from each other:** It gives the transformer the ability to learn from each other, because each layer of a transformer gives you a deeper and deeper representation of the meaning or thought of the input text.
*   **FFN (2 layer MLP):** The feed-forward network typically consists of a two-layer Multilayer Perceptron (MLP). The first layer expands the dimensionality of the input, while the second layer projects it back to the original dimensionality.

<----------section---------->

## Transformer’s Encoder

### Transformer’s Encoder
The encoder is responsible for creating a high-quality representation of the input sequence. Its key features are:

*   **Input Sequence Representation:** The encoder's primary goal is to compute a representation of the input sequence, capturing its meaning and structure. This representation will be used by the decoder to generate the output sequence.
*   **Additive Positional Encoding:** Since self-attention is order-agnostic, the encoder adds positional encoding to each word embedding. This provides information about the position of each word in the sequence, allowing the model to account for word order.
*   **Residual Connections:** Residual connections are used to foster the gradients flow, mitigating the vanishing gradient problem and enabling the training of deeper networks.
*   **Normalization Layers:** Normalization layers are employed to stabilize the network training, ensuring that the activations remain within a reasonable range.
*   **Position-Wise Feed-Forward Layer:** A position-wise feed-forward layer, applied to each sequence element independently, adds non-linearity to the model, allowing it to learn complex relationships.
*   **Stacking Encoders:** Each encoder produces an output with the same dimensionality as its input, meaning that it is possible to stack an arbitrary number of encoder blocks. The output of one encoder is fed as input to the next, enabling hierarchical representation learning.

<----------section---------->

## Decoder

### Decoder
The decoder generates the output sequence *y*<sub>1</sub>,…,*y*<sub>*m*</sub>, leveraging the information encoded in the intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub> produced by the encoder.

*   **Sequential Operation:** The decoder operates sequentially, generating the output tokens one at a time.
*   **Input at each step:** At each step *i*, the decoder uses the encoder's intermediate representation *z*<sub>1</sub>,…,*z*<sub>*t*</sub> and the previously generated output tokens *y*<sub>1</sub>,…,*y*<sub>*i*-1</sub> to generate the next output token *y*<sub>*i*</sub>.

### Decoder
The decoder is constructed from a stack of decoder blocks, each having a similar structure. The original Transformer paper used 6 decoder blocks. Each decoder block incorporates:

*   **Same Modules as Encoder:** The same modules used in the encoder block, such as self-attention, add & norm, and feed forward networks.
*   **Encoder-Decoder Attention:** An additional attention module is included, where the keys and values are derived from the encoder's intermediate representation (*z*<sub>1</sub>,…,*z*<sub>*t*</sub>). This allows the decoder to focus on the relevant parts of the input sequence when generating the output.
*   **Masked Self-Attention:** The self-attention module is modified to ensure that the query at position *i* only uses values at positions 1,…,i. This prevents the decoder from "peeking" at future tokens in the output sequence, which is crucial for training.

**Output Layer:**

*   **Linear Layer and Softmax:** On top of the last decoder block, a linear layer followed by a softmax activation function computes the probability of the next output element *y*<sub>*i*</sub>.
*   **Output Set Cardinality:** The final linear layer has a number of neurons that corresponds to the size of the output set, so that the transformer can figure out which token to output.

<----------section---------->

## Masked Multi-Head Attention
This type of Multi-Head Attention is used in the Decoder.

*   **Preventing Future Information Leakage:** Masked multi-head attention ensures that when generating the output at time *T*, the model only attends to outputs up to time *T*-1. This prevents the decoder from "cheating" by looking ahead at the tokens it's supposed to be predicting.
*   **Attention Mask (M):** This is achieved by masking the attention values. The attention mask *M* is a matrix that determines which attention weights are used and which are masked out (set to -inf, so the softmax will ignore them).
*   **R<sup>IxT</sup> & R<sup>TxT</sup>:** *R<sup>IxT</sup>* might represent the attention values before masking, where I is the input sequence length and T is the target sequence length. Masked attention values are then obtained after applying the mask *M*, resulting in a masked attention matrix.

<----------section---------->

## Encoder Decoder Attention
This describes how the Decoder uses the output of the encoder to produce output text.

*   **Keys and Values from Encoder:** The keys and values for the encoder-decoder attention module are taken from the encoder's outputs (*z*<sub>1</sub>,…,*z*<sub>*t*</sub>). This enables the decoder to access and leverage the information encoded in the input sequence.
*   **Queries from Decoder:** The queries are derived from the decoder's inputs, which are the previously generated output tokens.
*   **Consistent Encoder Output:** Every decoder block receives the same final encoder output. This ensures that all decoder layers have access to the same contextual information about the input sequence.

<----------section---------->

## Output

### Output
Describes the process of how the transformer outputs the decoded text.

*   **Linear Layer:** A linear layer is used to project the output of the decoder into the vocabulary space.
*   **Weight Tying:** The weights of this linear layer are often tied (shared) with the model's input embedding matrix. This can improve the model's generalization performance and reduce the number of parameters.
*   **Softmax:** Finally, a softmax function is applied to produce a probability distribution over the vocabulary, representing the model's prediction for the next token in the output sequence.

<----------section---------->

## Transformer’s pipeline
Describes the way in which the transformer deals with input data.

*   **Decoding Time Step:** The transformer processes the input one decoding time step at a time. The decoder produces a token, and then takes in the previously decoded tokens to output the next token in the sequence.
*   **ENCODER - DECODER:** This emphasizes the core architecture of the Transformer: the encoder processes the input sequence and the decoder generates the output sequence based on the encoded representation.
*   **Resource:** The provided link (https://poloclub.github.io/transformer-explainer/) links to an interactive explainer which can be useful to understand the finer details of how transformers work.

<----------section---------->

```
from
 
torch
 
import
 
Tensor
```

```
from
 
typing
 
import
 
Optional
, 
Any
```

```
class
 
CustomDecoderLayer
(nn.TransformerDecoderLayer):
```

```
    
def
 
forward
(
self
, tgt: Tensor, memory: Tensor,
```

```
            tgt_mask: Optional[Tensor] = 
None
,
```

```
            memory_mask: Optional[Tensor] = 
None
,
```

```
            tgt_key_padding_mask: Optional[Tensor] = 
None
```

```
            ) -> Tensor:
```

```
        
"""
Like decode but returns multi-head attention weights.
```

```
        tgt2 = 
self
.self_attn(
```

```
            tgt, tgt, tgt, attn_mask=tgt_mask,
```

```
            key_padding_mask=tgt_key_padding_mask)[
0
]
```

```
        tgt = tgt + 
self
.dropout1(tgt2)
```

```
        tgt = 
self
.norm1(tgt)
```

```
        tgt2, attention_weights = 
self
.multihead_attn(
```

```
            tgt, memory, memory,  # #1
```

```
            attn_mask=memory_mask,
```

```
            key_padding_mask=mem_key_padding_mask,
```

```
            need_weights=
True
)
```

```
        tgt = tgt + 
self
.dropout2(tgt2)
```

```
        tgt = 
self
.norm2(tgt)
```

```
        tgt2 = 
self
.linear2(
```

```
            
self
.dropout(
self
.activation(
self
.linear1(tgt))))
```

```
        tgt = tgt + 
self
.dropout3(tgt2)
```

```
        tgt = 
self
.norm3(tgt)
```

```
        
return
 tgt, attention_weights  # #2
```

```
class
 
CustomDecoder
(nn.TransformerDecoder):
```

```
    
def
 
__init__
(
self
, decoder_layer, num_layers, norm=
None
):
```

```
        
super
().__init__(
```

```
            decoder_layer, num_layers, norm)
```

```
    
def
 
forward
(
self
,
```

```
            tgt: Tensor, memory: Tensor,
```

```
            tgt_mask: Optional[Tensor] = 
None
,
```

```
            memory_mask: Optional[Tensor] = 
None
,
```

```
            tgt_key_padding_mask: Optional[Tensor] = 
None
```

```
            ) -> Tensor:
```

```
        
"""
Like TransformerDecoder but cache multi-head attention
```

```
        
self
.attention_weights = []  # #1
```

```
        output = tgt
```

```
        
for
 mod 
in
 
self
.layers:
```

```
            output, attention = mod(
```

```
                output, memory, tgt_mask=tgt_mask,
```

```
                memory_mask=memory_mask,
```

```
                tgt_key_padding_mask=tgt_key_padding_mask)
```

```
            
self
.attention_weights.append(attention) # #2
```

```
        
if
 
self
.norm 
is
 
not
 
None
:
```

```
            output = 
self
.norm(output)
```

```
        
return
 output
```

```
from
 
einops
 
import
 
rearrange
  # #1
```

```
class
 
TranslationTransformer
(nn.Transformer):  # #2
```

```
    
def
 
__init__
(
self
,
```

```
            device=DEVICE,
```

```
            src_vocab_size: int = VOCAB_SIZE,
```

```
            src_pad_idx: int = PAD_IDX,
```

```
            tgt_vocab_size: int = VOCAB_SIZE,
```

```
            tgt_pad_idx: int = PAD_IDX,
```

```
            max_sequence_length: int = 
100
,
```

```
            d_model: int = 
512
,
```

```
            nhead: int = 
8
,
```

```
            num_encoder_layers: int = 
6
,
```

```
            num_decoder_layers: int = 
6
,
```

```
            dim_feedforward: int = 
2048
,
```

```
            dropout: float = 
0.1
,
```

```
            activation: str = 
"
relu
"
```

```
        decoder_layer = CustomDecoderLayer(
```

```
            d_model, nhead, dim_feedforward,  # #3
```

```
            dropout, activation)
```

```
        decoder_norm = nn.LayerNorm(d_model)
```

```
        decoder = CustomDecoder(
```

```
            decoder_layer, num_decoder_layers,
```

```
            decoder_norm)  # #4
```

```
        
super
().__init__(
```

```
            d_model=d_model, nhead=nhead,
```

```
            num_encoder_layers=num_encoder_layers,
```

```
            num_decoder_layers=num_decoder_layers,
```

```
            dim_feedforward=dim_feedforward,
```

```
            dropout=dropout, custom_decoder=decoder)
```

```
        
self
.src_pad_idx = src_pad_idx
```

```
        
self
.tgt_pad_idx = tgt_pad_idx
```

```
        
self
.device = device
```

```
        
self
.src_emb = nn.Embedding(
```

```
            src_vocab_size, d_model)  # #5
```

```
        
self
.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)
```

```
        
self
.pos_enc = PositionalEncoding(
```

```
            d_model, dropout, max_sequence_length)  # #6
```

```
        
self
.linear = nn.Linear(
```

```
            d_model, tgt_vocab_size)  # #7
```
This extended context provides code examples and explanations related to implementing custom decoders and translation transformers using PyTorch. Specifically, it includes classes for:

*   `CustomDecoderLayer`:  A subclass of `torch.nn.TransformerDecoderLayer` that additionally returns multi-head self-attention weights during the forward pass. This is useful for analyzing the attention patterns of the decoder.
*   `CustomDecoder`: A subclass of `torch.nn.TransformerDecoder` that stores the attention weights from each layer in a list.
*   `TranslationTransformer`: A subclass of `torch.nn.Transformer` designed for translation tasks. It incorporates the custom decoder components and includes methods for preparing the source and target sequences with positional encodings and padding masks.

These code snippets can be helpful to anyone who is attempting to build upon the standard architecture of a transformer, and to provide more information about the way in which they function.
