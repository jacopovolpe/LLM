=== Extracted text from PDF ===
Natural Language Processing and Large Language ModelsCorso di Laurea Magistrale in Ingegneria Informatica Lesson 6Neural Networks for NLPNicola Capuano and Antonio GrecoDIEM – University of Salerno
Outline•Recurrent Neural Networks•RNN Variants•Building a Spam Detector•Intro to Text Generation•Building a Poetry Generator
Recurrent Neural Networks
Neural Networks and NLPNeural networks are widely used in text processing•A limitation of feedforward networks is the lack of memory•Each input is processed independently, without maintaining any state•To process a text, the entire sequence of words must be presented at onceThe entire text must become a single data•This is the approach used with BoW or TF-IDF vectors•A similar approach is averaging the Word Vectors of a text
Neural Networks with MemoryWhen you read a text, you:•process sentences and words one by one•maintain the memory of what you have read previously•An internal model is continuously updated as new information arrivesA Recurrent Neural Network adopts the same principle•It processes sequences of information by iterating on the elements of the sequence•E.g., the words of a text represented in the form of word embeddings•It maintains a state containing information about what has been seen so far
Recurrent Neural Networks•Circles are feedforward network layers composed of one or more neurons•The output of the hidden layer emerges from the network normally to the output layer•In addition, it is also fed back as input to the hidden layer along with the normal input in the next time step
250 CHAPTER  8 Loopy (recurrent) neural networks (RNNs)
 One-dimensional convolutions gave us a way to deal with these inter -token rela-
tionships by looking at windows  of words together. And the pooling layers discussed in
chapter 7 were specifically designed to handle slight word order variations. In this
chapter, we look at a differ ent approach. And through this approach, you’ll take a first
step toward the concept of memory  in a neural network. Inst ead of thinking about lan-
guage as a large chunk of data, you can begin to look at it as it’s created, token by
token, over time , in sequence.
8.1 Remembering with recurrent networks
Of course, the words in a do cument are rarely completely  independent of each other;
their occurrence influences or is influenced by occurrences of other words in the
document:
 The stolen car sped into the arena.
 The clown car sped into the arena.
Two different emotions may arise in the reader of these two sentences as the reader
reaches the end of the sentence. The two se ntences are identical in adjective, noun,
verb, and prepositional phrase construction. But that adjective swap early in the sen-
tence has a profound effect on what  the reader infers is going on.
    Can you find a way to model that relationship? A way
to understand that “arena” and even “sped” could take
on slightly different connotations when an adjective that
does not directly modify eith er occurred earlier in the
sentence?
     If you can find a way to remember  what just happened
the moment before (specifica lly what happened at time
step t when you’re looking at time step t+1), you’d be on
the way to capturing the patterns that emerge when cer-
tain tokens appear in patterns relative to other tokens in
a sequence. Recurrent neural nets  ( R N N s )  e n a b l e  n e u r a l
networks to remember the pa st words within a sentence.
    You can see in figure 8.3 that a single recurrent neu-
ron in the hidden layer adds a recurrent loop to “recy-
cle” the output of the hidden layer at time t. The output
at time t is added to the next input at time t+1. This new
input is processed by th e network at time step t+1 to cre-
ate the output for that hidden layer at time t+1. That out-
put at t+1 is then recycled back into the input again at
time step t+2, and so on. 1
1 In finance, dynamics, and feedback control, this is often called an auto-regressive moving average  ( A R M A )
model: https:/ /en.wikipedia.org/w iki/Autoregressive_model .
x (t +1)
y (t)
RNN
Input
layer
Recurrent loopHidden
layer
Output
layer
Figure 8.3
Recurrent neural net
 
Unrolling the RNN252 CHAPTER  8 Loopy (recurrent) neural networks (RNNs)
during backpropagation. But when looking at the three unfolded networks, remember
that they’re all different snapshots of the same network with a si ngle set of weights.
 Let’s zoom in on the original representa tion of a recurrent neural network before
it was unrolled. Let’s expose the input-weig ht relationships. The individual layers of
this recurrent network look like what you see in figures 8.5 and 8.6.
Figure 8.5 Detailed recurrent neural net at time step t = 0
x (t +1)
y (t)
RNN Same RNN “unrolled”
Input
layer
Hidden
layer
Output
layer
t =0
Input
layer
Hidden
layer
Output
layer
t =1
Input
layer
Hidden
layer
Output
layer
t =2
Input
layer
Hidden
layer
Output
layer
Figure 8.4 Unrolled recurrent neural net 
NeuronX 0
X 1
Y
X 2
h0
h1
h 2
Neuron
Neuron
Hidden Output layerInput vector
 
timeline
Inside the RNN (t = 0)
252 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
during backpropagation. But when looking at the three unfolded networks, remember
that they’re all different snapshots of the same network with a si ngle set of weights.
 Let’s zoom in on the original representa tion of a recurrent neural network before
it was unrolled. Let’s expose the input-weig ht relationships. The individual layers of
this recurrent network look like what you see in figures 8.5 and 8.6.
Figure 8.5 Detailed recurrent neural net at time step t  = 0
x ( t +1)
y (t)
RNN Same RNN “unrolled”
Input
layer
Hidden
layer
Output
layer
t =0
Input
layer
Hidden
layer
Output
layer
t =1
Input
layer
Hidden
layer
Output
layer
t =2
Input
layer
Hidden
layer
Output
layer
Figure 8.4 Unrolled recurrent neural net 
NeuronX 0
X 1
Y
X 2
h 0
h 1
h 2
Neuron
Neuron
Hidden Output layerInput vector
 
Note: The first input (t=0) has no pastHidden vector
Inside the RNN (t = 1)253Remembering with recurrent networks
Figure 8.6 Detailed recurrent neural net at time step t = 1
Each neuron in the hidden state has a set of  weights that it applies to each element of
each input vector, as a normal feedforwar d network. But now you have an additional
set of trainable weights that are applied to the output of the hidd en neurons from the
previous time step. The network can learn ho w much weight or importance to give the
events of the “past” as you input a sequence token by token.
TIP The first input in a sequence has no “past,” so the hidden state at t =0
receives an input of 0 from its t -1 self. An alternative way of “filling” the initial
state value is to first pass related but separate samples into the net one after
the other. Each sample’s final output is used for the t =0 input of the next
sample. You’ll learn how to preserve more of the information in your dataset
using alternative “filling” approaches in the section on statefulness  at the end
of this chapter.
Let’s turn back to the data: imagine you have a set of documents, each a labeled
example. For each sample, instead of passing  t h e  c o l l e c t i o n  o f  w o r d  v e c t o r s  i n t o  a
convolutional neural net all at once as in  the last chapter (see figure 8.7), you take
the sample one token at a time and pass the tokens individually into your RNN (see
figure 8.8).
NeuronX0
X1
Y
X2
h1
h2
Neuron
Input at t = 1 includes
output  from t = 0Raw output
from time t = 0
Raw input
@ time t = 1 
Neuron
Hidden Output layerInput vector
h0
 
Trainable weights:•Input to Hidden•Hidden to Output•Hidden to HiddenHidden vector from t = 0
Input at t = 1
Using RNNs with Text
254 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
 
Figure 8.7 Data into convolutional network
In your recurrent neural net, you pass in the word vector for the first token and get
the network’s output. You then pass in the se cond token, but you also pass in the out-
put from the first token! And then pass in  the third token along with the output from
the second token. And so on. The network has a concept of before and after, cause
and effect, some vague notion of time (see figure 8.8).
Figure 8.8 Data fed into a recurrent network
Hidden
layer
Output
Associated
label
the arenaintospedcarclownThe
Hidden
layer
Output
The
clown
car
sped
into
the
arena
Hidden
layer Hidden
layer
Hidden
layer Hidden
layer Hidden
layer Hidden
layer
Output
Associated
label
Recurrent neural net
 
254 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
 
Figure 8.7 Data into convolutional network
In your recurrent neural net, you pass in the word vector for the first token and get
the network’s output. You then pass in the se cond token, but you also pass in the out-
put from the first token! And then pass in  the third token along with the output from
the second token. And so on. The network has a concept of before and after, cause
and effect, some vague notion of time (see figure 8.8).
Figure 8.8 Data fed into a recurrent network
Hidden
layer
Output
Associated
label
the arenaintospedcarclownThe
Hidden
layer
Output
The
clown
car
sped
into
the
arena
Hidden
layer Hidden
layer
Hidden
layer Hidden
layer Hidden
layer Hidden
layer
Output
Associated
label
Recurrent neural net
 
254 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
 
Figure 8.7 Data into convolutional network
In your recurrent neural net, you pass in the word vector for the first token and get
the network’s output. You then pass in the se cond token, but you also pass in the out-
put from the first token! And then pass in  the third token along with the output from
the second token. And so on. The network has a concept of before and after, cause
and effect, some vague notion of time (see figure 8.8).
Figure 8.8 Data fed into a recurrent network
Hidden
layer
Output
Associated
label
the arenaintospedcarclownThe
Hidden
layer
Output
The
clown
car
sped
into
the
arena
Hidden
layer Hidden
layer
Hidden
layer Hidden
layer Hidden
layer Hidden
layer
Output
Associated
label
Recurrent neural net
 
254 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
 
Figure 8.7 Data into convolutional network
In your recurrent neural net, you pass in the word vector for the first token and get
the network’s output. You then pass in the se cond token, but you also pass in the out-
put from the first token! And then pass in  the third token along with the output from
the second token. And so on. The network has a concept of before and after, cause
and effect, some vague notion of time (see figure 8.8).
Figure 8.8 Data fed into a recurrent network
Hidden
layer
Output
Associated
label
the arenaintospedcarclownThe
Hidden
layer
Output
The
clown
car
sped
into
the
arena
Hidden
layer Hidden
layer
Hidden
layer Hidden
layer Hidden
layer Hidden
layer
Output
Associated
label
Recurrent neural net
 
254 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
 
Figure 8.7 Data into convolutional network
In your recurrent neural net, you pass in the word vector for the first token and get
the network’s output. You then pass in the se cond token, but you also pass in the out-
put from the first token! And then pass in  the third token along with the output from
the second token. And so on. The network has a concept of before and after, cause
and effect, some vague notion of time (see figure 8.8).
Figure 8.8 Data fed into a recurrent network
Hidden
layer
Output
Associated
label
the arenaintospedcarclownThe
Hidden
layer
Output
The
clown
car
sped
into
the
arena
Hidden
layer Hidden
layer
Hidden
layer Hidden
layer Hidden
layer Hidden
layer
Output
Associated
label
Recurrent neural net
 
254 C HAPTER  8 Loopy (recurrent) neural networks (RNNs)
 
Figure 8.7 Data into convolutional network
In your recurrent neural net, you pass in the word vector for the first token and get
the network’s output. You then pass in the se cond token, but you also pass in the out-
put from the first token! And then pass in  the third token along with the output from
the second token. And so on. The network has a concept of before and after, cause
and effect, some vague notion of time (see figure 8.8).
Figure 8.8 Data fed into a recurrent network
Hidden
layer
Output
Associated
label
the arenaintospedcarclownThe
Hidden
layer
Output
The
clown
car
sped
into
the
arena
Hidden
layer Hidden
layer
Hidden
layer Hidden
layer Hidden
layer Hidden
layer
Output
Associated
label
Recurrent neural net
 
Text: The clown car sped into the arena
Ignored output
RNN Training
255Remembering with recurrent networks
Now your network is remember ing something! Well, sort of. A few things remain for
you to figure out. For one, how does backprop agation even work in a structure like this?
8.1.1 Backpropagation through time
All the networks we’ve talked about so far ha ve a label (the target variable) to aim for,
and recurrent networks are no  exception. But you don’t have a concept of a label for
each token. You have a single label for a ll the tokens in each sample text. You only
have a label for the sample document.
 … and that is enough.
                                    Isadora Duncan
TIP We are speaking about tokens as the input to each time step of the net-
work, but recurrent neural nets work id entically with any sort of time series
data. Your tokens can be anything, disc rete or continuous: readings from a
weather station, musical notes, char acters in a sentence, you name it.
Here, you’ll initially look at the output of  the network at the last time step and com-
pare that output to the label. That’l l be (for now) the definition of the error . And the
error is what your network will ultimately try to minimize. But you now have some-
thing that’s a shift from what you had in the earlier chapters. For a given data sample,
you break it into smaller pieces that are fe d into the network sequentially. But instead
of dealing with the output generated by any of these “subsamples” directly, you feed it
back into the network.
 You’re only concerned with the final outp ut, at least for now. You input each token
from the sequence into your network and calculate the loss based on the output from
the last time step (token) in the sequence. See figure 8.9.
Hidden
layer
Today
Ignored
output
Hidden
layer
was
Ignored
output
Hidden
layer
a
Ignored
output
Hidden
layer
good
Ignored
output
Hidden
layer
day .
Ignored
output
Hidden
layer
error = y_true_label - y_output
Output
Figure 8.9 Only last output matters here
 
255Remembering with recurrent networks
Now your network is remember ing something! Well, sort of. A few things remain for
you to figure out. For one, how does backprop agation even work in a structure like this?
8.1.1 Backpropagation through time
All the networks we’ve talked about so far ha ve a label (the target variable) to aim for,
and recurrent networks are no  exception. But you don’t have a concept of a label for
each token. You have a single label for a ll the tokens in each sample text. You only
have a label for the sample document.
 … and that is enough.
                                    Isadora Duncan
TIP We are speaking about tokens as the input to each time step of the net-
work, but recurrent neural nets work id entically with any sort of time series
data. Your tokens can be anything, disc rete or continuous: readings from a
weather station, musical notes, char acters in a sentence, you name it.
Here, you’ll initially look at the output of  the network at the last time step and com-
pare that output to the label. That’l l be (for now) the definition of the error . And the
error is what your network will ultimately try to minimize. But you now have some-
thing that’s a shift from what you had in the earlier chapters. For a given data sample,
you break it into smaller pieces that are fe d into the network sequentially. But instead
of dealing with the output generated by any of these “subsamples” directly, you feed it
back into the network.
 You’re only concerned with the final outp ut, at least for now. You input each token
from the sequence into your network and calculate the loss based on the output from
the last time step (token) in the sequence. See figure 8.9.
Hidden
layer
Today
Ignored
output
Hidden
layer
was
Ignored
output
Hidden
layer
a
Ignored
output
Hidden
layer
good
Ignored
output
Hidden
layer
day .
Ignored
output
Hidden
layer
error = y_true_label - y_output
Output
Figure 8.9 Only last output matters here
 
Forward pass
RNN Training
255Remembering with recurrent networks
Now your network is remember ing something! Well, sort of. A few things remain for
you to figure out. For one, how does backprop agation even work in a structure like this?
8.1.1 Backpropagation through time
All the networks we’ve talked about so far ha ve a label (the target variable) to aim for,
and recurrent networks are no  exception. But you don’t have a concept of a label for
each token. You have a single label for a ll the tokens in each sample text. You only
have a label for the sample document.
 … and that is enough.
                                    Isadora Duncan
TIP We are speaking about tokens as the input to each time step of the net-
work, but recurrent neural nets work id entically with any sort of time series
data. Your tokens can be anything, disc rete or continuous: readings from a
weather station, musical notes, char acters in a sentence, you name it.
Here, you’ll initially look at the output of  the network at the last time step and com-
pare that output to the label. That’l l be (for now) the definition of the error . And the
error is what your network will ultimately try to minimize. But you now have some-
thing that’s a shift from what you had in the earlier chapters. For a given data sample,
you break it into smaller pieces that are fe d into the network sequentially. But instead
of dealing with the output generated by any of these “subsamples” directly, you feed it
back into the network.
 You’re only concerned with the final outp ut, at least for now. You input each token
from the sequence into your network and calculate the loss based on the output from
the last time step (token) in the sequence. See figure 8.9.
Hidden
layer
Today
Ignored
output
Hidden
layer
was
Ignored
output
Hidden
layer
a
Ignored
output
Hidden
layer
good
Ignored
output
Hidden
layer
day .
Ignored
output
Hidden
layer
error = y_true_label - y_output
Output
Figure 8.9 Only last output matters here
 
255Remembering with recurrent networks
Now your network is remember ing something! Well, sort of. A few things remain for
you to figure out. For one, how does backprop agation even work in a structure like this?
8.1.1 Backpropagation through time
All the networks we’ve talked about so far ha ve a label (the target variable) to aim for,
and recurrent networks are no  exception. But you don’t have a concept of a label for
each token. You have a single label for a ll the tokens in each sample text. You only
have a label for the sample document.
 … and that is enough.
                                    Isadora Duncan
TIP We are speaking about tokens as the input to each time step of the net-
work, but recurrent neural nets work id entically with any sort of time series
data. Your tokens can be anything, disc rete or continuous: readings from a
weather station, musical notes, char acters in a sentence, you name it.
Here, you’ll initially look at the output of  the network at the last time step and com-
pare that output to the label. That’l l be (for now) the definition of the error . And the
error is what your network will ultimately try to minimize. But you now have some-
thing that’s a shift from what you had in the earlier chapters. For a given data sample,
you break it into smaller pieces that are fe d into the network sequentially. But instead
of dealing with the output generated by any of these “subsamples” directly, you feed it
back into the network.
 You’re only concerned with the final outp ut, at least for now. You input each token
from the sequence into your network and calculate the loss based on the output from
the last time step (token) in the sequence. See figure 8.9.
Hidden
layer
Today
Ignored
output
Hidden
layer
was
Ignored
output
Hidden
layer
a
Ignored
output
Hidden
layer
good
Ignored
output
Hidden
layer
day .
Ignored
output
Hidden
layer
error = y_true_label - y_output
Output
Figure 8.9 Only last output matters here
 
Backpropagation
Backpropagated error
What are RNNs Good For?RNNs can be used in several ways:
     8.1 What are RNNs good for?
     The previous deep learning architectures you’ve learned about are great for processing
short bits of text - usually individual sentences. RNNs promise to break through that text
length barrier and allow your NLP pipeline to ingest an infinitely long sequence of text.
And not only can they process unending text, but they can also generate text for as long
as you like. RNNs open up a whole new range of applications like generative
conversational chatbots and text summarizers that combine concepts from many
different places within your documents.
    Type Description Applications
One to Many
One input tensor used to
generate a sequence of
output tensors
Generate chat messages,
answer questions, describe
images
Many to One
sequence of input tensors
gathered up into a single
output tensor
Classify or tag text
according to its language,
intent, or other
characteristics
Many to Many
a sequence of input tensors
used to generate a
sequence of output tensors
Translate, tag, or
anonymize the tokens
within a sequence of
tokens, answer questions,
participate in a conversation
    This is the superpower of RNNs, they process sequences of tokens or vectors. You are no
longer limited to processing a single, fixed-length vector. So you don’t have to truncate
and pad your input text to make your round text the right shape to fit into a square hole.
And an RNN can generate text sequences that go on and on forever if you like. You don’t
have to stop or truncate the output at some arbitrary maximum length that you decide
ahead of time. Your code can dynamically decide when enough is enough.
    
412
© Manning Publications Co. To comment go to liveBook
Licensed to Nicola Capuano <nicola@capuano.biz>
What are RNNs Good For?

RNNs for Text GenerationWhen used for text generation…•The output of each time step is as important as the final output•Error is captured and backpropagated at each step to adjust all network weights
258 CHAPTER  8 Loopy (recurrent) neural networks (RNNs)
sample, the network will  settle (assuming it converges) on the weight for that input to
that neuron that best  handles this task.
BUT YOU DO CARE  WHAT  CAME  OUT OF THE EARLIER  STEPS
Sometimes you may care about the entire sequence generated by each of the interme-
diate time steps as well. In chapter 9, you’ ll see examples where the output at a given
time step t is as important as the output at the final time step. Figure 8.11 shows a path
for capturing the error at any given time step  and carrying that ba ckward to adjust all
the weights of the networ k during backpropagation.
Figure 8.11 All outputs matter here
This process is like the normal backpropagation through time for n time steps. In this
case, you’re now backpropagating the error from multiple sources at the same time.
But as in the first example, the weight corrections are additi ve. You backpropagate
from the last time step all the way to the first, summing up what you’ll change for each
weight. Then you do the same with the error calculated at the seco nd-to-last time step
and sum up all the changes all the way back to t =0. You repeat this process until you
get all the way back down to time step 0 and then backpropagate it as if it were the
only one in the world. You then apply th e grand total of the updates to the corre-
sponding hidden layer weights all at once.
 In figure 8.12, you can see that the erro r is backpropagated fr om each output all
the way back to t=0, and aggregated, before finally applying changes to the weights.
This is the most important ta keaway of this section. As with a standard feedforward
network, you update the weights only after  you have calculated the proposed change
in the weights for the entire backpropagation step for that input (or set of inputs). In
the case of a recurrent neural net, this backpropagation includes the updates all the
way back to time t =0.
Hidden
layer
Today
y0 y1 y2 y3 y4 y5
Hidden
layer
was
Hidden
layer
a
Hidden
layer
good
Hidden
layer
day .
Hidden
layer
error = sum([y_true_label[i] - y[i] for i in range(6)])
 
RNN Variants
Bidirectional RNNA Bidirectional RNN has two recurrent hidden layers•One processes the input sequence forward •The other processes the input sequence backward•The output of those two are concatenated at each time stepBy processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN•Example: they wanted to pet the dog whose fur was brown
Bi-directional RNN272 CHAPTER  8 Loopy (recurrent) neural networks (RNNs)
Figure 8.13 Bidirectional recurrent neural net
The basic idea is you arrange two RNNs right next to each other, passing the input
into one as normal and the same input backward  into the other net (see figure 8.13).
The output of those two are then concatenated  at each time step to the related (same
input token) time step in the other network. You take the output of the final time step
in the input and concatenate it with the ou tput generated by the same input token at
the first time step of the backward net.
TIP Keras also has a go_backwards  keyword argument. If  this is set to True ,
Keras automatically flips the input se quences and inputs them into the net-
work in reverse order. This is the second half of a bidirectional layer.
If you’re not using a bidirectional wr apper, this keyword can be useful,
because a recurrent neural network (due  to the vanishing gradients problem)
is more receptive to data at the end of the sample than at the beginning. If
you have padded your samples with <PAD>  tokens at the end, all the good,
juicy stuff is buried deep in the input loop. go_backwards  can be a quick
way around this problem.
With these tools you’re well on your way to not just predicting and classifying text, but
actually modeling language itself and how it ’s used. And with that deeper algorithmic
understanding, instead of just parroting text your model has seen before, you can
generate completely new statements!
8.5.3 What is this thing?
Ahead of the Dense layer you have a vector that is of shape (number of neurons x 1)
coming out of the last time  step of the Recurrent layer for a given input sequence.
This vector is the parallel to the thought vector  you got out of the convolutional neural
Forward
hidden
layer
Backward
hidden
layer
y0 y1 y2
See Spot run
Forward
hidden
layer
Backward
hidden
layer
Forward
hidden
layer
Backward
hidden
layer
Error
backpropogated
through time
Error
backpropogated
through time
(forward?)
+++
 
276 CHAPTER  9 Improving retention with long short-term memory networks
 In LSTMs, the rules that govern the information stored in the state (memory) are
trained neural nets themselves—therein lies  the magic. They can be trained to learn
what to remember, while at the same time the rest of the recurrent net learns to pre-
dict the target label! With the introducti on of a memory and state, you can begin to
learn dependencies that st retch not just one or two tokens away, but across the
entirety of each data sample. With thos e long-term dependencies in hand, you can
start to see beyond the words themselves and into something deeper about language.
 With LSTMs, patterns that humans take for granted and process on a subconscious
level begin to be available to your model. And with those patterns, you can not only
more accurately predict sample classification s, but you can start to generate novel text
using those patterns. The state of the art in th is field is still far from perfect, but the
results you’ll see, even in yo ur toy examples, are striking.
 So how does this thing work (see figure 9.1)?
Figure 9.1 LSTM network and its memory
The memory state is affected by the input and also affects the layer output just as in a
normal recurrent net. But that memory state persists across all the time steps of the
time series (your sentence or  document). So each input can have an effect on the
memory state as well as an effect on the hidden layer output. The magic of the mem-
ory state is that it learns  what to remember at the same ti me that it learns to reproduce
the output, using standard backpropagat ion! So what does this look like?
 
x(t+1)
y (t)
LSTM
Input
layer
Normal recurrenceHidden
layer
Output
layer
Memory
‘state’
 
LSTMRNNs should theoretically retain information from inputs seen many timesteps earlier but…•They struggle to learn long-term dependencies•Vanishing Gradient: as more layers are added, as the network becomes difficult to trainLong Short-Term Memory networks are designed to solve this problem:•Introduces a state updated with each training example•The rules to decide what information to remember and what to forget are trained
Unrolling the LSTM277LSTM
Figure 9.2 Unrolled LSTM network and its memory
First, let’s unroll a standard recurrent ne ural net and add your memory unit. Figure
9.2 looks similar to a normal recurrent neural net. However, in addition to the activa-
tion output feeding into the next time-step version of the layer, you add a memory
state that also passes through time steps of  the network. At each time-step iteration,
the hidden recurrent unit has access to th e memory unit. The addition of this mem-
ory unit, and the mechanisms that interact wi th it, make this quite a bit different from
a traditional neural network la yer. However, you may like to  know that it’s possible to
design a set of traditional recurrent neur al network layers (a computational graph)
that accomplishes all the computations th at exist within an LSTM layer. An LSTM
layer is just a highly specialized recurrent neural network.
TIP In much of the literature, 6  the “Memory State” block shown in figure 9.2
is referred to as an LSTM cell  rather than an LSTM neuron , because it contains
two additional neurons or gates ju st like a silicon computer memory cell .7
When an LSTM memory cell  is combined with a sigmoid activation function to
output a value to the next LSTM cell , this structure, cont aining multiple inter-
acting elements, is referred to as an LSTM unit . Multiple LSTM units  are com-
bined to form an LSTM layer . The horizontal line running across the unrolled
recurrent neuron in figure 9.2 is the signal holding the memory or state. It
becomes a vector with a dimension for each LSTM cell  a s  t h e  s e q u e n c e  o f
tokens is passed into a multi-unit LSTM layer .
6 A good recent example of LSTM terminology usage is Alex Graves' 2012 Thesis “Supervised Sequence Label-
ling with Recurrent Neural Networks”: https:/ /mediatum.ub.tu m.de/doc/673554/file.pdf .
7 See the Wikipedia article “Memory cell” ( https:/ /en.wikipedia.org/wiki/Memory_cell_(computing) ).
t =0
Input
layer
Hidden
layer
Output
layer
t =1
Input
layer
Hidden
layer
Output
layer
t =2
Input
layer
Hidden
layer
Output
layer
Memory state
 
LSTM allow past information to be reinjected later, thus fighting the vanishing-gradient problem
GRUGated Recurrent Unit (GRU) is an RNN architecture designed to solve the vanishing gradient problemMain Features•Like LSTM, but with a simpler architecture•GRU lacks a separate memory state, relying solely on the hidden state to store and transfer information across timesteps•Fewer parameters than LSTM, making it faster to train and more computationally efficient•The performance is comparable to LSTM, particularly in tasks with simpler temporal dependencies
Stacked LSTMLayering enhances the model’s ability to capture complex relationshipsNote: The output at each timestep serves as the input for the corresponding timestep in the next layer
309LSTM
 Those are just two of the RNN/LSTM de rivatives out there. Experiments are ever
ongoing, and we encourage you to join the fun. The tools are all readily available, so
finding the next newest greatest it eration is in the reach of all. 
9.1.10 Going deeper
It’s convenient to think of the memory unit as encoding a specific representation of
noun/verb pairs or sentence-to-sentence verb  tense references, but that isn’t specifi-
cally what’s going on. It’s just a happy byproduct of the patterns that the network
learns, assuming the training went well. Li ke in any neural network, layering allows
the model to form more-complex representati ons of the patterns in the training data.
And you can just as easily stack LSTM layers (see figure 9.13).
Figure 9.13 Stacked LSTM
Stacked layers are much more computationally expensive to train. But stacking  them
takes only a few seconds in Ke ras. See the following listing.
>>> from keras.models import Sequential
>>> from keras.layers import LSTM
>>> model = Sequential()
>>> model.add(LSTM(num_neurons, return_sequences=True,
... input_shape=X[0].shape))
>>> model.add(LSTM(num_neurons_2, return_sequences=True))
Listing 9.32 Two LSTM layers
State 2
State 1
LSTM
cell 1
LSTM
cell 1
LSTM
cell 1
LSTM
cell 2
LSTM
cell 2
LSTM
cell 2
Output
layer
Each LSTM layer is a cell with its own gates and state vector.
Stacked LSTM
Output
layer
Output
layer
Input
layer
Input
layer
Input
layer
t =0 t =1 t =2
 
Building a Spam Detector
The DatasetDownload the dataset from:https://archive.ics.uci.edu/dataset/228/sms+spam+collection 

Read the Dataset
Tokenize and Generate WEs

Split the dataset
Train an RRN model

Plot the Training History
Plot the Training History

Report and Confusion Matrix
Report and Confusion Matrix

Using RNN Variants•Bi-directional RRN:model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(64)))•LSTM:model.add(keras.layers.LSTM(64))•Bi-directional LSTM:model.add(keras.layers.Bidirectional(keras.layers.LSTM(64)))•GRU:model.add(keras.layers.GRU(64))•Bi-directional GRU:model.add(keras.layers.Bidirectional(keras.layers.GRU(64)))
Using Ragged Tensors A Ragged Tensor is a tensor that allows rows to have variable lengths•Useful for handling data like text sequences, where each input can have a different number of elements (e.g., sentences with varying numbers of words)•Avoids the need for padding/truncating sequences to a fixed length•Reduces overhead and improves computational efficiency by directly handling variable-length data•Available in TensorFlow since version 2.0•In PyTorch, similar functionality is provided by Packed Sequences
Using Ragged Tensors
Using Ragged Tensors

Intro to Text Generation
Generative ModelsA class of NLP models designed to generate new text•… that is coherent and syntactically correct•... based on patterns and structures learned from text corporaGenerative vs Discriminative•Discriminative models are mainly used to classify or predict categories of data•Generative models can produce new and original dataRNN can be used to generate text• Transformers are better (we will discuss them later)
Applications•Machine TranslationAutomatically translating text from one language to another•Question AnsweringGenerating answers to questions based on a given context•Automatic SummarizationCreating concise summaries of longer texts•Text CompletionPredicting and generating the continuation of a given text•Dialogue SystemsCreating responses in conversational agents and chatbots•Creative WritingAssisting in generating poetry, stories, and other creative texts
Language ModelA mathematical model that determine the probability of the next token given the previous ones•It captures the statistical structure of the language (latent space)•Once created, can be sampled to generate new sequences•An initial string of text (conditioning data) is provided•The model generates a new token•The generated token is added to the input data•the process is repeated several times•This way, sequences of arbitrary length can be generated
LM TrainingAt each step, the RNN receives a token extracted from a sentence in the corpus and produces an output299LSTM
working the same way, aggregating the errors by adjusting all your weights at the end of
the sequence.
 So the first thing you need to do is adjust your training set labels. The output vec-
tor will be measured not against a given classification  label but against the one-hot
encoding of the next ch aracter in the sequence.
Hidden
layer
Hidden
layer
Hidden
layer
Hidden
layer
y1 y2 y3 y4
Today was a good
Hidden
layer
y0
<start>
Hidden
layer
y5
day
Hidden
layer
y6
.
Today was a good <stop> day .
Expected output is the next token in the sample. Shown here on word level.
Actual
output
Expected
output
Figure 9.10 Next word prediction 
Hidden
layer
Hidden
layer
Hidden
layer
Hidden
layer
y1 y2 y3 y4
Toda
o
Hidden
layer
y0
<start>
Td a y
Hidden
layer
y5
y
.
Hidden
layer
y6
.
<stop>
Expected output is the next token in the sample. Shown here on character level.
Actual
output
Expected
output
…
Figure 9.11 Next character prediction 
 
LM TrainingThe output is compared with the expected token (the next one in the sentence)299LSTM
working the same way, aggregating the errors by adjusting all your weights at the end of
the sequence.
 So the first thing you need to do is adjust your training set labels. The output vec-
tor will be measured not against a given classification  label but against the one-hot
encoding of the next ch aracter in the sequence.
Hidden
layer
Hidden
layer
Hidden
layer
Hidden
layer
y1 y2 y3 y4
Today was a good
Hidden
layer
y0
<start>
Hidden
layer
y5
day
Hidden
layer
y6
.
Today was a good <stop> day .
Expected output is the next token in the sample. Shown here on word level.
Actual
output
Expected
output
Figure 9.10 Next word prediction 
Hidden
layer
Hidden
layer
Hidden
layer
Hidden
layer
y1 y2 y3 y4
Toda
o
Hidden
layer
y0
<start>
Td a y
Hidden
layer
y5
y
.
Hidden
layer
y6
.
<stop>
Expected output is the next token in the sample. Shown here on character level.
Actual
output
Expected
output
…
Figure 9.11 Next character prediction 
 
LM TrainingThe comparison generates an error, which is used to update the weights of the network via backpropagation•Unlike traditional RNNs, where backpropagation occurs only at the end of the sequence, errors are propagated at each step299LSTM
working the same way, aggregating the errors by adjusting all your weights at the end of
the sequence.
 So the first thing you need to do is adjust your training set labels. The output vec-
tor will be measured not against a given classification  label but against the one-hot
encoding of the next ch aracter in the sequence.
Hidden
layer
Hidden
layer
Hidden
layer
Hidden
layer
y1 y2 y3 y4
Today was a good
Hidden
layer
y0
<start>
Hidden
layer
y5
day
Hidden
layer
y6
.
Today was a good <stop> day .
Expected output is the next token in the sample. Shown here on word level.
Actual
output
Expected
output
Figure 9.10 Next word prediction 
Hidden
layer
Hidden
layer
Hidden
layer
Hidden
layer
y1 y2 y3 y4
Toda
o
Hidden
layer
y0
<start>
Td a y
Hidden
layer
y5
y
.
Hidden
layer
y6
.
<stop>
Expected output is the next token in the sample. Shown here on character level.
Actual
output
Expected
output
…
Figure 9.11 Next character prediction 
 
SamplingDuring utilization:•Discriminative models always select the most probable output based on the given input•Generative models sample from the possible alternatives:•Example: if a word has a probability of 0.3 of being the next word in a sentence, it will be chosen approximately 30% of the timeTemperature: a parameter (T) used to regulate the randomness of sampling•A low temperature (T<1) makes the model more deterministic•A high temperature (T>1) makes the model more creative
Temperature!!=#$%&'(%!);!!"=!!∑#!#.Where…•p is the original probability distribution•pi is the probability of token i •T>0 is the chosen temperature•q’  is the new distribution affected by the temperature274 CHAPTER 8 Generative deep learning
Higher temperatures result in sampling distributions of higher entropy that will generate more
surprising and unstructured generated data, whereas a lower temperature will result in less ran-
domness and much more predictable generated data (see figure 8.2). 
8.1.4 Implementing character-level LSTM text generation
Let’s put these ideas into practice in a Keras implementation. The first thing you need
is a lot of text data that you can use to learn a language model. You can use any suffi-
ciently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. In
this example, you’ll use some of the writings of Nietzsche, the late-nineteenth century
German philosopher (translated into English). The language model you’ll learn will
thus be specifically a model of Nietzsche’s writing style and topics of choice, rather
than a more generic model of the English language.
PREPARING THE DATA
Let’s start by downloading the corpus and converting it to lowercase.
import keras
import numpy as np
path = keras.utils.get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('Corpus length:', len(text))
Listing 8.2 Downloading and parsing the initial text file
temperature = 0.01 temperature = 0.2 temperature = 0.4
temperature = 0.6
Discrete elements (characters)
Probability of sampling element
temperature = 0.8 temperature = 1.0
Figure 8.2 Different reweightings of one probability distribution. Low temperature = more 
deterministic, high temperature = more random.
Licensed to   <null>
274 CHAPTER 8 Generative deep learning
Higher temperatures result in sampling distributions of higher entropy that will generate more
surprising and unstructured generated data, whereas a lower temperature will result in less ran-
domness and much more predictable generated data (see figure 8.2). 
8.1.4 Implementing character-level LSTM text generation
Let’s put these ideas into practice in a Keras implementation. The first thing you need
is a lot of text data that you can use to learn a language model. You can use any suffi-
ciently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. In
this example, you’ll use some of the writings of Nietzsche, the late-nineteenth century
German philosopher (translated into English). The language model you’ll learn will
thus be specifically a model of Nietzsche’s writing style and topics of choice, rather
than a more generic model of the English language.
PREPARING THE DATA
Let’s start by downloading the corpus and converting it to lowercase.
import keras
import numpy as np
path = keras.utils.get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('Corpus length:', len(text))
Listing 8.2 Downloading and parsing the initial text file
temperature = 0.01 temperature = 0.2 temperature = 0.4
temperature = 0.6
Discrete elements (characters)
Probability of sampling element
temperature = 0.8 temperature = 1.0
Figure 8.2 Different reweightings of one probability distribution. Low temperature = more 
deterministic, high temperature = more random.
Licensed to   <null>
274 CHAPTER 8 Generative deep learning
Higher temperatures result in sampling distributions of higher entropy that will generate more
surprising and unstructured generated data, whereas a lower temperature will result in less ran-
domness and much more predictable generated data (see figure 8.2). 
8.1.4 Implementing character-level LSTM text generation
Let’s put these ideas into practice in a Keras implementation. The first thing you need
is a lot of text data that you can use to learn a language model. You can use any suffi-
ciently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. In
this example, you’ll use some of the writings of Nietzsche, the late-nineteenth century
German philosopher (translated into English). The language model you’ll learn will
thus be specifically a model of Nietzsche’s writing style and topics of choice, rather
than a more generic model of the English language.
PREPARING THE DATA
Let’s start by downloading the corpus and converting it to lowercase.
import keras
import numpy as np
path = keras.utils.get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('Corpus length:', len(text))
Listing 8.2 Downloading and parsing the initial text file
temperature = 0.01 temperature = 0.2 temperature = 0.4
temperature = 0.6
Discrete elements (characters)
Probability of sampling element
temperature = 0.8 temperature = 1.0
Figure 8.2 Different reweightings of one probability distribution. Low temperature = more 
deterministic, high temperature = more random.
Licensed to   <null>Building a Poetry Generator
Leopardi Poetry Generator•Download the corpus from https://elearning.unisa.it/ 
Extract the Training SamplesWe will use characters rather than words as tokens

Build and Train the Model
Build and Train the Model

Generate a new Poetry
Define Helper Functionsfirst
Generate a new Poetry

ReferencesNatural Language Processing IN ACTIONUnderstanding, analyzing, and generating text with Python Chapters 8 and 9
Natural Language Processing and Large Language ModelsCorso di Laurea Magistrale in Ingegneria Informatica Lesson 6Neural Networks for NLPNicola Capuano and Antonio GrecoDIEM – University of Salerno

=== Extracted Text from images (OCR) ===
Natural Language Processing and
Large Language Models

Corso di Laurea Magistrale in Ingegneria Informatica

Lesson 6
Neural Networks
for NLP

Nicola Capuano and Antonio Greco

DIEM — University of Salerno

Outline
* Recurrent Neural Networks
© RNN Variants
* Building a Spam Detector
T-@

® Intro to Text Generation Vn

* Building a Poetry Generator | @_.

Y

Recurrent Neural
Networks

Neural Networks and NLP

Neural networks are widely used in text processing
® Alimitation of feedforward networks is the lack of memory

® Each input is processed independently, without maintaining any
state

® To process a text, the entire sequence of words must be
presented at once

The entire text must become a single data
° This is the approach used with BoW or TF-IDF vectors

® Asimilar approach is averaging the Word Vectors of a text

Neural Networks with Memory

When you read a text, you:
® process sentences and words one by one

® maintain the memory of what you have read previously

® Aninternal model is continuously updated as new information arrives

A Recurrent Neural Network adopts the same principle

° It processes sequences of information by iterating on the
elements of the sequence

° E.g., the words of a text represented in the form of word embeddings

® It maintains a state containing information about what has been
seen so far

Recurrent Neural Networks
* Circles are feedforward network
layers composed of one or more
neurons
x(t#1) * The output of the hidden layer
| emerges from the network
“ayer. } _)Recurentloop == normally to the output layer

v(t ° In addition, it is also fed back as

input to the hidden layer along
layer

with the normal input in the next
time step

Unrolling the RNN

Same RNN “unrolled”

t=0 t=1 t=2

Hidden Hidden
layer layer

x(t+1)

timeline

Inside the RNN (t = 0)

Input vector Hidden Output layer

Hidden
vector

Note: The first input (t=o)
has no past

Inside the RNN (t = 1)

Input vector Hidden Output layer

Input
att=1

Trainable weights:

* Input to Hidden
Hidden vector

° Hi tput
from t=o idden to Outpu

© Hidden to Hidden

Using RNNs with Text

Text: The clown car sped
into the arena

Ignored
utput

RNN Training

Forward pass —————>

Hidden Hidden Hidden Hidden Hidden
layer layer layer layer layer

Ignored
output

Ignored Ignored Ignored Ignored

output output output output

error = y_true_label - y_output

RNN Training

————  —— Backpropagation

Hidden Hidden Hidden Hidden Hidden Hidden
layer layer ]  \ layer | ~~ \ layer layer /  \_ layer

Ignored
output

Ignored Ignored Ignored Ignored

output output output output

Backpropagated error = y_true_label - y_output
error

What are RNNs Good For?

RNNs can be used in several ways:

One input tensor used to Generate chat messages,
One to Many generate a sequence of answer questions, describe
output tensors images
. Classify or tag text
sequence of input tensors ‘n t " ,
. . according to its language,
Many to One gathered up into a single . g guag
intent, or other
output tensor
characteristics

Translate, tag, or
a sequence of input tensors | anonymize the tokens
Many to Many used to generate a within a sequence of
sequence of output tensors tokens, answer questions,
AN participate in a conversation

What are RNNs Good For?

one-to-one one-to-many many-to-one many-to-many

a A i le
Oe. © Oe OO

boa da
ea

| Pu) | | PLEL PU

ey

RNNs for Text Generation

When used for text generation...
® The output of each time step is as important as the final output

® Error is captured and backpropagated at each step to adjust all
network weights

error = sum([y_true_label[i] - yl[i] for i in range(6)])

RNN Variants

Bidirectional RNN

A Bidirectional RNN has two recurrent hidden layers
°® One processes the input sequence forward
° The other processes the input sequence backward

° The output of those two are concatenated at each time step

By processing a sequence both ways, a bidirectional RNN
can catch patterns that may be overlooked by a
unidirectional RNN

° Example: they wanted to pet the dog whose fur was brown

Y

Bi-directional RNN

Error
| backpropogated
through time

Backward
hidden

Forward
hidden

Backward
hidden

Forward
hidden

Backward
hidden

Forward
hidden

Error
backpropogated
through time

LSTM

RNNs should theoretically retain information from inputs
seen many timesteps earlier but...

LSTM

° They struggle to learn long-term dependencies
y 9g g p

® Vanishing Gradient: as more layers are added, as
the network becomes difficult to train

Long Short-Term Memory networks
are designed to solve this problem:

® Introduces a state updated with eer
each training example State

The rules to decide what information to

remember and what to forget are trained

Output
layer

Unrolling the LSTM

LSTM allow past
information to be
reinjected later, thus
fighting the vanishing-
gradient problem

Memory state

Output Output Output
layer layer layer

GRU

Gated Recurrent Unit (GRU) is an RNN architecture
designed to solve the vanishing gradient problem

Main Features
° Like LSTM, but with a simpler architecture

® GRU lacks a separate memory state, relying solely on the hidden
state to store and transfer information across timesteps

° Fewer parameters than LSTM, making it faster to train and more
computationally efficient

The performance is comparable to LSTM, particularly in tasks
with simpler temporal dependencies

Stacked LSTM

Layering enhances
the model's ability
to capture complex
relationships

Note: The output at
each timestep serves as
the input for the
corresponding timestep
in the next layer

Building a Spam Detector

The Dataset

Download the dataset from:

https://archive.ics.uci.edu/dataset/228/sms+spam+collection

SMS Spam Collection

Donated on 6/21/2012

The SMS Spam Collection is a public set of SMS labeled messages that have been collected
for mobile phone spam research.

Dataset Characteristics | Subject Area Associated Tasks
Multivariate, Text, Domain- Computer Science Classification, Clustering
Theory

Feature Type # Instances # Features

Real 5574 -

Read the Dataset

import pandas as pd

df = pd.read_csv("datasets/sms_spam.tsv", delimiter='\t', \
header = None, names = ['label', ‘text'])

df
label text
ham Go until jurong point, crazy.. Available only ...
ham Ok lar... Joking wif u oni...

spam Free entry in 2 a wkly comp to win FA Cup fina...

ham U dun say so early hor... U c already then say...

mB WN = OC

ham Nah | don't think he goes to usf, he lives aro...

5567 spam _ This is the 2nd time we have tried 2 contact u...
5568 ham Will U b going to esplanade fr home?
5569 ham Pity, * was in mood for that. So...any other s...
5570 ham The guy did some bitching but | acted like i'd...

5571 ham Rofl. Its true to its name

5572 rows x 2 columns

Tokenize and Generate WEs

import spacy, numpy as np
nlp = spacy.load('en_core_web_md') # loads the medium model with 30@-dimensional WEs

# Tokenize the text and save the WEs
corpus = []
for sample in df['text']:
doc = nlp(sample, disable=["tagger", “parser”, “attribute_ruler", \
| "lemmatizer", “ner"]) # only tok2vec
corpus.append([token.vector for token in doc])

# Pad or truncate samples to a fixed length
maxlen = 50
zero_vec = [0] * len(corpus[®@] [0])
for i in range(len(corpus)):
if len(corpus[i]) < maxlen:
corpus[i] += [zero_vec] * (maxlen - len(corpus[i])) # pad
else:
corpus[i] = corpus[i] [:maxlen] # truncate

corpus = np.array(corpus)
corpus. shape

(5572, 50, 300)

Split the dataset

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode the labels
encoder = LabelEncoder()
labels = encoder. fit_transform(df['label'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size = 0.2)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

((4457, 50, 300), (1115, 50, 300), (4457,), (1115,))

Train an RRN model

# pip install tensorflow keras
import keras

model = keras.models.Sequential()

model.add(keras. layers. Input(shape = (X_train.shape[1], X_train.shape[2])))

model. add(keras. layers.SimpleRNN (64) )

model. add(keras. layers.Dropout (0.3) )

model. add(keras. layers.Dense(1, activation='sigmoid'))

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [‘accuracy']})
model.summary()

history = model. fit(X_train, y_train, batch_size = 512, epochs = 20,
validation_data = (X_test, y_test))

Layer (type) Output Shape

simple_rnn (SimpleRNN) (None, 64) | (23,360 |
dropout (Dropout) (None, 64) fF
dense (Dense) (None, 1) pS

Plot the Training History

from matplotlib import pyplot as plt

def plot(history, metrics):
fig, axes = plt.subplots(1, len(metrics), figsize = (15, 5))
for i, metric in enumerate(metrics):
ax = axes[i]
ax.plot(history.history(metric], label='Train')
ax.plot(history.history['val_' + metric], label='Validation')
ax.set_title(f'Model {metric.capitalize()}')
ax.set_ylabel(metric.capitalize())
ax.set_xlabel('Epoch')
ax. legend(loc='upper left')
ax.grid()
plt.tight_layout()
plt.show()

plot(history, ['loss', ‘accuracy'])

Plot the Training History

from matplotlib import pyplot as plt

def plot(history, metrics):

fig, axes = plt.subplots(1, len(metrics), figsize = (15, 5))

for i, metric in enumerate(metrics):
ax = axes[i]
ax.plot(history.history(metric], label='Train')
ax.plot(history.history['val_' + metric], label='Validation')
ax.set_title(f'Model {metric.capitalize()}')

Model Loss Model Accuracy

| — Trin | — Train
— Validation — validation

0.95 +

Report and Confusion Matrix

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns

def print_report(model, X_test, y_test, encoder):
y_pred = model.predict(X_test).ravel()
y_pred_class = (y_pred > 0.5).astype(int) # convert probabilities to classes

y_pred_lab = encoder. inverse_transform(y_pred_class)
y_test_lab = encoder. inverse_transform(y_test)
print(classification_report(y_test_lab, y_pred_lab, zero_division = @))

cm = confusion_matrix(y_test_lab, y_pred_lab)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot = True, fmt = 'd', cmap = ‘Blues’,
xticklabels = encoder.classes_, yticklabels = encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt. show()

Report and Confusion Matrix

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns

def print_report(model, X_test, y_test, encoder):
y_pred = model.predict(X_test).ravel(- Confusion Matrix
y_pred_class = (y_pred > @.5).astype(

y_pred_lab = encoder. inverse_transfor

800
v test lab = encoder.inverse transfor €& 8
precision recall fi-score support t le
uv 600
ham 0.96 0.97 0.97 972 g
spam 0.81 0.75 0.78 143 | YF 's
vo
=]
accuracy 0.95 1115 F='r 400
macro avg 0.89 0.86 0.87 1115 Flele Ee
weighted avg 0.94 0.95 0.94 1115 S- 16 27 | 500
plt.title('Confusion Matrix")
plt. show() ham a

Predicted Labels

Using RNN Variants

* Bi-directional RRN:

model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(64) ))

° LSTM:

model.add(keras.layers.LSTM(64) )

* Bi-directional LSTM:

model.add(keras.layers.Bidirectional(keras.layers.LSTM(64) ))

© GRU:

model.add(keras.layers.GRU(64))

Bi-directional GRU:

model.add(keras.layers.Bidirectional(keras.layers.GRU(64) ))

Using Ragged Tensors

A Ragged Tensor is a tensor that allows rows to have
variable lengths

° Useful for handling data like text sequences, where each input can
have a different number of elements (e.g., sentences with varying
numbers of words)

° Avoids the need for padding/truncating sequences to a fixed
length

® Reduces overhead and improves computational efficiency by
directly handling variable-length data

© Available in TensorFlow since version 2.0

In PyTorch, similar functionality is provided by Packed Sequences

Using Ragged Tensors

import tensorflow as tf

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(original_corpus, labels, test_size=0.2)

# Convert sequences into RaggedTensors to handle variable-length inputs
X_train_ragged = tf.ragged.constant(X_train)

X_test_ragged = tf.ragged.constant(X_test)

# Build the model

model = keras.models.Sequential()

model.add(keras. layers. Input(shape=(None, 300), ragged = True))
model.add(keras. layers.SimpleRNN(64) )

model. add(keras. layers.Dropout (@.3) )

model.add(keras. layers.Dense(1, activation='sigmoid'))

model.compile(loss = ‘binary_crossentropy', optimizer = ‘adam', metrics = ['accuracy'])
model. summary ( )

# Train the model using RaggedTensors
history = model. fit(X_train_ragged, y_train, batch_size=512, epochs=20,

| validation_data=(X_test_ragged, y_test) )
plot(history, ['loss', ‘accuracy'])
print_report(model, X_test_ragged, y_test, encoder)

Using Ragged Tensors

Model Loss Model Accuracy

—— Train —— Train
0.6 |. — Validation 0.95 | — Validation

0.4 fol
ES 5 0.80
g
0.3
0.75
0.2
0.70
0.1 0.65
0.0 25 5.0 75 10.0 12.5 15.0 75 0.0 2.5 5.0 75 10.0 12.5 15.0 17.5
Epoch Epoch
Confusion Matrix
35/35 [Sssssssssssssssssssss=========] - Qs 6ms/step
precision recall f1-score support 800
18
ham Q.97 @.98 @.98 980 600
spam 0.86 0.79 0.82 135 3g
s
accuracy 0.96 1115 F 400

spam

Macro avg @.91 0.89 @.90 1115
weighted avg 0.96 0.96 @.96 1115 4 28 107
- 200
ham spam
Predicted Labels

Intro to Text Generation

Generative Models

A class of NLP models designed to generate new text
° ... that is coherent and syntactically correct

° ... based on patterns and structures learned from text corpora

Generative vs Discriminative

® Discriminative models are mainly used to classify or predict
categories of data

* Generative models can produce new and original data

RNN can be used to generate text

° Transformers are better (we will discuss them later)

Applications

Machine Translation
Automatically translating text from one language to another

Question Answering
Generating answers to questions based on a given context

Automatic Summarization
Creating concise summaries of longer texts

Text Completion
Predicting and generating the continuation of a given text

Dialogue Systems
Creating responses in conversational agents and chatbots

Creative Writing
Assisting in generating poetry, stories, and other creative texts

Language Model

A mathematical model that determine the probability of
the next token given the previous ones

° It captures the statistical structure of the language (latent space)
°* Once created, can be sampled to generate new sequences

* An initial string of text (conditioning data) is provided

° The model generates a new token

° The generated token is added to the input data

° the process is repeated several times

° This way, sequences of arbitrary length can be generated

LM Training

At each step, the RNN receives a token extracted from a
sentence in the corpus and produces an output

Actual
output

LM Training

The output is compared with the expected token (the
next one in the sentence)

Actual

Hidden Hidden Hidden Hidden Hidden Hidden Hidden
layer -_ layer -_ layer -_ layer -_ layer - layer -_ layer
Expected

LM Training

The comparison generates an error, which is used to
update the weights of the network via backpropagation

° Unlike traditional RNNs, where backpropagation occurs only at
the end of the sequence, errors are propagated at each step
Actual

Hidden Hidden Hidden
layer /2_\ layer /-—\ layer
Expected

Hidden
layer

Sampling

During utilization:

* Discriminative models always select the most probable output
based on the given input

* Generative models sample from the possible alternatives:

° Example: ifa word has a probability of 0.3 of being the next word ina
sentence, it will be chosen approximately 30% of the time

Temperature: a parameter (T) used to regulate the
randomness of sampling

° Alow temperature (T<1) makes the model more deterministic

° Ahigh temperature (T>1) makes the model more creative

Temperature

log(p)\ 4
. r yg

® pis the original probability distribution

Where...

p; is the probability of token i
T>o is the chosen temperature

q’ is the new distribution affected by the temperature

temperature = 0.01 temperature = 0.2 temperature = 0.4 temperature = 0.6 temperature = 1.0

|
e

ee ee ee ee eee
5 ee ee ee ee ee ee ee

Pi
9
a
Q
oO
@
©
3
S

Building a Poetry
Generator

Leopardi Poetry Generator

* Download the corpus from https://elearning.unisa. it/

# Load the corpus
with open('datasets/leopardi.txt', 'r') as f:
text = f.read()

# Get the unique characters in the corpus
chars = sorted(list(set(text)))
char_indices = dict((c, i) for i, c in enumerate(chars) )

indices_char = dict((i, c) for i, c in enumerate(chars) )

print("Corpus length: {}; total chars: {}".format(len(text), len(chars) ))

Corpus length: 134628; total chars: 77

Extract the Training Samples

maxlen = 40 # length of the extracted sequences

We will use
samples = [] # holds the extracted sequences
characters targets = [] # holds the next character for each sequence

rather than # Extract sequences of maxlen characters

for i in range(@, len(text) - maxlen):
words as samples.append(text[i: i + maxlen] )
tokens targets.append(text[i + maxlen] )

print('Number of samples: {}'.format(len(samples) ) )

; Number of samples: 134588
import numpy as np

# Initialize the training data
X = np.zeros((len(samples), maxlen, len(chars)), dtype = bool)
y = np.zeros((len(samples), len(chars)), dtype = bool)

# One-hot encode samples and targets
for i, sample in enumerate(samples):
for j, char in enumerate(sample):
Xi, j, char_indices[char]] = 1
yli, char_indices[targets[i]]] = 1

Build and Train the Model

model = keras.models.Sequential()

model. add(keras. layers. Input(shape = (X.shape[1], X.shape[2])))

model. add(keras. layers.LSTM(128) )

model.add(keras. layers.Dense(len(chars), activation = 'softmax'))

model.compile(loss = ‘categorical_crossentropy', optimizer = "adam", metrics = ['accuracy'])
model. summary ()

history = model. fit(X, y, batch_size = 128,| epochs = 10Q,| shuffle = True)

model. save( 'models/leopardi.keras')

Y 83m 31.0s

Layer (type) Output Shape
lstm (LSTM) (None, 128) 105,472
dense (Dense) (None, 77) | 9, 933 |

Build and Train the Model

model = keras.models.Sequential()

model. add(keras. layers. Input(shape = (X.shape[1], X.shape[2])))

model. add(keras. layers.LSTM(128) )

model.add(keras. layers.Dense(len(chars), activation = 'softmax'))

model.compile(loss = ‘categorical_crossentropy', optimizer = "adam", metrics = ['accuracy'])
model. summary ()

history = model. fit(X, y, batch_size = 128,| epochs = 10Q,| shuffle = True)

model. save( 'models/leopardi.keras')

Y 83m 31.0s

Model Loss Model Accuracy

— Train

Epoch

Generate a new Poetry

import random, sys

Define # Sample the next character
def sample(preds, temperature):
Helper preds = np.asarray(preds).astype('float64')
. preds = np.log(preds) / temperature
Functions exp_preds = np.exp(preds)

fj preds = exp_preds / np.sum(exp_preds)
irst probas = np.random.multinomial(1, preds, 1)
return np.argmax(probas)

# Generate text from a seed
def generate_text(seed, length = 400, temperature = Q.5):
sys.stdout.write(seed)
seed = seed[-maxlen:].rjust(maxlen) # adjust the seed length
for i in range(length):
xX = np.zeros((1, maxlen, len(chars)))
for t, char in enumerate(seed):
| x[@, t, char_indices[char]] = 1
preds = model.predict(x, verbose = @) [@]
next_char = indices_char[sample(preds, temperature) ]
seed = seed[1:] + next_char
sys.stdout.write(next_char)
sys.Stdout. flush()
print("\n")

Generate a new Poetry

# Generate text from a random two lines Leopardi's seed
lines = text.splitlines()

start_index = random.randint(®@, len(lines) - 2)
generate_text("\n". join( lines [start_index:start_index + 2]))

# Generate text from an Ungaretti's seed
generate_text("""Ognuno sta solo sul cuor della terra
trafitto da un raggio di sole:

ed @ subito sera.""")

I fusi delle Parche. Ogni giornale, Ognuno sta solo sul cuor della terra

Gener vario di lingue e di colonne, trafitto da un raggio di sole:

E cosi senza voce ad altro innoca ed @ subito sera.

Dell'acquianza e l'alta, e la colona. Era di te stano al volto si cola

E tu con soliarti e dilettoso in terra Dell'acceli stetto i carsi, o cara si soglie
Di lor voler nelle sua vita, e il corro. Seduta il petto dolo

E di gli occhi son o le seco possa, L'altro di rovanta.

Con quando a te per lo specia e il vento Altri in terra di schielata, e la villa
Dell'umana cui di luna, e il nuco Stal che tu con l'etra colonterati,

Di quest'avira speranza e l'amori, Se che il cor tempo, e la terra in tutto,
Che di te per la morte a lei pativa. Un giorno E di te che te ricolonda

Dell'armi e delle amor preverea perro La stravente vendesti in sul fatto estremo
Vai per mi parer la vita io ragio Di colorati suoi l'ora gia seggerdo

Di tuo de' corsaggiar, che solo, e chiara Del cielo accolor dell'animora, ancora

Che di lor vita e vivi in tutto scordo. Quella vita morte, e di corre i migio.

References

Natural Language Processing INACTION

Understanding, analyzing, and generating text with Python : Natural
Chapters 8 and 9 jig Language

it rocessing

Bm» IN ACTION

Natural Language Processing and
Large Language Models

Corso di Laurea Magistrale in Ingegneria Informatica

Lesson 6
Neural Networks
for NLP

Nicola Capuano and Antonio Greco

DIEM — University of Salerno